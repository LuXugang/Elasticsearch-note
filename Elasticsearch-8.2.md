# [Elasticsearch-8.2](https://luxugang.github.io/Elasticsearch/2022/0905/Elasticsearch-8-2/)（2024/03/26）

## What is Elasticsearch?
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/elasticsearch-intro.html)

**You know, for search (and analysis)**

&emsp;&emsp;Elasticsearch是Elastic Stack中核心的分布式搜索和分析引擎。Logstash和Beats帮助收集，聚合以及丰富你的数据并存储在Elasticsearch。Kibana使你能够交互式地探索、可视化和分享对数据的见解，并管理和监视stack。Elasticsearch用于索引，查询以及分析。

&emsp;&emsp;Elasticsearch提供对所有类型的数据的近实时搜索（near real-time）和分析。无论是结构化还是非结构化的文本，数值类型的数据，或者地理位置数据，Elasticsearch都能有效的进行存储并以某种方式进行索引来实现快速查询。你可以不仅仅是简单的数据检索而是可以进一步的对信息进行聚合来发现你数据中的趋势和patterns。随着你的数据和查询体量的增大，Elasticsearch的分布式功能使你的部署能够无缝地（seamless）随之增长。

&emsp;&emsp;虽然不是每一个问题都是一个查询问题，Elasticsearch为在各种用例中处理数据提供了速度（speed）和灵活性（flexibility）。


- 添加一个搜索框（search box）到一个app或者网页中
- 存储和分析日志，指标以及安全事件数据
- 使用machine learning自动对你的数据行为构建实时的模型
- 使用Elasticsearch作为一个存储引擎自动化业务工作流（business workflows）
- 使用Elasticsearch作为一个管理，集成以及分析空间信息地理信息系统（GIS: geographic information system）
- 使用Elasticsearch作为一个生物信息学研究工具（ bioinformatics research tool）来存储以及处理基因数据

&emsp;&emsp;我们不断的因为用户使用新颖（novel）的搜索方式而感到惊讶（amaze）。但是不管你的使用案例是否类似与这些中的一种，或者你正在使用Elasticsearch解决（tackle）一个新的问题，你在Elasticsearch中处理你数据， 文档以及索引都是一样的。

### Data in documents and indices
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/documents-indices.html)

&emsp;&emsp;Elasticsearch是一个分布式的文档存储。Elasticsearch 存储已序列化为 JSON 文档的复杂数据结构，而不是将信息存储为列式数据行。当你的集群中有多个Elasticsearch节点，存储的文档跨集群分布并能立即从任何节点访问。

&emsp;&emsp;文档在存储之后，它会被索引（index）并且在能在[near real time](#Near-real-time-search)--一秒内完全的用于搜索。Elasticsearch使用了称为倒排表（inverted index）的数据结构，它能够用于快速的全文检索。inverted index列出了出现在所有文档中的每一个unique word并识别出每一个单词所在的所有文档。

&emsp;&emsp;索引（index）可以认为是一个优化后的文档集合，每一篇文档是一个字段（field）的集合，每一个字段是包含数据的一个键值对（key-value pair）。默认情况下，Elasticsearch会索引所有字段中的数据并且每一种索引字段（indexed field）都有专门的优化后的数据结构。例如，text field存储在倒排索引（inverted index）中，numeric和geo field存储在[BKD](https://www.amazingkoala.com.cn/Lucene/gongjulei/2019/0422/Bkd-Tree)树中。使用每一种字段的数据结构进行组合（assemble）和返回查询结果的能力使得Elasticsearch特别的快。

&emsp;&emsp;Elasticsearch同样有schema-less的能力，意味着不用显示的指定如何处理一篇文档中的不同的字段就可以直接对文档进行索引。当开启dynamic mapping后，Elasticsearch能自动的检测并添加新的字段到索引中。默认的行为使得索引以及探索你的数据变得简单。只需要开始索引文档，Elasticsearch就会进行检测并将booleans，floating point，integer values，dates和strings映射成Elasticsearch中合适的数据类型。

&emsp;&emsp;最终，当你比Elasticsearch更了解自己的数据并且想要按照自己的方式来处理。你可以定义规则来控制dynamic mappings以及显示的（explicit）定义mapping来完全的控制如何对字段进行存储和索引。

&emsp;&emsp;定义你自己的mapping可以让你：

- 区分出全文检索字段（full-text string ）跟精确匹配字段（exact value string field）
- 执行特定语言的文本分析
- 为部分匹配对字段进行优化
- 使用自定义的date formats
- 使用`geo_point` 和 `geo_shape`这些不能被自动检测的数据类型

&emsp;&emsp;基于不同的目的，用不同的方式索引同一个字段通常是很有用的。例如你可能想要将一个字符串字段索引为text field用于全文检索以及keyword用于排序、聚合。或者你可能会选择使用多个语言分词器来处理包含用户输入的内容。

&emsp;&emsp;在索引期间应用到full-text字段的analysis chain在查询期间同样需要使用。当你查询一个full-text 字段，在索引中查找term前，它的请求文本（query text）也会经历（undergo）相同的analysis。

### Information out: search and analyze
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-analyze.html)

&emsp;&emsp;当使用Elasticsearch作为一个文档存储（document store），检索文档以及文档的元信息（metadata）时，你能够轻松访问全套搜索能力，其能力来源是因为构建在Apache Lucene 搜索引擎库之上。

&emsp;&emsp;Elasticsearch提供了一套简单的，容易理解的（coherent）的REST API，用于管理集群，索引以及查询数据。出于测试的目的，你可以简单的通过命令行或者Kibana中的Developer Console直接提交一个请求。在你的应用中，你可以选择语言并使用[Elasticsearch client](https://www.elastic.co/guide/en/elasticsearch/client/index.html)：Java, JavaScript, Go, .NET, PHP, Perl, Python 或者 Ruby。

#### Searching your data

&emsp;&emsp;Elasticsearch REST APIs支持结构化查询（structured query），全文检索，以及复杂的查询，比如query的组合。结构化查询类似你在SQL中构造的查询类型。例如，你可以在`employee`索引中查询`gender`和`age`字段并且根据`hire_date`字段对匹配的结果进行排序。全文检索会找到满足查询条件的所有的文档并且根据相关性（relevance，how good a match they are for your search terms）排序。

&emsp;&emsp;除了查询不同的term，你还可以执行短语查询（phrase search），相似度查询（similarity search），前缀查询（prefix search）以及获得autocomplete suggestions。

&emsp;&emsp;想要查询地理位置或者其他数值类型的数据的话，Elasticsearch将这类非文本的数据索引到一个优化后的数据结构（BKD）使得支持高性能的地址位置和数值查询。

&emsp;&emsp;你可以使用Elasticsearch中JSON风格的查询语言（[Query DSL](#Query-DSL)）来访问所有的查询能力。你也可以构造[SQL-style query](#Overview（SQL）)查询/聚合数据，以及使用JDBC和ODBC驱动使得更多的第三方应用通过SQL使用Elasticsearch。

#### Analyzing your data

&emsp;&emsp;Elasticsearch的聚合（aggregation）能让你构建复杂的数据汇总并获得关键指标的洞见（insight），模式（pattern）以及趋势（trend）。聚合能让你回答下面的问题，而不是仅仅如谚语中所说的needle in a haystack：
Near real-time search

- haystack中有多少个needle？
- needle的平均长度
- 每个生产商（manufacturer）制造的needle的median length
- 过去的六个月中，每个月添加到haystack的needle的数量

&emsp;&emsp;你可以使用聚合回答更多subtle问题，例如：

- 最受欢迎的needle生产商是哪家？
- 是否存在不寻常或者异常的（anomalous）needle？

&emsp;&emsp;由于聚合使用了查询中使用的相同的数据结构，所以非常的快，使得可以实时的分析以及可视化你的数据。报表跟dashboard可以随着你的数据的变更而更新，使得你可以基于最新的信息采取措施（take action）。

&emsp;&emsp;聚合是跟查询请求一起执行的。你可以在单个请求中对相同的数据进行查询，过滤，以及分析。因为聚合要在某个查询的上下文中计算，你不仅仅能展示70号needle的数量统计，你还能展示满足你的策略的needle：比如说70号的不沾针（non-stick embroidery needles）。

#### But wait, there’s more

&emsp;&emsp;想要自动分析你的时序数据吗？你可以使用[machine learning](https://www.elastic.co/guide/en/machine-learning/8.2/ml-ad-overview.html)功能创建你的数据中普通行为（normal behavior）的准确基线以及识别出异常模式（anomalous pattern）。使用machine learning，你可以检测下面的信息：

- Anomalies related to temporal deviations in values, counts, or frequencies
- Statistical rarity
- Unusual behaviors for a member of a population

&emsp;&emsp;And the best part? 你不需要指定算法，模型或者其他数据科学相关的配置就可以实现上面的功能。

### Scalability and resilience: clusters, nodes, and shards
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/scalability.html#scalability)

&emsp;&emsp;Elasticsearch总是可用的（available）并且根据你的需要进行扩展。It does this by being distributed by nature。你可以向一个集群中添加服务（节点）来提高承载力（capacity），Elasticsearch可以跨所有可用的节点，自动的分布/查询（distribute/query）数据。不需要overhaul你的应用，Elasticsearch知道如何平衡多个节点的集群来提供扩展以及高可用能力。The more nodes, the merrier。

&emsp;&emsp;Elasticsearch是如何工作的？在底层实现中，一个Elasticsearch index只是一个或多个物理分片（physical shards）的逻辑组合（logical grouping）。每一个分片实际上是一个self-contained index。通过将索引中的文档分布到多个分片，将分片分布到多个节点，Elasticsearch可以确保冗余（ensure redundancy），使得应对硬件故障以及节点添加到集群后，查询能力的提升。随着集群的增长（收缩），Elasticsearch能自动的迁移（migrate）分片来rebalance集群。

&emsp;&emsp;分片的类型有两种：主分片跟副本分片（primary  and replica shard）。索引中的每一篇文档属于某一个主分片。一个副本分片是某个主分片的拷贝。副本分片提供了数据的冗余来应对硬件故障以及提高例如查询或者检索一篇文档的读取请求的能力。

&emsp;&emsp;某个索引中的主分片数量在索引创建后就固定了。但是副本分配的数量可以在任何时间内更改，不会影响（interrupt）索引或者查询操作。

#### It depends…

&emsp;&emsp;对于分片大小和索引的主分片数量，有很多性能考虑点以及trade off。分片越多，维护这些索引的开销就越大。分片大小越大，当Elasticsearch需要rebalance集群时，移动分片花费的时间越大。

&emsp;&emsp;在很多较小的分片上查询时，在每一个分片上的查询很快，但是分片越多，查询次数就越多，开销就越大，所以在数量较小，体积较大的分片上的查询可能就更快。In short…it depends。

&emsp;&emsp;As a starting point：

- 将分片的平均大小保持在几个GB以及十几个GB之间。对于基于时间的数据，通常来说，分片的大小在20GB到40GB之间
- 避免出现大量分片的问题。一个节点可以拥有的分片数量跟可用的堆内存成正比。一般来说（as a general rule），每1GB的堆内存中的分片数量应该小于20个。

&emsp;&emsp;对于你的用例，确定最佳配置的最好方式是通过[testing with your own data and queries](https://www.elastic.co/cn/elasticon/conf/2016/sf/quantitative-cluster-sizing)。

#### In case of disaster

&emsp;&emsp;集群中的节点之间需要良好的，可靠的连接。若要能提供一个较好的连接，你通常会将节点放在相同的数据中心或者附近的数据中心。然而为了高可用，你同样需要避免单点故障（single point of failure）。某个地区（location）发生停电后，其他地区必须能接管服务。这个问题的答案就是使用CCR（Cross-cluster replication）。

&emsp;&emsp;CCR提供了一种从主集群（primary cluster）自动同步索引到secondary remote cluster的方法，即secondary remote cluster作为一个热备（hot backup）。如果primary cluster发生了故障，secondary cluster可以进行接管。你可以使用CCR创建secondary cluster，给地理上靠近（geo-proximity）这个secondary cluster的用户提供读取请求。

#### Care and feeding

&emsp;&emsp;与任何企业系统一样，你需要工具来secure，manage，以及monitor你的Elasticsearch 集群。Security，monitoring，以及administrative features都集成到了Elasticsearch，使得你可以使用[Kibana](https://www.elastic.co/guide/en/kibana/8.2/introduction.html)作为控制中心来管理集群。比如[data rollups](#Rolling-up-historical-data)、[index lifecycle management](#ILM-Manage-the-index-lifecycle)这些功能能帮助你根据时间来管理你的数据。

## Set up Elasticsearch
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/setup.html)

&emsp;&emsp;这个章节介绍了如何设置Elasticsearch并且使其运行，包含的内容有：

- 下载
- 安装
- 启动
- 配置

#### Supported platforms

&emsp;&emsp;官方给出的操作系统以及JVM的支持列表见[Support Matrix](https://www.elastic.co/support/matrix).Elasticsearch在这些平台上都经过了测试，但是在列表外的其他平台上还是有可能可以正常工作的。

#### Java（JVM）Version

&emsp;&emsp;Elasticsearch使用Java构建，并且每次发布都捆绑了一个OpenJDK，这个OpenJDK由JDK维护者使用GPLv2+CE协议维护。建议使用捆绑的JDK，它位于Elasticsearch home目录中名为jdk的目录中。

&emsp;&emsp;可以通过设置环境变量`ES_JAVA_HOME`来使用你自己的Java版本。如果你一定要使用一个跟捆绑的JVM不一样的Java版本，我们建议你使用Java的长期支持版本[LTS ](https://www.oracle.com/technetwork/java/eol-135779.html)。如果使用了一个错误Java版本的，Elasticsearch将不会启动。当你使用了自己的JVM后，绑定的JVM目录可能会被删除。

#### Use delicated hosts 使用专用的主机

&emsp;&emsp;在生产上，我们建议你在专用的主机或者主服务器上（primary service）运行Elasticsearch。假定Elasticsearch是主机上或者容器上唯一的资源密集型的应用，那么一些Elasticsearch的功能比如自动化分配JVM堆大小就能实现。比如说，你可能同时运行Metribeat跟Elasticsearch来做集群统计，那么就应该将resource-heavy的Logstash部署在它自己的主机上。

### Installing Elasticsearch
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/install-elasticsearch.html)

##### Hosted Elasticsearch

&emsp;&emsp;你可以在自己的设备上运行Elasticsearch或者也可以使用AWS, GCP, and Azure上[专用的Elasticsearch服务器](https://www.elastic.co/cn/cloud/elasticsearch-service/signup?baymax=docs-body&elektra=docs)。

##### Installing Elasticsearch Yourself

&emsp;&emsp;Elasticsearch以下列的打包方式呈现：

##### Configuration Management Tools

&emsp;&emsp;我们同样提供了下列的配置管理工具有助于大型的部署

- [Puppet](https://github.com/voxpupuli/puppet-elasticsearch)
- [Chef](https://github.com/elastic/cookbook-elasticsearch)
- [Ansible](https://github.com/elastic/ansible-elasticsearch)

#### Install Elasticsearch from archive on Linux or MacOS
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/targz.html#install-macos)

&emsp;&emsp;在Linux和MacOS平台下，Elasticsearch是作为一个`.tar.gz`的归档文件。

&emsp;&emsp;这个安装包同时包含了免费和订阅的特性，[30天试用](#License-settings)可以使用所有的功能。

&emsp;&emsp;Elasticsearch最新的稳定版本可以从这个页面下载，其他版本可以从过去的发布页面下载。

>NOTE： Elasticsearch中捆绑了一个OpenJDK，这个OpenJDK由JDK维护者使用GPLv2+CE协议维护，如果要使用自己的Java版本,

##### Download and install archive for Linux

&emsp;&emsp;Linux下Elasticsearch-7.15.2归档文件的下载以及安装如下所示：

```text
wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.15.2-linux-x86_64.tar.gz
wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.15.2-linux-x86_64.tar.gz.sha512
shasum -a 512 -c elasticsearch-7.15.2-linux-x86_64.tar.gz.sha512 
tar -xzf elasticsearch-7.15.2-linux-x86_64.tar.gz
cd elasticsearch-7.15.2/ 
```

&emsp;&emsp;上文第三行Compares the SHA of the downloaded `.tar.gz` archive and the published checksum，which should output `elasticsearch-{version}-linux-x86_64.tar.gz: OK`。

&emsp;&emsp;上文第五行就是所谓的 `$ES_HOME`。

##### Download and install archive for MacOS

&emsp;&emsp;MacOS下Elasticsearch-7.15.2归档文件的下载以及安装如下所示：

```text
wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.15.2-darwin-x86_64.tar.gz
wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.15.2-darwin-x86_64.tar.gz.sha512
shasum -a 512 -c elasticsearch-7.15.2-darwin-x86_64.tar.gz.sha512 
tar -xzf elasticsearch-7.15.2-darwin-x86_64.tar.gz
cd elasticsearch-7.15.2/ 
```

&emsp;&emsp;上文第三行Compares the SHA of the downloaded `.tar.gz` archive and the published checksum，which should output `elasticsearch-{version}-linux-x86_64.tar.gz: OK`。

&emsp;&emsp;上文第五行就是所谓的 `$ES_HOME`。

##### Enable automatic creation of system indices

&emsp;&emsp;一些商业功能会自动创建索引。默认情况下，Elasticsearch可以被配置为允许自动创建索引，并且不需要再做额外的步骤。然而，如果你关闭了自动创建索引， 你必须在`elasticsearch.yml`中配置[action.auto_create_index](#Index-APi)来允许商业功能创建下面的索引：

```text
action.auto_create_index: .monitoring*,.watches,.triggered_watches,.watcher-history*,.ml*
```

>IMPORTANT：如果你正在使用Logstash或者Beats，那么你很有可能需要在action.auto_create_index配置额外的索引名，确切的名称取决去你的本地配置。如果你不能保证正确的名称，你可以考虑将索引名设置为\*，这样就会自动创建所有的索引


##### Running Elasticsearch from the command line

&emsp;&emsp;Elasticsearch可以通过下面的命令行启动：

```text
./bin/elasticsearch
```

&emsp;&emsp;如果Elasticsearch keystore有密码保护，你会被提示（be prompted to）输入keystore's的密码，详细内容见[安全配置](#Secure-settings)。

&emsp;&emsp;默认情况下，Elasticsearch将日志打印到控制台（标准输出）以及[日志目录](#Important-Elasticsearch-configuration)的`<clustername>.log`文件中。Elasticsearch在启动期间会生成一些日志，但一旦初始化完成，它将继续在前台（foreground）运行并且不再生成任何日志直到一些值的记录的信息产生。在Elasticsearch运行期间，你可以通过HTTP接口访问默认的9200端口与Elasticsearch交互。输入`Ctrl-c`来停止Elasticsearch。

>NOTE：Elasticsearch安装包中的所有脚本要求的Bash版本需要支持arrays并且假定/bin/bash是存在的，同样的，Bash在这个路径是存在的或者能通过动态链接找到

##### Checking that Elasticsearch is running

&emsp;&emsp;你能通过HTTP请求访问localhost:9200来测试Elasticsearch节点是否在运行中：

```java
GET /
```

&emsp;&emsp;你应该能收到类似下面的回应（response）

```java
{
  "name" : "Cp8oag6",
  "cluster_name" : "elasticsearch",
  "cluster_uuid" : "AT69_T_DTp-1qgIJlatQqA",
  "version" : {
    "number" : "7.15.2",
    "build_flavor" : "default",
    "build_type" : "tar",
    "build_hash" : "f27399d",
    "build_date" : "2016-03-30T09:51:41.449Z",
    "build_snapshot" : false,
    "lucene_version" : "8.9.0",
    "minimum_wire_compatibility_version" : "1.2.3",
    "minimum_index_compatibility_version" : "1.2.3"
  },
  "tagline" : "You Know, for Search"
}
```

##### Running as a daemon

&emsp;&emsp;在命令行指定`-d`使得Elasticsearch成为一个后台程序，并且通过参数`-p`生成一个`pid`文件记录进程号。

```text
./bin/elasticsearch -d -p pid
```

&emsp;&emsp;如果Elasticsearch keystore有密码保护，你会被提示（be prompted to）输入keystore's的密码，详细内容见[安全配置](#Secure-settings)。

&emsp;&emsp;在`$ES_HOME/logs/`目录能找到日志信息。

&emsp;&emsp;通过杀死`pid`文件中记录的进程号来关闭Elasticsearch。

```text
pkill -F pid
```

>NOTE：tar.gz的安装包中不包含systemd组件。如果要把Elasticsearch作为一个服务管理，用RPM或者Debian的安装包进行安装。

##### Configuring Elasticsearch on the command line

&emsp;&emsp;Elasticsearch默认装载(load)的配置文件路径为`$ES_HOME/config/elasticsearch.yml`，这个配置文件中的格式见[Configuring Elasticsearch](#Configuring-Elasticsearch)。

&emsp;&emsp;在配置文件中可以指定的任何设置（settings）都可以通过命令行的参数指定，使用`-E`语法：

```text
./bin/elasticsearch -d -Ecluster.name=my_cluster -Enode.name=node_1
```

>TIP：一般来说，任何集群范围（cluster-wide）的配置（比如说 cluster.name）都应该在elasticsearch.yml文件中配置，同时具体节点的任意配置比如说node.name可以通过命令行参数指定

##### Directory layout of archives

&emsp;&emsp;归档发行版 (archive distribution)是self-contained（意思就是所有的文件都在同一个目录中，Elasticsearch还有安装包发行版package distribution，通过这种方式安装后，文件会分散在不同的目录中）。所有的文件和目录默认情况下都在`$ES_HOME`下，`$ES_HOME`在解压归档后会被创建。

&emsp;&emsp;这是一种非常方便的方式因为你不需要创建任何目录来开始使用Elasticsearch，并且当卸载Elasticsearch时只要简单的移除`$ES_HOME`。然而我们建议把配置文件目录config directory、数据目录data directory、以及日志目录log directory的位置进行变更，以免在以后重要的数据被删除。

| 类型Type |                       描述Description                        | 默认位置Default Location |                         设置Setting                          |
| :------: | :----------------------------------------------------------: | :----------------------: | :----------------------------------------------------------: |
|   home   |             Elasticsearch home目录或者`$ES_HOME`             |  解压归档后创建这个目录  |                                                              |
|   bin    | 二进制脚本，包含`elasticsearch`用于启动一个节点以及`elasticsearch-plugin`用于安装插件 |      `$ES_HOME/bin`      | [ES_PATH_CONF](#Configuring-Elasticsearch) |
|   conf   |            包含`elasticsearch.yml`的所有配置文件             |    `$ES_HOME/config`     |                                                              |
|   data   |               每一个索引/分片的数据文件的位置                |     `$ES_HOME/data`      |                          path.data                           |
|   logs   |                         日志文件位置                         |     `$ES_HOME/logs`      |                          path.logs                           |
| plugins  |             插件文件位置。每个插件在一个子目录中             |    `$ES_HOME/plugins`    |                                                              |
|   repo   | Shared file system repository locations. Can hold multiple locations. A file system repository can be placed in to any subdirectory of any directory specified here. |      NOT_configured      |                          path.repo                           |

##### Next steps

&emsp;&emsp;现在你建好了一个Elasticsearch测试环境。在你开始认真做开发护着进入Elasticsearch的生产环境，你必须做一些额外的设置：

- 学习如何[配置Elasticsearch](#Configuring-Elasticsearch)
- 配置[重要的Elasticsearch设置](#Important-Elasticsearch-configuration)
- 配置[重要的系统设置](#Important-system-configuration)

#### Install Elasticsearch with Docker
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/docker.html)

### Configuring Elasticsearch
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/settings.html)

&emsp;&emsp;Elasticsearch附带（ship with）了很好的默认配置并且只需要较小的配置。在一个运行中的集群中，大部分的设置可以通过[Cluster update settings](#Cluster-update-settings-API) API进行更改。

&emsp;&emsp;配置文件应该包含对指定节点（node-specific）的设置，例如`node.name`以及路径，或者是能让节点加入集群的配置，例如`cluster.name`以及`network.host`。

##### Config files location

&emsp;&emsp;Elasticsearch有三个配置文件：

- `elasticsearch.yml` 用于配置 Elasticsearch
- `jvm.options` 用于配置Elasticsearch JVM 设置
- `log4j2.properties` 用于配置 Elasticsearch日志记录

&emsp;&emsp;这三个文件都在config目录中，它们的默认路径取决于使用哪种方式安装：归档发行版（archive distribution）`tar.gz`或者`zip`还是安装包发行版（package distribution）Debian 或者RPM安装包。

&emsp;&emsp;对于归档发行版，config目录位置默认是`$ES_HOME/config`。config目录的位置能通过`ES_PATH_CONFIG`这个环境变量进行更改：

```text
ES_PATH_CONF=/path/to/my/config ./bin/elasticsearch
```

&emsp;&emsp;或者，通过在命令行或profile文件中`export` `ES_PATH_CONFIG`这个环境变量。

&emsp;&emsp;对于安装包发行版，config目录的位置默认在`/etc/elasticsearch`，config目录的位置可以通过`ES_PATH_CONF`这个环境变量变更。注意的是在shell中设置这些是不够的。Instead, this variable is sourced from `/etc/default/elasticsearch` (for the Debian package) and `/etc/sysconfig/elasticsearch` (for the RPM package). You will need to edit the`ES_PATH_CONF=/etc/elasticsearch` entry in one of these files accordingly to change the config directory location.

##### Config file format

&emsp;&emsp;配置格式是[YAML](https://yaml.org)，下面是更改数据和日志目录的例子

```text
path:
    data: /var/lib/elasticsearch
    logs: /var/log/elasticsearch
```

&emsp;&emsp;配置也可以进行平铺（flattened）

```text
path.data: /var/lib/elasticsearch
path.logs: /var/log/elasticsearch
```

&emsp;&emsp;在YAML中，你可以格式化non-scalar的值写成序列：

```text
discovery.seed_hosts:
   - 192.168.1.10:9300
   - 192.168.1.11
   - seeds.mydomain.com
```

&emsp;&emsp;尽管下面的方式不是很常见（Thought less common），你也可以把non-scala的值写成数组：

```text
discovery.seed_hosts: ["192.168.1.10:9300", "192.168.1.11", "seeds.mydomain.com"]
```

##### Environment variable substitution

&emsp;&emsp;在配置文件中可以通过使用`${...}`这类符号的方式引用环境变量：

```text
node.name:    ${HOSTNAME}
network.host: ${ES_NETWORK_HOST}
```

&emsp;&emsp;环境变量的值必须是简单的String值，用逗号分隔的String值会被Elasticsearch解析成一个list。例如，Elasticsearch将下面的环境变量的String值根据逗号切分到list中。

```shell
export HOSTNAME="host1,host2"
```

##### Cluster and node setting types

&emsp;&emsp;集群跟节点的设置基于它们是如何被配置可以分类为：

###### Dynamic（settings）

&emsp;&emsp;在一个正在运行的集群上，你可以使用[cluster update settings API](#Cluster-update-settings-API)来配置或更新动态设置。你也可以使用`elasticsearch.yml`对本地未启动或者关闭的节点配置动态设置。

&emsp;&emsp;使用[cluster update settings API](#Cluster-update-settings-API)更新可以是永久的（persistent），即使集群重启也生效，也可以是临时的（transient），集群在重启后就重置了。你也可以通过使用API把配置设置为null也能重置使用persistent或者transient更新过的设置。

&emsp;&emsp;如果你用多种方式配置了一样的设置，那么Elasticsearch会根据下面的优先顺序（order of precedence）来应用设置（apply settings）。

1. Transient Setting
2. Persistent Setting
2. `elasticsearch.yml`  setting
2. Default setting value

&emsp;&emsp;例如，你可以使用一个transient setting覆盖persistent setting或者`elasticsearch.yml`。然而，在`elasticsearch.yml`上的变更不会覆盖定义好的（defined）transient 或者 persistent setting。

>TIP：如果你使用Elasticsearch Service，使用[user settings](https://www.elastic.co/guide/en/cloud/current/ec-add-user-settings.html)功能来配置所有的设置。这个方法能让Elasticsearch自动的拒绝（reject）掉任何会破坏你集群的设置。
如果你在自己的设备（hardware）上运行Elasticsearch，可以使用cluster update settings API来配置集群动态设置。对于集群或者节点的静态设置只使用elasticsearch.yml来配置。使用API不会要求重启并且保证所有节点都被配置成相同的值。

>WARNING: We no longer recommend using transient cluster settings. Use persistent cluster settings instead. If a cluster becomes unstable, transient settings can clear unexpectedly, resulting in a potentially undesired cluster configuration. See the [Transient settings migration guide](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/transient-settings-migration-guide.html).

###### Static（settings） 

&emsp;&emsp;静态设置只能在未启动或者关闭的节点上使用`elasticsearch.yml`来配置。

&emsp;&emsp;静态配置必须在集群中相关的所有节点上一一配置。

#### Important Elasticsearch configuration
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/7,15/important-settings.html)

&emsp;&emsp;Elasticsearch只需要很少的配置就能启动。但是在生产中使用你的集群时必须要考虑这些条目：

- [Path settings](#Path-settings)
- [Cluster name setting](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/important-settings.html#cluster-name)
- [Node name setting](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/important-settings.html#node-name)
- [Network host settings](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/important-settings.html#network.host)
- [Discovery settings](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/important-settings.html#discovery-settings)
- [Heap size settings](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/important-settings.html#heap-size-settings)
- [JVM heap dump path setting](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/important-settings.html#heap-dump-path)
- [GC logging settings](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/important-settings.html#gc-logging)
- [Temporary directory settings](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/important-settings.html#es-tmpdir)
- [JVM fatal error log setting](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/important-settings.html#error-file-path)
- [Cluster backups](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/important-settings.html#important-settings-backups)

&emsp;&emsp;我们的[Elastic Cloud service](https://www.elastic.co/cloud/elasticsearch-service/signup?baymax=docs-body&elektra=docs)能自动配置这些条目，使得你的集群处在生产就绪状态。

##### Path settings

&emsp;&emsp;Elasticsearch把你的索引数据和数据流（data streams）都写到`data`目录中。

&emsp;&emsp;Elasticsearch本身也会生产应用日志，包含集群健康信息以及集群操作信息，并且写入到`logs`目录中。

&emsp;&emsp;对于在MacOS和Linux下使用`.tar.gz`, 以及Windows下使用`zip`方式安装时，`data`和`logs`目录默认是`$ES_HOME`下的子目录，然而在升级Elasticsearch时，位于`$ES_HOME`下的文件是存在风险的。

&emsp;&emsp;在生产中，我们强烈建议在`elasticsearch.yml`中设置`path.data`和`path.logs`将`data`和`logs`目录放到`$ES_HOME`以外的位置。其他安装方式（rpm、macOS Homebrew、Windows `.msi`）默认会把`data`和`logs`目录放到`$ES_HOME`以外的位置。

&emsp;&emsp;支持在不同的平台，`path.data`和`path.logs`可以是不同的值：

- Unix-like systems
  - Linux and macOS installations support Unix-style paths:
```text
path:
  data: /var/data/elasticsearch
  logs: /var/log/elasticsearch
```

- Windows
  - Windows installations support DOS paths with escaped backslashes:

```text
path:
  data: "C:\\Elastic\\Elasticsearch\\data"
  logs: "C:\\Elastic\\Elasticsearch\\logs"
```

>WARNING：不要更改data目录中的任何内容（content），也不要运行可能会影响data目录内容的程序。如果Elasticsearch以外的程序更改了data目录的内容，那么Elasticsearch可能会失败，报告损坏或者其他数据不一致性（data inconsistencies）的问题，也可能工作正常但是丢失了一些你的数据。

##### Multiple data paths

>WARNING: 在7.13.0版本中废弃了

&emsp;&emsp;如果有这个需要，你可以在`path.data`中指定多个路径。Elasticsearch会在这些路径上存储节点的数据，但是会在相同的路径上存储同一个shard的数据。

&emsp;&emsp;Elasticsearch不会在多个路径上去平衡shards的写入。在某个路径上的高磁盘使用率会触发整个节点的[high disk usage watermark](#Cluster-level-shard-allocation-and-routing-settings])。触发后，Elasticsearch不会在这个节点增加shard，尽管这个节点的在其他路径上有可用的空间。如果你需要额外的磁盘空间，我们建议你增加新的节点而不是增加data的路径。

- Unix-like systems
  - Linux and macOS installations support Unix-style paths:
```text
path:
  data:
    - /mnt/elasticsearch_1
    - /mnt/elasticsearch_2
    - /mnt/elasticsearch_3
```

- Windows
  - Windows installations support DOS paths with escaped backslashes:

```text
path:
  data:
    - "C:\\Elastic\\Elasticsearch_1"
    - "E:\\Elastic\\Elasticsearch_1"
    - "F:\\Elastic\\Elasticsearch_3"
```

##### Migrate from multiple data paths

&emsp;&emsp;在7.13版本中弃用了多个数据路径的支持，将会在未来的版本中移除。

&emsp;&emsp;作为多个数据路径的替代方案，你可以通过使用硬件虚拟化层（如RAID）或软件虚拟化层（如Linux上的逻辑卷管理器（LVM）或Windows上的存储空间）来创建跨多个磁盘的文件系统。如果你希望在单台机器上使用多个数据路径，则必须为每个数据路径运行一个节点。

&emsp;&emsp;如果你正在一个[highly available cluster](#Designing-for-resilience)中使用多个数据路径，你可以无需停机，使用类似[rolling restart](#Rolling-restart)的方式对一个节点使用单个数据路径来实现迁移：轮流关闭每一个节点，并用一个或多个配置为使用单个数据路径的节点替换它。跟进一步的说，对于每一个当前有多个数据路径的节点来说，你应该按照下面的步骤进行迁移。原则上你可以在升级到8.0版本时执行这类迁移，但是我们建议在升级版本前先完成单个数据路径的迁移工作。

1. 执行快照防止你的数据出现灾难性的问题
2. （可选）使用[allocation filter](#Cluster-level-shard-allocation-filtering)将目标节点的数据进行迁移

```text
PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.exclude._name": "target-node-name"
  }
}
```
你可以使用[cat allocation API](#cat-allocation-API)来跟踪数据迁移的进度。如果某些分片没有迁移，那么[cluster allocation explain API](#Cluster-allocation-explain-API)会帮助你查明原因。

3. 按照[rolling restart process](#Rolling restart)中的步骤执行，包括关闭目前节点。
4. 确保你的集群是`yellow`或者`green`，使得你的集群中至少有一个节点有每一个分片的拷贝。
5. 如果执行了上面的第二步，现在需要移除allocation filter。
```text
PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.exclude._name": null
  }
}
```
6. 通过删除数据路径的方式来丢弃被停掉的节点拥有的数据。
7. 重新配置你的存储。使用LVM或者Storage Spaces将你的磁盘合并到单个文件系统中。确保你重新配置的存储有足够的空间存储数据
8. 通过设置`elasticsearch.yml`文件中的`path.data`来重新配置你的节点。如果有需要的话，安装更多的节点，并使用它们自己各自的`path.data`。
9. 启动新的节点并按照[rolling restart process](#Rolling restart)中的其他步骤执行
10. 确保你的集群健康是`green`，使得所有的分片都已经分配结束

&emsp;&emsp;你也可以在你的集群中增加一些单数据路径的节点，使用[allocation filters](#Cluster-level shard allocation filtering)将你的数据迁移到新的节点中，然后将这些旧的节点从集群中移除。这个方法会临时增大集群中的体积，所以只有你的集群有这个膨胀能力才能使用这个方法。

&emsp;&emsp;如果你目前使用多个数据路径，但你的集群并不具备高可用性，则可以通过拍摄快照、创建具有所需配置的新集群并将快照还原到其中来迁移到非废弃配置。

##### Cluster name setting

&emsp;&emsp;在一个集群中，一个节点只有跟集群中的其他节点共享他的`cluster.name`才能加入到这个集群中。默认的名称是`elasticsearch`，但是你应该把这个名称改成一个合适的名称，这个名称能描述这个集群的目的。

```text
cluster.name: logging-prod
```

>IMPORTANT：在不同的环境中不要重复使用相同的集群名，否则节点可能会加入到错误的集群中。

##### Node name setting

&emsp;&emsp;Elasticsearch使用`node.name`作为可读（human-readable）的一个特定的Elasticsearch实例。在很多APIs的返回中（response）会用到。当Elasticsearch启动后，节点名称默认是服务器的hostname，可以在`elasticsearch.yml`中显示配置：

```text
node.name: prod-data-2
```

##### Network host setting

&emsp;&emsp;默认清下，Elasticsearch只绑定类似127.0.0.1以及\[::1\]的回调地址（lookback address）。这样的话能够在单个服务上允许一个集群的多个节点用于开发以及测试。但是一个[resilient production cluster](#Designing-for-resilience)必须是包含在其他服务上的节点。尽管还有许多[newwork settings](#Networking)，但是通常你只需要配置`network.host`：

```text
network.host: 192.168.1.10
```

> 当你提供了network.host的值，Elasticsearch会假设你正在从开发模式转到生产模式，并且会在系统启动时升级warning到exception的检查。见[development and production modes](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/system-config.html#dev-vs-prod)的差异。

##### Discovery and cluster formation settings(setting)

&emsp;&emsp;在进入生产前配置两个重要的discovery和cluster formation设置，使得这个集群中的节点能互相发现并且选出一个master节点。

###### discovery.seed_hosts

&emsp;&emsp;如果没有配置任何网络设置，Elasticsearch会绑定可见的回调地址并且扫描本地的9300到9305端口去连接同一个服务上的其他节点。这种行为提供了一种自动集群体验，而无需进行任何配置。即开箱即用（out of box）。

&emsp;&emsp;当你想要组成（form）一个集群，它包含的节点在其他服务器上时，可以使用[静态](#Static)的`discovery.seed_hosts`设置。这个设置可以提供一个集群中其他节点的list，这些节点是候选的主节点（mater-eligible），并且有可能是live的，并且能在[discovery process](#Discovery)中作为可以联系的节点来选出种子选手（and contactable to seed the discovery process。翻译能力有限，只可意会，不可言传）。这个设置可以将集群中所有候选的主节点的地址用YAML中的序列或者数组的风格书写。每一个地址可以是IP地址或者hostname，hostname可以通过DNS关联一个或多个IP地址。

```text
discovery.seed_hosts:
   - 192.168.1.10:9300
   - 192.168.1.11 
   - seeds.mydomain.com 
   - [0:0:0:0:0:ffff:c0a8:10c]:9301 
```

1. 如果IP地址中没有指定端口号，那么就是默认的9300，默认的端口号也可以被[override](#Discovery)
1. 如果hostname关联多个IP 地址，那么当前节点会试图发现hostname关联的所有地址上的节点
1. IPv6地址使用方括号包裹（enclosed in square brackets）

&emsp;&emsp;如果候选的主节点的节点没有固定的名称或者地址，可以使用[alternative hosts provider](#Discovery) 来动态的找到它们的地址。

###### cluster.initial_master_nodes

&emsp;&emsp;当你第一次启动Elasticsearch集群，在[cluster bootstrapping](#Bootstrapping-a-cluster)阶段确定在第一次参与投票的候选主节点。在[development mode](#Bootstrap-Checks)中，with no discovery settings configured, this step is performed automatically by the nodes themselves。

&emsp;&emsp;因为auto-bootstrapping是内在不安全的，在生产模式中启动一个新的集群时，你必须显式列出候选的主节点并且它们的投票要被统计在第一轮的选票中。你可以使用`cluster.initial_master_nodes`设置来列出这些候选的主节点。

> IMPORTANT：当一个新的集群第一次形成之后，从每个节点的配置中移除cluster.initial_master_nodes设置。当重启一个集群或者往已存在的集群中添加一个新的节点时，不要使用该配置。

```text
discovery.seed_hosts:
   - 192.168.1.10:9300
   - 192.168.1.11
   - seeds.mydomain.com
   - [0:0:0:0:0:ffff:c0a8:10c]:9301
cluster.initial_master_nodes: 
   - master-node-a
   - master-node-b
   - master-node-c
```

1. 通过`node.name`来确定最初的master节点身份，默认是hostname。要确保`cluster.initial_master_nodes`跟`node.name`的值是一致的。如果节点的名称使用了全限定域名（fully-qualified domain name），比如master-node-a.example.com，那么你必须在cluster.initial_master_nodes中使用FQDN。相反的，如果只是仅仅使用了hostname而没有尾随的限定符，那么在cluster.initial_master_nodes也不能带尾随的限定符

&emsp;&emsp;见 [bootstrapping a cluster](#Bootstrapping-a-cluster)和[discovery and cluster formation settings](#Discovery-and-cluster-formation)。

##### Heap size settings

&emsp;&emsp;默认情况下，Elasticsearch会根据节点的[roles](#Node)跟内存大小来自动的设置JVM堆的大小。我们建议使用默认的大小，它适用于大部分生产环境。

>NOTE：堆大小的自动配置需要[bundle JDK](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/setup.html#jvm-version)，如果使用自定义的JRE位置，需要Java 14以及更新的JRE。

&emsp;&emsp;如果有必要，你可以通过[setting the JVM heap size](#Advanced-configuration)手动覆盖默认值。

##### JVM heap dump path setting

&emsp;&emsp;默认情况下，Elasticsearch会配置JVM，使得OOM异常转存（dump）到`data`目录。在基于ROM跟Debian的安装包中，`data`目录位于/var/lib/elasticsearch。在Linux、MacOs以及Windows发行版中，`data`目录位于Elasticsearch安装目录的根目录。

&emsp;&emsp;如果默认的路径用于接受heap dump不是很合适，那么可以通过在文件`jvm.options`中修改`-XX:HeapDumpPath=...`。

- 如果你指定了一个目录，JVM将会基于正在运行的实例，用它的PID来为heap dump文件命名
- 如果你指定了一个固定的文件名而不是一个目录，那么这个文件必须不存在，否则heap dump会失败

##### GC logging settings

&emsp;&emsp;默认情况下，Elasticsearch开启了垃圾回收日志。在`jvm.options`文件中配置，并且输出到Elasticsearch的`logs`文件中。默认的配置中，每64M就会轮换一次日志，最多消耗2G的磁盘空间。

&emsp;&emsp;你可以命令行来配置JVM的日志打印，见[JEP 158: Unified JVM Logging](https://openjdk.java.net/jeps/158)。除非你直接更改`jvm.options`文件，Elasticsearch的默认配置会被应用（apply）除了你自己的设置。如果要关闭默认配置，第一步先通过提供参数`-Xlog:disable`关闭日志打印，然后提供你自己的命令选项。以上操作会关闭所有的JVM日志打印，所以要确定好可用的选项来满足你的需求。

&emsp;&emsp;见 [Enable Logging with the JVM Unified Logging Framework](https://docs.oracle.com/en/java/javase/13/docs/specs/man/java.html#enable-logging-with-the-jvm-unified-logging-framework)了解更多在原始的JEP（origin JEP）中不包含的虚拟机选项。

##### Examples

&emsp;&emsp;下面的例子中创建了`$ES_HOME/config/jvm.options.d/gc.options`并包含一些虚拟机选项：把默认的GC 日志输出到`/opt/my-app/gc.log`。

```text
# Turn off all previous logging configuratons
-Xlog:disable

# Default settings from JEP 158, but with `utctime` instead of `uptime` to match the next line
-Xlog:all=warning:stderr:utctime,level,tags

# Enable GC logging to a custom location with a variety of options
-Xlog:gc*,gc+age=trace,safepoint:file=/opt/my-app/gc.log:utctime,pid,tags:filecount=32,filesize=64m
```

&emsp;&emsp;下面的例子中配置一个[Elasticsearch Docker container](# Install Elasticsearch with Docker)，将GC的debug日志输出到`stderr`，让容器编排器处理输出。

```text
MY_OPTS="-Xlog:disable -Xlog:all=warning:stderr:utctime,level,tags -Xlog:gc=debug:stderr:utctime"
docker run -e ES_JAVA_OPTS="$MY_OPTS"
```

##### Temporary directory settings

&emsp;&emsp;默认情况下，Elasticsearch会使用一个私有的临时目录，它由一个启动脚本创建，并位于系统的临时目录中。

&emsp;&emsp;在一些Linux发行版中，一些系统工具会清楚 `/tmp` 目录下长时间未被访问的文件。这会导致Elasticsearch的私有临时目录可能会被删除。移除私有的临时目录会导致一些问题，比如一些功能随后会访问这些私有临时目录。

&emsp;&emsp;如果你使用`.deb`或者`.rpm`安装的Elasticsearch，并且在`systemd`下运行，那么Elasticsearch使用的私有临时目录不会被周期性的清除（periodic cleanup）。

&emsp;&emsp;如果你想要在Linux跟MacOS上运行`.tar.gz`发行版，可以考虑为Elasticsearch创建一个专用的临时目录，并且该目录所在路径中不包含旧的文件或目录，并且这个目录只有Elasticsearch有访问权限。最后在Elasticsearch启动前设置环境变量`$ES_TMPDIR`。

##### JVM fatal error log setting

&emsp;&emsp;默认情况下，Elasticsearch会配置JVM将fatal错误日志写到默认的日志目录中。在`RPM`和`Debian`安装中，目录位置为`/var/log/elasticsearch`，在`Linux、MacOs、Windows`发行版中，`logs`目录位于Elasticsearch的安装目录。

&emsp;&emsp;当遇到一些fatal错误日志，它由JVM产生，比如说段错误（segmentation fault）。如果接受这些日志的路径不是很合适，可以在`jvm.options`中修改`-XX:ErrorFile=...`。

##### Cluster backups

&emsp;&emsp;在发生灾难时，[snapshot](#Snapshot-and-restore)能防止数据永久丢失。Snapshot lifecycle management是最简单的办法来对集群实现定期备份。见[Create a snapshot](#Create a snapshot)。

>WARNING：快照是唯一可靠并且被支持的方式（supported way）来备份集群。
你不能通过拷贝data目录中的节点数据进行备份。没有一种支持方式（supported way）从文件系统层面的备份来恢复数据。如果你想尝试从这种备份中恢复集群，可能会失败并且Report出corruption或者文件丢失或者其他数据一致性的问题，或者出现成功的悄无声息的丢失一些你的数据

#### Secure settings
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/secure-settings.html)

&emsp;&emsp;有些设置是敏感的（sensitive），仅仅依靠文件系统的权限来保护这些值是不够。对于这种情况，Elasticsearch提供了keystore和 [elasticsearch-keystore tool](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/elasticsearch-keystore.html) 来管理这些设置。

>IMPORTANT：只有一些设置被设计为从keystore中读取。但是keystore没有验证一些不支持的设置。增加不支持的设置会导致Elasticsearch启动失败。

&emsp;&emsp;所有在keystore中的变更只有在Elasticsearch重启后才生效。

&emsp;&emsp;keystore中设置跟配置文件`elasticsearch.yml`中的普通的设置项一样都需要在每个节点上指定。当前，所有的安全设置都是特定于节点的，即所有的节点上必须设置相同的值。

##### Reloadable secure settings

&emsp;&emsp;跟`elasticsearch.yml`中的设置值一样，keystore中的内容无法自动的应用（apply）到正在运行的Elasticsearch节点，需要重启。然而有一些安全设置被标记为reloadable，这些设置可以被[reload](#Nodes-reload-secure-settings-API)。

&emsp;&emsp;对于安全设置的值，不管是否可以reloadable，集群中的所有节点都必须一致。在安全设置变更后，执行`bin/elasticsearch-keystore`调用下面的请求：

```text
POST _nodes/reload_secure_settings
{
  "secure_settings_password": "keystore-password" 
}
```

1. Elasticsearch keystore会对密码进行加密。

&emsp;&emsp;上述API会解密并且重新读取集群中所有节点的整个keystore，但是只有reloadable的安全设置可以被应用（apply）。其他的安全设置不会生效直到下一次重启。一旦API调用返回了，意味着reload完成了，即所以依赖这些设置的内部数据结构都已经发生了变更。Everything should look as if the settings had the new value from the start。

&emsp;&emsp;当更改了多个reloadable安全设置后，在集群中的每一个节点上修改后，随后发起[reload_secure_settings](#Nodes-reload-secure-settings-API)的调用，而不用在每一次变更后reload。

&emsp;&emsp;下面是reloadable安全设置

- [The Azure repository plugin](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/repository-azure-client-settings.html)
- [The EC2 discovery plugin](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/discovery-ec2-usage.html#_configuring_ec2_discovery)
- [The GCS repository plugin](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/repository-gcs-client.html)
- [The S3 repository plugin](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/repository-s3-client.html)
- [Monitoring settings](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/monitoring-settings.html)
- [Watcher settings](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/notification-settings.html)

#### Auditing security settings
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/auditing-settings.html#auditing-settings)

&emsp;&emsp;你可以使用[audit logging](#Enable-audit-logging)来记录安全相关的事件，比如认证失败，拒绝连接，以及数据访问事件。另外也会记录通过API访问安全配置的操作，例如创建、更新、移除[native](#Native-user-authenticationedit)、[built-in](#Built-in-users) users，[roles](#Create-or-update-roles-API])、[role mappings](#Create-or-update-role-mappings-API) 以及 [API keys](#Create-API-key-API)。

>TIP：日志审计只在某些订阅中提供，详细信息见 https://www.elastic.co/subscriptions

&emsp;&emsp;在配置后，集群上的所有节点都需要设置一遍。静态设置，比如说`xpack.security.audit.enabled`，在每个节点的`elasticsearch.yml`中都必须配置。对于动态的日志设置，使用[cluster update settings API](#Cluster-update-settings-API) 可以让所有节点的设置一致。

##### General Auditing Settings

###### xpack.security.audit.enabled

&emsp;&emsp;（[Static](#Static（settings） )）设置为true来开启。默认值为false。在每个节点上日志事件会被放到一个名为`<clustername>_audit.json`的专用的文件中。

&emsp;&emsp;如果开启了，集群中的所有节点的`elasticsearch.yml`都需要配置该设置。

##### Audited Event Settings

&emsp;&emsp;日志事件以及其他一些信息比如获取什么样的日志可以通过下面的设置来控制：

###### xpack.security.audit.logfile.events.include

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）在日志输出中指定事件类型（[kind of events](#Audit-events)），设置为`_all`会彻底的记录所有的日志事件，但通常不建议这么做因为会非常的verbose。默认值是一个list包含：`access_denied`, `access_granted`, `anonymous_access_denied`, `authentication_failed`, `connection_denied`, `tampered_request`, `run_as_denied`, `run_as_granted`, `security_config_change`。

###### xpack.security.audit.logfile.events.exclude

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）从包含的（[kind of events](#Audit-events)）列表中指定排除部分选项。当xpack.security.audit.logfile.events.include的值设置为`_all`时，然后用`xpack.security.audit.logfile.events.exclude`来进行指定选项的排除是不错的方式。默认值是空的list。

###### xpack.security.audit.logfile.events.emit_request_body

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）用于指定是否将REST请求的请求body作为日志事件的属性，这个设置可以用于[audit search queries](#Auditing-search-queries)。

&emsp;&emsp;默认是false，那么请求body不会被打印出来。

>IMPORTANT：注意的是，当日志事件中包含请求body时，一些敏感数据可能以明文形式被记录，即使是一些安全相关API，比如修改用户密码，认证信息在开启后有被泄露（filter out）的风险

##### Local Node Info Settings

###### xpack.security.audit.logfile.emit_node_name

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）指定在每一个日志事件中，节点名称[node name](#Important-Elasticsearch-configuration)是否作为其中的一个字段。默认值是false。

###### xpack.security.audit.logfile.emit_node_host_address

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）指定在每一个日志事件中，节点的地址是否作为其中的一个字段。默认值是false。

###### xpack.security.audit.logfile.emit_node_host_name

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）指定在每一个日志事件中，节点的主机名是否作为其中的一个字段。默认值是false。

###### xpack.security.audit.logfile.emit_node_id

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）指定在每一个日志事件中，节点的id是否作为其中的一个字段。不同于[node name](#Important-Elasticsearch-configuration)，管理员可以在配置文件中修改节点id，节点id在集群启动后将不可变更，默认值是true。

##### Audit Logfile Event Ignore Policies

&emsp;&emsp;下面的设置会影响[ignore policies](#Logfile-audit-events-ignore-policies)，它们能更精细的（fine-grained）控制日志事件打印到日志文件中。拥有相同police_name的设置将会组合到一个策略（police）中。如何一个事件匹配到任意一条策略的所有条件，该日志事件将被忽略并不会被打印。大多数的日志事件都遵照（subject to）忽略策略。唯一的例外（sole exception）是`security_config_change`类型的事件，无法被过滤掉（filter out），除非完全的通过`xpack.security.audit.logfile.events.exclude`进行exclude。

###### xpack.security.audit.logfile.events.ignore_filters.\<policy_name>.users

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）用户名称列表或者名称通配值（wildcards）。匹配该值的所有用户的日志事件不会打印出来。

###### xpack.security.audit.logfile.events.ignore_filters.\<policy_name>.realms

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）authentication realm的列表或者通配值，匹配该值的所有realm的日志事件不会打印出来。

###### xpack.security.audit.logfile.events.ignore_filters.\<policy_name>.actions

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）action名称的列表或者通配值，action的名称可以在日志事件的`action`字段中找到， 匹配该值的所有action的日志事件不会打印出来。

###### xpack.security.audit.logfile.events.ignore_filters.\<policy_name>.roles

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）角色列表或者角色通配值（wildcards）。匹配该值并且拥有该角色的所有用户的日志事件不会打印出来。如果用户拥有多个角色，而一些角色没有被策略覆盖到，那么该策略不会覆盖其日志事件。

###### xpack.security.audit.logfile.events.ignore_filters.\<policy_name>.indices

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）索引名称列表或者通配值，日志事件中所有的索引名都匹配后才不会打印。如果事件中涉及（concern）了多条索引，并且一些索引没有被该策略覆盖到，那么该策略不会覆盖日志事件。

#### Circuit breaker settings
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/circuit-breaker.html)

&emsp;&emsp;Elasticsearch包含多种熔断器来防止导致OOM的操作。每一种熔断器指定了内存的使用限制。另外，父级别的熔断器指定了内存的总量，限制子级别的熔断器的内存使用总量。

&emsp;&emsp;除了特别的说明，这些设置能通过[cluster-update-settings](#Cluster-update-settings-API) API在运行中的集群上进行动态的更新。

&emsp;&emsp;关于熔断器提供的报错信息，见[Circuit breaker errors](#Fix-common-cluster-issues)。

##### Parent circuit breaker

&emsp;&emsp;父级别的熔断器可以通过下面的设置进行配置：

###### indices.breaker.total.use_real_memory

&emsp;&emsp;（[Static](#Static（settings） )）如果该值为true，那么父级别的熔断器会考虑实际的内存使用量，否则考虑的是子级别的熔断器预定（reserved）的内存量总和。默认值为true。

###### indices.breaker.total.limit

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）当`indices.breaker.total.use_real_memory`为false，并且虚拟机的内存使用量达到70%，或者当`indices.breaker.total.use_real_memory`为true时，并且虚拟机的内存使用量达到95%，所有的父熔断器开始限制内存使用。

##### Field data circuit breaker

&emsp;&emsp;Filed data熔断器会估算把字段filed载入到[field data cache](#Field-data-cache-settings)所使用的堆内存量。如果载入后会导致缓存超过了先前定义的内存限制值，熔断器将会停止该操作并返回错误。

###### indices.breaker.fielddata.limit

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）fielddata breaker的限制值，默认是40%的JVM堆内存量。

###### indices.breaker.fielddata.overhead

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）该配置是一个常量，所有field data的估算值乘以这个常量来计算出最终的估算值。默认是值1.03。

##### Request circuit breaker

&emsp;&emsp;Request熔断器允许Elasticsearch防止每一个请求的数据结构（例如，在一次请求中，聚合计算占用的内存量）超过一定量的内存量。

###### indices.breaker.request.limit

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）Request熔断器的限制值，默认是60%的JVM堆内存量。

###### indices.breaker.request.overhead

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）该配置是一个常量，所有Request的估算值乘以这个常量来计算出最终的估算值。默认是值1。

##### In flight requests circuit breaker

&emsp;&emsp;in flight Request熔断器允许Elasticsearch限制所有在传输或者HTTP层的请求的内存使用量不超过在一个节点上的相对内存总量。内存使用量基于请求本身的content length。This circuit breaker also considers that memory is not only needed for representing the raw request but also as a structured object which is reflected by default overhead。

###### network.breaker.inflight_requests.limit

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）in flight Request熔断器的限制值，默认是60%的JVM堆内存量。

###### network.breaker.inflight_requests.overhead

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）该配置是一个常量，所有in flight Request的估算值乘以这个常量来计算出最终的估算值。默认是值2。

##### Accounting requests circuit breaker

&emsp;&emsp;accounting熔断器允许Elasticsearch限制某些请求已经结束，但是相关内存还未被释放的内存量。比如说Lucene的段占用的内存。

###### indices.breaker.accounting.limit

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）accounting熔断器的限制值，默认是100%的JVM堆内存量。这意味着受到父级熔断器的配置限制。

###### indices.breaker.accounting.overhead

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）该配置是一个常量，所有accounting Request的估算值乘以这个常量来计算出最终的估算值。默认是值1。

##### Script compilation circuit breaker

&emsp;&emsp;和上文中的熔断器稍微不同的是，Script compilation熔断器限制了在一个时间周期内inline script compilation的数量。

&emsp;&emsp;见[scripting](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-scripting-using.html) 文档中`prefer-parameters`章节的内容来了解更多。

###### script.max_compilations_rate

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）Limit for the number of unique dynamic scripts within a certain interval that are allowed to be compiled. Defaults to 150/5m, meaning 150 every 5 minutes。

##### Regex circuit breaker

&emsp;&emsp;写的不好的正则表达式会降低（degrade）集群稳定性以及性能。regex熔断器限制了[Painless scripts](https://www.elastic.co/guide/en/elasticsearch/painless/8.2/painless-regexes.html)中正则表达式的使用以及复杂性。

###### script.painless.regex.enabled

&emsp;&emsp;（[Static](#Static（settings） ))）允许painless脚本中使用正则表达式，该值可以是：

- limited（默认值）
  - 允许使用正则表达式但是使用集群设置中的[script.painless.regex.limit-factor](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/circuit-breaker.html#script-painless-regex-limit-factor)限制正则表达式的复杂性
- true
  - 允许使用正则表达式并且不限制正则表达式的复杂性。regex熔断器关闭
  
- false
  - 不允许使用正则表达式。包含任何正则表达式的painless脚本都会返回错误
  
###### script.painless.regex.limit-factor

&emsp;&emsp;（[Static](#Static（settings） ))）用来限制painless脚本中正则表达式的字符数量。Elasticsearch通过当前设置的值与脚本输入的值的长度的乘积值作为限制值。

&emsp;&emsp;比如说输入值`foobarbaz`的长度为9，如果`script.painless.regex.limit-factor`的值为6，那么基于`foobarbaz`的正则表达式的长度为54（6 \* 9）。如果超过这个限制值，将触发regex熔断器并返回错误。

&emsp;&emsp;只有`script.painless.regex.enabled`为`limited`时，Elasticsearch才会使用该限制值。

#### Cluster-level shard allocation and routing settings
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-cluster.html)

&emsp;&emsp;Shard allocation说的是分配分片到节点的过程。该过程会在initial recovery、副本（replica）分配、rebalancing或者增加或者移除节点时发生。

&emsp;&emsp;master node的其中一个应用是决定如何分配分片到节点，何时在节点之间移动分片来平衡集群。

&emsp;&emsp;以下的一些设置用来控制分配分片的过程：

- [Cluster-level shard allocation settings](#Cluster-level-shard-allocation-settings)用来控制分片和平衡操作
- [Disk-based shard allocation settings](#Disk-based-shard-allocation-settings)描述了Elasticsearch如何考虑（take into account）磁盘空间等等相关信息
- [Shard allocation awareness](#Shard-allocation-awareness) 和[Forced awareness](#Forced-awareness)用来控制在不同的rack和可见区域上发布分片
- [Cluster-level shard allocation filtering](#Cluster-level shard allocation filtering)允许一些节点或者组内的节点不会被分配分片，使得这些节点能够被关闭（decommissioned）

&emsp;&emsp;除此之外，还有一些其他的配置，见[Miscellaneous cluster settings](#Miscellaneous-cluster-settings)。

##### Cluster-level shard allocation settings

&emsp;&emsp;你可以使用下面的设置来控制分片的分配以及恢复：

###### cluster.routing.allocation.enable

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）开启或者关闭某些分片类型的分配：

- `all`：（default）所有类型的分配都可以被分配
- `primaries`：只有主分片才能被分配
- `new_primaries`：只有主分片中新的索引才能被分配
- `none`：任何分片中的任何索引都不能被分配

&emsp;&emsp;在节点重启后，上述的配置不会影响本地主分片的恢复。重启后的节点上的未分配的主分片的allocation id如果匹配了集群状态中的active allocation ids，那么会立即恢复。

###### cluster.routing.allocation.node_concurrent_incoming_recoveries

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）在一个节点上允许同时进行恢复incoming recoveries的并发数量。incoming recoveries中的分片（大多数是副本分片或者就是分片正在重新分配位置relocating）将被分配到当前节点上。默认值是2。

###### cluster.routing.allocation.node_concurrent_outgoing_recoveries

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）在一个节点上允许同时进行恢复outgoing recoveries的并发数量。outgoing recoveries中的分片（大多数是当前节点上的主分片或者就是分片正在重新分配位置relocating）将被分配到当前节点上。默认值是2。

###### cluster.routing.allocation.node_concurrent_recoveries

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）一种快捷方法shortcut来设置`cluster.routing.allocation.node_concurrent_incoming_recoveries`和`cluster.routing.allocation.node_concurrent_outgoing_recoveries`。

###### cluster.routing.allocation.node_initial_primaries_recoveries

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）在一个节点重启后，副本分片replica的恢复是通过网络实现的，然而一个未分配的主分片unassigned primary则是通过读取本地磁盘数据恢复的。在同一个节点上通过这种本地恢复的方式是很快的，默认值是4。

###### cluster.routing.allocation.same_shard.host

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）该值允许执行一个检查，防止在单个主机上上分配多个相同的分片，通过主机名以及主机地址来描述一台主机。默认值是false。意味着默认不会进行检查。只有在同一台机器上的多个节点启动后，该值才会应用（apply）。

##### Shard rebalancing settings

&emsp;&emsp;当每一个节点上的分片数量是相等并且在任何节点的任何索引上没有集中分片（concentration of shards），那么集群是平衡的。Elasticsearch会运行一个自动程序（automatic process）称为rebalancing，它会在集群中的节点之间移动分片来提高平衡性。rebalancing遵守类似 [allocation filtering](#Cluster-level-shard-allocation-filtering) 和 [forced awareness](#Forced-awareness)的分片分配规则。这些规则会阻止完全的平衡集群，因此rebalancing会努力（strive to）在你配置的规则下尽可能的去平衡集群。如果你正在使用 [data tiers](#Data-tiers)，那么Elasticsearch会自动的应用分配过滤规则将每一个分配放到合适的数据层，这些规则意味着平衡器在每一层中独立工作。

&emsp;&emsp;你可以使用下面的设置在集群中控制分片的平衡：

###### cluster.routing.rebalance.enable

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）为指定的分片类型开启或关闭rebalancing：

- `all`：（default）所有类型的分片都允许rebalancing
- `primaries`：只有主分片才允许rebalancing
- `replicas`：只有副本分片才允许rebalancing
- `none`：任何类型分片的任何索引都不允许rebalancing

###### cluster.routing.allocation.allow_rebalance

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）用于指定什么时候允许分片rebalancing

- `always`：总是允许rebalancing
- `indices_primaries_active`：只有当集群中的所有主分片都被分配后
- `indices_all_active`：（default）只有当集群中所有的分片（primaries and replicas）都被分配后

###### cluster.routing.allocation.cluster_concurrent_rebalance

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）用于控制集群范围（cluster wide）内分片平衡并发数，默认值是2。注意的是这个设置仅仅控制因为集群不平衡导致的分片重定位（shard relocating）的并发数。这个设置不会限制因 [allocation filtering](#Cluster-level-shard-allocation-filtering) 和 [forced awareness](#Forced-awareness)导致的分片重定位。

##### Shard balancing heuristics settings

&emsp;&emsp;基于每个节点上分片的分配情况计算出一个weight来实现rebalancing，并且通过在节点之间移动分片来降低heavier节点的weight并且提高lighter节点的weight。当不存在可能使任何节点的权值与其他节点的权值更接近超过可配置阈值的分片移动时，集群是平衡的。下面的设置允许你控制计算的细节

###### cluster.routing.allocation.balance.shard

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）为节点上分配的分片总数定义一个weight因子。默认值是0.45f。提高这个值会使集群中所有节点的分片数量趋于均衡。

###### cluster.routing.allocation.balance.index

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）为每一个索引的分片数量定义一个weight因子。默认值是0.55f。提高这个值会使集群中所有节点上的每一个索引的的分片数量趋于均衡。

###### cluster.routing.allocation.balance.threshold

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）该值时候一个非负的浮点之后，Minimal optimization value of operations that should be performed。提高这个值将导致集群在优化分片平衡方面不那么积极（leess aggressive）。

>NOTE：无论balancing算法的结果如何，rebalancing可能因为forced awareness或者allocation filtering而无法执行

##### Disk-based shard allocation settings

&emsp;&emsp;基于磁盘的分片分配器能保证所有节点都有足够的磁盘空间而不用执行更多的分片移动。它基于一对阈值来分配分片：low wateremark 和 high watermark。主要的目的是保证在每一个节点不会超过high watermark，或者是暂时的超过（overage is temporary）。如果某个节点上超过了high  watermark，那么Elasticsearch会通过把分片移动到集群中的其他节点来的方式来解决。

>NOTE：节点上有时（from time to time）出现临时的超过high watermark是正常的

&emsp;&emsp;分配器总是尝试通过禁止往已经超过low watermark的节点分配更多分配的方式来让节点远离（clear of）high watermark。重要的是，如果所有的节点都已经超过了low watermark那么不会有新的分片会被分配并且Elasticsearch不会通过在节点间移动分片的方式来让磁盘使用率低于high watermark。你必须保证在你的集群中有足够的磁盘空间并且总存在一些低于low watermark的节点。

&emsp;&emsp;通过基于磁盘的分片分配器进行的分片移动必须满足（satisfy）其他的分片分配规则，例如 [allocation filtering](#Cluster-level-shard-allocation-filtering) 和 [forced awareness](#Forced-awareness)。如果这些规则过于严格，它们还可以防止碎片移动，以保持节点的磁盘使用在控制之下。如果你正在使用 [data tiers](#Data-tiers)，那么Elasticsearch会自动的应用分配过滤规则将每一个分配放到合适的数据层，这些规则意味着平衡器在每一层中独立工作。

&emsp;&emsp;如果某个节点填满磁盘的速度比Elasticsearch将分片移动到其他地方的速度还要快就会有磁盘被填满的风险。为了防止出现这个问题，万不得已的情况下（ as a last resort），一旦磁盘使用达到`flood-stage` watermark，Elasticsearch会阻塞受到影响的节点上的分片索引的写入。但仍然继续将分片移动到集群中的其他节点。当受到影响的节点上的磁盘使用降到high watermark，Elasticsearch会自动的移除write block。

> TIP：集群中的节点各自使用容量不相同的磁盘空间是正常的。集群的[balance](#Shard rebalancing settings)取决与每一个节点上分片的数量以及分片中的索引。基于下面的两个理由，集群的balance既不会考虑分片的大小，也不会考虑每一个节点上磁盘可用的空间：
> - 磁盘的使用随着时间发生变化。平衡不同节点的磁盘使用将会有很多更多的分片移动，perhaps even wastefully undoing earlier movements。移动一个分片会消耗例如I/O资源以及网络带宽，并且可能从文件系统缓存中换出（evict）数据。这些资源最好用于你的查询和索引。
> - 集群中每一个节点有相同的磁盘使用在性能上通常没有有不同磁盘使用的性能高，只要所有的磁盘不是太满
> 

&emsp;&emsp;你可以使用下面的设置控制基于磁盘的分配：

###### cluster.routing.allocation.disk.threshold_enabled

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）默认值为`true`。设置为`false`则关闭基于磁盘的分配。

###### cluster.routing.allocation.disk.watermark.low

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）控制磁盘使用量的水位下限（low watermark）。默认值为`85%`。意味着Elasticsearch不会将分片分配到磁盘使用超过85%的节点上。该值也可以是一个字节单位的值（例如`500mb`），使得当磁盘空间小于指定的值时就不让Elasticsearch分配分片到这个节点。这个设置不会影响新创建的索引的主分片，但是会组织副本分配的创建。

###### cluster.routing.allocation.disk.watermark.high

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）控制磁盘使用量的水位上限。默认值为`90%`，意味着Elasticsearch将对磁盘使用量超过90%的节点上的分片进行relocate。该值也可以是一个字节单位的值（跟low watermark类似）。使得当磁盘空间小于指定的值时就把该节点上的分片进行relocate。这个设置会影响所有分片的分配，无论之前是否已经分配。

###### cluster.routing.allocation.disk.watermark.enable_for_single_data_node

&emsp;&emsp;（[Static](# Static（settings） )) 在更早的发布中，当做出分配决策时，对于单个数据节点的集群是不会考虑disk watermark的。在7.14之后被值为deprecated并且在8.0移除。现在这个设置唯一合法的值为`true`。这个设置在未来的发布中移除。

###### cluster.routing.allocation.disk.watermark.flood_stage

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）控制flood stage watermark。只要至少有一个节点超过该值，分配到这个节点的一个或者分片对应的索引会被Elasticsearch强制置为read-only index block（`index.blocks.read_only_allow_delete`）。这个设置是防止节点发生磁盘空间不足最后的手段。当磁盘使用量降到high watermark后会自动释放index block。

> NOTE：你不能在设置中混合使用比例值（percentage）和字节值（byte value）。要么所有的值都是比例值，要么都是字节值。这种强制性的要求使得Elasticsearch可以进行一致性的处理。另外要确保low disk threshold要低于high disk threshold，并且high disk threshold要低于flood stage threshold。

&emsp;&emsp;下面的例子中在索引`my-index-000001`上重新设置read-only index block：

```text
PUT /my-index-000001/_settings
{
  "index.blocks.read_only_allow_delete": null
}
```

###### cluster.routing.allocation.disk.watermark.flood_stage.frozen

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）用于专用的frozen node，控制flood stage watermark，默认值为95%

###### cluster.routing.allocation.disk.watermark.flood_stage.frozen.max_headroom

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）用于专用的frozen node，控制flood stage watermark的head room。当`cluster.routing.allocation.disk.watermark.flood_stage.frozen`没有显示设置时默认值为20GB。该值限制（cap）了专用的frozen node上磁盘的空闲量。

###### cluster.info.update.interval

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）Elasticsearch定时检查集群中每一个节点上磁盘使用情况的时间间隔。默认值为`30s`。

> NOTE：比例值说的是（refer to）已使用的磁盘空间，而字节值说的是剩余磁盘空间。这可能会让人疑惑，因为它弄反了高和低的含义。比如，设置low watermark为10GB，high watermark为5GB是合理的，反过来设置的话就不行

&emsp;&emsp;下面的例子讲low watermark的值设置为至少100gb，high watermark的值设置为至少50gb，flood stage watermark的值为10gb，并且每一分钟进行检查：

```text
PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.disk.watermark.low": "100gb",
    "cluster.routing.allocation.disk.watermark.high": "50gb",
    "cluster.routing.allocation.disk.watermark.flood_stage": "10gb",
    "cluster.info.update.interval": "1m"
  }
}
```

##### Shard allocation awareness

&emsp;&emsp;你可以自定义节点属性作为感知属性（awareness attributes）让Elasticsearch在分配分片时考虑你物理硬件的配置。如果Elasticsearch知道哪些节点在同一台物理机上、同一个机架（rack）、或者同一个区域，使得在发布主分片跟副本分片时能最低限度的降低丢失所有副本分片的风险。

&emsp;&emsp;当使用（[Dynamic](#Dynamic（settings）)）的`cluster.routing.allocation.awareness.attributes`的设置开启awareness attribute后，分片只会被分配到设置了awareness attributes的节点上。当你使用多个awareness attribute时，Elasticsearch在分配分片时会单独地（separately）考虑每一个attribute。

> NOTE: attribute values的数量决定了每一个位置上副本分配的分配数量。如果在每一个位置上的节点数量是unbalanced并且有许多的副本，副本分片可能不会被分配

###### Enabling shard allocation awareness

&emsp;&emsp;开启分片的分配awareness，需要：

1. 使用一个自定义的节点属性来指定每个节点的位置。如果你想要Elasticsearch在不同的机架间发布分片，你可能需要在配置文件`elasticsearch.yml`上设置一个名为`rack_id`的属性。

```text
node.attr.rack_id: rack_one
```

&emsp;&emsp;你也可以在启动的时候设置自定义的属性。

```text
./bin/elasticsearch -Enode.attr.rack_id=rack_one
```

2. 在每一个master-eligible node的配置文件`elasticsearch.yml`上设置`cluster.routing.allocation.awareness.attributes`，告诉Elasticsearch在分配分片的时候要考虑一个或者多个awareness attribute。

```text
cluster.routing.allocation.awareness.attributes: rack_id
```

&emsp;&emsp;用逗号分隔多个属性。

&emsp;&emsp;你也可以使用[cluster-update-settings](#Cluster-update-settings API) API来设置或者更新集群的awareness attributes。

&emsp;&emsp;基于上述的配置例子，如果你启动了两个节点并且都设置`node.attr.rack_id`为`rack_one`并且为每个index设置5个主分片，每个主分片设置一个副本分片，那么这两个节点都包含所有的主分片以及副本分片。

&emsp;&emsp;如果你增加了两个节点并且都设置`node.attr.rack_id`为`rack_two`，Elasticsearch会把分片移动到新的节点，ensuring（if possible）相同的副本分片不会在相同的机架上。

&emsp;&emsp;如果`rack_two`失败并关闭了它的两个节点，默认情况下，Elasticsearch会将丢失的副本分片分配给`rack_one`中的节点。为了防止特定分片的多个副本被分配到相同的位置，可以启用forced awareness。

###### Forced awareness

&emsp;&emsp;默认情况下，如果一个位置失败了（one location fails），Elasticsearch将缺失的分片分配到剩余的位置。可能所有的位置都有足够的资源来存放主分片跟副本分片，也有可能某个位置无法存储所有的分片。

&emsp;&emsp;为了防止某个位置因为失败事件（event of failure）而造成负载过高（overload），你可以设置`cluster.routing.allocation.awareness.force`使得副本分片不会被分配直到其他位置可以被分配。

&emsp;&emsp;比如说，如果你有一个awareness attribute 名为`zone`并且在两个节点分别配置`zone1`跟`zone2`，那么在只有一个zone可用的情况，你可以使用forced awareness来防止Elasticsearch分配副本分片。

```text
cluster.routing.allocation.awareness.attributes: zone
cluster.routing.allocation.awareness.force.zone.values: zone1,zone2
```

&emsp;&emsp;为awareness attribute指定所有可能的值。

&emsp;&emsp;基于上述的配置例子，如果启动了两个节点并且设置`node.attr.zone`的值为`zone1`并且为每个index设置5个分片，每个分片设置一个副本分片，那么Elasticsearch会创建索引并且分配5个主分片但是不会有副本分片。只有某个节点设置`node.attr.zone`的值为`zone2`才会分配副本分片。

##### Cluster-level shard allocation filtering

&emsp;&emsp;你可以使用cluster-level shard allocation filters来控制Elasticsearch从任何索引分配分片的位置。结合[per-index allocation filtering](#Index-level shard allocation filtering)和[allocation awareness](#Shard allocation awareness)来应用集群级别（cluster wide）的filter。

&emsp;&emsp;Shard allocation filters可以基于自定义的节点属性或者内建的属性：`_name`, `_host_ip`, `_publish_ip`, `_ip`, `_host`,`_id` 和` _tier`。

&emsp;&emsp;`cluster.routing.allocation`设置是动态的[Dynamic](#Dynamic（settings）)，允许将live indices从一组节点上移动到其他组。只有在不破坏路由约定（routing constraint）下才有可能重新分配分片，比如说不会将主分片和它的副本分片分配到同一个节点上。

&emsp;&emsp;最常用的cluster-level shard allocation filtering用例是当你想要结束（decommission）一个节点。要在关闭节点之前将分片移出节点，您可以创建一个过滤器，通过其 IP 地址排除该节点：

```text
PUT _cluster/settings
{
  "persistent" : {
    "cluster.routing.allocation.exclude._ip" : "10.0.0.1"
  }
}
```

##### Cluster routing settings

###### cluster.routing.allocation.include.{attribute}

&emsp;&emsp;([Dynamic](#Dynamic（settings）))将分配分片到一个节点，这个节点的`{attribute}`至少是用逗号隔开的多个属性中的一个。

###### cluster.routing.allocation.require.{attribute}

&emsp;&emsp;([Dynamic](#Dynamic（settings）))只将分片分配到一个节点，这个节点的`{attribute}`包含所有用逗号隔开的多个属性。

###### cluster.routing.allocation.exclude.{attribute}

&emsp;&emsp;([Dynamic](#Dynamic（settings）))不将分片分配到一个节点，这个节点的`{attribute}`包含用逗号隔开的多个属性中的任意一个。

&emsp;&emsp;cluster allocation settings支持下面的内建属性：

|    \_name    | 根据node name匹配节点 |
| :---------: | :--: |
|  \_host\_ip   | 根据host IP 地址（hostname关联的IP）匹配节点 |
| \_publish\_ip | 根据发布的IP地址匹配节点 |
|     \_ip     | 根据\_host\_ip或者\_publish\_ip匹配节点 |
|    \_host    | 根据hostname匹配节点 |
|     \_id     | 根据node id匹配节点 |
|    \_tier    | 根据节点的[data tier](#Data tiers)角色匹配节点 |

> NOTE:  `_tier`的过滤基于[ node roles](#Node)，只有部分角色是[ data tier](#Data tiers) 角色，并且[generic data role](#Data node)将会匹配任何的tier filtering。a subset of roles that are data tier roles, but the generic data role will match any tier filtering.

&emsp;&emsp;在指定attribute values时可以使用通配符，例如：

```text
PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.exclude._ip": "192.168.2.*"
  }
}
```

##### Miscellaneous cluster settings

###### Metadata

&emsp;&emsp;通过下面的设置可以将整个集群设置为read-only：

###### cluster.blocks.read_only

&emsp;&emsp;([Dynamic](#Dynamic（settings）)) 让整个集群read only（索引不支持写操作），metadata不允许更改（创建或者删除索引）

###### cluster.blocks.read_only_allow_delete

&emsp;&emsp;([Dynamic](#Dynamic（settings）)) 与`cluster.blocks.read_only`一样，但是允许删除索引来释放资源。

> WARNING: 不要依赖这些设置来防止集群发生变更。任何有[cluster-update-settings API](#Cluster update settings API)  访问权限的用户都可以让集群再次read-write

##### Cluster shard limit

&emsp;&emsp;基于集群中的节点数量，集群中分片的数量有一个soft limit。这是为了防止可能无意中破坏集群稳定性的操作。

> IMPORTANT：这个限制作为一个安全网（safety net），不是一个推荐的大小。你集群中可以安全的支持准确的分片数量取决于你的硬件配置跟工作负载，但是在大部分case下，这个limit应该还不错，因为默认的limit是非常大的。

&emsp;&emsp;如果一个操作，比如创建新的索引，恢复snapshot index，或者打开一个关闭的索引会导致集群中的分片数量超过限制。那这个操作会失败并且给出shard limit的错误。

&emsp;&emsp;如果由于node membership的变化或者设置的变更使得集群已经超过了限制，所有创建或者打开索引的操作都会失败直到下文中将介绍的提高limit或者[关闭](#Open index API)[删除](#Delete index API)一些索引使得分片的数量降到了limit以下。

&emsp;&emsp;对于normal indices（non-frozen），每一个non-frozen data node上cluster shard limit的默认值是1000，对于frozen indices，每一个frozen data node上的默认值是3000。所有打开的索引的主分片跟副本分片的数量都会纳入到统计中（count toward the limit），包括未分配的分片。例如5个主分片和2个副本的数量为15个分片。关闭的索引不会参与分片的统计。

&emsp;&emsp;你可以使用下面的方式动态的调整cluster shard limit：

###### cluster.max_shards_per_node

&emsp;&emsp;([Dynamic](#Dynamic（settings）)) 限制集群中主分片跟副本分片的总数量。Elasticsearch使用下面的公式统计limit：

&emsp;&emsp;`cluster.max_shards_per_node * number of non-frozen data nodes`

&emsp;&emsp;关闭的索引不会纳入到统计中。默认是`1000`。没有data node的集群是没有限制的。

&emsp;&emsp;超过上限后，Elasticsearch会reject创建更多分片的请求。比如集群的`cluster.max_shards_per_node`的值是`100`，并且三个data node就有`300`的分片数量限制。如果集群中已经包含了296个分片，Elasticsearch会reject增加5个或者更多分片数量的请求。

&emsp;&emsp;注意的是frozen shard有它们自己独立的限制。

###### cluster.max_shards_per_node.frozen

&emsp;&emsp;([Dynamic](#Dynamic（settings）)) 限制集群中主分片跟replica frozen shard的总数量。Elasticsearch使用下面的公式统计limit：

&emsp;&emsp;`cluster.max_shards_per_node * number of frozen data nodes`

&emsp;&emsp;关闭的索引不会纳入到统计中。默认是`3000`。没有frozen data node的集群是没有限制的。

&emsp;&emsp;超过上限后，Elasticsearch会reject创建更多frozen shard的请求。比如集群的`cluster.max_shards_per_node.frozen`的值是`100`，并且三个frozen data node就有`300`的分片数量限制。如果集群中已经包含了296个分片，Elasticsearch会reject增加5个或者更多frozen shard数量的请求。

> NOTE: These setting do not limit shards for individual nodes. 可以使用设置[cluster.routing.allocation.total_shards_per_node](#cluster.routing.allocation.total_shards_per_node)来限制每一个节点上的分片数量。

##### User-defined cluster metadata

&emsp;&emsp;可以通过Cluster Settings API来存储以及查询用户自定义的集群元信息（User-defined cluster metadata）。可以用来存储任意的（arbitrary），很少更改（infrequently-changing）的关于集群的信息，而不需要创建索引来存储这些信息。使用以`cluster.metadata`为前缀的任意的key来存储信息。例如，在`cluster.metadata.administrator`这个key下存储管理员的email地址，如下所示：

```text
PUT /_cluster/settings
{
  "persistent": {
    "cluster.metadata.administrator": "sysadmin@example.com"
  }
}
```

> IMPORTANT： User-defined cluster metadata不要存储敏感的或者机密信息。任何人都可以通过[Cluster Get Settings API](#Cluster get settings API)访问存储在User-defined cluster metadata的信息，并且会被记录在Elasticsearch日志中。

##### Index tombstones

&emsp;&emsp;集群状态（cluster state）维护者index tombstone来显示的告知（denote）被删除的索引。集群状态维护tombstone的数量，通过下面的设置来进行控制：

###### cluster.indices.tombstones.size

（[Static](#Static（settings） )）当发生删除时，Index tombstones会阻止不属于集群的节点加入集群并重新导入索引，就好像delete was never issued。为了防止增长的太大，我们只保留最新的`cluster.indices.tombstones.size`数量的删除，默认值是500。You can increase it if you expect nodes to be absent from the cluster and miss more than 500 deletes. We think that is rare, thus the default. Tombstones don’t take up much space, but we also think that a number like 50,000 is probably too big.

&emsp;&emsp;如果 Elasticsearch 遇到当前集群状态中不存在的索引数据，则这些索引被认为是dangling。 例如，如果你在 Elasticsearch 节点离线时删除超过 `cluster.indices.tombstones.size` 的索引，就会发生这种情况。

&emsp;&emsp;你可以使用[Dangling indices API](#Dangling indices API)来进行管理。

##### Logger

&emsp;&emsp;这个设置用来控制日志，使用`logger.`为前缀的key来[动态的](#Dynamic（settings）)更新。发布下面的请求（issue this request）来提高`indices.recovery`模块的日志等级为`DEBUG`：

```text
PUT /_cluster/settings
{
  "persistent": {
    "logger.org.elasticsearch.indices.recovery": "DEBUG"
  }
}
```

##### Persistent tasks allocation

&emsp;&emsp;Plugin可以创建一系列称为persistent task的任务。这些任务通常是long-lived并且存储在cluster state中，并且允许在一次full cluster restart后恢复（revive）这些任务。

&emsp;&emsp;每一次创建一个persistent task时，master node会负责将任务分配到集群中的一个节点。然后被分配的节点会pick up这个任务然后在本地执行。通过下面的设置来控制被分配的persistent task的运行：

###### cluster.persistent_tasks.allocation.enable

([Dynamic](#Dynamic（settings）))开启或者关闭persistent task的分配：

- all - (default) 允许persistent task被分配到节点
- none - 不允许分配任何类型的persistent task

&emsp;&emsp;这个设置不会影响已经执行的persistent task。只有最新创建的，或者必须要重新被分配的persistent task才会收到这个设置的影响。

###### cluster.persistent_tasks.allocation.recheck_interval

([Dynamic](#Dynamic（settings）))master node会自动的检查在cluster state发生重大变化（change significantly）后persistent tast是否需要被分配。然而，也会有其他的一些因素，比如说memory usage，这个会影响persistent task能否被分配但这不会引起cluster state的变更。这个设置控制了检查的间隔。默认是30秒。最小允许的时间为10秒。

#### Cross-cluster replication settings
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ccr-settings.html#ccr-recovery-settings)

&emsp;&emsp;可以使用[cluster update settings API](#Cluster update settings API)动态更新集群中的CCR（cross-cluster replication）设置。

##### Remote recovery settings

&emsp;&emsp;下面的设置可以用于在[remote recoveries](#Initializing followers using remote recovery)中限制数据传输速率：

###### ccr.indices.recovery.max_bytes_per_sec ([Dynamic](#Dynamic（settings）))

&emsp;&emsp;用来限制每一个节点上inbound和outbound remote recovery traffic。由于这个设置是应用到每一个节点上，但是可能存在许多的节点并发的执行remote recoveries，remote recovery占用的流量可能比这个限制高。如果这个值设置的太高，那么会存在这么一种风险，即正在进行中的recoveries会过度消费带宽 ，使得集群变得不稳定。这个设置应该同时被leader和follower clusters使用。例如，如果在leader上设置了`20mb`，leader将只能发送`20mb/s`到follower，即使follower可以达到`60mb/s`。默认值为`40mb`。

##### Advanced remote recovery settings

&emsp;&emsp;下面的专家级设置（expert settings）用于管理remote recoveries时的资源消费。

###### ccr.indices.recovery.max_concurrent_file_chunks ([Dynamic](#Dynamic（settings）))

&emsp;&emsp;控制每个recovery中并发请求文件块（file chunk）的数量。由于多个remote recoveries已经在并发运行，增加这个专家级设置可能只有助于没有达到总的inbound and outbound remote recovery traffic的单个分片的remote recovery，即上文中的`ccr.indices.recovery.max_bytes_per_sec`。`ccr.indices.recovery.max_concurrent_file_chunks`的默认值为`5`。允许最大值为`10`。

###### ccr.indices.recovery.chunk_size ([Dynamic](#Dynamic（settings）))

&emsp;&emsp;控制在文件传输时，follower请求的文件块的大小。默认值为`1mb`。

###### ccr.indices.recovery.recovery_activity_timeout ([Dynamic](#Dynamic（settings）))

&emsp;&emsp;控制recovery的活跃（activity）超时时间。这个超时时间主要是应用在leader集群上。在处理recovery期间，leader cluster必须打开内存资源提供数据给follower。如果leader在周期时间内没有接收到follower的接收请求，leader会关闭资源。默认值为`60s`。

###### ccr.indices.recovery.internal_action_timeout ([Dynamic](#Dynamic（settings）))

&emsp;&emsp;控制在remote recovery处理期间每一个网络请求的超时时间。某个单独的动作（individual action）超时会导致recovery失败。默认值为`60s`。

#### Discovery and cluster formation settings

&emsp;&emsp;[Discovery and cluster formation](#Discovery and cluster formation)受到下面设置的影响：

##### discovery.seed_hosts

&emsp;&emsp;（[Static](#Static（settings） ))）提供集群中master-eligible node的地址列表。可以是单个包含地址，用逗号隔开的字符串。每一个地址有`host:port`或者`host`的格式。`host`可以是hostname（可以用DNS解析）、IPv4地址、IPv6地址。IPv6地址必须用{}包裹。如果hostname用DNS解析出多个地址，Elasticsearch会使用所有的地址。DNS基于[JVM DNS caching](#DNS cache settings)进行查找（lookup）。如果没有指定`port`，会有序检查下面的设置来找到`port`：

1. `transport.profiles.default.port`
2. `transport.port`

&emsp;&emsp;如果没有设置上面的配置，则`port`的默认值为`9300`。`discovery.seed_hosts`的默认值为`["127.0.0.1", "[::1]"]`。见[discovery.seed_hosts](#discovery.seed_hosts)。

###### discovery.seed_providers

&emsp;&emsp;（[Static](#Static（settings） ))）指定[seed hosts provider](#Seed hosts providers)的类型，用于获取启动发现程序（discovery process）时需要用到的seed nodes地址。默认情况为[settings-based seed hosts provider](#Settings-based seed hosts provider)，它包含了`discovery.seed_hosts`中的seed node 地址。

###### discovery.type

&emsp;&emsp;（[Static](#Static（settings） ))）指定Elasticsearch是否形成一个多节点的（multiple-node）的集群，默认值为`multi-node`，意味着Elasticsearch在形成一个集群时会发现其他节点并且允许其他节点随后加入到这个集群。如果设置为`single-node`，Elasticsearch会形成一个单节点（single-node）集群并且suppresses the timeout set by `cluster.publish.timeout`。当你想要使用这个设置的时候，见[Single-node discovery](#Single-node discovery)了解更多的信息

###### cluster.initial_master_nodes

&emsp;&emsp;（[Static](#Static（settings） ))）在全新的集群中初始化master-eligible node集合。默认情况下是空值，意味着这个节点想要加入到一个引导后（bootstrapped）的集群。集群一旦形成后需要移除这个设置。当重启一个节点或者添加一个新的节点到现有的集群中时不要使用这个设置。见[cluster.initial_master_nodes](#cluster.initial_master_nodes)。

##### Expert settings

&emsp;&emsp;[Discovery and cluster formation](#Discovery and cluster formation)受到下面专家级设置（expert settings）的影响，当然我们不推荐修改这些设置的默认值。

> WARNING：如果你调整了集群中的这些设置，可能会无法正确的形成一个集群或者可能集群变得不稳定，或者无法处理（intolerant）对于一些故障。

###### discovery.cluster_formation_warning_timeout

&emsp;&emsp;（[Static](#Static（settings） ))）尝试形成一个集群时，多长时间后仍然没有形成集群，开始记录未形成集群的警告日志的设置。默认值为`10s`。如果在`discovery.cluster_formation_warning_timeout`的时间内没有形成一个集群，那节点会记录一个包含`master not discovered`的warnning message，它描述了当前discovery process当前的状态。

###### discovery.find_peers_interval

&emsp;&emsp;（[Static](#Static（settings） ))）一个节点尝试新一轮discovery的时间间隔。默认值为`1s`。

###### discovery.probe.connect_timeout

&emsp;&emsp;（[Static](#Static（settings） ))）尝试连接每一个地址的连接等待时间。默认值为`30s`。

###### discovery.probe.handshake_timeout

&emsp;&emsp;（[Static](#Static（settings） ))）尝试通过handshake识别remote node的等待时间。默认值为`30s`

###### discovery.request_peers_timeout

&emsp;&emsp;（[Static](#Static（settings） ))）在认为请求失败时，询问其peer后的等待时间。默认值为`3s`。

###### discovery.find_peers_warning_timeout

&emsp;&emsp;（[Static](#Static（settings） ))）节点尝试discover它的peers并开始记录为什么尝试连接失败的verbose message的时间。默认值为`3m`。

###### discovery.seed_resolver.max_concurrent_resolvers

&emsp;&emsp;（[Static](#Static（settings） ))）DNS并发解析seed node的地址时允许的并发量。默认值为`10`。

###### discovery.seed_resolver.timeout

&emsp;&emsp;（[Static](#Static（settings） ))）DNS解析seed note的地址的超时时间。默认值为`5s`。

###### cluster.auto_shrink_voting_configuration

&emsp;&emsp;([Dynamic](#Dynamic（settings）))控制[voting configuration](#Voting configurations)是否自动的去除（shed）不能担任voting的节点（departed node），只要它至少还包含3个节点。默认值为`true`。如果设置为`false`。voting configuration不会自动的进行收缩，你必须通过[voting configuration exclusions API](#Voting configuration exclusions API)手动的移除不能担任voting的节点（departed node）。

###### cluster.election.back_off_time

&emsp;&emsp;（[Static](#Static（settings） ))）
（未完成）

###### cluster.election.duration

###### cluster.election.initial_timeout

#### Field data cache settings
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-fielddata.html)

&emsp;&emsp;field data cache中包含了[field data](#fielddata mapping parameter)和[global ordinals](#eager_global_ordinals)，它们都是用来让某些字段能够用于聚合。由于是on-heap的数据结构，所以很有必要监控缓存的使用情况。

##### Cache size

&emsp;&emsp;构建cache中实体（entities）的开销是很昂贵的，所以默认情况下会保留载入到内存的entities。默认的cache大小是没有限制的，cache大小会不断增大直到达到[field data circuit breaker](#Field data circuit breaker)设定的上限。这种行为可以配置。

&emsp;&emsp;如果设置了cache大小的上限，cache将会清除cache中最近更新的实体（If the cache size limit is set, the cache will begin clearing the least-recently-updated entries in the cache）。这个设置会自动的避免circuit breaker limit，代价就是需要重新构建cache。

&emsp;&emsp;如果达到了circuit breaker limit，后续会增加cache大小的请求将被阻止。在这种情况下你必须要手动[clear the cache](#Clear cache API)。

###### indices.fielddata.cache.size

&emsp;&emsp;（[Static](#Static（settings） ))） field data cache的最大值。比如节点堆空间大小的`38%`，或者是一个absolute value，`12GB`。默认是没有限制。如果选择设置这个值，这个值应该小于[Field data circuit breaker limit](#Field data circuit breaker)。

##### Monitoring field data

&emsp;&emsp;你可以使用[nodes stats API](#Nodes stats API)和[cat fielddata API](#cat fielddata API)来监控filed data的内存使用情况以及 field data circuit breaker。


#### Index lifecycle management settings in Elasticsearch
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-settings.html#index-lifecycle-step-wait-time-threshold)

##### Index level settings

###### index.lifecycle.origination_date

###### index.lifecycle.parse_origination_date

###### index.lifecycle.step.wait_time_threshold

#### Index management settings
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-management-settings.html#stack-templates-enabled)

##### stack.templates.enabled

#### Index recovery settings
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/recovery.html)

##### Recovery settings

#### Indexing buffer settings
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indexing-buffer.html)

&emsp;&emsp;indexing buffer用来缓存新添加的文档，当缓冲区满后，缓存中的文档会被写入到一个段中并且存放到磁盘上。Indexing buffer的大小被节点上的所有分片共享。

&emsp;&emsp;下面的设置都是静态的必须在集群中的每一个节点上配置：

##### indices.memory.index_buffer_size

&emsp;&emsp;（[Static](#Static（settings） )）可以是百分比或者是字节单位的数值。默认是`10%`意味着堆内存的10%会被分配给节点用于Indexing buffer，并且被节点上所有的分片共享。

##### indices.memory.min_index_buffer_size

&emsp;&emsp;（[Static](#Static（settings） )）如果`index_buffer_size`指定为百分比，那这个设置用来指定为一个绝对最小值（absolute minimum）。默认是是`48m`。

##### indices.memory.max_index_buffer_size

&emsp;&emsp;（[Static](#Static（settings） )）如果`index_buffer_size`指定为百分比，那这个设置用来指定为一个绝对最大值（absolute maximum）。默认是无限制的。

#### License settings
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/license-settings.html)

#### Local gateway settings
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-gateway.html)

##### Dangling indices

&emsp;&emsp;当一个节点加入到集群，如果它发现本地数据目录中存在一些分片，这些分片在集群中的其他节点上不存在，那么就认为这些分片属于"dangling" index。你可以使用[Dangling indices API](#Dangling indices API)列出，导入或者删除dangling index。

#### Logging
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/logging.html)

#### Machine learning settings in Elasticsearch
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ml-settings.html)

#### Node
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-node.html)

&emsp;&emsp;任何时候在你启动了一个Elasticsearch的实例时，即你正在启动一个节点（Node）。相互连接的节点集合成为一个集群（[cluster](#Cluster-level shard allocation and routing settings)）。如果你运行了 一个单个节点的Elasticsearch，那么你拥有一个只有一个节点的集群。

&emsp;&emsp;默认情况下，集群中的每个节点都可以处理[ HTTP and transport](#Networking )的传输。transport layer只用来处理节点之间的交互，HTTP layer用于REST clients 。

&emsp;&emsp;所有的节点都知道集群中的其他节点并且可以转发客户端的请求到合适的节点。

##### Node roles

&emsp;&emsp;你可以在`elasticsearch.yml`中设置`node.roles`来定义一个节点的角色。如果你设置了`node.roles`，这个节点只会被分配为你指定的角色。如果你不指定`node.roles`，这个节点会被分配为下面的角色：

- master
- data
- data_content
- data_hot
- data_warm
- data_cold
- data_frozen
- ingest
- ml
- remote_cluster_client
- transform

> IMPORTANT：如果你设置了`node.roles`，你要保证你的集群需要你指定的这些角色。每一个集群需要有下面的角色：
>
> - master
> - (data_content和data_hot) 或者 data
>
> 一些Elastic Stack 功能同样要求指定下面的节点角色：
>
> - 跨集群搜索（Cross-cluster search）和跨集群副本（Cross-cluster replication）需要`remote_cluster_client`角色
> - Stack Monitoring和ingest pipeline需要`ingest`角色
> - Fleet，Elastic Security应用，和transforms需要`transform`角色。`remote_cluster_client`角色同样需要用于这些功能中的cross-cluster search
> - Machine learning功能，比如异常检测（anomaly detection）需要`ml`角色

&emsp;&emsp;随着集群的增长，特别是集群中有大规模的machine learning任务、continuous transforms，我们建议从专用的data node、machine learning node、transform node中分离出master-eligible节点。

|  [Master-eligible node](#Master-eligible node)  | 拥有`master`角色的节点，使得节点有资格被[选举](#Discovery and cluster formation)为master node来控制整个集群 |
| :-------------------------------------------------: | :----------------------------------------------------------: |
|             [Data node](#Data node)             | 拥有`data`角色的节点，data node保留数据并且执行相关操作例如CRUD、查询、聚合。一个拥有`data`角色的节点可以添加任何其他特定的数据节点角色，例如hot 、warm等 |
|           [Ingest node](#Ingest node)           | 拥有`ingest`角色的节点，ingest node可以对文档进行[ingest pipeline](#Ingest pipelines)使得可以在索引前transform或者丰富文档。对于繁重的ingest，使用专用的ingest node并且让拥有`master`或者`data`角色的节点不要有`ingest`角色 |
|  [Remote-eligible node](#Remote-eligible node)  | 拥有`remote_cluster_client`角色的节点，使得这个节点有资格成为一个remote client |
| [Machine learning node](#Machine learning node) | 拥有`ml`角色的节点，如果你想要使用machine learning功能，你的集群中至少要有一个machine learning node。见[Machine learning settings](#Machine learning settings in Elasticsearch)和[Machine learning in the Elastic Stack](https://www.elastic.co/guide/en/machine-learning/8.2/index.html)了解更多信息 |
|        [Transform node](#Transform node)        | 拥有`transform`角色的节点。如果想要使用transform，你的集群中至少要有一个transform node。见[Transforms settings](#Transforms settings in Elasticsearch)和[Transforming data](#Transforming data)了解更多信息 |

> NOTE：Coordinating node
> 一些像查询请求或者块索引（bulk-indeing）的请求可能涉及不同data node上的数据。一次查询请求，例如，搜索请求分两个阶段执行，由接收客户端请求的节点进行协调 - coordinating node。
> 
> 在scatter阶段，coordinating node将请求转发给持有数据的数据节点。 每个数据节点在本地执行请求并将其结果返回给coordinating node。 在收集（gather）阶段，协调节点将每个数据节点的结果减少为单个全局结果集。
> 
> 每个节点都隐含的是一个coordinating node。 这意味着通过设置 `node.roles` 为空的角色列表的节点将作为coordinating node，并且不能被关闭这种设定。 因此，这样的节点需要有足够的内存和 CPU 才能处理收集阶段。

##### Master-eligible node

&emsp;&emsp;master node负责轻量级（lightweight）的集群范围（cluster-wide）的一些工作，比如创建、删除一个索引，跟踪哪些节点是集群的一部分以及决定给哪里节点分配分片。稳定的master node对于集群的健康是十分重要的。

&emsp;&emsp;一个不是[voting-only](#Voting-only master-eligible node)的master-eligible node会在[master election process](#Discovery and cluster formation)中被选为master node。

> IMPORTANT: master node必须有一个`path.data`目录，其内容在重启后仍然存在，跟data node一样，这个目录中存放了集群的元数据（cluster metadata）。集群元数据描述了如何读取存储在data node上的数据，所以如果丢失了集群元数据就无法读取data node上的数据。


###### Dedicated master-eligible node

&emsp;&emsp;满足被选举为master的节点为了能完成它的职责所需要的资源对于集群的健康是十分重要的。如果被选举为master的节点因为其他的工作导致超负荷了，那么集群将无法较好的运行。避免master node工作超负荷最可靠的方法就是将所有master-eligible node只配置为`master`角色，让它们作为专用的master-eligible node（dedicated master-eligible node），让它们集中做好管理集群的工作。Master-eligible node可以跟[coordinating nodes](#Coordinating only node)一样将客户端的请求路由到集群中的其他节点，但是你不应该让专用的master-eligible node做这些事情。

&emsp;&emsp;在一些小规模或者轻负载（lightly-loaded）的集群中，如果master-eligible node有其他的角色和职责也可以正常的运行，但是，一旦集群中包含多个节点，通常使用专用的master-eligible node是有意义的。

&emsp;&emsp;通过设置下面的配置来创建一个专用的master-eligible节点：

```text
node.roles: [ master ]
```

###### Voting-only master-eligible node

&emsp;&emsp;voting-only master-eligible node是会参与[master elections](#Discovery and cluster formation)的节点，但是不会成为集群的master节点。特别在于一个voting-only node可以成为选举过程中的tiebreaker。

&emsp;&emsp;使用"master-eligible"这个词来描述一个voting-only node看起来有点让人困惑，因为这种节点是完全没资格成为master节点的。使用这个术语是因为历史的原因（This terminology is an unfortunate consequence of history）：master-eligible node是那些参与选举、集群状态发布（cluster state publication）期间执行一些任务的节点，而voting-only node有着相同的工作职责（responsibility）只是它们不会被选举为master节点。

&emsp;&emsp;通过添加`master`和`voting_only`角色来配置一个master-eligible node为voting-only node。下面的例子创建了一个voting-onyl data node：

```text
node.roles: [ data, master, voting_only ]
```

> IMPORTANT：只有拥有`master`角色的节点才可以标记为拥有`voting_only`角色。

&emsp;&emsp;高可用（HA：high availability）的集群要求至少有三个master-eligible node，它们中的至少两个节点不是voting-only node。这样的集群才能够在其中一个节点发生故障后选出一个master节点。

&emsp;&emsp;由于voting-only node不会被选举为master节点，所以相较于真正的master node，它们需要较少的堆以及较低性能的CPU。然而所有的master-eligible node，包括voting-only node，相较于集群中的其他节点，它们需要相当快速的持久存储以及一个可靠的低延迟的网络连接，因为它们处于[publishing cluster state updates](#Publishing the cluster state)的关键路径（critical path）上。

&emsp;&emsp;Voting-only master-eligible node在集群中可能被分配其他的角色。比如说一个节点同时是data node或者Voting-only master-eligible node。一个专用的voting-only master-eligible node 在集群中是不会有其他的角色的。通过下面的方式来创建一个专用的voting-only master-eligible nodes：

```text
node.roles: [ master, voting_only ]
```

##### Data node

&emsp;&emsp;data note保留了你索引的文档的分片。data note处理像CRUD、查询、聚合的操作。这些操作有I/O-, memory-, and CPU-intensive。监控这些资源以及工作负载大后增加更多的data node是非常重要的。

&emsp;&emsp;拥有专用的data node最主要的好处是将master和data角色分开出来。

&emsp;&emsp;通过下面的方式创建一个专用的data node：

```text
node.roles: [ data ]
```

&emsp;&emsp;在[多层部署架构](#Data tiers)（multi-tier）中，你可以根据为data node分配不同数据层对应的数据角色（data role）：`data_content`,`data_hot`, `data_warm`, `data_cold`, 或者`data_frozen`。一个节点可以属于多个层，但是已经拥有一个指定数据角色的节点不能有通用的`data`角色（generic data role）。

##### Content data node

&emsp;&emsp;Content data node属于[content tier](#Content tier)的一部分。存储在content tier上的数据通常是iterm的集合比如说产品目录（product catalog）或者文章归档（article archive）。跟时序数据不同的是，这些内容的价值随着时间的流逝相对保持不变的，所以根据这些数据的寿命（age）将它们移到性能不同的数据层是不合理的。Content data通常有长时间保留（retention）的要求，并且也希望无论这些数据的寿命的长短，总是能很快的检索到。

&emsp;&emsp;Content tier node会为查询性能做优化-优先优化IO吞吐使得可以处理复杂的查询，聚合并且能很快的返回结果。同时这类节点也用来索引，content data通常没有例如log和metric这种时序数据一样高的ingest rate。从弹性的角度（resiliency perspective）看，当前层的索引应该配置为使用一个或者多个副本（replica）。

&emsp;&emsp;Content tier是必须要有的（required）。系统索引以及其他不是[data stream](#Data streams)的索引都会被自动分配到content tier。

&emsp;&emsp;通过下面的方式创建一个专用的content  node：

```text
node.roles: [ data_content ]
```

##### Hot data node

&emsp;&emsp;hot data node是 [hot tier](#Hot tier)的一部分。hot tier是Elasticsearch中时序数据的入口点（entry point），并且保留最近，最频繁搜索的时序数据。hot tier节点上的读写速度都需要很快，要求更多的硬件资源和更快的存储（SSDs）。出于弹性目的，hot tier上的索引应该配置一个或多个副本分片。

&emsp;&emsp;hot tier是必须要有的。[data stream](#Data streams)中新的索引会被自动分配到hot tier。

&emsp;&emsp;通过下面的方式创建一个专用的hot node:

```text
node.roles: [ data_hot ]
```

##### Warm data node

&emsp;&emsp;warm data node是[warm tie](#Warm tier)的一部分。一旦时序数据的访问频率比最近索引的数据（recently-indexed data）低了，这些数据就可以移到warm tier。warm tier通常保留最近几周的数据。允许更新数据，但是infrequent。warm tier节点不需要像hot tier一样的快。出于弹性目的，warm tier上的索引应该配置一个或多个副本分片。

&emsp;&emsp;通过下面的方式创建一个专用的warm node:

```text
node.roles: [ data_warm ]
```

##### Cold data node

&emsp;&emsp;cold data node是[cold tier](#Cold tier)的一部分。当你不再经常（regular）搜索时序数据了，那可以将它们从warm tier移到cold tier。数据仍然可以被搜索到，在这一层的通常会被优化成较低的存储开销而不是查询速度。

&emsp;&emsp;为了更好的节省存储（storage saveing），你可以在cold tier保留[fully mounted indices](#Fully mounted index)的[searchable snapshots](#Searchable snapshots)。跟常规索引（regular index）不同的是，这些fully mounted indices不需要副本分片来满足可靠性（reliability），一旦出现失败事件，可以从底层（underlying）snapshot中恢复。这样可以潜在的减少一般的本地数据存储开销。snapshot仓库要求在cold tier使用fully mounted indices。Fully mounted indices只允许读取，不能修改。

&emsp;&emsp;另外你可以使用cold tier存储常规索引并且使用副本分片的方式，而不是使用searchable snapshot，这样会帮你在较低成本的硬件上存储较老的索引，但是相较于warm tier不会降低磁盘空间。

&emsp;&emsp;通过下面的方式创建一个专用的cold node:

```text
node.roles: [ data_cold ]
```

##### Frozen data node

&emsp;&emsp;frozen data node是[frozen tier](#Frozen tier)的一部分。一旦数据不需要或者很少（rare）被查询，也许就可以将数据从cold tier移到frozen tier，where it stays for the rest of its life。

&emsp;&emsp;frozen tier需要用到snapshot repository。frozen tier使用[partially mounted indices](#Partially mounted index)的方式存储以及从snapshot repository中载入数据。这样仍然可以让你搜索frozen数据并且可以减少本地储存（local storage）和操作开销（operation cost）。因为Elasticsearch必须有时从snapshot repository中提取（fetch）数据，在frozen tier的查询速度通常比cold  tier慢。

##### Ingest node

&emsp;&emsp;Ingest node可以执行pre-processing pipelines。由一个或者多个ingest processor组成。基于ingest processor执行的操作类型以及需要的资源，拥有一个专用的ingest node还是有意义的，使得这个节点只运行指定的任务。

&emsp;&emsp;通过下面的方式创建一个专用的Ingest node:

```text
node.roles: [ ingest ]
```

##### Coordinating only node

&emsp;&emsp;如果你的节点不需要（take away）处理master职责（duty）、保留数据、pre-process文档的能力，那么你还剩下一个coordinating only node，只能处理路由请求、查询的reduce phase、以及分布式的块索引（bulk Indexing）。本质上coordinating only node表现为像一个智能的负载均衡节点。

&emsp;&emsp;在大规模的集群中将coordinating node角色从data node和master-eligible node中剥离（offload）出来能体现coordinating only nodes的益处。这些节点加入到集群中并且接收完整的[cluster state](#Cluster state API)，就像每一个其他的节点一样，它们使用cluster state将请求路由到合适的地方。

> WARNING：集群中添加太多的coordinating only nodes会增加整个集群的负担（burden），因为被选举为master的节点必须等待从每一个节点更新后的cluster state的回应。不能过度夸大（overstate）coordinating only node的益处-data node也可以实现跟coordinating only nodes相同的目的（same purpose）

&emsp;&emsp;通过下面的方式创建一个专用的coordinating node:

```text
node.roles: [ ]
```

##### Remote-eligible node

&emsp;&emsp;一个remote-eligible node扮演一个cross-cluster client的角色以及连接[remote clusters](#Remote clusters)。一旦连接后，你可以使用[cross-cluster search](#Search across clusters)搜索远程的集群。你可以使用[cross-cluster replication](#Cross-cluster replication)在集群间同步数据。

```test
node.roles: [ remote_cluster_client ]
```

##### Machine learning node

&emsp;&emsp;Machine learning node运行任务并且处理machine learning API的请求。见[Machine learning settings](#Machine learning settings in Elasticsearch)查看更多的信息

&emsp;&emsp;通过下面的方式创建一个专用的machine learning node:

```text
node.roles: [ ml, remote_cluster_client]
```

&emsp;&emsp;`remote_cluster_client`是可选的，但是强烈建议配置进去。否则当使用machine learning jobs 或者 datafeed时，cross-cluster search会失败。如果在你的异常检测任务中使用corss-cluster search，在所有的master-eligible node上同样需要`remote_cluster_client`这个角色。否则 datafeed无法开始，见[Remote-eligible node](#Remote-eligible node)。

##### Transform node

&emsp;&emsp;Transform node运行transforms并且处理transform API的请求。见[Transforms settings](#Transforms settings in Elasticsearch)。

&emsp;&emsp;通过下面的方式创建一个专用的transform node:

```text
node.roles: [ transform, remote_cluster_client ]
```

&emsp;&emsp;`remote_cluster_client`是可选的，但是强烈建议配置进去。否则当使用transforms时，cross-cluster search会失败。见[Remote-eligible node](#Remote-eligible node)。

##### Changing the role of a node

&emsp;&emsp;每一个data node在磁盘上维护下面的数据：

- 分配给节点的每一个分片的分片数据
- 分配在这个节点的每一个分片的索引元数据（index metadata），以及
- 集群范围（cluster-wide）的元数据，例如settings和index templates

&emsp;&emsp;同样的，每一个master-eligible node在磁盘上维护下面的数据：

- 集群中每一个索引的索引元数据，以及
- 集群范围（cluster-wide）的元数据，例如settings和index templates

&emsp;&emsp;每一个节点在启动时会检查[数据路径](#path.data)的内容。如果发现了非预期（unexpected）的数据，节点将拒绝启动。这样就可以避免导入会导致红色的集群健康的[unwanted dangling indices](#Dangling indices)。更准确的说，如果在启动时发现磁盘上有分片数据但是这个节点没有`data`角色，那么这个节点不会启动。如果在启动时发现磁盘上有索引元数据但是这个节点没有`data`和`master`角色，那么这个节点不会启动。

&emsp;&emsp;可以通过更改`elasticsearch.yml`然后重启节点来修改节点的角色。这就是众所周知的 `repurposing`一个节点。为了能满足上文中描述的对非预期的检查，在启动没有`data`或者`master`角色的节点前，你必须执行一些额外的步骤来为repurposing做准备。

- 如果你想要通过repurposing来移除一个data note的`data`角色，你应该先通过[allocation filter](#Cluster-level shard allocation settings)安全的将节点上的所有分片数据迁移到集群中的其他节点上。
- 如果你想要通过repurposing来移除`data`和`master`节点，最简单的办法是启动一个全新的节点，这个节点设置一个空的数据路径以及想要的节点角色。你会发现使用[allocation filter](#Cluster-level shard allocation settings)可以安全的先将分片数据迁移到集群中的其他地方

&emsp;&emsp;如果没有办法按照这些额外的步骤实现你的目的，你可以使用[elasticsearch-node repurpose](#Changing the role of a node)工具来删除阻止节点启动的多余的数据（excess data）。

##### Node data path settings

###### path.data

&emsp;&emsp;每个data node、master-eligible node都要访问一个数据目录，这个目录中存放了分片、索引和集群元数据。`path.data`的默认值是`$ES_HOME/data`但是可以在`elasticsearch.yml`中配置一个全路径或者一个相对于`$ES_HOME`的相对路径：

```text
path.data:  /var/elasticsearch/data
```

&emsp;&emsp;跟其他的节点设置一样，可以在命令行上指定：

```text
./bin/elasticsearch -Epath.data=/var/elasticsearch/data
```

&emsp;&emsp;`path.data`的内容在启动时必须存在，因为这是你数据存储的位置。Elasticsearch 要求文件系统像由本地磁盘支持一样运行，但这意味着只要远程存储正常运行，它就可以在配置正确的远程块设备（例如 SAN）和远程文件系统（例如 NFS）上正常工作，与本地存储没有什么不同。你可以在同一个文件系统上运行多个Elasticsearch节点，但是每个节点要有自己的数据路径。

&emsp;&emsp;Elasticsearch集群的性能常常受制于底层存储的性能，所以你必须保证你的存储能有可以接受的性能范围内。一些远程存储的性能很差，尤其是在 Elasticsearch 施加（impose）的那种负载下，所以在使用特定的存储架构之前，请确保仔细地对你的系统进行基准测试。

> TIP：如果使用了`.zip`或者`.tar.gz`的发行版，`path.data`的值应该设置到Elasticsearch home目录以外的路径上，这样删除home目录时不会删除你的数据！RPM和Debian发行版已经为你做了这些工作。

> IMPORTANT：不要修改数据目录中的任何东西或者运行可能会影响这个目录中内容的程序。如果一些不是Elasticsearch的操作修改了数据目录的内容，那么Elasticsearch可能会发生故障，报告出corruption或者其他data inconsistencies，或者工作正常但是轻微的丢失了一些你的数据。不要尝试做文件系统级别的数据目录的备份。因为没有可用的方法来恢复这类备份。而是应该用 [Snapshot and restore](#Snapshot and restore)做安全的备份。不要在数据目录做病毒扫描，病毒扫描会使得Elasticsearch不能正确的工作并且可能会修改数据目录的内容。数据目录不包含可执行文件，因此病毒扫描只会发现误报

##### Other node settings

&emsp;&emsp;更多的跟节点相关的设置可以在[Configuring Elasticsearch](#Configuring Elasticsearch) 和 [Important Elasticsearch configuration](#Important Elasticsearch configuration)找到，包括：

- [cluster.name](#Cluster name setting)
- [node.name](#Node name setting)
- [network settings](#Networking)

#### Networking 
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-network.html)

##### Commonly used network settings

##### Binding and publishing

##### Advanced HTTP settings

###### http.max_content_length

##### Advanced transport settings

###### Long-lived idle connections

#### Node query cache settings
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-cache.html)

&emsp;&emsp;[filter context](#Filter context)中的查询结果缓存在node query cache中使得可以快速的查询，这属于每个节点上的查询缓存（query cache），由所有的分片共享。缓存使用了LRU eviction policy：当缓存满了以后，最近最少使用的查询结果会被踢出（evict），为新的查询查询让路。你不能查看（inspect）查询缓存的内容。

&emsp;&emsp;Term query以及在filter context使用外的查询没有缓存资格（eligible）。

&emsp;&emsp;默认情况下，缓存保留最多10000个query，以及占用最多10%的总内存。若要判断某个query是否有资格被缓存，Elasticsearch维护了一个query history来track occurrence。

&emsp;&emsp;缓存在每个段上进行，并且段中至少包含10000篇文档并且每个段中的文档数量至少是分片中文档总数的3%。由于是基于段的缓存，因此段的合并会使缓存的查询失效。

&emsp;&emsp;下面的设置是`static`并且必须在集群中每个data node上配置：

##### indices.queries.cache.size

&emsp;&emsp;（[Static](#Static（settings） )）为filter cache控制内存大小。可以是一个百分比的值，比如5%或者一个精确值，例如`512mb`。默认值为`10%`。

##### Query cache index settings

&emsp;&emsp;下面的设置属于index setting，可以在每一个索引上配置。只能在索引创建时或者一个[closed index](#Open index API)上设置。

###### index.queries.cache.enabled

&emsp;&emsp;（[Static](#Static（settings） )）控制是否开启查询缓存。可以是`true`（默认值）或者`false`。

#### Search settings
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-settings.html)

&emsp;&emsp;下面的expert setting用来管理全局的查询和聚合限制。

##### indices.query.bool.max_clause_count

&emsp;&emsp;（[Static](#Static（settings） ) integer）一个查询能包含的clause的最大数量。默认值为`4096`。

&emsp;&emsp;这个设置限制了一颗query tree能包含的clause的最大数量。4096这个默认值已经是非常高了，通常来说该值是够用的。这个限制会应用于重写query（rewrite query），所以不仅仅是`bool` query，所有会重写为bool query的query，例如`fuzzy` query，都会有较高数量的clause。这个限制的目的是防止查询的范围变得很大，占用过多的CPU和内存。如果你一定要提高这个值，保证你已经竭尽全力尝试了所有其他的选项，否则就不要这么做。较高的值会导致性能降低以及内存问题，特别是在负载高或者资源较少的集群上。

&emsp;&emsp;Elasticsearch提供了一些工具来防止遇到这个clause数量上限的问题，例如在[terms](#Terms query) query中，它允许查询许多不一样的值，但是它只作为一个clause。或者是带有[index_prefixes](#index_prefixes)选项的[text](#Text type family)字段，它会执行prefix query，这个query会膨胀为较高数量的term，但是只作为一个term query。

##### search.max_buckets

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)，integer）在一次响应中，[aggregation buckets ](#Bucket aggregations)的最大分桶数量。默认值为65535。

&emsp;&emsp;尝试返回超过这个限制的请求会返回一个错误。

##### indices.query.bool.max_nested_depth

&emsp;&emsp;（[Static](#Static（settings） ) integer）bool query的最大深度。默认值为`20`。

&emsp;&emsp;这个设置限制了bool query的嵌套深度。深度大的bool query可能会导致stack overflow。

#### Security settings in Elasticsearch

##### Realm settings

###### Kerberos realm settings

###### OpenID Connect realm settings

#### Shard request cache settings
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/shard-request-cache.html)

&emsp;&emsp;当一个查询请求运行在一个或者多个索引上时，每一个参与的分片会在本地执行查询然后将结果返回到coordinating note，节点会结合（combine）分片级别（shard-level）的结果到一个"全局的"的结果集中。

&emsp;&emsp;分片层的请求缓存模块会在每一个分片上缓存本地的结果。这允许经常使用（并且可能很重）的搜索请求几乎立即返回结果。请求缓存（request cache）非常适用于日志用例，因为这类数据只有最近的索引会持续更新，从旧的索引中获取的结果可以直接从缓存中返回。

> IMPORTANT：默认情况下，请求缓存在`size=0`时仅仅缓存查询请求的结果。所以不会缓存`hits`，但是它会缓存`hits.total`, [aggregations](#Aggregations)以及[suggestions](#Suggesters)。
>
> 大多数使用`now`（见 [Date Math](#Date Math)）的请求不能被缓存。
>
> 使用脚本的查询使用了一些不确定的API调用不会被缓存，比如说`Math.random()` 或者 `new Date()` 。

##### Cache invalidation

&emsp;&emsp;The cache is smart。它可以跟非缓存的查询一样的近实时查询。

&emsp;&emsp;当分片[refresh](#Near real-time search)后文档发生了变更或者当你更新了mapping时，缓存的结果会自动的被废止。换句话说，你总是可以从缓存中获得与未缓存搜索请求相同的结果。

&emsp;&emsp;refresh的间隔时间越长意味着缓存的结果保留时间越长，即使文档已经发生了变化。如果缓存满了，基于LRU（least recently used）机制来丢弃缓存。

&emsp;&emsp;可以通过[clear-cache API](#Clear cache API)手动将缓存置为过期的（expired）：

```text
POST /my-index-000001,my-index-000002/_cache/clear?request=true
```

##### Enabling and disabling caching

&emsp;&emsp;缓存默认开启，可以在创建一个新的索引时来关闭缓存：

```text
PUT /my-index-000001
{
  "settings": {
    "index.requests.cache.enable": false
  }
}
```

&emsp;&emsp;可以通过[update-settings](#Update index settings API) API来手动的开启或关闭缓存：

```text
PUT /my-index-000001/_settings
{ "index.requests.cache.enable": true }
```

##### Enabling and disabling caching per request

&emsp;&emsp;`request_cache` 这个query-string参数可以用于在每一次请求中来开启或关闭缓存。如果设置了这个参数，会覆盖索引层（index-levele）的设置：

```text
GET /my-index-000001/_search?request_cache=true
{
  "size": 0,
  "aggs": {
    "popular_colors": {
      "terms": {
        "field": "colors"
      }
    }
  }
}
```

&emsp;&emsp;请求中的`size`如果大于0的话则不会缓存即使在索引设置中开启了缓存。要缓存这些请求，你需要使用的query-string参数。

##### Cache key

&emsp;&emsp;使用完整的JSON计算出的hash值作为cache key。这意味着如果JSON发生了变更，比如说key的先后顺序发生了变化，那么对应的cache key就无法被识别。

> TIP：大多数 JSON 库都支持规范模式（canonical mode），可确保 JSON的key始终以相同的顺序发出。这种规范模式可以在应用程序中使用，以确保始终以相同的方式序列化请求。

##### Cache settings

&emsp;&emsp;缓存在节点层进行管理，缓存大小的最大值默认是堆内存的1%。可以通过`config/elasticsearch.yml`修改：

```text
indices.requests.cache.size: 2%
```

&emsp;&emsp;同样的你可以使用`indices.requests.cache.expire`为缓存结果设置一个TTL，但是并没有理由要这么做。因为过时的结果（stale result）会在refresh后自动的废止。这个设置只是为了completeness（This setting is provided for completeness' sake only）。

##### Monitoring cache usage

&emsp;&emsp;缓存的大小（字节）和evictions的数量可以通过[indices-stats](#Index stats API)接口来查看：

```text
GET /_stats/request_cache?human
```

&emsp;&emsp;或者通过[nodes-stats](#Nodes stats API)接口查看：

```text
GET /_nodes/stats/indices/request_cache?human
```

#### Snapshot and restore settings

##### SLM settings

###### slm.history_index_enabled

###### slm.retention_schedule

###### slm.retention_duration

###### repositories.url.allowed_urls

#### Transforms settings in Elasticsearch
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/transform-settings.html)

#### Thread pools
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-threadpool.html)

&emsp;&emsp;一个节点会使用多个线程池来管理内存消费（memory consumption）。许多的线程池使用队列hold住pending中的请求而不是丢弃。

&emsp;&emsp;节点中有很多的线程池，下面介绍主要的几个线程池：

###### generic

&emsp;&emsp;用于一般的操作（例如background node discovery）。线程池类型是[scaling](#scaling)。

###### search

&emsp;&emsp;用于count/search/suggest操作。线程池类型是[fixed](#fixed)。线程数量为**int((# of [allocated processors](#Allocated processors setting) * 3) / 2) + 1**。队列大小为`1000`。

###### search_throttled

&emsp;&emsp;用于在`search_throttled indices`上的count/search/suggest/get操作。线程池类型为`fixed`，线程数量为`1`，队列大小为`100`。

###### search_coordination

&emsp;&emsp;用于轻量级的查询相关的协同操作（search-related coordination）。线程池类型为`fixed`，线程数量为**a max of min(5, (# of [allocated processors](#Allocated processors setting)) / 2)**，队列大小为`1000`。

###### get

&emsp;&emsp;用于get操作，线程池类型为`fixed`，线程数量为[# of allocated processors](#Allocated processors setting)，队列大小为`1000`。

###### analyze

&emsp;&emsp;用于analyzer请求，线程池类型为`fixed`，线程数量为`1`，队列大小为`16`。

###### write

&emsp;&emsp;用于单篇文档的 index/delete/update以及bulk的请求，线程池类型为`fixed`，线程数量为[# of allocated processors](#Allocated processors setting)，队列大小为`10000`。这个线程池的最大值是**1 + [# of allocated processors](#Allocated processors setting)**。

###### snapshot

&emsp;&emsp;用于snapshot/restore请求，线程池类型为[scaling](#scaling)，keep-alived的值为`5m`以及**a max of min(5, ([# of allocated processors](#Allocated processors setting)) / 2)**。

###### snapshot_meta

&emsp;&emsp;用于读取snapshot repository metadata的请求，线程池类型为[scaling](#scaling)，keep-alived的值为`5m`以及**a max of min(50, ([# of allocated processors](#Allocated processors setting)) / 3)**。

###### warmer

&emsp;&emsp;用于segment warm-up的操作，线程池类型为[scaling](#scaling)，keep-alived的值为`5m`以及**a max of min(5, ([# of allocated processors](#Allocated processors setting)) / 2)**。

###### refresh

&emsp;&emsp;用于refresh的操作，线程池类型为[scaling](#scaling)，keep-alived的值为`5m`以及**a max of min(10, ([# of allocated processors](#Allocated processors setting)) / 2)**。

###### fetch_shard_started

&emsp;&emsp;用于列出分片状态（list shard state）的操作，线程池类型为[scaling](#scaling)，keep-alived的值为`5m`以及**a default maximum size of 2 * [# of allocated processors](#Allocated processors setting)**。

###### fetch_shard_store

&emsp;&emsp;用于列出分片存储（list shard store）的操作，线程池类型为[scaling](#scaling)，keep-alived的值为`5m`以及**a default maximum size of 2 * [# of allocated processors](#Allocated processors setting)**。

###### flush

&emsp;&emsp;用于[flush](#Flush API)和[translog](#Translog) `fsync`的操作，线程池类型为[scaling](#scaling)，keep-alived的值为`5m`以及**a max of min(10, ([# of allocated processors](#Allocated processors setting)) / 2)**。

###### force_merge

&emsp;&emsp;用于[force merge](#Force merge API)操作，线程池类型为[fixed](#fixed)，线程数量为`1`，队列大小为`unbounded`。

###### management

&emsp;&emsp;用于集群管理，线程池类型为[scaling](#scaling)，keep-alived的值为`5m`以及**a default maximum size of 5**。

###### system_read

&emsp;&emsp;用于读取系统索引的操作，线程池类型为[fixed](#fixed)，keep-alived的值为`5m`以及**a default max of min(5, ([# of allocated processors](#Allocated processors setting)) / 2)**。

###### system_write

&emsp;&emsp;用于写入系统索引的操作，线程池类型为[fixed](#fixed)，keep-alived的值为`5m`以及**a default max of min(5, ([# of allocated processors](#Allocated processors setting)) / 2)**。

###### system_critical_read

&emsp;&emsp;用于读取关键系统索引（critical system indices）的操作，线程池类型为[fixed](#fixed)，keep-alived的值为`5m`以及**a default max of min(5, ([# of allocated processors](#Allocated processors setting)) / 2)**。

###### system_critical_write

&emsp;&emsp;用于写入关键系统索引（critical system indices）的操作，线程池类型为[fixed](#fixed)，keep-alived的值为`5m`以及**a default max of min(5, ([# of allocated processors](#Allocated processors setting)) / 2)**。

###### watcher

&emsp;&emsp;用于[watch executions](#Watcher)。线程池类型为[fixed](#fixed)，以及**a default max of min(5 * ([# of allocated processors](#Allocated processors setting)) , 50)**，队列大小为`50`。

&emsp;&emsp;线程池的设置是[static](#Static（settings） )，通过编辑`elasticsearch.yml`更改。下面的例子中更改了`write`线程池中的线程数量：

```text
thread_pool:
    write:
        size: 30
```

##### Thread pool types

&emsp;&emsp;下文中介绍的是线程池的类型以及对应的参数：

###### fixed

&emsp;&emsp;`fixed`类型的线程池中有固定数量的线程，以及一个队列（通常是有界的），当线程池中没有可用线程时，队列会hold住请求。

&emsp;&emsp;`size`参数控制了线程的数量。

&emsp;&emsp;`queue_size`控制了队列中的请求数量，当没有可用线程时，队列会hold住请求。默认情况下，该值为`-1`意味着队列是无界的。但一个新的请求到来时，如果队列已满则abort这个请求。

```text
thread_pool:
    write:
        size: 30
        queue_size: 1000
```

###### scaling

&emsp;&emsp;`scaling`类型的线程池中拥有动态数量的线程。线程数量跟工作负载成正比（proportional）。线程数量在参数`core`和`max`之间变化。

&emsp;&emsp;`keep_alive`参数用于决定当线程池中的线程没有任何工作时，呆在线程池中的最大时间。

```text
thread_pool:
    warmer:
        core: 1
        max: 8
        keep_alive: 2m
```

##### Allocated processors setting

&emsp;&emsp;处理器（processor）的数量会被自动的检测。线程池的会基于处理器的数量自动的设置。在有些情况下，覆盖（override）处理器的数量是很有用的。可以显示的（explicit）设置`node.processors`。

```text
node.processors: 2
```

&emsp;&emsp;以下的情况要显示的覆盖设置`node.processors`：

1. If you are running multiple instances of Elasticsearch on the same host but want Elasticsearch to size its thread pools as if it only has a fraction of the CPU, you should override the node.processors setting to the desired fraction, for example, if you’re running two instances of Elasticsearch on a 16-core machine, set node.processors to 8. Note that this is an expert-level use case and there’s a lot more involved than just setting the node.processors setting as there are other considerations like changing the number of garbage collector threads, pinning processes to cores, and so on.
2. Sometimes the number of processors is wrongly detected and in such cases explicitly setting the node.processors setting will workaround such issues.

&emsp;&emsp;若要检查检查到的处理器的数量，使用nodes info AP带上`os`这个flag。 

#### Advanced configuration
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/advanced-configuration.html#advanced-configuration)

### Important system configuration
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-discovery.html)

#### Configuring system settings
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/setting-system-settings.html)

&emsp;&emsp;根据你安装Elasticsearch时使用的安装包，以及你正在使用的操作系统，系统设置的配置位置略有不同。

&emsp;&emsp;当使用`.zip`或者`.tar.gz`的安装包时，可以通过下面的方式配置系统设置：

- 在[ulimit](#ulimit)中临时设置
- 在[/etc/security/limits.conf](#/etc/security/limits.conf)中永久设置

&emsp;&emsp;当使用RPM或者Debian发行版时，大多数的系统设置在[system configuration file](#Sysconfig file)中设置，然而在使用systemd的系统中，要求在[systemd configuration file](#Systemd configuration)中指定system limit。

##### ulimit

&emsp;&emsp;在linux系统中，`ulimit`用于临时更改资源限制（resource limit）。Limits通常需要使用root用户进行设置，然后切换到普通用户启动Elasticsearch。例如，将文件句柄（file handler）的最大数量设置为65,536（ulimit -n），你可以按照下面的方式进行设置：

```text
sudo su  
ulimit -n 65535 
su elasticsearch 
```

&emsp;&emsp;第1行，切换到root用户
&emsp;&emsp;第2行，设置打开的文件最大数量（max number of open files）
&emsp;&emsp;第3行，切换到普通用户来启动Elasticsearch
&emsp;&emsp;这个新的限制（new limit）只适用于当前会话（current session）
&emsp;&emsp;你可以通过`ulimit -a`查询（consult）当前目前所有的limit

##### /etc/security/limits.conf

&emsp;&emsp;在linux系统中，可以通过编辑文件`/etc/security/limits.conf`为linux的某个系统用户设置persistent limits。如果要为`elasticsearch`这个系统用户设置可以打开的文件最大数量为65,535，在文件中添加一行下面的内容：

```text
elasticsearch  -  nofile  65535
```

&emsp;&emsp;这个变更只有在`elasticsearch`这个用户下次打开一个新的session时才会生效。

> NOTE: Ubuntu and `limits.conf`
> 在Ubuntu中，由`init.d`启动的进程会忽略`limits.conf`文件，如果需要让这个文件生效，可以编辑`/etc/pam.d/su`然后注释下面这一行的内容：`# session    required   pam_limits.so`

##### Sysconfig file

&emsp;&emsp;当使用RPM或者Debian安装包时，可以在系统配置文件中指定系统变量：

|  RPM   | /etc/sysconfig/elasticsearch |
| :----: | :--------------------------: |
| Debian |  /etc/default/elasticsearch  |

&emsp;&emsp;然而system limits需要通过[systemd](#Systemd configuration)来指定。

##### Systemd configuration

&emsp;&emsp;当在使用systemd的系统上使用RPM或者Debian安装包时，system limits需要通过[systemd](https://en.wikipedia.org/wiki/Systemd)来指定。

&emsp;&emsp;systemd service file（`/usr/lib/systemd/system/elasticsearch.service`）中包含了默认的limit。

&emsp;&emsp;你可以新增一个文件名为`/etc/systemd/system/elasticsearch.service.d/override.conf`（或者你可以执行`sudo systemctl edit elasticsearch`，它会在默认编辑器中自动打开文件），在这个文件中做如下的变更：

```text
[Service]
LimitMEMLOCK=infinity
```

&emsp;&emsp;完成更改后，运行下面的命令来reload units：

```text
sudo systemctl daemon-reload
```

#### Disable swapping
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/setup-configuration-memory.html)

&emsp;&emsp;大多数的操作系统会为文件系统缓存（file system cache）尽可能多的使用内存并且急切的（eagerly）将unused application memory交换出去（swap out）。这会导致部分JVM堆甚至executable page被交换到磁盘上。

&emsp;&emsp;Swapping对性能、节点稳定影响是非常大的，无论如何（at all cost）都要避免。这会导致垃圾回收的时间达到分钟级别而不是毫秒级别，使得节点的响应时间变慢甚至与节点的断开连接。在弹性分布式系统中，让操作系统杀死节点反而更有效。

&emsp;&emsp;有三种方法关闭swapping。较好的方法（prefer option）是完全的关闭swapping。如果不能用这个方法，那么基于你的环境选择最小化swappiness还是内存锁定。

##### Disable all swap files

&emsp;&emsp;通常Elasticsearch是一个box中唯一运行的服务，并且通过JVM参数来控制内存的使用，就没有必要开启swapping。

&emsp;&emsp;在linux系统中，你可以运行下面的命令临时关闭swapping：

```text
sudo swapoff -a
```

&emsp;&emsp;这个操作不需要重启Elasticsearch。

&emsp;&emsp;若要永久关闭，你需要编辑文件`/etc/fstab`，然后将所有包含`swap`的内容都注释。

&emsp;&emsp;在Windows上，可以通过`System Properties → Advanced → Performance → Advanced → Virtual memory`达到相同的目的。

##### Configure swappiness

&emsp;&emsp;要确保Linux系统上另一个可用的选项：sysctl的`vm.swappiness`的值是`1`。这个选项会降低kernel进行swap的趋势，在正常情况下（normal circumstances）不会进行swapping，并且允许在紧急情况下运行将整个系统进行swapping。

###### Enable bootstrap.memory_lock

&emsp;&emsp;另一种方式是通过在Linux/Unix系统上使用[mlockall](https://pubs.opengroup.org/onlinepubs/007908799/xsh/mlockall.html)或者是Windows系统上的[VirtualLock](https://learn.microsoft.com/en-us/windows/win32/api/memoryapi/nf-memoryapi-virtuallock?redirectedfrom=MSDN)，将进程地址空间锁定到RAM，防止Elasticsearch堆内存被swapping。

> NOTE: 一些平台在使用了memory lock后仍然会swap off-heap memory。为了防止swap off-heap memory，转而尝试[disable all swap files](#Disable all swap files)。

&emsp;&emsp;在`elasticsearch.yml`中设置`bootstrap.memory_lock`为`true`来开启memory lock：

```text
bootstrap.memory_lock: true
```

> WARNING：如果尝试分配比可用内存更多的内存量，`mlockall`可能会导致JVM或者shell session退出。

&emsp;&emsp;在启动Elasticsearch之后，你可以通过下面这个请求输出的`mlockall`的值检查是否应用了设置：

```text
GET _nodes?filter_path=**.mlockall
```

&emsp;&emsp;如果你看到`mlockall`的值为`false`，那么意味着`mlockall`请求失败了。你可以在日志中看到包含`Unable to lock JVM Memory`的更详细的信息。

&emsp;&emsp;在Linux/Unix系统上最有可能导致失败的原因是运行中的Elasticsearch没有权限来lock memory。可以通过下面的方式授权：

- `.zip` and `.tar.gz`
  - 在启动Elasticsearch前切换到root用户并执行[ulimit -l unlimited](#ulimit)，或者在`/etc/security/limits.conf`中设置`memlock`为`unlimited`。

```text
# allow user 'elasticsearch' mlockall
elasticsearch soft memlock unlimited
elasticsearch hard memlock unlimited
```

- RPM and Debian
  - 在[systemd configuration](#Systemd configuration)中将`LimitMEMLOCK`设置为`infinity`

&emsp;&emsp;另一个导致`mlockall`可能的原因是[the JNA temporary directory (usually a sub-directory of /tmp) is mounted with the noexec option](#Ensure JNA temporary directory permits executables)，可以使用`ES_JAVA_OPTS`环境变量为JNA指定一个临时的目录来解决：

```text
export ES_JAVA_OPTS="$ES_JAVA_OPTS -Djna.tmpdir=<path>"
./bin/elasticsearch
```

&emsp;&emsp;或者在配置文件jvm.options中设置这个JVM标志。

#### File Descriptors
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/file-descriptors.html)

>NOTE：File Descriptors仅仅适用于Linux以及MacOs，在Windows平台上运行Elasticsearch可以忽略。在 Windows 上，JVM 使用仅受可用资源限制的 [API](https://learn.microsoft.com/zh-cn/windows/win32/api/fileapi/nf-fileapi-createfilea?redirectedfrom=MSDN)

&emsp;&emsp;Elasticsearch会使用很多file descriptors 或者 file handles。文件描述符用完可能是灾难性的，并且很可能会导致数据丢失。要确保将open files descriptors的数量提高到65,536或者更高用于Elasticsearch的运行。

&emsp;&emsp;对于`.zip`和`.tar.gz`的安装包，在Elasticsearch启动前使用root用户执行`ulimit -n 65535`，或者在`/etc/security/limits.conf`设置`nofile`的值为`65535`。

&emsp;&emsp;在MacOs平台上，你必须要使用JVM参数`-XX:-MaxFDLimit`让Elasticsearch使用更高的file descriptors限制。

&emsp;&emsp;RPM和Debian已经默认将file descriptors的最大值设置为65535并且不需要进一步的配置。

&emsp;&emsp;你可以使用[Nodes stats](#Nodes stats API) API检查每一个节点上的`max_file_descriptors`：

```text
GET _nodes/stats/process?filter_path=**.max_file_descriptors
```

#### Virtual memory
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/vm-max-map-count.html)

&emsp;&emsp;Elasticsearch默认使用[mmapfs](#mmapfs)存储索引文件。默认情况下操作系统会将mmap count限制的太低导致OOM的异常。

&emsp;&emsp;在Linux下，你可以使用root通过下面的命令来提高上限：

```text
sysctl -w vm.max_map_count=262144
```

&emsp;&emsp;可以在`/etc/sysctl.conf`中更新`vm.max_map_count`来永久的设置该值。在重启后使用`sysctl vm.max_map_count`命名来验证刚才的变更。

&emsp;&emsp;RPM跟Debian安装包会自动进行配置。不需要进行额外的配置。

#### Number of threads
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/max-number-of-threads.html)

&emsp;&emsp;Elasticsearch会使用线程池用于不同类型的操作。重要的是当有需要时就会创建新的线程。保证Elasticsearch能创建的线程数量至少可以有4096个。

&emsp;&emsp;在启动Elasticsearch前切换到root设置[ulimit -u 4096](#ulimit)，或者在[/etc/security/limits.conf](#/etc/security/limits.conf)中将`nproc`设置为4096。

&emsp;&emsp;在`systemd`下运行的service会自动的为Elasticsearch进程设置线程数量。不需要额外的配置。 

#### DNS cache settings
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/networkaddress-cache-ttl.html)

#### Ensure JNA temporary directory permits executables
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/executable-jna-tmpdir.html)

#### TCP retransmission timeout
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/system-config-tcpretries.html)

### Bootstrap Checks
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/bootstrap-checks.html)

#### Development vs. production mode

### Discovery and cluster formation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-discovery.html)

&emsp;&emsp;Discovery and cluster formation负责节点发现，选举master，形成集群，以及每一次发生变更时集群状态的发布。

&emsp;&emsp;下面的processes和settings是discovery and cluster formation的 一部分：

- [Discovery](#Discovery)
  - Discovery是当master未知时，节点之间用来相互发现的步骤，例如一个节点刚刚启动或者当上一个master node发生故障时

- [Quorum-based decision making](#Quorum-based decision making)
  - Elasticsearch如何基于quorum-based voting mechanism来做出决策，即使一些节点发生了故障

- [Voting configurations](#Voting configurations)
  - Elasticsearch在节点加入以及离开集群时如何自动的更新voting configuration

- [Bootstrapping a cluster](#Bootstrapping a cluster)
  - 当Elasticsearch集群第一次启动时Bootstrapping a cluster是必须。在[development node](#Development vs. production mode)中，如果没有配置discovery settings，这将由节点本身自动执行。由于auto-bootstrapping是[inherently unsafe](#Quorum-based decision making)，在[production mode](#Development vs. production mode)中要求节点显示配置（[explicitly configure](#Bootstrapping a cluster)）bootstrappping。

- [Adding and removing master-eligible nodes](#Adding and removing nodes)
  - 建议在集群中设置一个小规模的并且固定数量的master-eligible node，只通过添加或者移除master-ineligible node来扩大或者缩小集群。然而也有一些情况需要将一些master-eligible node 添加到集群，或者从集群中移除。 这块内容描述的是添加或者移除master-eligible node的过程，包括在同一时间移除超过一半数量的master-eligible node时额外必须要执行的步骤。

- [Publishing the cluster state](#Publishing the cluster state)
  - Cluster state publishing描述的是被选举为master的node更新集群中所有节点的集群状态的过程

- [Cluster fault detection](#Cluster fault detection)
  - Elasticsearch执行健康检查来检测以及移除有故障的节点

- [Settings](#Discovery and cluster formation settings)
  - 让用户通过设置来影响（influence）discovery，cluster formation，master election和fault detection processes

#### Discovery
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-discovery-hosts-providers.html)

&emsp;&emsp;Discovery是cluster formation module发现其他节点并形成一个集群的过程。这个过程发生在当你启动一个Elasticsearch节点或者当一个节点认为master node发生故障并且等到发现master node或者选出新的master node后继续运行时。

&emsp;&emsp;这个过程和上一次已知的集群中的master-eligible node的地址一起，开始于一个或者多个[seed hosts providers](#Seed hosts providers)中提供的seed address列表。这个过程有两个阶段：首先，通过连接每一个地址来尝试明确节点是否连接并且验证是否为master-eligible node。然后如果成功了，它会共享它已知的master-eligible node的remote node并且等待响应。然后该节点探测（prove）它刚刚发现的所有新节点，请求它们的对等节点（peer），等等。

&emsp;&emsp;如果当前节点不是master-eligible node，那它会继续处理直到它发现了被选举为master的node。如果没有发现master node那么这个节点将在`discovery.find_peers_interval`后重试，默认值是`1s`。

&emsp;&emsp;如果当前节点是master-eligible node，那它会继续处理直到它要么发现了一个master node，要么已经发现了足够多masterless master-eligible node并完成master的选举。如果这两个都没有发生，则在`discovery.find_peers_interval`后重试，默认值是`1s`。

&emsp;&emsp;一旦一个master被选举出来，通常情况下它会一直是master node，直到它被故意（deliberate）停止。如果[falult detection](#Cluster fault detection)发现集群发生了故障，也有可能作为master并停止。如果一个节点不是master node，它则继续discovery的过程。

##### Troubleshooting discovery

&emsp;&emsp;大多数情况下，discovery和选举的过程会很快完成。并且master node在很长的一段时间内都是会master。

&emsp;&emsp;如果你的集群没有一个稳定（stable）的master，许多的功能不能正确工作。那么Elasticsearch会报出错误给客户端并记录日志。你必须在解决其它问题前先处理master node的不稳定问题。master node选举出来之前或者master node稳定前将无法解决其他任何的问题。

&emsp;&emsp;如果你有一个稳定的master node但是一些节点不能发现它或者加入，这些节点会报出错误给客户端并记录日志。在解决其它问题前你必须先解决这些节点无法加入到集群的障碍（obstacle）。节点不能加入集群前将无法解决其他任何问题。

&emsp;&emsp;如果集群在超过好几秒后都没有选举出master，说明master不稳定或者其他节点无法发现或者加入join a stable master。那Elasticsearch会在日志中记录其原因。如果这个问题超过好几分钟还存在，Elasticsearch会在日志中记录额外的日志。若要正确的处理discovery和选举问题，可以收集并分析所有节点上最新的5分钟内的日志。

&emsp;&emsp;下面的内容介绍了一些常见的discovery和选举问题。

###### No master is elected

&emsp;&emsp;当一个节点赢的选举，日志中会包含`elected-as-master`的信息并且所有的节点会记录一条包含`master node changed`的信息来标识最新赢的选举的master node。

&emsp;&emsp;如果没有选出master node并且没有节点赢得选举，所有的节点会重复的在日志中记录问题：`org.elasticsearch.cluster.coordination.ClusterFormationFailureHelper`。默认情况下，每10秒发生一次。

&emsp;&emsp;master的选举只会涉及master-eligible node，所以这种情况下集中查看master-eligible node的日志。这些节点的日志会说明master选举的要求，比如用于discovery的相关节点。

&emsp;&emsp;如果日志显示Elasticsearch不能从quorum中发现足够多的节点。你必须解决阻止Elasticsearch发现缺失节点的原因。缺失的节点需要用于重新构造（reconstruct）cluster metadata。没有cluster metadata，你集群中的数据是没有意义的（meaningless）。cluster metadata存储在集群中master-eligible node的子集中。如果不能发现quorum ，那么缺失的节点是拥有cluster metadata的节点。

&emsp;&emsp;保证有足够多的节点来形成quorum并且每个节点能相互通过网络联系到对方。如果选举问题持续了好几分钟，Elasticsearch会额外的报出网络连接的细节。如果你不能启动足够多的节点来形成quorum，那么启动一个新的集群并且从最新的snapshot中恢复数据。参考[Quorum-based decision making ](#Quorum-based decision making)了解更多信息。

&emsp;&emsp;如果日志显示Elasticsearch已经发现了 a possible quorum of nodes。那么集群不能选出master的原因通常是其中的节点不能发现quorum。检查其他master-eligible node上的日志并且保证它们已经发现了足够多的节点来形成一个quorum。

###### Master is elected but unstable

&emsp;&emsp;当一个节点赢得了选举，它的日志中会包含`elected-as-master`的信息。如果重复出现这个日志，说明选举出的节点不稳定。在这种情况下，集中在master-eligible node中的日志中查看为什么选举出的胜利者后来又停止成为master并且又触发另一个选举的原因。

###### Node cannot discover or join stable master

&emsp;&emsp;如果选举出的master node是一个稳定的节点，但是某个节点不能发现并加入到集群，它会在日志中使用`ClusterFormationFailureHelper`重复的报出日志。其他受到影响的节点和选举为master的节点上可能会有一些关于这个问题的额外的日志。

###### Node joins cluster and leaves again

&emsp;&emsp;如果某个节点加入到了集群但是Elasticsearch认为它出了故障并随后将再次从集群中移除。见[Troubleshooting an unstable cluster](#Troubleshooting an unstable cluster)了解更多信息。

##### Seed hosts providers

&emsp;&emsp;默认情况下，cluster formation module提供了两个seed hosts provider来配置seed node列表：[settings-based](#Settings-based seed hosts provider)和[file-based](#File-based seed hosts provider)。可以扩展为支持云环境以及通过[discovery plugins](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/discovery.html)的其他形式的seed hosts provider。seed hosts provider使用`discovery.seed_providers`进行配置，这个是settings-base 这种provider的默认方式。这个设置接受一个不同的provider的列表，允许你能用多种方式来为你的集群找到seed hosts。

&emsp;&emsp;每一个seed hosts provider提供（yield）seed node的IP地址或者hostname。如果返回的是hostname，那么需要使用DNS来查找对应的IP地址。如果一个hostname对应了对个IP地址，那么Elasticsearch会尝试找到所有的地址。如果hosts provider没有显示的（explicit）给出节点的TCP port，则会隐式的（implicit）使用port range中的第一个：`transport.profiles.default.port`或者`transport.port`（如果没有设置`transport.profiles.default.port`）。通过DSN查找IP地址的并发数量由`discovery.seed_resolver.max_concurrent_resolvers`控制，默认值是`10`。超时时间由`discovery.seed_resolver.timeout`控制，默认值是`5s`。注意的是DNS查找IP地址受限于[JVM DNS caching](#DNS cache settings)。

###### Settings-based seed hosts provider

&emsp;&emsp;Settings-based seed hosts provider使用了节点设置来配置一个静态的seed nodes的地址列表。这些地址可以是hostname或者是IP地址。使用hostname时会在每一次discovery时去查找对应的IP地址。

&emsp;&emsp;下面使用了[discovery.seed_hosts](#discovery.seed_hosts)静态设置：

```text
discovery.seed_hosts:
   - 192.168.1.10:9300
   - 192.168.1.11 
   - seeds.mydomain.com 
```

&emsp;&emsp;第3行，将默认使用`transport.profiles.default.port`，如果没有配置`transport.profiles.default.port`则使用`transport.port`
&emsp;&emsp;第4行，如果这个hostname对应了多个IP地址，Elasticsearch会尝试连接每一个IP地址

###### File-based seed hosts provider

&emsp;&emsp;File-based seed hosts provider通过外部的文件来配置hosts列表。这个文件发生变更后，Elasticsearch就会重新加载，所以不需要重启节点就可以使用变更后的seed nodes的地址。例如运行在Docker容器中的Elasticsearch就可以很方便的在节点启动时，如果给定的IP地址无法连接时就可以动态的提供。

&emsp;&emsp;若要开启file-based的discovery，那么在`elasticsearch.yml`中配置`file`即可：

```text
discovery.seed_providers: file
```

&emsp;&emsp;然后按照下面的格式在`$ES_PATH_CONF/unicast_hosts.txt`中创建一个文件。任何时刻在`unicast_hosts.txt`文件中发生的变更都会被Elasticsearch捕获并使用新的hosts列表。

&emsp;&emsp;注意的是file-based discovery 插件增强了`elasticsearch.yml`中的unicast hosts list：如果在`discovery.seed_hosts`中有合法的seed address，那么Elasticsearch会使用这些地址并额外使用`unicast_hosts.txt`中的地址。

&emsp;&emsp;`unicast_hosts.txt`文件中每一行是一个node entry。每一个node entry由host（hostname或者IP地址）和一个可选的transport port 组成。如果指定了port，必须是写在host后面并且用`:`分隔。如果没有指定port，默认使用`transport.profiles.default.port`，如果没有配置`transport.profiles.default.port`则使用`transport.port`。

&emsp;&emsp;例如，下面的`unicast_hosts.txt` 中有四个node参与discovery，其中一些使用了默认的端口号：

```text
10.10.10.5
10.10.10.6:9305
10.10.10.5:10005
# an IPv6 address
[2001:0db8:85a3:0000:0000:8a2e:0370:7334]:9301
```

&emsp;&emsp;可以使用hostname来代替IP地址，但是需要上文描述的DNS。IPv6的地址必须要给定端口号。

&emsp;&emsp;你可以在文件中添加注释， 所有的注释必须以`#`开头

###### EC2 hosts provider

&emsp;&emsp;[EC2 discovery plugin](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/discovery-ec2.html)使用[AWSAPI](https://github.com/aws/aws-sdk-java)来 查找seed node。

###### Azure Classic hosts provider

&emsp;&emsp;[Azure Classic discovery plugin](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/discovery-azure-classic.html)使用Azure Classic API来查找seed node。

###### Google Compute Engine hosts provider

&emsp;&emsp;[GCE discovery plugin ](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/discovery-gce.html)使用GCE API来查找seed node。

#### Quorum-based decision making
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-discovery-quorums.html#modules-discovery-quorums)

&emsp;&emsp;选举出master node和变更集群状态是两个基本的任务，需要所有的master-eligible node一起工作执行。重要的是即使一些节点发生了故障也要让这些工作具有鲁棒性。Elasticsearch中达到这个鲁棒性的方式是：考虑每一个从`quorum`中接受到成功的响应。`quorum`即集群中的master-eligible node集合。只需要一部分节点响应的好处在于在一些节点发生故障后也不会影响集群继续工作。quorum是经过谨慎选择的，这样集群中不会出现裂脑（split brain）的场景：集群被划分为两块，每一块会作出与另一块不一样的决策。

&emsp;&emsp;Elasticsearch允许你向正在运行中的集群添加或者移除master-eligible node。在很多场景中只要按需简单的启动或者节点即可。见[Adding and removing nodes](#Adding and removing nodes)。

&emsp;&emsp;通过更新集群中的[voting configuration](#Voting configurations)使得在添加/移除节点时，Elasticsearch能维护一个最佳的容错能力（Elasticsearch maintains an optimal level of fault tolerance）。voting configuration是master-eligible的集合，它们的响应会被计数用来对选举出新的master和提交（commit）新的集群状态做出决策。只有voting configuration中的超过半数才能做出决策。通常情况下，voting configuration跟当前集群中所有的master-eligible集合是相同的，然而在有些情况下会有不同。

&emsp;&emsp;为了保证集群仍然可用（remain available），你必须不能在同一时间（at the same time）停止一半或者更多的voting configuration中的节点。只要超过一半的voting node是可用的，集群就能正常的工作。例如，如果有3个或者4个master-eligible node，集群可以容忍一个不可用的节点。如果2个或者更少的master-eligible node，它们必须都是可用的。

&emsp;&emsp;一个节点加入/离开集群时，选举为master的节点必须发布（issue）集群状态的更新，调整（adjustment）voting configuration，这个过程可能会花费一点时间。在移除更多节点之前等待调整结束是非常重要的。

##### Master elections

&emsp;&emsp;Elasticsearch在启动阶段以及当master node发生故障时使用选举的处理方式来达成一致选出master node。任何的master-eligible都可以开始一个选举，通常来说第一次选举都会成功（normally the first election that takes place will succeed）。只有当两个节点碰巧同时开始选举时，选举才会失败，所以每个节点上的选举都是随机安排的，以降低这种情况发生的概率。节点会重试选举，直到选举出master node。backing off on failure，使得选举会最终的成功。The scheduling of master elections are controlled by the [master election settings](#cluster.election.back_off_time)。

##### Cluster maintenance, rolling restarts and migrations

&emsp;&emsp;许多的集群维护任务包括临时的关闭一个或者多个节点然后重新启动它们。默认情况下，如果其中一个master-eligible node下线了，Elasticsearch仍然可用。比如在[rolling upgrade](#Rolling restart)期间。如果多个节点停止并且再次启动，Elasticsearch会自动的进行恢复，例如在[full cluster restart](#Full-cluster restart)期间。上述说到的场景中，不需要使用APIs来做出额外的措施，because the set of master nodes is not changing permanently。

#### Voting configurations
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-discovery-voting.html)

&emsp;&emsp;每一个Elasticsearch集群中都有一个voting configuration，通过[master-eligible nodes](#Master-eligible node)集合的投票计数来作出一些决策，例如选举出新的master或者提交一个新的集群状态。决策只有在voting configuration 中节点大多数都响应后（超过半数）可以通过。	

&emsp;&emsp;通常情况下voting configuration和当前集群中所有的master-eligible node集合相同。然而在某些情况下可能会不同。

> IMPORTANT：为了保证集群仍然可用（remain available），你必须不能在同一时间（at the same time）停止一半或者更多的voting configuration中的节点。只要超过一半的voting node是可用的，集群就能正常的工作。例如，如果有3个或者4个master-eligible node，集群可以容忍一个不可用的节点。如果2个或者更少的master-eligible node，它们必须都是可用的。

&emsp;&emsp;在某个节点加入或者离开集群时，Elasticsearch会自动的对voting configuration作出对应的变更来保证集群尽可能的具有弹性（resilient）。非常重要的一点是：你从集群中移除更多节点之前，要等待调整（adjustment）完成，见[Adding and removing nodes.](#Adding and removing nodes)。

&emsp;&emsp;当前的voting configuration存储在集群状态中，你可以按照以下的方式来查看当前的内容：

```text
GET /_cluster/state?filter_path=metadata.cluster_coordination.last_committed_config
```

> NOTE：当前的voting configuration不需要跟集群中的所有的可用的master-eligible node集合相同。改变（alert）voting configuration会涉及进行投票，所以节点加入或者离开集群时会花一点时间来调整configuration。同样的，也会存在most resilient configuration包含不可用的节点或者不包含可用的节点。在这种情况下，voting configuration就会跟集群中的master-eligible node不相同。

&emsp;&emsp;规模大（large）的voting configuration 通常更具有弹性。所以Elasticsearch更喜欢（prefer to）在master-eligible node加入到集群后把它们添加到voting configuration中。同样的，如果一个在voting configuration中的节点离开了集群并且集群中有另一个不在voting configuration中的master-eligible node，那么会去交换这两个节点。voting configuration的大小没有发生变化但是提高了弹性。

&emsp;&emsp;节点离开集群后，从voting configuration中移除节点不是一件简单的事情。不同的策略有不同的好处跟缺点，所以正确的选择取决于集群如何被使用。你可以通过[cluster.auto_shrink_voting_configuration setting](#cluster.auto_shrink_voting_configuration)来控制voting configuration是否要自动的收缩（shrink）。

> NOTE：如果`cluster.auto_shrink_voting_configuration`设置为`true`（默认值，推荐值）。那么集群中会至少有3个master-eligible node。Elasticsearch仍然可以处理集群状态的更新，只要除了其中一个maste-eligible node，其他的所有节点都是健康的。

&emsp;&emsp;在某些情况下，Elasticsearch能够容忍多个节点的丢失， but this is not guaranteed under all sequences of failures。如果`cluster.auto_shrink_voting_configuration`设置为`false`。你必须手动的去voting configuration中移除离开集群的节点（departed node）。使用[voting exclusions API ](#Voting configuration exclusions API)来达到想要的弹性等级。

&emsp;&emsp;无论怎么配置，Elasticsearch都不会受到裂脑导致不一致的困扰。`cluster.auto_shrink_voting_configuration`这个设置只会只会影响在一些节点发生故障后的可用性问题，以及影响节点离开/加入集群时 administrative task的执行。

##### Even numbers of master-eligible nodes

&emsp;&emsp;正常来说集群中应该有奇数个master-eligible node。如果是偶数个，Elasticsearch会将其中一个排除在voting configuration以外来保证奇数个。这种omission不会降低集群的容错能力。事实上还有略微提升：如果集群受到网络分区的影响，将其划分为大小相等的两部分，那么其中一部分将包含大多数voting configuration，并能够继续运行。如果所有的master-eligible node的投票都进行了统计，那么任何一方都不会包含严格多数的节点，因此集群将无法做任何处理（make any progress）。

&emsp;&emsp;例如，如果在集群中有4个master-eligible node并且voting configuration包含这四个节点。任何quorum-based的决策都要求至少三个投票。这意味着这个集群可以容忍单个master-eligible node的丢失。如果集群被两等分，没有一半可以获得三个master-eligible node，集群就不会做任何处理。如果voting configuration只有三个master-eligible node，集群仍然可以容忍丢失一个节点，但是quorum-based的决策要求3个投票节点中的2个。即使发生了分裂，其中一半会包含3个投票节点的2个节点，所以这一半仍然是可用的。

##### Setting the initial voting configuration

&emsp;&emsp;当一个全新的集群第一次启动，必须选出第一个master node。要完成这个选举，需要知道用于投票计数的master-eligible集合。这个初始化的voting configuration即`bootstrap configuration`并且在[cluster bootstrapping process](#Bootstrapping a cluster)中设置。

&emsp;&emsp;重要的一点是`bootstrap configuration`明确指定了在第一次选举中参与的节点。 It is not sufficient to configure each node with an expectation of how many nodes there should be in the cluster. It is also important to note that the bootstrap configuration must come from outside the cluster: there is no safe way for the cluster to determine the bootstrap configuration correctly on its own。

&emsp;&emsp;如果boostrap configuration没有正确设置，当你启动一个全新的集群时，会有形成两个不同集群的风险。这种情况会导致数据丢失：很有可能在你注意到这个问题前已经启动了2个集群并且不可能在随后合并这两个集群。

> NOTE：To illustrate the problem with configuring each node to expect a certain cluster size，想象一下启动一个三个节点的集群，每一个节点知道自己将成为三个节点集群中的一员。三个节点的大多数是2，所以正常情况下前两个启动的节点能发现对方并且形成一个集群，并且第三个节点会在随后很短的时间内加入它们。然而想象下如果错误当启动了四个节点而不是三个。在这种情况下，就会有足够的节点来形成两个集群。当然，如果每一个节点是手动启动的，那不大可能启动太多的节点。如果使用了自动化编排（orchestrator），这当然有可能会发生这种情况。特别是编排器不具备弹性（resilient）而导致网络分区的错误。

&emsp;&emsp;最初的quorum只有在整个集群最开始启动的时候需要。加入到一个已经建立好的集群的节点可以安全的从被选举为master节点上获取所有的信息。注意的是之前作为集群一部分的节点将在重新启动时会将所有需要的信息存储到磁盘中。

#### Bootstrapping a cluster
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-discovery-bootstrap-cluster.html)

&emsp;&emsp;最开始启动Elasticsearch时要求一个集群中的一个或者多个初始化的[master-eligible node](#Master-eligible node)集合，需要显示的定义，即`cluster bootstrapping`。仅要求在集群第一次启动时：已经加入到一个集群的节点会将信息存储在它们的数据目录中用于[full cluster restart](#Full cluster restart upgrade)并且新加入到集群的节点会从集群中被选举master的节点上获取信息。

&emsp;&emsp;最初的master-eligible node集合定义在[cluster.initial_master_nodes setting](#cluster.initial_master_nodes)中。配置中要包含每一个master-eligible node的下面其中一个条目：

- 节点的[node name](#Node name setting)
- 如果没有设置`node.name`则要求有节点的hostname。因为`node.name`默认就是节点的hostname。你可以使用fully-qualified hostname或者bare hostname，取决于[depending on your system configuration](#Node name formats must match)。
- 如果无法使用`node.name`，则使用节点的[transport publish address](#Binding and publishing)的IP地址。通常是[network.host](#Commonly used network settings)对应的IP地址，但是[this can be overridden](#Advanced network settings)
- 节点的publish address的IP地址跟端口号。如果无法使用`node.name`并且多个节点共享同一个IP地址，可以使用的格式为：`IP:PORT`。

&emsp;&emsp;在你启动一个master-eligible node时，你可以通过命令行或者`elasticsearch.yml`的方式提供这个设置。集群形成之后，将这个设置从每一个节点中移除。不能对master-**ineligible** node，已经加入集群的master-eligible node以及重启一个或者多个节点配置这个设置。

&emsp;&emsp;技术上来讲只在集群中的单个master-eligible node上设置`cluster.initial_master_nodes`就足够了并且只在设置中提及这个单个节点。但是会在集群完全形成前缺乏容错能力。因此最好使用三个master-eligible node进行引导，每一个节点中都配置了包含三个节点的`cluster.initial_master_nodes`。

> WARNING：你必须在每一个节点上设置完全相同的`cluster.initial_master_nodes`，使得在引导期间只会形成一个集群，避免数据丢失的风险。

&emsp;&emsp;对于拥有三个master-eligible node（[node name](#Node name setting)是 master-a, master-b and master-c）的集群，配置如下：

```text
cluster.initial_master_nodes:
  - master-a
  - master-b
  - master-c
```

&emsp;&emsp;跟其他的节点设置一样，也可以通过命令行的方式指定the initial set of master nodes 来启动Elasticsearch：

```text
bin/elasticsearch -E cluster.initial_master_nodes=master-a,master-b,master-c
```

##### Node name formats must match

&emsp;&emsp;`cluster.initial_master_nodes`列表中使用的node name必须和节点的属性`node.name`完全一样。默认情况下node name被设置为机器的hostname，这个值在你的系统配置中可能不是fully-qualified。如果每一个node name是fully-qualified domain name，例如`master-a.example.com`，那你在`cluster.initial_master_nodes`列表中也必须使用fully-qualified domain name。如果你的node name是bare hostname (without the .example.com suffix) ，那你在`cluster.initial_master_nodes`列表中也必须使用bare hostname。如果你同时使用了fully-qualified 和 bare hostnames，或者其他无法匹配node.name 和 cluster.initial_master_nodes的值，那集群无法成功形成并会看到下面的日志信息：

```text
[master-a.example.com] master not discovered yet, this node has
not previously joined a bootstrapped (v7+) cluster, and this
node must discover master-eligible nodes [master-a, master-b] to
bootstrap a cluster: have discovered [{master-b.example.com}{...
```

&emsp;&emsp;这个消息显示了node name：`master-a.example.com` 和 `master-b.example.com`以及`cluster.initial_master_nodes`中的值：`master-a`和`master-b`。日志中明显的指出它们无法完全匹配。

##### Choosing a cluster name

&emsp;&emsp;[cluster.name](#Cluster name setting)这个设置能让你创建多个彼此相互隔离的集群。节点之间在第一次相互连接时会验证集群名。Elasticsearch只会形成一个所有节点的集群名都相同的集群。默认值是`elasticsearch`，不过建议修改这个值来体现出集群的逻辑名称。

##### Auto-bootstrapping in development mode

&emsp;&emsp;默认情况下，每一个节点在第一次启动时会自动的引导（bootstrap）自己进入一个单节点的集群。如果配置了下面任意的一个设置，自动引导会被替代：

- `discovery.seed_providers`
- `discovery.seed_hosts`
- `cluster.initial_master_nodes`

&emsp;&emsp;若要添加一个节点到现有的一个集群中，那么配置`discovery.seed_hosts`或者其他相关的设置，这样新的节点就可以发现集群中现有的master-eligible节点。若要引导一个多节点的集群，配置上文中[section on cluster bootstrapping](#Bootstrapping a cluster)描述的`cluster.initial_master_nodes`。

###### Forming a single cluster

&emsp;&emsp;一旦Elasticsearch的节点加入到一个现有的集群，或者引导出一个集群后，他不会加入到其他的集群。Elasticsearch不会将两个不同的已经形成的集群进行合并，即使你随后尝试配置所有的节点到一个节点中。这是因为没法在不丢失数据的情况下合并不同的集群。你可以通过`GET /`在每一个节点上获取集群的UUID来检查形成的不同集群。

&emsp;&emsp;如果你想要添加一个节点到现有的集群中而不是引导出不同的单节点集群，那你必须：

1. 关闭节点
2. 通过删除[data folder](#path.data)的方式来完全擦除（wipe）节点
3. 配置`discovery.seed_hosts`或者 `discovery.seed_providers`以及其他相关的discover 设置
4. 重启节点并且验证下节点是否加入到集群而不是形成自己的单节点集群

&emsp;&emsp;如果你是要形成一个新的多节点集群而不是引导出多个单节点集群，那你必须

1. 关闭所有的节点
2. 通过删除每一个节点的[data folder](#path.data)的方式来完全擦除（wipe）所有的节点
3. 按照上文描述的方式来配置`cluster.initial_master_nodes`
4. 配置`discovery.seed_hosts`或者 `discovery.seed_providers`以及其他相关的discover 设置
5. 重启所有的节点并验证它们形成了单个集群

#### Publishing the cluster state
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cluster-state-publishing.html)

&emsp;&emsp;master node是集群中唯一可以对集群状态（cluster states）做出变更的节点。master node一次处理一批集群状态的更新。计算变更请求并更新集群状态到集群中的所有节点。每一次的发布开始于master向集群中所有的节点广播更新后的集群状态。每一个节点都响应确认，但未应用最新接收的状态。一旦master收集到足够的来自master-eligible node的确认，这个新的集群状态被认为是提交（commit）了，然后master node广播另一个消息让节点应用提交的集群状态。每一个节点收到这条消息后，应用更新后的状态，然后发送一个确认给master。

&emsp;&emsp;master允许在有限的时间内完成集群状态更改到发布给所有的节点。定义在`cluster.publish.timeout`设置中，默认值`30s`。从publication开始算时间。如果在新的集群状态提交前达到了这个时间，集群状态的变更会被reject然后master会认为自己出现了故障。然后开始新的选举。

&emsp;&emsp;如果在达到`cluster.publish.timeout`之前新的集群状态提交了，master会认为这次变更成功了。master会等待所有节点应用更新状态后的确认或者等待超时，然后开始处理并发布下一次的集群状态更新。如果master没有收到一些确认（比如一些节点还没有确认它们已经应用了当前的更新）这些节点被认为是它们的集群状态落后于master最新的状态。默认值是`90s`。如果仍然没有成功的应用集群状态更新，那么它会被认为发生了故障并且从集群中移除。

&emsp;&emsp;发布的集群状态的更新内容通常是跟上一次状态的差异，这样能减低时间以及网络带宽。例如为集群状态中部分索引的更新mappings时，只要节点上有上一次的集群状态，那么就可以只发布这些索引更新对应的差异到集群中的节点上。如果一个节点缺失了上一个集群状态，比如说重新加入到了集群，master会发送完整的集群状态到这个节点，使得这个节点后续能接收差异更新。

> NOTE：Elasticsearch是基于点对点的系统，节点之间相互直连。high-throughput的APIs（index，delete，Search）通常不会跟master node交互。master node负责全局的集群状态并且当节点加入或者离开时重新分配分片。每次集群状态发生变化后，最新的状态会被发布到集群中的节点上

#### Cluster fault detection
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cluster-fault-detection.html)

&emsp;&emsp;被选举为master的节点会周期性的检查集群中的每个节点来保证它们仍是连接的并且健康的。每一个节点同样周期性的检查master node的健康。这些检查分别被称为` follower checks`和`leader checks`。

&emsp;&emsp;Elasticsearch允许这些检查偶尔的出现失败或者超时的问题并且不采取任何行动。只有在连续多次检查（consecutive check）失败后才会认为这个节点发生了故障。你可以通过[cluster.fault_detection.\* settings](#Discovery and cluster formation settings)来控制故障检测的行为。

&emsp;&emsp;如果master node检测到某个节点失去了连接，这种情况会被视为立即失败（immediate failure）。master会绕过（bypass）超时和重试这些设置，并将这个节点从集群中移除。同样的，如果某个节点检测到master node失去了连接，这种情况也会被视为立即失败。这个节点会绕过（bypass）超时和重试这些设置，尝试discovery阶段来尝试找到/选举一个新的master。

&emsp;&emsp;另外，每一个节点会周期性的往data path中写入一个小文件然后删除它来验证data path是否健康。如果节点发现它的data path不健康，随后会从集群中移除直到data path恢复。你可以通过[monitor.fs.health settings](#Discovery and cluster formation settings)来控制这个行为。

&emsp;&emsp;如果某个节点在一个合理的时间内无法应用更新后的集群状态，master node也会将其从集群中移除。超时时间默认是2分钟，这个时间从更新集群状态开始。参考[Publishing the cluster state](#Publishing the cluster state)了解更多信息。

##### Troubleshooting an unstable cluster

&emsp;&emsp;一般情况下，节点只有在故意关闭（deliberately shutdown）时才离开集群。如果某个节点意外的（unexpectedly）离开集群，解决这个问题是非常重要的。集群中的节点意外的离开集群会造成集群不稳定并且会产生一些问题。比如说：

- 集群的健康可能是黄色或者红色
- 一些分片可能会初始化并且其他的分片可能会失败
- 查询，索引以及监控可能会失败并且在日志中抛出异常
- 索引`.security`可能会不可用，阻塞集群的访问
- master node可能会由于频繁的集群状态更新变的忙碌

&emsp;&emsp;若要解决集群中的问题，首先要保证集群中有一个[stable master](#Troubleshooting discovery)，优先于其他的问题，集中精力关注意外退出的节点。直到集群中有一个稳定的master node以及稳定的节点成员才有可能解决其他的问题。

&emsp;&emsp;诊断数据（Diagnostics）和统计数据在一个不稳定的集群中通常是没有参考价值的。这些工具只是实时的在某个时间给出集群状态的视图。应该查看一段时间内集群的行为模式（pattern of behavior）。特别要关注master node上的日志，当某个节点离开集群时，master node的日志中会有以下类似的信息（为了可读性添加了换行符）

```text
[2022-03-21T11:02:35,513][INFO ][o.e.c.s.MasterService    ]
    [instance-0000000000] node-left[
        {instance-0000000004}{bfcMDTiDRkietFb9v_di7w}{aNlyORLASam1ammv2DzYXA}{172.27.47.21}{172.27.47.21:19054}{m}
            reason: disconnected,
        {tiebreaker-0000000003}{UNw_RuazQCSBskWZV8ID_w}{bltyVOQ-RNu20OQfTHSLtA}{172.27.161.154}{172.27.161.154:19251}{mv}
            reason: disconnected
        ], term: 14, version: 1653415, ...
```

&emsp;&emsp;这个消息说的是master node(instance-0000000000)上的`MasterService`正在处理一个`node-left`的任务。并列出了被移除的节点以及移除的原因。其他的节点可能有类似的日志，但是细节较少：

```text
[2020-01-29T11:02:36,985][INFO ][o.e.c.s.ClusterApplierService]
    [instance-0000000001] removed {
        {instance-0000000004}{bfcMDTiDRkietFb9v_di7w}{aNlyORLASam1ammv2DzYXA}{172.27.47.21}{172.27.47.21:19054}{m}
        {tiebreaker-0000000003}{UNw_RuazQCSBskWZV8ID_w}{bltyVOQ-RNu20OQfTHSLtA}{172.27.161.154}{172.27.161.154:19251}{mv}
    }, term: 14, version: 1653415, reason: Publication{term=14, version=1653415}
```

&emsp;&emsp;集中关注master node上`MasterService`中抛出的日志，它包含更多的细节。如果你无法查看到来自`MasterService`的日志，检测下面的内容：

- 你正在查看的是被选举为master的节点上的日志
- 日志记录的时间段是正确的
- 日志等级需要`INFO`

&emsp;&emsp;节点在开始/结束 follow一个master node时会记录包含`master node changed`的日志，你可以使用这些信息来观察一段时间内的view of the state of the master 。

&emsp;&emsp;节点重启的会先离开集群然后再次加入到集群中。重新加入后，`MasterService`会记录它正在处理`node-join`的任务。你可以从master的日志中得知节点重启的信息，因为`node-join`相关日志描述的是节点的`joining after restart`。在较老的Elasticsearch版本中，你可以通过查看`node-left`和`node-join`中的"ephemeral" ID来判断节点的重启。每次节点的启动，这个 ephemeral ID各不相同。如果某个节点意外的重启，你需要查看这个节点的日志了解它为什么关闭了。

&emsp;&emsp;如果节点没有重启，你应该在`node-left`日志中查看节点离开集群（departure）的原因。有下面三个可能的原因：

- `disconnected`：master node到被移出集群的节点之间的连接关闭了
- `lagging`：master发布了集群状态的更新，但是被移出集群的节点没有在允许的时间内应用这次更新。默认情况下是2分钟。参考[Discovery and cluster formation settings](#Discovery and cluster formation settings)了解更多信息
- `followers check retry count exceeded`：master node往节点连续多次发送了健康检查，这些检查被reject或者超时了。默认情况下，每次的健康检查会在10秒后超时，Elasticsearch会在连续3次的健康检查失败后移除这个节点。参考[Discovery and cluster formation settings](#Discovery and cluster formation settings)了解更多信息

###### Diagnosing disconnected nodes

&emsp;&emsp;Elasticsearch被设计为运行在一个相对可靠的网络上。它在节点间打开一定数量的TCP端口并且期望这些连接一直保持打开。如果连接关闭，Elasticsearch会尝试重新连接。因此要限制偶尔的小故障（occasional blip）对集群的影响，即使是受到影响的节点暂时离开了集群。相反，重复丢失连接会严重影响运行。

&emsp;&emsp;集群中master node和其他节点之间的连接是特别重要的。被选举为master的节点从来不会自发地（spontaneous）关闭跟其他节点的outbound连接。同样的，一旦建立了连接，节点也不会自发地关闭outbound连接，除非节点关闭。

&emsp;&emsp;如果你看到因为`disconnected`的原因意外的（unexpected）关闭了。可能是Elasticsearch以外的原因导致了连接关闭。一个常见的问题是防火墙配置错误：错误的超时时间（improper timeout），或者策略跟Elasticsearch[不兼容](#Long-lived idle connections)。也有可能是由于其他的连接问题，比如由于硬件故障网络阻塞（network congestion）导致包的丢失。如果你是高级用户（advanced user），你可以配置下面的内容来获取更多关于网络异常的信息：

```text
logger.org.elasticsearch.transport.TcpTransport: DEBUG
logger.org.elasticsearch.xpack.core.security.transport.netty4.SecurityNetty4Transport: DEBUG
```

&emsp;&emsp;极端的情况下，你可能需要进行抓包并使用`tcpdump`对其进行分析，查看节点之间的是否出现丢包或者被网络上的其他设备reject。

###### Diagnosing lagging nodes

&emsp;&emsp;Elasticsearch需要每一个节点能合理的快速处理集群状态的更新。节点花费太长的时间来处理集群状态的更新对集群是不利的。master会以`lagging`的原因将节点移除。参考[Discovery and cluster formation settings](#Discovery and cluster formation settings)了解更多的信息。

&emsp;&emsp;通常导致lagging的原因是节点的性能问题。然而也有可能因为严重的网络延迟。若要排除（rule out）网络延迟，确保[合理配置了](#TCP retransmission timeout)`net.ipv4.tcp_retries2`。包含`warn threshold`的日志信息可能会提供更多的根因。

&emsp;&emsp;如果你是高级用户（advance user），你可以通过配置下面的内容获取更多节点被移除时的正在处理的信息：

```text
logger.org.elasticsearch.cluster.coordination.LagDetector: DEBUG
```

&emsp;&emsp;当开启这个日志后，Elasticsearch会尝试在发生故障的节点上运行[Nodes hot threads ](#Nodes hot threads API)并在master节点的日志里面记录运行结果。

###### Diagnosing follower check retry count exceeded nodes

&emsp;&emsp;Elasticsearch需要每一个节点要成功响应网络消息（network message）并且合理的快速响应。如果某个节点reject请求或者完全不响应请求，可能会对集群造成损坏。如果多个连续的检查都失败了，那么Elasticsearch会以`follower check retry count exceeded`的原因将节点移除。并且在`node-left`消息中指出多少个连续的不成功的检查以及有多少个请求超时了。参考[Discovery and cluster formation settings](#Discovery and cluster formation settings)了解更多的信息。

&emsp;&emsp;超时和失败（timeout and failure）可能是因为网络延迟或者节点的性能问题。确保[合理的配置](#TCP retransmission timeout)`net.ipv4.tcp_retries2`来消除可能导致这种不稳定的网络延迟。日志消息中包含`warn threshold`的日志可能会有导致这种不稳定的原因。

&emsp;&emsp;如果上一次检查失败并抛出了异常，那这个异常会被报出，通常会报出需要解决的问题。如果发生了超时，也许需要理解下成功的检查中涉及到的步骤顺序：

1. master上运行在线程`elasticsearch[master][scheduler][T#1]`的`FollowerChecker`告知`TransportService`往follower节点发送检查请求的消息。
2. master上运行在线程`elasticsearch[master][transport_worker][T#2]`的`TransportService`将检查请求消息传给操作系统。
3. master上的操作系统将消息转化成一个或者多个包通过网络发送出去。
4. 在master与follower之间的各种的路由，防火墙和设备转发这些包，这个过程中可能有fragmenting或者defragmenting
5. 在follower节点上的操作系统接收到包并通知Elasticsearch
6. follower上运行在线程`elasticsearch[follower][transport_worker][T#3]`的`TransportService`读取了包，然后重新构造并处理了检查请求。通常来说，检查很快就成功。如果是这样，线程马上构造一个响应并传递给操作系统
7. 如果检查没有马上成功（例如，最近一次选举开始了）
   a. follower上运行在线程`elasticsearch[follower][cluster_coordination][T#4]`的`FollowerChecker`处理这次请求。它开始构造响应并告诉`TransportService`向master发送响应。
   b. follower上运行在线程` elasticsearch[follower][transport_worker][T#3]`的`TransportService`将响应传给操作系统。
8.  follower上的操作系统将响应转化成一个或者多个包通过网络发送出去。
9.  在master与follower之间的各种的路由，防火墙和设备转发这些包，这个过程中可能有fragmenting或者defragmenting
10.  在master节点上的操作系统接受到包并通知Elasticsearch
11.  master上运行在线程`elasticsearch[master][transport_worker][T#2]`的`TransportService`读取了包，重新构造了检查响应，然后进行处理，只要这次检查没有超时。

&emsp;&emsp;存在很多的事情会延缓检查的完成并且导致它超时：

1. 将检查请求传递给`TransportService`后可能有long GC或者virtual machine pause。
2. 可能会长时间的等待`transport_worker`线程变成可用，或者在将检查请求传递给操作系统前可能有long GC或者virtual machine pause。
3. master上的系统错误（例如，a broken network card）可能会延缓通过网络发送消息，possibly indefinitely。
4. 一路上交互的设备可能会延缓，丢弃，或者破坏包。master上的操作系统会等待并根据`net.ipv4.tcp_retries2`重新发送（retransmit）没有确认的或者被破坏的包。我们建议[reducing this value](#TCP retransmission timeout)，因为默认值会导致很长的延迟。
5. follower上的系统错误（例如，a broken network card）可能会延缓通过网络发送的消息，possibly indefinitely。
6. 可能会长时间的等待`transport_worker`变成可用，或者在follower上处理请求的过程中可能有long GC或者virtual machine pause。
7. 可能会长时间的等待`cluster_coordination`变成可用，可能又要等待`transport_worker`变成可用。处理请求的过程中可能有long GC或者virtual machine pause。
8. follower上的系统错误（例如，a broken network card）可能会延缓通过网络发送的响应
9. 一路上交互的设备可能会延缓，丢弃，或者破坏包导致重发（retransmissions）、
10. master上的系统错误（例如，a broken network card）可能会延缓通过网络接收响应
11. 可能会长时间的等待`transport_worker`变成可用来处理响应，可能有long GC或者virtual machine pause。

&emsp;&emsp;若要确定follower check超时的原因。我们可以根据下面的内容来缩小下（narrow down）原因：

- GC pause会在GC日志中记录，Elasticsearch默认会emit这些信息。通常也可以根据main node日志中的`JvmMonitorService`。使用这些日志来确定是否是GC导致的延迟
- VM pause同样会影响在同一个host上的其他process。VM pause还会导致系统时钟中断，Elasticsearch将在其日志中报告这一点
- Packet capture可以展示系统层和网络层的错误。特别是当你同时捕获master和故障节点上的网络流量时。用于follower check的连接不用于任何其他流量，因此仅从流模式就可以很容易地识别出来，即使使用了TLS：几乎每秒钟都会有几百个字节发送到每个方向，首先是master的请求，然后是follower的响应。你应该能够观察到这种连接上的任何重传、数据包丢失或其他延迟。
- 长时间等待特定线程成为可用的这种问题可以通过跟踪stack dump（例如使用`jstack`）或者profiling trace（例如使用Java Flight Recorder）来观察节点离开集群前（departure）几秒的信息。[Nodes hot threads](#Nodes hot threads AP) API有时候也能产生一些有用的信息。但需要记住的是，这个API还需要许多`transport_worker`和`cluster_coordination`线程用于across集群中的节点。API可能会受到你正在诊断的问题的影响。`jstack`更可靠点因为他不要求任何JVM线程。涉及到follower check的线程是`transport_worker`和`cluster_coordination`。因此不应该长时间等待这两个线程。在Elasticsearch的日志中可能也有长时间等待线程的信息。参考[Networking threading model](#Networking)了解更多的信息。

### Add and remove nodes in your cluster
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/add-elasticsearch-nodes.html)

&emsp;&emsp;当你启动了一个Elasticsearch的实例就是启动了一个节点。Elasticsearch集群就是一组节点的集合，这些节点有相同的`cluster.name`的属性。节点加入或者离开节点，集群都会自动的识别并且最终将数据分布到可用的节点上。

&emsp;&emsp;如果你正在运行单个实例的Elasticsearch，你的集群中只有一个节点。所有的主分片在单个节点上。副本分片无法被分配，因此你的集群颜色是黄色的。集群有完整的功能但是在出现故障时有丢失数据的风险。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/elas_0202.png">

&emsp;&emsp;向集群中添加节点能提高承载力和可靠性（capacity and reliability）。默认情况下，一个节点既是data node并且又是一个有资格成为master的节点来控制集群。你也可以配置一个新的节点用于特定的目的，例如处理[ingest request](#Ingest pipelines)，见[Nodes](#Node)了解更多信息。

&emsp;&emsp;向集群中添加更多节点后，它会自动的分配副本分片。当所有的主分片和副本分片都可用后（active），集群状态会变更为绿色。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/elas_0204.png">

&emsp;&emsp;你可以在你的本地机器上运行多个节点来体验多个节点的集群的行为。若要添加一个节点到一个运行在你本地机器上的集群，你需要：

1. 启动一个新的Elasticsearch实例
2. 在`elasticsearch.yml`文件中的`cluster.name`配置中指定集群名称。例如若要将一个节点添加到名为`logging-prod`的集群中，那么在`elasticsearch.yml`中添加一行： cluster.name: "logging-prod" 

&emsp;&emsp;若要添加一个节点到运行在多个机器上的集群，你必须要[set discovery.seed_hosts](#discovery.seed_hosts)，使得新的节点可以发现集群中的其他节点。

&emsp;&emsp;更多discover和shard allocation的信息见[Discovery and cluster formation](#Discovery and cluster formation)和[Cluster-level shard allocation and routing settings](#Cluster-level shard allocation and routing settings)。

#### Master-eligible nodes

&emsp;&emsp;随着节点的添加和移除，Elasticsearch通过自动的更新集群的投票配置（cluster's voting configuration）来保持最佳的容错水平。通过[master-eligible nodes](#Master-eligible node)的投票计数来作出一些决策，例如选举出新的master或者提交一个新的集群状态。

&emsp;&emsp; 建议在集群中设置一个小规模的并且固定数量的master-eligible node，只通过添加或者移除master-ineligible node来扩大或者缩小集群。然而也有一些情况需要将一些master-eligible node 添加到集群，或者从集群中移除。

##### Adding master-eligible nodes

&emsp;&emsp;如果你想要在你的集群中添加一些节点，对新节点进行简单配置来找到现有的集群然后启动它们。如果这样做是合适的话（ if it is appropriate to do so），Elasticsearch会将新的节点添加到voting configuration中。

&emsp;&emsp;在master选举或者加入到一个现有的已经形成的集群中时，节点会发送一个join request到master，使得正式的添加到集群中。

##### Removing master-eligible nodes

&emsp;&emsp;在移除master-eligible node时，非常重要的一点是不要在同一时间移除太多的节点。例如如果当前有七个master-eligiable node，然后你想要降到三个。不是简单的马上将其中四个节点停止就行：这么做将只剩下三个节点，少于[voting configuration](#Voting configurations)中的一半，意味着集群不能再执行任何的动作。

&emsp;&emsp;更准确的说，如果在同一时间关闭一半或者更多的master-eligible node，集群通常会变成不可用。可以通过再次启动被移除的节点就可以让集群重新上线（back online）。

&emsp;&emsp;只要集群中至少有3个master-eligible node，作为一般的规则要一个接着一个的去移除节点，给于集群足够的时间自动的调整（[adjust](#Quorum-based decision making)）voting configuration，适应新的节点集合对应的容错级别。

&emsp;&emsp;如果只剩下两个master-eligible node，它们都不能被安全的移除因为它们都依靠对方才能进行可靠的运行。若要移除这些节点中的一员，需要通知Elasticsearch让这个节点不再是voting configuration中的一部分。投票权利应该给于其他的节点。这样你就可以让这个节点下线并且不会阻止其他节点的正常运行。排除在voting configuration之外的节点仍然可以正常工作，但是Elasticsearch会尝试将这个节点从voting configuration中移除并不再需要它的投票。重要的是，Elasticsearch不会自动的将排除在voting configuration之外的节点重新回到voting configuration中。一旦节点成功的自动配置为voting configuration之外的节点，它就可以安全的关闭并且不会影响集群master-level的可用性。可以通过[Voting configuration exclusions ](#Voting configuration exclusions API)将节点排除在voting configuration之外：

```text
# Add node to voting configuration exclusions list and wait for the system
# to auto-reconfigure the node out of the voting configuration up to the
# default timeout of 30 seconds
POST /_cluster/voting_config_exclusions?node_names=node_name

# Add node to voting configuration exclusions list and wait for
# auto-reconfiguration up to one minute
POST /_cluster/voting_config_exclusions?node_names=node_name&timeout=1m
```


### Full-cluster restart and rolling restart
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/restart-cluster.html)

&emsp;&emsp;可能会存在一些情况[where you want to perform a full-cluster restart ](#Encrypt internode communication-1)或者rolling restart。对于[full-cluster restart](#Full-cluster restart)，需要关闭并重启集群中所有的节点，而对于[rolling restart](#Rolling restart)，则是需要在某一时间只关闭一个节点，这样提供的服务就不会停止。

#### Full-cluster restart

1. **Disable shard allocation**

&emsp;&emsp;当你关闭了一个data node，allocation process将位于该节点上的主分片复制到其他节点前会等待`index.unassigned.node_left.delayed_timeout`（默认一分钟），因为这会占用很多I/O。由于节点很快就要重启，那这个I/O是没有必要的。你可以在关闭[data node](#Data node)前[disabling allocation of replica](#Cluster-level shard allocation and routing settings)，免得在超时前没有重启好节点。

```text
PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.enable": "primaries"
  }
}
```

2. **Stop indexing and perform a flush**

&emsp;&emsp;执行[flush](#Flush API)加快分片恢复。

```text
POST /_flush
```

&emsp;&emsp;2.1 **Temporarily stop the tasks associated with active machine learning jobs and datafeeds**（Optional）

&emsp;&emsp;Machine leanring功能需要[subscription](https://www.elastic.co/cn/subscriptions)。

&emsp;&emsp;当你关闭一个集群时，你有两个选项来处理machine learning jobs和datafeeds。

- 临时暂停跟你的machine learning jobs和datafeeds相关的任务，使用[set upgrade mode API](#Set upgrade mode API)防止打开新的任务:

```text
POST _ml/set_upgrade_mode?enabled=true
```

- 当你关闭了upgrade mode，会使用上一次自动保存的模型状态（model state）来恢复任务。这个选项可以避免在关机期间管理活跃的任务（active job）的开销，并且快于显示的停止datafeeds和关闭任务jobs。

- [Stop all datafeeds and close all jobs](https://www.elastic.co/guide/en/machine-learning/8.2/stopping-ml.html)。这个选项会在关闭时保存模型状态。当集群重启后，你重新打开jobs时，它们能使用完全相同的模型。然而相较于使用upgrade mode，保存最新的模型状态会需要更长的时间，特别是你拥有很多的jobs或者说jobs有很大的模型状态。

&emsp;&emsp;2.2 **Shut down all nodes**

- 如果你使用`systemd`运行Elasticsearch：

```text
sudo systemctl stop elasticsearch.service
```

- 如果你使用SysV `init`允许Elasticsearch：

```text
sudo -i service elasticsearch stop
```

- 如果将Elasticsearch作为一个后台应用运行：

```text
kill $(cat pid)
```

3. **Perform any needed changes**
4. **Restart nodes**

&emsp;&emsp;如果你有专用的master node，那可以在处理你的data node前，先启动它们，然后等待它们形成一个集群并选举出一个master。你可以通过日志查看处理进程。

&emsp;&emsp;一旦有足够的master-eligible node相互的发现了对方，它们就会形成一个集群并且选举出master。在这个时候，你可以使用[cat health](#cat health API)和[cat nodes](#cat nodes API)来监控加入到集群的节点。

```text
GET _cat/health

GET _cat/nodes
```

&emsp;&emsp;`_cat/health`返回的`status`列显示了集群中每一个节点的状态: `red`，`yellow`或者`green`。

5. **Wait for all nodes to join the cluster and report a status of yellow**

&emsp;&emsp;当一个节点加入到集群，它开始恢复存储在本地的所有的主分片。[\_cat/health](#cat health API) API最开始会报出值为`red`的`status`，意思是还有主分片没有被分配。

&emsp;&emsp;某个节点一旦恢复了本地的分片，`status`会切换到`yellow`，意思是所有的主分片已经恢复，但不意味着所有的副本分片已经分配好。这是意料之中的事，因为你还没有重新开启副本分片的分配。延迟副本分片的分片直到所有的节点的`status`为`yellow`，使得master能将副本分片分配到那些已经有本地分片拷贝的节点上。

6. **Re-enable allocation**

&emsp;&emsp;当所有的节点加入到集群后，并且恢复好了它们的主分片后，将`cluster.routing.allocation.enable`恢复到它的默认值来重新开启分配：

```text
PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.enable": null
  }
}
```

&emsp;&emsp;一旦重新开启了分配，集群开始将副本分片分配到data node。在这个时间点恢复索引和查询是安全的，但如果你能等待所有的分片和副本分片成功的分配结束并且所有节点的状态都是`green`，那你的集群能更快的恢复。

&emsp;&emsp;你可以使用[\_cat/health ](#cat health API)和[\_cat/recovery](#cat recovery API) APIs来监控处理进程：

```text
GET _cat/health

GET _cat/recovery
```

7. **Restart machine learning jobs**（Optional）

&emsp;&emsp;如果你临时暂停了machine learning jobs相关的任务，使用[set upgrade mode API](#Set upgrade mode API)将它们返回到活跃状态：

```text
POST _ml/set_upgrade_mode?enabled=false
```

&emsp;&emsp;如果你在停止节点前关闭了所有的machine learning jobs，可以在Kibana中打开jobs并开始datafeed或者使用[open jobs](#Open anomaly detection jobs API)和[start datafeed ](#Start datafeeds API) APIs。

#### Rolling restart

1. **Disable shard allocation**

&emsp;&emsp;当你关闭了一个data node，allocation process将该节点上的主分片复制到其他节点前会等待`index.unassigned.node_left.delayed_timeout`（默认一分钟），因为这会占用很多I/O。由于节点很快就要重启，那这个I/O是没有必要的。你可以在关闭[data node](#Data node)前[disabling allocation of replica](#Cluster-level shard allocation and routing settings)，免得在超时前没有重启好节点。

```text
PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.enable": "primaries"
  }
}
```

2. **Stop non-essential indexing and perform a flush**（Optional）

&emsp;&emsp;尽管你可以在rolling restart期间继续执行索引操作，但如果你临时的停止没有必要的索引操作并执行[flush](#Flush API)可以让分片恢复的更快

```text
POST /_flush
```

3. **Temporarily stop the tasks associated with active machine learning jobs and datafeeds** （Optional）

&emsp;&emsp;Machine leanring功能需要[subscription](https://www.elastic.co/cn/subscriptions)。

&emsp;&emsp;当你关闭一个集群时，你有两个选项来处理machine learning jobs和datafeeds。

- 临时暂停跟你的machine learning jobs和datafeeds相关的任务，使用[set upgrade mode API](#Set upgrade mode API)防止打开新的任务:

```text
POST _ml/set_upgrade_mode?enabled=true
```

- 当你关闭了upgrade mode，会使用上一次自动保存的模型状态（model state）来恢复任务。这个选项可以避免在关机期间管理活跃的任务（active job）的开销，并且快于显示的停止datafeeds和关闭任务jobs。

- [Stop all datafeeds and close all jobs](https://www.elastic.co/guide/en/machine-learning/8.2/stopping-ml.html)。这个选项会在关闭时保存模型状态。当集群重启后，在你重新打开jobs时，它们能使用完全相同的模型。然而相较于使用upgrade mode，保存最新的模型状态会需要更长的时间，特别是你拥有很多的jobs或者说jobs有很大的模型状态。

- 如果你执行了一个rolling restart，你也可以让machining learning jobs继续运行。当你关闭一个machine leaning node时，它的jobs会自动的移动到其他节点并恢复模型状态（model state）。这个选项可以让你的jobs继续运行但是会增加集群中的负载。

4. **Shut down a single node in case of rolling restart**

- 如果你使用`systemd`运行Elasticsearch：

```text
sudo systemctl stop elasticsearch.service
```

- 如果你使用SysV `init`允许Elasticsearch：

```text
sudo -i service elasticsearch stop
```

- 如果将Elasticsearch作为一个后台应用运行：

```text
kill $(cat pid)
```

5. **Perform any needed changes**
6. **Restart the node you changed**

&emsp;&emsp;启动节点后并通过查看日志文件或者提交一个`_cat/nodes`请求来确认该节点加入到了集群：

```text
GET _cat/nodes
```

7. **Reenable shard allocation**

&emsp;&emsp;对于data node，一旦它加入到集群后，就可以移除`cluster.routing.allocation.enable`这个设置来开启分片分配并开始使用这个节点：

```text
PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.enable": null
  }
}
```

8. **Repeat in case of rolling restart**

&emsp;&emsp;当节点恢复后并且集群已经稳定，对每一个需要变更的节点重复这些步骤。

9. **Restart machine learning jobs**（Optional）

&emsp;&emsp;如果你临时暂停了machine learning jobs相关的任务，使用[set upgrade mode API](# Set upgrade mode API)将它们返回到活跃状态：

```text
POST _ml/set_upgrade_mode?enabled=false
```

&emsp;&emsp;如果你在停止节点前关闭了所有的machine learning jobs，可以在Kibana中打开jobs并开始datafeed或者使用[open jobs](#Open anomaly detection jobs API)和[start datafeed ](#Start datafeeds API) APIs。

### Remote clusters
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/remote-clusters.html)

&emsp;&emsp;你可以将本地集群（local cluster）与其他Elasticsearch集群进行连接，即remote cluster。remote cluster可以位于不同的数据中心或者地理区域，其包含的index和data stream可以被CCR（cross-cluster replication）用于复制或者在一个本地集群使用CCS（cross-cluster search）。

&emsp;&emsp;在[CCR](#Cross-cluster replication)中，提取（ingest）的是remote cluster上的索引数据。`leader index`可以被复制（replicate）到你本地集群的一个或者多个只读的follower index上。使用CCR创建一个多集群可以提供灾备（discovery recovery），让数据的地理位置靠近你的用户，或者建立一个中心化的集群用于出报表。

&emsp;&emsp;[Cross-cluster search](#Search across clusters)能让你对一个或者多个remote cluster运行一个查询请求。这个能力可以让每一个区域都有一个全局视图的集群。允许你从本地集群执行单个查询请求，并从所有连接的remote cluster中返回数据。

&emsp;&emsp;开启并配置security对本地集群和remote cluster都是很重要的。当本地集群连接到一个remote cluster，本地集群上Elasticsearch的superuser能获取remote cluster上完整的读取权限。为了能安全的使用CCR和CCS，在所有连接的节点上开启[enable security](#Configure remote clusters with security)并且至少在每一个节点的transport level配置 Transport Layer Security (TLS) 。

&emsp;&emsp;此外，操作系统级别的本地管理员如果能够充分访问Elasticsearch配置文件和私钥，就有可能接管remote cluster。确保你的安全策略中包括在操作系统级别保护本地和remote cluster。

&emsp;&emsp;若要注册一个remote cluster，使用[sniff mode](#Sniff mode) 或[proxy mode ](#Proxy mode)[connect the local cluster](#Connect to remote clusters)和remote cluster中的节点。注册完remote cluster后，为CCR和CCS [configure privileges](#Configure roles and users for remote clusters)。

##### Sniff mode

&emsp;&emsp;在sniff mode中，使用名称和seed node list来创建集群。当注册了一个remote cluster后，它的cluster state从seed node中获取并且最多三个`gateway nodes`作为remote cluster requests的一部分。这个模式要求gateway node 的publish address能被本地集群访问到。

&emsp;&emsp;Sniff模式是默认的连接模式。

&emsp;&emsp;`gateway node`的选择取决于下面的标准：

- Version：remote node必须和注册的集群兼容：
  - 相同主版本（major version）的节点可以相互连接。例如7.0可以跟任何7.x节点通信
  - 只有主版本的最后一个小版本可以跟下一个主版本通信。在6.x系列，6.8可以跟任意的7.x的节点通信。同时6.7只可以跟7.0通信
  - 版本兼容性是对称的，意味着如果6.7可以跟7.0通信，那么7.0也可以跟6.7通信。下面的表描述（depict）了本地集群跟remote cluster之间的兼容性

| **Remote cluster** | 5.0–5.5 | 5.6  | 6.0–6.6 | 6.7  | 6.8  | 7.0  | 7.1–7.16 | 7.17 | 8.0–8.2 |
| :----------------: | :-----: | :--: | :-----: | :--: | :--: | :--: | :------: | :--: | :-----: |
|      5.0–5.5       |    √    |  √   |    ×    |  ×   |  ×   |  ×   |    ×     |  ×   |    ×    |
|        5.6         |    √    |  √   |    √    |  √   |  √   |  ×   |    ×     |  ×   |    ×    |
|      6.0–6.6       |    ×    |  √   |    √    |  √   |  √   |  ×   |    ×     |  ×   |    ×    |
|        6.7         |    ×    |  √   |    √    |  √   |  √   |  √   |    ×     |  ×   |    ×    |
|        6.8         |    ×    |  √   |    √    |  √   |  √   |  √   |    √     |  √   |    ×    |
|        7.0         |    ×    |  ×   |    ×    |  √   |  √   |  √   |    √     |  √   |    ×    |
|      7.1–7.16      |    ×    |  ×   |    ×    |  ×   |  √   |  √   |    √     |  √   |    ×    |
|        7.17        |    ×    |  ×   |    ×    |  ×   |  √   |  √   |    √     |  √   |    √    |
|      8.0–8.2       |    ×    |  ×   |    ×    |  ×   |  ×   |  ×   |    ×     |  √   |    √    |


> IMPORTANT：Elastic仅支持在这些配置的一个子集上进行跨集群搜索，见[Supported cross-cluster search configurations](#Supported cross-cluster search configurations)。

- Role：默认情况下，任何non-[master-eligible](#Master-eligible node) node 都可以作为一个gateway node。专用的master node永远不会被选为gateway node
- attributes：你可以通过设置[cluster.remote.node.attr.gateway](#Sniff mode remote cluster settings)设置为`true`来定义gateway node。然而，这样的节点仍然需要上面的要求

##### Proxy mode

&emsp;&emsp;在proxy模式中，使用一个名称和单个代理地址来创建一个集群。当你注册了一个remote cluster，数量可配置的socket 连接对代理地址开放。这个proxy要求将这些连接路由到remote cluster。Proxy mode不要求remote cluster node‘有用于访问的publish address。

&emsp;&emsp;proxy模式不是默认的连接模式并且必须要配置。Proxy mode跟sniff mode有相同的版本兼容要求。

> IMPORTANT：Elastic仅支持在这些配置的一个子集上进行跨集群搜索，见[Supported cross-cluster search configurations](#Supported cross-cluster search configurations)。

#### Configure remote clusters with security
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/remote-clusters-security.html)

#### Connect to remote clusters
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/remote-clusters-connect.html)

&emsp;&emsp;本地集群使用[ transport interface](#Networking)建立于remote cluster的连接。本地集群中的coordinating node会跟remote cluster中指定的节点建立[long-lived](#Long-lived idle connections) TCP连接。Elasticsearch要求这些连接保持打开，即使连接空闲（idle）了很长一段时间。

> NOTE：你必须要有`manage` cluster privilege用于跟remote cluster进行连接

&emsp;&emsp;若要在Kibana的Stack Management 中添加一个remote cluster：

1. 从侧边导航栏选择**Remote Clusters**
2. 指定remote cluster（Cluster A）的Elasticsearch endpoint URL，或者IP地址，hostname以及transport端口（默认值`9300`）。例如`cluster.es.eastus2.staging.azure.foundit.no:9400`或者`192.168.1.1:9300`

&emsp;&emsp;或者使用[cluster update settings API ](#Cluster update settings API)添加一个remote cluster。你也可以为本地集群中的每一个node [dynamically configure](########## Dynamically configure remote clusters) remote cluster。若要对本地集群中不同的节点配置remote cluster，那么在每一个节点的`elasticsearch.yml`中定义[static settings](#Statically configure remote clusters)。

&emsp;&emsp;连接remote cluster之后，[configure roles and users for remote clusters](#Configure roles and users for remote clusters)。

&emsp;&emsp;下面的请求添加了一个名为`cluster_one`的remote cluster，`cluster alias`是一个唯一标识符代表连接的remote cluster用于区分本地和远程的索引。

```text
PUT /_cluster/settings
{
  "persistent" : {
    "cluster" : {
      "remote" : {
        "cluster_one" : {    
          "seeds" : [
            "127.0.0.1:9300" 
          ]
        }
      }
    }
  }
}
```

&emsp;&emsp;第6行，这个remote cluster的`cluster alias`是`cluster_one`·
&emsp;&emsp;第8行，指定remote cluster中seed node 的hostname跟transport port

&emsp;&emsp;你可以使用[remote cluster info API ](#Remote cluster info API)来验证本地集群是否于remote cluster连接成功。

```text
GET /_remote/info
```

&emsp;&emsp;API的响应指出本地集群跟`cluster alias`为`cluster_one`的集群连接成功：

```text
{
  "cluster_one" : {
    "seeds" : [
      "127.0.0.1:9300"
    ],
    "connected" : true,
    "num_nodes_connected" : 1,  
    "max_connections_per_cluster" : 3,
    "initial_connect_timeout" : "30s",
    "skip_unavailable" : false, 
    "mode" : "sniff"
  }
}
```

&emsp;&emsp;第7行，在remote cluster中，跟本地集群连接的节点数量
&emsp;&emsp;第10行，如果通过CCS进行查询时节点不可用，是否要跳过这个集群

##### Dynamically configure remote clusters

&emsp;&emsp;使用[cluster update settings API](#Cluster update settings API)动态的配置集群中每一个节点上的remote 设置。下面的请求增加了三个remote cluster：`cluster_one`，`cluster_two`，`cluster_three`。

&emsp;&emsp;参数`seed`指定了remote cluster中seed node的hostname和[transport port](#Advanced transport settings)（默认值`9300`）

&emsp;&emsp;参数`mode`决定了连接模式，默认值是[sniff](#Sniff mode)。因为`cluster_one`没有指定`mode`，所以使用默认值。`cluster_two`和`cluster_three`都显示的使用了不同的模式：

```text
PUT _cluster/settings
{
  "persistent": {
    "cluster": {
      "remote": {
        "cluster_one": {
          "seeds": [
            "127.0.0.1:9300"
          ]
        },
        "cluster_two": {
          "mode": "sniff",
          "seeds": [
            "127.0.0.1:9301"
          ],
          "transport.compress": true,
          "skip_unavailable": true
        },
        "cluster_three": {
          "mode": "proxy",
          "proxy_address": "127.0.0.1:9302"
        }
      }
    }
  }
}
```

&emsp;&emsp;你可以在初始化配置后动态的为一个remote cluster更新。下面的请求为`cluster_two`更新了compress settings，为`cluster_three`更新 了compress settings 以及ping schedule settings

> NOTE：当compression和ping schedule settings更改后，所有现有的节点连接必须关闭并重新打开，可能会导致正在运行的请求失败。

```text
PUT _cluster/settings
{
  "persistent": {
    "cluster": {
      "remote": {
        "cluster_two": {
          "transport.compress": false
        },
        "cluster_three": {
          "transport.compress": true,
          "transport.ping_schedule": "60s"
        }
      }
    }
  }
}
```

&emsp;&emsp;你可以通过将每一个remote cluster settings的值赋值为`null`的方式将一个remote cluster从一个cluster settings中删除。下面的请求将`cluster_two`从cluster settings中移除，留下了`cluster_one`和`cluster_three`：

```text
PUT _cluster/settings
{
  "persistent": {
    "cluster": {
      "remote": {
        "cluster_two": {
          "mode": null,
          "seeds": null,
          "skip_unavailable": null,
          "transport.compress": null
        }
      }
    }
  }
}
```

##### Statically configure remote clusters

&emsp;&emsp;如果你在`elasticsearch.yml`中指定了设置，带有这些设置的节点才能连接remote cluster并且服务remote cluster的请求。

> NOTE：对于每一个节点，使用[cluster update settings API](#Cluster update settings API)指定的cluster settings优先在`elasticsearch.yml`中的设置

&emsp;&emsp;在下面的例子中，`cluster_one`, `cluster_two`, 和`cluster_three`是随意的（arbitrary）取的名称（aliases）用来代表连接的集群。这些名称随后用于区分本地和远程的索引。

```text
cluster:
    remote:
        cluster_one:
            seeds: 127.0.0.1:9300
        cluster_two:
            mode: sniff
            seeds: 127.0.0.1:9301
            transport.compress: true      
            skip_unavailable: true        
        cluster_three:
            mode: proxy
            proxy_address: 127.0.0.1:9302 
```

&emsp;&emsp;第8行，`cluster_two`中显示的开启了Compression
&emsp;&emsp;第9行，`cluster_two`中失去连接的remote cluster是可选的
&emsp;&emsp;第12行，`cluster_three`中用于连接的代理地址

#### Configure roles and users for remote clusters
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/remote-clusters-privileges.html#remote-clusters-privileges-ccs)

&emsp;&emsp;在[connecting remote clusters](#Connect to remote clusters)之后，你需要同时在本地和远程集群中创建一个用户角色（user role）并且赋予必要的privilege。这些角色要求可以使用CCR（cross-cluster replication）和CCS（cross-cluster search）。

> IMPORTANT：你必须在本地和远程集群使用相同的角色。例如，下面用于CCR的配置在本地和远程集群上使用了`remote-replication`角色名。然而，你可以在每一个集群上指定不同的角色定义。

&emsp;&emsp;你可以用Kibana的Stack Management，从侧边导航栏中选择**Security > Roles**来管理用户和角色。你也可以使用[role management APIs](#Role mappings)动态的添加，更新，移除并且检索角色。当你使用APIs在`native` realm中管理角色时，角色存储在内部的Elasticsearch index中。

&emsp;&emsp;下面的请求使用了[create or update roles API](#Create or update roles API)。你必须至少有`manage_security` 的cluster privilege来使用这个API。

##### Configure privileges for cross-cluster replication

&emsp;&emsp;CCR用户在本地集群和remote cluster上要求有不同的 index和cluster privilege。使用下面的请求分别在本地集群和remote cluster上创建不同的角色，然后创建一个用户拥有这些角色。

###### Remote cluster

&emsp;&emsp;在包含leader index的remote cluster上，CCR角色需要`read_ccr`的cluster privilege，以及在leader index上的`monitor`和`read` privilege

> NOTE：如果请求导致了[on behalf of other users](#Submitting requests on behalf of other users)，那么用户在remote cluster上必须要有`run_as`的privilege。

&emsp;&emsp;下面的请求在remote cluster上创建了一个`remote-replication`的角色：

```text
POST /_security/role/remote-replication
{
  "cluster": [
    "read_ccr"
  ],
  "indices": [
    {
      "names": [
        "leader-index-name"
      ],
      "privileges": [
        "monitor",
        "read"
      ]
    }
  ]
}
```

###### Local cluster

&emsp;&emsp;在包含follower index的本地集群上，`remote-replication` 角色需要`manage_ccr`的cluster privilege，follower index的`monitor`，`read`，`write`，和`manage_follow_index` privilege。

&emsp;&emsp;下面的请求在本地集群上创建了一个`remote-replication`的角色：

```text
POST /_security/role/remote-replication
{
  "cluster": [
    "manage_ccr"
  ],
  "indices": [
    {
      "names": [
        "follower-index-name"
      ],
      "privileges": [
        "monitor",
        "read",
        "write",
        "manage_follow_index"
      ]
    }
  ]
}
```

&emsp;&emsp;在每一个集群上创建完`remote-replication`的角色后，使用[create or update users API](#Create or update users API)在本地集群上创建一个用户，并赋予`remote-replication`的角色。例如，下面的请求赋予一个名为`cross-cluster-user`的用户`remote-replication`的角色：

```text
POST /_security/user/cross-cluster-user
{
  "password" : "l0ng-r4nd0m-p@ssw0rd",
  "roles" : [ "remote-replication" ]
}
```

> NOTE：在本地集群上你只需要创建这个用户。

&emsp;&emsp;你可以随后[configure cross-cluster replication](#Tutorial: Set up cross-cluster replication)来跨中心复制你的数据。

##### Configure privileges for cross-cluster search

&emsp;&emsp;CCS用户在本地集群和remote cluster上要求有不同的 index和cluster privilege。使用下面的请求分别在本地集群和remote cluster上创建不同的角色，然后创建一个用户拥有这些角色。

###### Remote cluster

&emsp;&emsp;在remote cluster上，CCS角色要求为目标索引有`read`和`read_cross_cluster` privilege。

> NOTE：如果请求导致了[on behalf of other users](#Submitting requests on behalf of other users)，那么用户在remote cluster上必须要有`run_as`的privilege。

&emsp;&emsp;下面的请求在remote cluster上创建了一个`remote-search`的角色：

```text
POST /_security/role/remote-search
{
  "indices": [
    {
      "names": [
        "target-indices"
      ],
      "privileges": [
        "read",
        "read_cross_cluster"
      ]
    }
  ]
}
```

###### Local cluster

&emsp;&emsp;在本地集群上，集群用于初始化CCS，用户只需要`remote-search`角色。role privilege可以是空。

&emsp;&emsp;下面的请求在本地集群上创建了一个`remote-search`角色：

```text
POST /_security/user/cross-search-user
{
  "password" : "l0ng-r4nd0m-p@ssw0rd",
  "roles" : [ "remote-search" ]
}
```

> NOTE：在本地集群上你只需要创建这个用户。

&emsp;&emsp;有`remote-search`角色的用户随后可以[search across clusters](#Search across clusters)。

##### Configure privileges for cross-cluster search and Kibana

&emsp;&emsp;当使用Kibana跨多个集群搜索时，一个两步骤的赋权过程决定了用户是否可以访问remote cluster上的data streams和index：

- 首先，本地集群确定了用户是否被授权访问remote cluster。本地集群就是Kibana连接的那个集群
- 如果用户有权限，remote cluster随后确定用户是否能访问指定的data stream和index

&emsp;&emsp;为了保证Kibana用户能访问remote cluster，赋予它们一个在remote cluster上指定索引的 read privilege。你可以在remote cluster中指定data streams和index：`<remote_cluster_name>:<target>`。

&emsp;&emsp;例如，你可能将Logstash data索引到本地集群并且周期性的将older time-based的索引归档到你的remote cluster上。你想要在这两个集群上进行搜索，那你必须在两个集群上同时enable Kibana users。

###### Local cluster

&emsp;&emsp;在本地集群上，创建一个`logstash-reader`的角色，然后在`logstash-*`上授予（grant）`read`和`view_index_metadata`的privilege。

> NOTE：如果你将本地集群配置为其他集群的remote cluster，你本地集群上的`logstash-reader`角色同样需要赋予`read_cross_cluster` privilege。

```text
POST /_security/role/logstash-reader
{
  "indices": [
    {
      "names": [
        "logstash-*"
        ],
        "privileges": [
          "read",
          "view_index_metadata"
          ]
    }
  ]
}
```

&emsp;&emsp;给你的Kibana用户赋予一个角色能[access to Kibana](https://www.elastic.co/guide/en/kibana/8.2/xpack-security-authorization.html)以及`logstash_reader`角色。例如，下面的请求创建了`cross-cluster-kibana`用户并赋予`kibana-access`和`logstash-reader`的角色。

```text
PUT /_security/user/cross-cluster-kibana
{
  "password" : "l0ng-r4nd0m-p@ssw0rd",
  "roles" : [
    "logstash-reader",
    "kibana-access"
    ]
}
```

###### Remote cluster

&emsp;&emsp;在remote cluster上，创建一个`logstash-reader`并赋予`read_cross_cluster`的privilege以及`logstash-*`索引的`read`和`view_index_metadata`的privilege。

```text
POST /_security/role/logstash-reader
{
  "indices": [
    {
      "names": [
        "logstash-*"
        ],
        "privileges": [
          "read_cross_cluster",
          "read",
          "view_index_metadata"
          ]
    }
  ]
}
```

#### Remote cluster settings

##### Sniff mode remote cluster settings


## Index Modules
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-modules.html)

&emsp;&emsp;Index Modules是用于索引创建的模块，控制与索引相关的所有方面。

#### Index Settings

&emsp;&emsp;索引层的设置（index level settings）可以根据每一个索引进行配置，索引配置可以划分为：

##### static

&emsp;&emsp;只能在索引创建期间或者对一个[closed index](#Open-index API)进行设置。

##### dynamic（index modules）

&emsp;&emsp;使用[update-index-settings](#Update-index-settings-API)对live index的索引配置进行变更。

```text
在一个关闭的索引上更改静态或者动态的索引设置可能会导致错误的设置，这些设置可能会更改（rectify）或者重建（recreating）索引（不包含被删除的数据）
```

#### Static index settings

&emsp;&emsp;下面是所有的静态索引设置，这些设置跟任何特定的索引模块都没有关联（associate）：

##### index.number_of_shards

&emsp;&emsp;一个索引应该拥有的主分片数量。默认值是1。这个设置只能在索引创建期间设置。不能对关闭的索引进行设置。

```text
每个索引的分片的数量上限是1024。这个限制值是一个安全限制，它基于资源分配防止索引的创建导致集群的不稳定（destabilize）。这个限制可以通过对集群中所有节点的系统属性（system property）export ES_JAVA_OPTS="-Des.index.max_number_of_shards=128"进行变更
```

##### index.number_of_routing_shards

&emsp;&emsp;整数值，跟`index.number_of_shards`一起用来将文档路由到主分片中，见[\_routing field](#_routing-field)。

&emsp;&emsp;Elasticsearch在对索引进行[split](#Split an index)时会使用这个值。例如，一个拥有5个分片，并且`index.number_of_routing_shards`的值设置为30时，那么每个分片可以按照因子2或者3进行划分，换句话说，可以按照下面的方式进行划分：

- 5 -> 10 -> 30 （5个分片中的每个分片先分为2份，然后10个分片的每个分片再分为3份）
- 5 -> 15 -> 30（5个分片中的每个分片先分为3份，然后15个分片中的每个分片再分为2份）
- 5 -> 30（5个分片中的每个分片分为6份）

&emsp;&emsp;默认值取决于索引的主分片数量，默认情况下根据切分因子2进行切分，并且最多切分出1024个分片。

```text
在Elasticsearch 7.0及以后的版本中，该值会影响文档分配到分片中。当使用自定义的路由规则来重新写入索引reindexing时，你必须显示指定index.number_of_routing_shards来维护文档的分配。见related breaking change（https://www.elastic.co/guide/en/elasticsearch/reference/7.0/breaking-changes-7.0.html#_document_distribution_changes）。
```

##### index.codec

&emsp;&emsp;默认使用[LZ4](https://www.amazingkoala.com.cn/Lucene/yasuocunchu/2019/0226/LZ4)对store data进行压缩，但是可以设置为`best_compression`，其使用[DEFLATE](https://en.wikipedia.org/wiki/Deflate)有更高压缩率，代价是降低了存储字段[stored filed](https://www.amazingkoala.com.cn/Lucene/suoyinwenjian/2020/1013/索引文件之fdx&&fdt&&fdm)的性能。如果你更新了压缩类型compression type，新的数据在段合并后会被应用（apply）。可以使用[force merge](#Force merge API)对段进行强制合并。

##### index.routing_partition_size

&emsp;&emsp;使用自定义的[路由值](#Routing to an index partition)，路由到一定数量的分片中（区别于路由到某一个分片，这里指的是可以被路由到某个分片集合中，集合的大小即index.routing_partition_size）。默认值是1并且只能在创建索引时设置，该值必须比`index.number_of_shards`小，除非`index.number_of_shards`的值就是1。见[Routing to an index partition](#Routing to an index partition)查看如何使用该值。

##### index.soft_deletes.enabled

&emsp;&emsp;该值描述了当前索引是否开启soft deletes。Soft deletes只能在创建索引时配置并且只能在Elasticsearch6.5及以后的版本的索引上配置。默认值为true。

- 从7.6.0开始后，创建索引时设置该值为false的功能已被弃用。

##### index.soft_deletes.retention_lease.period

&emsp;&emsp;在分片的history retention lease过期之前，保留的最长时间。Shard history retention leases能保证在Lucene层的段合并后soft deletes仍然能被保留（retain）。如果soft deletes在follower节点上生成副本期间被合并了merge away，那么在随后的处理中会因为在leader节点上不完整的history而导致失败。默认值为12h。

##### index.load_fixed_bitset_filters_eagerly

&emsp;&emsp;该值描述了[cached filters](#Query and filter context)是否为nested queries提前载入了pre-loaded。可选值是true(默认值)和false。

##### index.shard.check_on_startup

```text
WARNING：该配置仅针对专家用户。因为该配置允许在分片启动时进行一些开销较大的操作（processsing）并且只有在诊断集群中的问题时才有用。如果你确定要使用，你应该只是临时使用并且在不需要后移除这个配置
```

&emsp;&emsp;Elasticsearch在分片生命周期的不同时刻自动对其内容执行完整性检查。比如当恢复一个副本replica时，会对传输的每一个文件进行校验和的验证或者拍摄快照时（take a snapshot）。当启动一个节点或者完成一个分片的恢复跟重分配时，同样的会在打开一个分片时对重要的文件进行完整性的验证。你可以在对其进行拍摄快照并放到新的仓库中（fresh repository）或者恢复到一个新的节点后，手动的对整个分片进行完整性的校验。

&emsp;&emsp;当前配置用来描述Elasticsearch在打开一个分片时是否要对其进行额外的完整性检查。如果检查出corruption，那么阻止这个分片被打开，该配置接受下面的值：

- false
  - 在打开分片时不进行额外的corruption检查，该值是默认值并且是推荐值

- checksum
  - 通过分片中的每一个文件的校验和来校验分片的内容，这将检测从磁盘读取的数据与Elasticsearch最初写入的数据不同的情况，例如由于未检测到的磁盘损坏或其他硬件故障。这些检查需要从磁盘读取整个分片，这需要大量的时间和IO带宽，并可能通过从文件系统缓存中清除重要数据来影响集群性能

- true
  - 执行跟`checksum`一样的检查，同样要检查分片的logical inconsistencies，比如RAM问题或者硬件故障导致正在写入的索引被corrupted。这些检查要求从磁盘中读取所有的分片，所以它会带来大量的时间跟IO带宽并且对分片内容进行各种检查时又会带来大量的时间，CPU以及内存开销。

#### Dynamic index settings

&emsp;&emsp;下面列出的是所有的动态索引设置，这些设置与任何特定的index module没有关联。

##### index.number_of_replicas

&emsp;&emsp;每个主分片的副本分片（replica）的数量，默认值是1。

```text
WARNING：：该配置如果设置为0，可能导致在节点重启时临时可用性损失或者导致数据永久丢失
```

##### index.auto_expand_replicas

&emsp;&emsp;基于集群中节点的数量自动增加（auto-expand）副本分片的数量。该值可以设置为一个用破折号来区分上下界的值（例如: 0-5），或者使用`all`作为上界（例如：0-all）。默认值是false。注意的是这种自动增加副本分片数量只会考虑[allocation filtering](#Index-level shard allocation filtering)规则，会忽视其他的分配规则例如[total shards per node](#Total shards per node)，如果适用的规则（applicable rules）阻止分配所有的副本分片，会导致集群变成`yellow`。

&emsp;&emsp;如果上界是`all`，那么[shard allocation awareness](#Cluster-level shard allocation and routing settings)跟[cluster.routing.allocation.same_shard.host](#Cluster-level shard allocation and routing settings)这两个配置在这个索引上会被忽略。

##### index.search.idle.after
&emsp;&emsp;某个分片在多久未收到搜索或者请求后被认定为空闲的，默认值是30秒。

##### index.refresh_interval

&emsp;&emsp;多久执行一次refresh操作。该操作使得最近的修改能够被搜索到。默认值是1秒。可以被设置为`-1`来关闭refresh。如果没有显示的（explicit）设置这个值，那么分片在至少`index.search.idle.after`时间内没有收到搜索请求则不会收到后台刷新，直到分片收到查询请求。搜索时如果遇到某个分片正在进行refresh，那么会等待下一次后台refresh，这种行为的目的是在没有搜索请求时能优化bulk Indexing。为了避免这种行为，应该显式设置1s的值作为刷新间隔。

##### index.max_result_window

&emsp;&emsp;当前索引返回的结果最大值，基于`from + size`，默认值是10000。搜索请求占用的堆内存和时间与`from + size`成比例。见[scroll](#Paginate search results)或[Search After](#Search after) ，替换成这些方式来获得更多的搜索结果。

##### index.max_inner_result_window

&emsp;&emsp;`from + size`的最大值，用于索引的inner hits definition以及top hits aggregation。默认值为`100`。inner hits和top hits aggregation占用堆内存并且跟`from + size` 的大小成比例，该值用于限制内存的使用。

##### index.max_rescore_window

&emsp;&emsp;`rescore`请求中`window_size`的最大值。默认值为`index.max_result_window`，即`10000`。查询请求会占用堆内存并且跟`max(window_size, from + size)`的大小成比例，该值用于限制内存的使用。

##### index.max_docvalue_fields_search

&emsp;&emsp;某次查询中允许`docvalue_fileds`数量最大值。默认值为`100`。查询Doc-value字段有一定的开销因为可能需要查看对每一个字段中的每一个文档。

##### index.max_script_fields

&emsp;&emsp;某次查询中允许`script_fields`数量最大值。默认值为`32`。

##### index.max_ngram_diff

&emsp;&emsp;The maximum allowed difference between min_gram and max_gram for NGramTokenizer and NGramTokenFilter。默认值为`1`。

##### index.max_shingle_diff

&emsp;&emsp;[shingle token filter](#Shingle token filter)中max_shingle_size和min_shingle_size之间允许的最大不同。默认值为`3`。

##### index.max_refresh_listeners

&emsp;&emsp;索引中每一个分片上的可用的refresh listeners数量最大值。这些listeners用于实现[refresh=wait_for](#?refresh(api))。

##### index.analyze.max_token_count

&emsp;&emsp;使用\_analyze API允许生成token数量最大值。默认值为`10000`。

##### index.highlight.max_analyzed_offset

&emsp;&emsp;在高亮请求中，允许处理（analyze）的字符数量最大值。这个设置只有在text上请求高亮，并且没有设置offset或者term vectors才会应用。默认值为`1000000`。

##### index.max_terms_count

&emsp;&emsp;Terms Query中可以使用的term数量最大值。默认值为`65536`。

##### index.max_regex_length

&emsp;&emsp;Regexp Query中可以使用的regex的长度最大值。默认值为`1000`。

##### index.query.default_field

&emsp;&emsp;（string or array of strings）通配模版（Wildcard(`*`) patterns）会匹配到一个或者多个字段。下面的请求类型默认查询这些匹配到的字段：

- [More like this](#More like this query)
- [Multi-match](#Multi-match query)
- [Query string](#Query string query)
- [Simple query string](#Simple query string query)

&emsp;&emsp;默认值为`*`，意味着匹配所有eligible字段用于[term-level queries](#Term-level queries)，除了metadata字段。

##### index.routing.allocation.enable

&emsp;&emsp;用于控制索引的分片分配。该参数可以设置为：

- `all`（默认值）- 允许对所有的分片使用shard rebalancing
- `primaries` - 只允许对主分片使用shard rebalancing
- `replicas` - 只允许对副本分配（replica）使用shard rebalancing
- `none` - 不允许对分片使用shard rebalancing

##### index.gc_deletes

&emsp;&emsp;[a deleted document’s version number](#Delete API)仍然可用的时长，它用于[further versioned operations](#Index API)。默认值为`60s`。

##### index.default_pipeline

&emsp;&emsp;用于索引的默认的[ingest pipeline](#Ingest-pipelines)。如果设置了默认的pipeline但pipeline不存在，索引请求则会失败。可以使用`pipeline`参数覆盖默认的pipeline。特定的pipeline名称`none`意味着不允许任何pipeline 。

##### index.final_pipeline

&emsp;&emsp;索引的final [ingest pipeline](#Ingest-pipelines)。如果设置了final pipeline并且该pipeline不存在，索引请求则会失败。 final pipeline总是在请求中指定的pipeline以及默认的pipeline之后运行。特定的pipeline名称`none`意味着不允许任何pipeline 。

> NOTE：你不能使用一个final pipeline修改`_index`字段，如果pipeline尝试进行修改，索引请求则会失败。

##### index.hidden

&emsp;&emsp;该值描述的是索引是否默认隐藏。隐藏的索引不会在匹配到通配表达式到后返回。使用了`expand_wildcards`参数的请求会受到该参数的控制。可选值为`true`或者`false`。

#### Settings in other index modules

&emsp;&emsp;index module中其他可用的index setting：

- [Analysis](#Text analysis)

&emsp;&emsp;用于定义analyzers, tokenizers, token filters and character filters。

- [Index shard allocation](#Index Shard Allocation)

&emsp;&emsp;控制分片如何，何时分配到哪一个节点。

- [Mapping](#Mapper)

&emsp;&emsp;为索引开启/关闭dynamic mapping。

- [Merging](#Merge)

&emsp;&emsp;控制后台合并处理程序如何对分片进行和并。

- [Similarities](#Similarity module)

&emsp;&emsp;配置自定义的Similarity设置自定义查询结果的打分方式。

- [Slowlog](#Slow Log)

&emsp;&emsp;控制如何通过日志记录slow Query和slow fetch。

- [Store](#Store)

&emsp;&emsp;配置用于访问分片数据的文件系统。

- [Translog](#Translog)

&emsp;&emsp;控制transaction log以及后台flush操作。

- [History retention](#History retention)

&emsp;&emsp;控制索引的历史操作。

- [Indexing pressure](#Indexing pressure)

&emsp;&emsp;Configure indexing back pressure limits

#### X-Pack index settings

- [Index lifecycle management](#Index lifecycle management settings in Elasticsearch)

&emsp;&emsp;为索引指定生命周期策略以及rollover alias。

### Analysis
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-modules-analysis.html)

&emsp;&emsp;The index analysis module acts as a configurable registry of analyzers用于将一个字符串字段转化为不同的term：

- 添加到倒排索引（inverted index）中使得文档可以被搜索到
- 用于high level query，比如[match query](#Match query)，生成用于搜索的term

&emsp;&emsp;见[Text analysis](#Text analysis)了解配置细节。

### Index Shard Allocation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-modules-allocation.html)

&emsp;&emsp;这个组件提供了per-index settings来控制分片到节点的分配：

- [Shard allocation filtering](#Index-level shard allocation filtering)：控制哪些分片分配到哪些节点中。
- [Delayed allocation](#Delaying allocation when a node leaves)：节点离开集群后，未分配的（unassigned）分片延缓分配到节点。
- [Total shards per node:](#Total shards per node)：每一个节点上同一个索引的分片的数量限制
- [Data tier allocation](#Index-level data tier allocation filtering)：控制[data tiers](#Data tiers)上的索引分配

#### Index-level shard allocation filtering
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/shard-allocation-filtering.html)

&emsp;&emsp;你可以使用shard allocation filtering来控制Elasticsearch如何为一个指定的索引进行分片的分配。索引层（index-level）的filtering跟 [cluster-wide allocation filtering](#Cluster-level shard allocation filtering) 和 [allocation awareness](#Shard allocation awareness)结合应用。

&emsp;&emsp;Shard allocation filtering可以基于节点上自定义的属性或者内置属性例如`_name`, `_host_ip`, `_publish_ip`, `_ip`, `_host`, `_id`, `_tier` 以及`_tier_preference`。 [Index lifecycle management](#ILM: Manage the index lifecycle)使用基于自定义的节点属性的filtering来决定在进行阶段转变时如何重新分配分片。

&emsp;&emsp;设置（setting）`cluster.routing.allocation`是动态的，使得可以让live index从一些节点移到其他节点。只有在不会破坏其他路由限制（routing constraint）时才会重新分配分片，比如说不会在一台机器上同时分配主分片和副本。

&emsp;&emsp;例如，你可以使用自定义的节点属性来指示（indicate）一个节点的性能属性并且使用shard allocation filtering将指定的索引路由到级别最合适的硬件中（the most appropriate class of hardware）。

##### Enabling index-level shard allocation filtering

&emsp;&emsp;基于自定义的节点属性进行过滤（筛选）：

1. 在每一个节点的配置文件`elasticsearch.yml`中指定过滤节点属性。例如，如果你有`small`, `medium`, 和 `big`三类节点，你可以添加一个`size`属性，基于节点的大小进行过滤。

```text
node.attr.size: medium
```

&emsp;&emsp;你也可以在节点启动时指定自定义的属性：

```text
./bin/elasticsearch -Enode.attr.size=medium
```

2. 在索引中添加shard allocation filtering信息。`index.routing.allocation`这个设置支持三种类型的过滤：`include`, `exclude`和`require`。例如，告诉Elasticsearch使用`index.routing.allocation.include`将索引`text`的分片在值为`big`或者`medium`的节点上进行分配：

```text
PUT test/_settings
{
  "index.routing.allocation.include.size": "big,medium"
}
```

&emsp;&emsp;如果你指定多个条件并要求节点同时满足才能在其节点上进行分片的分配：

- 如果指定了任意`require`类型的条件，所有的条件都需要满足
- 如果指定了任意`exclude`类型的条件，所有的条件都不能满足
- 如果指定了任意`include`类型的条件，至少一个条件需要满足

&emsp;&emsp;例如，将索引`text`的分片移到机架值为`ranck1`并且大小为`big`的节点，你可以这么指定：

```text
PUT test/_settings
{
  "index.routing.allocation.require.size": "big",
  "index.routing.allocation.require.rack": "rack1"
}
```

##### Index allocation filter settings

###### index.routing.allocation.include.{attribute}

&emsp;&emsp;将索引分配给一个节点，这个节点的`{attribute}`属性中至少包含用逗号隔开的值。

###### index.routing.allocation.require.{attribute}

&emsp;&emsp;将索引分配给一个节点，这个节点的`{attribute}`属性中包含所有用逗号隔开的值。

###### index.routing.allocation.exclude.{attribute}

&emsp;&emsp;将索引分配给一个节点，这个节点的`{attribute}`属性中不包含用逗号隔开的值。

&emsp;&emsp;索引的分配设置支持下列的内置属性：

|    \_name    |                     根据节点名称进行匹配                     |
| :-----------: | :----------------------------------------------------------: |
|  \_host_ip   |                     根据host ip进行匹配                      |
| \_publish_ip |            根据发布的IP（publish IP）地址进行匹配            |
|     \_ip     |           根据`_host_ip`或者`_publish_ip`进行匹配            |
|    \_host    |                     根据hostname进行匹配                     |
|    \_id    |                      根据节点id进行匹配                      |
|    \_tier    | 根据[data tier](#Data tiers)角色匹配，见[data tier allocation filtering](#Index-level data tier allocation filtering)了解更多细节 |

&emsp;&emsp;`_tier`这种过滤属性基于[node](#Node)角色，只有部分角色是 [data tier](#Index-level data tier allocation filtering)角色并且一般的（generic）[data role](#Data node)会匹配到任意的tier filtering。

&emsp;&emsp;你可以使用通配符来指定属性值，例如：

```text
PUT test/_settings
{
  "index.routing.allocation.include._ip": "192.168.2.*"
}
```

#### Delaying allocation when a node leaves
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/delayed-allocation.html)

&emsp;&emsp;无论什么原因，有意为之，或者其他原因导致节点离开集群后，master的反应是：

- 将副本分片提升为主分片来代替在那个节点上的主分片
- 分配副本分片来代替缺失的副本分片（假设有足够的节点）
- 在剩余的节点上均匀地平衡分片

&emsp;&emsp;这些动作通过尽快的保证每一个分片拥有副本分片的方式来保护集群不会丢失数据。

&emsp;&emsp;尽管我们在[node level](#Index recovery settings)和[cluster level](#Cluster-level shard allocation)层限制（throttle）了并发恢复（concurrent recovery），但是这个"shard-shuffle"仍然会对集群造成一些额外的负载，如果节点很有可能很快重新加入到节点，那就没有必要做上面的动作了。想象下下面的场景：

- 节点5失去了网络连接
- master将副本分片提升为主分片，原来的主分片在节点5上
- master将新的副本分片分配到集群中的其他节点
- 每一个新的副本分片都要通过网络从主分片中复制获得
- 更多分片移动到其他节点来平衡集群
- 节点5在几分钟后回到了集群
- master将分片分配给节点5来平衡集群

&emsp;&emsp;如果master可以等待几分钟，那么缺失的分片就可以以最小的网络代价将分片分配给Node5。已经被自动[flush](#Flush API)的空闲分片（没有收到索引请求的分片为idle shard）的处理过程会更快。

&emsp;&emsp;因为动态设置`index.unassigned.node_left.delayed_timeout`，默认值为`1m`，节点离开集群后，未分配（unassigned）的分配（allocate）时间可能会被延后。

&emsp;&emsp;可以在一个live index（或者所有的索引）上更新这个设置：

```text
PUT _all/_settings
{
  "settings": {
    "index.unassigned.node_left.delayed_timeout": "5m"
  }
}
```

&emsp;&emsp;开启延迟分配后，上面的场景就会变成：

- 节点5失去了网络连接
- master将副本分片提升为主分片，原来的主分片在节点5上
- master记录日志：unassigned shard的分片被延后了，以及延后时间
- 由于存在unassigned replica shards，集群的颜色为黄色
- `timeout`过期前，节点5在几分钟后回到了集群
- 缺失的副本分片重新分配到节点5（and sync-flushed shards recover almost immediately）

> NOTE：This setting will not affect the promotion of replicas to primaries, nor will it affect the assignment of replicas that have not been assigned previously. In particular, delayed allocation does not come into effect after a full cluster restart. Also, in case of a master failover situation, elapsed delay time is forgotten (i.e. reset to the full initial delay).

##### Cancellation of shard relocation

&emsp;&emsp;如果延后分配超时，master会将缺失的分片分配给其他的节点并开始恢复。如果缺失的节点重新回到集群，并且它的分片跟主分片有相同的sync-id，shard recovery会被取消并synced shard会用于恢复。

&emsp;&emsp;因为这个理由，默认的`timeout`只要设置为一分钟：即使shard relocation已经开始，取消恢复而是使用synced shard有更小的代价。

##### Monitoring delayed unassigned shards

&emsp;&emsp;因为这个timeout设置导致延后分配的分片数量可以通过[cluster health API](#Cluster health API)查看：

```text
GET _cluster/health 
```

&emsp;&emsp;第1行，这个请求会返回一个`delayed_unassigned_shards`的值。

##### Removing a node permanently

&emsp;&emsp;如果某个节点不再回到集群，你可能想让Elasticsearch马上分配缺失的分片，只要将timeout的值更新为0：

```text
PUT _all/_settings
{
  "settings": {
    "index.unassigned.node_left.delayed_timeout": "0"
  }
}
```

&emsp;&emsp;缺失的分片开始恢复后，你就可以重新设置这个值。

#### Index recovery prioritization
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/recovery-prioritization.html)

&emsp;&emsp;whenever possible，未分配的索引会根据优先级依次进行恢复。根据下面的条件对索引进行排序：

- 可选的`index.priority`的值（higher before lower）
- 索引的创建时间（higher before lower）
- 索引的名称（higher before lower）

&emsp;&emsp;这意味着默认情况下，新的索引会先于旧的索引进行恢复。

&emsp;&emsp;使用每一个索引中可自动更新的`index.priority`来自定义索引的优先顺序。例如：

```text
PUT index_1

PUT index_2

PUT index_3
{
  "settings": {
    "index.priority": 10
  }
}

PUT index_4
{
  "settings": {
    "index.priority": 5
  }
}
```

&emsp;&emsp;在上面的例子中：

- 首先恢复`index_3`，因为它有最高的`index.priority`
- 接着恢复`index_4`，因为它有第二高的`index.priority`
- 然后恢复`index_2`，因为相较于`index_1`它是最近创建的
- 最后恢复`index_1`

&emsp;&emsp;这个设置可以是一个整数，可以通过[update indices settings](#Update index settings API)更新。

```text
PUT index_4/_settings
{
  "index.priority": 1
}
```

#### Total shards per node
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/allocation-total-shards.html)

&emsp;&emsp;cluster-level的分片分配器（shard allocator）会尝试将一个索引的多个分片尽可能的分布在多个节点上。然而由于你拥有的分片和索引的数量，以及它们的大小，可能没法总是平均的进行分配。

&emsp;&emsp;下面的动态设置允许你在每一个节点上，硬性限制（hard limit）单个索引允许的分片数量：

##### index.routing.allocation.total_shards_per_node

- 单个节点上允许分配的分片（副本分片和主分片）总数。默认没有限制。

&emsp;&emsp;你也可以限制一个节点上的分片总数并且regardless of index：

##### cluster.routing.allocation.total_shards_per_node

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）每一个节点上允许分配的主分片和副本分片的最大数量。默认值`-1`（没有限制）。

&emsp;&emsp;Elasticsearch会在分片分配时检查这个值，例如，某个集群的`cluster.routing.allocation.total_shards_per_node`的值为`100`，并且三个节点使用下面的分片分配：

- Node A： 100个分片
- Node B：98个分片
- Node C：1个分片

&emsp;&emsp;如果Node C发生故障，Elasticsearch会将分片分配到Node B。分配到Node A会超过node A的分片限制。

> WARNING：这些设置强行（impose）设置了限制会导致一些分片无法被分配。使用时需要注意这个问题。

#### Index-level data tier allocation filtering
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/data-tier-shard-filtering.html)

&emsp;&emsp;你可以通过index-level的设置`_tier_preference`来控制将索引分配到哪个[data tier ](#Data tiers)中。

&emsp;&emsp;这个设置对应的data node roles：

- [data_content](#Content data node)
- [data_hot](#Hot data node)
- [data_warm](#Warm data node)
- [data_cold](#Cold data node)
- [data_frozen](#Frozen data node)

> NOTE：[date](#Data node) role不是合法的data tier因此不能用于`_tier_preference`。The frozen tier stores [partially mounted indices](#Partially mounted index) exclusively


##### Data tier allocation settings

###### index.routing.allocation.include.\_tier\_preference

&emsp;&emsp;该参数的值是一个list。首先将索引分配到list中第一个data tier中，该节点必须是可用的。防止无法在期望的（prefer）data tier分配索引时导致无法分配这个索引。例如，如果你设置`index.routing.allocation.include._tier_preference`为`data_warm,data_hot`，索引会分配到拥有`data_warm`角色的节点上，如果在warm tier中没有节点，但是有拥有`data_hot`角色的节点，那么就分配到这个节点中。Used in conjuction with [data tiers](#Data tier index allocation)。

### Index blocks 

[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-modules-blocks.html#index-blocks-read-only)

#### Index block settings

##### index.blocks.read_only

### Merge
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-modules-merge.html)

&emsp;&emsp;Elasticsearch中的一个分片即Lucene中的索引，Lucene中的索引由一个或多个段（segment）组成。Segment是内部存储单元（internal storage element），用于存储数据，并且是不可改变的（immutable）。小段会周期性的合并到大段中并expunge deletes。

&emsp;&emsp;合并过程使用auto-throttle来平衡合并跟其他行为，例如查询之间的硬件资源的使用。

#### Merge scheduling

&emsp;&emsp;merge scheduler（ConcurrentMergeScheduler）控制了在需要进行合并时合并的执行过程。合并在不同的线程中运行，当使用的线程数量达到最大值后，后续的合并需要等待merge thread可用后才继续执行。

&emsp;&emsp;merge scheduler支持下面的动态参数：

##### index.merge.scheduler.max_thread_count

&emsp;&emsp;单个分片上可以同时合并的线程数量。默认值为`Math.max(1, Math.min(4 <<node.processors, node.processors>> / 2))`。这个值在固态硬盘（solid-state-disk）上 work well。如果你的索引工作在spinning platter drives，将这个值降为`1`。

### Similarity module
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-modules-similarity.html)

### Slow log
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-modules-translog.html)

#### Search Slow Log

&emsp;&emsp;分片层的慢查询日志（slow search log）允许将慢查询（query and fetch phases）记录到一个指定的日志文件中。

&emsp;&emsp;可以同时为query跟fetch设置阈值，见下面的例子：

```text
index.search.slowlog.threshold.query.warn: 10s
index.search.slowlog.threshold.query.info: 5s
index.search.slowlog.threshold.query.debug: 2s
index.search.slowlog.threshold.query.trace: 500ms

index.search.slowlog.threshold.fetch.warn: 1s
index.search.slowlog.threshold.fetch.info: 800ms
index.search.slowlog.threshold.fetch.debug: 500ms
index.search.slowlog.threshold.fetch.trace: 200ms
```

&emsp;&emsp;上述的设置都是动态的，可以通过[update indices settings](#Update index settings API)设置，例如：

```text
PUT /my-index-000001/_settings
{
  "index.search.slowlog.threshold.query.warn": "10s",
  "index.search.slowlog.threshold.query.info": "5s",
  "index.search.slowlog.threshold.query.debug": "2s",
  "index.search.slowlog.threshold.query.trace": "500ms",
  "index.search.slowlog.threshold.fetch.warn": "1s",
  "index.search.slowlog.threshold.fetch.info": "800ms",
  "index.search.slowlog.threshold.fetch.debug": "500ms",
  "index.search.slowlog.threshold.fetch.trace": "200ms"
}
```

&emsp;&emsp;默认情况下阈值是关闭的（默认值为-1）。

&emsp;&emsp;由于是分片层级别范围，意味着记录的是对于特定分片的查询。日志中不包含全部的查询请求，而是通过广播到每一个分片去执行日志的记录。跟请求级别（request level）相比，这种分片层级别的日志记录的好处是能关联到某个机器上的情况。

&emsp;&emsp;在`log4j2.properties`文件中配置查询慢日志。

#### Identifying search slow log origin

&emsp;&emsp;明确是什么导致了慢查询通常是非常有用的。如果在请求的header中带有`X-Opaque-ID`，那么在查询慢日志（Search Slow log）中会增加一个额外的`id`字段来描述user ID。

```text
{
  "type": "index_search_slowlog",
  "timestamp": "2030-08-30T11:59:37,786+02:00",
  "level": "WARN",
  "component": "i.s.s.query",
  "cluster.name": "distribution_run",
  "node.name": "node-0",
  "message": "[index6][0]",
  "took": "78.4micros",
  "took_millis": "0",
  "total_hits": "0 hits",
  "stats": "[]",
  "search_type": "QUERY_THEN_FETCH",
  "total_shards": "1",
  "source": "{\"query\":{\"match_all\":{\"boost\":1.0}}}",
  "id": "MY_USER_ID",
  "cluster.uuid": "Aq-c-PAeQiK3tfBYtig9Bw",
  "node.id": "D7fUYfnfTLa2D7y-xw6tZg"
}
```

#### Index Slow log

&emsp;&emsp;索引慢日志跟查询慢日志是类似的。以`_index_indexing_slowlog.json.`为后缀的日志以及阈值使用跟查询慢日志相同的配置方式：

```text
index.indexing.slowlog.threshold.index.warn: 10s
index.indexing.slowlog.threshold.index.info: 5s
index.indexing.slowlog.threshold.index.debug: 2s
index.indexing.slowlog.threshold.index.trace: 500ms
index.indexing.slowlog.source: 1000
```

&emsp;&emsp;上述的设置都是动态的，可以通过[update indices settings](#Update index settings API)设置，例如：

```text
PUT /my-index-000001/_settings
{
  "index.indexing.slowlog.threshold.index.warn": "10s",
  "index.indexing.slowlog.threshold.index.info": "5s",
  "index.indexing.slowlog.threshold.index.debug": "2s",
  "index.indexing.slowlog.threshold.index.trace": "500ms",
  "index.indexing.slowlog.source": "1000"
}
```

&emsp;&emsp;默认情况下，Elasticsearch会在慢日志（slowlog）中记录`_source`中前1000个字符。你可以通过`index.indexing.slowlog.source`更改这个限制。设置为`0`或者`false`将不会记录`source`字段。如果设置为`true`则会记录完整的`_source`并且不关心其内容的大小。`_source`中的内容被格式化（format）过使得可以作为日志中的单独一行。如果不格式化`_source`的内容是件重要的事情，那么可以通过将`index.indexing.slowlog.reformat`设置为`false`来关闭formatting，这样会使得`_source`中的内容以多行的形式出现在日志中。

&emsp;&emsp;在`log4j2.properties`文件中配置索引慢日志。

#### Slow log levels

&emsp;&emsp;可以通过设置查询\索引慢日志日志级别来设置合适的阈值，来关闭（switch off）冗长的（more verbose）设置。例如可以设置`index.indexing.slowlog.level: INFO`那么我们就可以设置`index.indexing.slowlog.threshold.index.debug`跟`index.indexing.slowlog.threshold.index.trace`为`-1`。

### Store
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-modules-store.html)

&emsp;&emsp;Store模块允许你控制索引数据在磁盘上的存储和访问方式。

> NOTE：这是一个low-level的设置。一些store的实现有较差的并发性或者没有对堆内存的使用进行优化。我们建议使用默认值（sticking to the defaults）

##### File system storage types

&emsp;&emsp;现在有不同的文件系统实现或者存储类型（file system implementations or storage types）。默认情况下，Elasticsearch会基于操作系统环境来选择最佳实现。

&emsp;&emsp;Storage type可以通过在`config/elasticsearch.yml`文件中配置来显示的为所有的索引进行设置：

```text
index.store.type: hybridfs
```

这是一个静态设置，可以在创建索引时基于每个索引进行设置：

```text
PUT /my-index-000001
{
  "settings": {
    "index.store.type": "hybridfs"
  }
}
```

> WARNING：这是一个专家（expert-only）设置并且可能在未来的版本中移除。

&emsp;&emsp;下面的内容列出了所有支持的不同类型的存储：

###### fs

&emsp;&emsp;默认的文件系统实现。会根据操作系统环境选择最佳的实现方式。目前在所有支持的系统上都是`hybridfs`，但可能会发生变化。

###### simplefs

&emsp;&emsp;deprecated::[7.15, "simplefs已经被弃用并将在8.0中移除"，转而用niofs或者其他文件系统。Elasticsearch 7.15 及以后的版本中使用niofs作为simplefs这个存储类型会提供相对于simplefs更好或者相同的性能]

&emsp;&emsp;Simple FS 类型是使用随机访问文件的文件系统存储的简单实现（straightforward implementation）（对应Lucene中的[SimpleFsDirectory](https://www.amazingkoala.com.cn/Lucene/Store/2019/0613/Directory（上）)）。这种实现有较差的并发性能（使用多线程会有瓶颈）并且没有对堆内存的使用进行优化。

###### niofs

&emsp;&emsp;NIO FS类型使用NIO将分片索引存储在文件系统上（对应Lucene中的[NIOFSDirectory](https://www.amazingkoala.com.cn/Lucene/Store/2019/0613/Directory（上）)）。他允许多线程并发的读取相同的文件。由于在SUN Java实现中存在一个bug，所以不建议在Windows上使用这个类型并且没有对堆内存的使用进行优化。

###### mmapfs

&emsp;&emsp;MMap FS 类型通过将文件映射到内存（mmap）将分片索引存储在文件系统上（对应Lucene中的[MMapDirectory](https://www.amazingkoala.com.cn/Lucene/Store/2019/0613/Directory（上）)）。内存映射占用了进程中的一部分虚拟内存地址空间，等于被映射文件的大小。在使用这个之前确保你拥有大量的[virtual address space](#Virtual memory)。

###### hybridfs

&emsp;&emsp;`hybridfs`类型是`niofs`和`mmapfs`的混合类型，基于不同类型的文件的读取访问模式选择最佳的文件系统类型。目前只有Lucene term dictionary，norms和doc values文件是内存映射的。其他的文件使用Lucene的[NIOFSDirectory](https://www.amazingkoala.com.cn/Lucene/Store/2019/0613/Directory（上）)打开。跟`mmapfs`一样，在使用这个之前确保你拥有大量的[virtual address space](#Virtual memory)。

&emsp;&emsp;你可以通过`node.store.allow_mmap`来限制（restrict）`mmapfs`以及`hybridfs`的使用。这是一个布尔类型的设置描述是否允许内存映射（memory-mapping）。默认值是允许的。这个设置是很有用的，比如说，如果你处于无法控制创建大量内存映射的环境中，那么你需要禁用使用内存映射的能力。

#### Preloading data into the file system cache
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/preload-data-to-file-system-cache.html)


### Translog
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-modules-translog.html)

&emsp;&emsp;Lucene中的[commit](https://www.amazingkoala.com.cn/Lucene/Index/2019/0906/文档提交之commit（一）)会将更改（changes）持久化到磁盘，这是一种开销很大的操作，所以不能在每次索引或者删除操作后执行commit。如果在索引过程中退出或者硬件错误，那么在一次commit后，下一次commit前这个期间发生的变更会被移除（丢失）。

&emsp;&emsp;Lucene中，每一次单独的变更都执行commit的话其开销是很大的，所以每一个分片都会把写操作（writes operation）写入到事务日志（transaction log）中，即所谓的translog。所有的索引和删除操作在Lucene中建立索引后（未持久化到磁盘）会被添加到translog中，在发生crash时间后，最近被确认的（acknowledge）（已经写入Lucene的内存意味着确认，但未持久化到磁盘）但是不在Lucene commit中的操作，在分片恢复时会使用translog进行恢复。

&emsp;&emsp;Elasticsearch中执行[flush](#Flush API)即执行Lucene中的commit操作，并且开始一个新的translog generation。flush会在后台自动执行使得不会让translog记录太多操作，这样在恢复期间不会因为重新（replay）执行translog中的操作使得花费很多的时间。也可以通过API来执行flush，但这几乎是没必要的。



#### Translog settings

&emsp;&emsp;translog中的数据只有在它是`fsync`并且commit后会持久化到磁盘上。硬件故障的事件或者是操作系统崩溃或者JVm崩溃或者分片失败，上一次commit之后的translog数据都会丢失。

&emsp;&emsp;`index.translog.durability`默认设置为`request`意味着在translog在成功`fsynced`并且commit到主分片以及每一个分配的副本后，Elasticsearch将只报告成功的索引、删除、更新、bulk request给client。如果`index.translog.durability`设置为`async`,那么Elasticsearch只会每隔`index.translog.sync_interval`对translog进行`fsynced`并且commit，意味着当节点恢复时，任何crash之前的操作可能会丢失。

&emsp;&emsp;下面每个索引的[dynamically updatable](#Index APIs)设置控制translog的行为：

##### index.translog.sync_interval

&emsp;&emsp;translog fsynced 到磁盘并且commit的间隔，不考虑写操作，默认时间是5s。不允许设置100ms以下的值。

##### index.translog.durability

&emsp;&emsp;translog是否在每一次的index、delete、update或者bulk request都进行`fsynced`并且commit。可以设置为下面的值：

######  request

&emsp;&emsp;（默认值）每一个请求都进行`fsynced`并且commit。硬件错误事件时，所有确认的写操作已经commit到磁盘。

###### async

&emsp;&emsp;每隔`index.translog.sync_interval`进行`fsynced`并且commit。在出现失败事件时，上一次自动commit后的所有确认的写操作都会被丢弃

##### index.translog.flush_threshold_size

&emsp;&emsp;所有在Lucene中没有安全持久化的操作都会存储在translog中，这个操作对读操作是可见的，在分片被停后并且必须要恢复时，它们用于重新索引操作。这个设置控制了这些操作占用总量的最大值，防止恢复时占用太长的时间。当达到最大值时会触发flush，生成一个新的Lucene commit。默认值是`512mb`。


### History retention
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-modules-history-retention.html)

&emsp;&emsp;Elasticsearch有时候需要replay在分片上执行的一些操作。例如，如果一个副本分片简单的offline了，相较于从头开始（from scratch）构造这个副本分片，只replay它在offline期间丢失的操作有着更高的效率。同样的，[cross-cluster replication](#Cross-cluster replication)的工作方式为：在leader cluster执行操作，然后在follower cluster上replay这些操作。

&emsp;&emsp;Elasticsearch对索引的写操作在Lucene层来说只有[两个操作](https://www.amazingkoala.com.cn/Lucene/Index/2019/0626/文档的增删改（上）)：索引（添加）一篇文档或者删除现有的文档。更新操作的实现方式是先删除旧的文档然后索引一篇新的文档。索引一篇文档到Lucene的操作包含了所有用于replay的信息，但是对于文档的删除不是这样的。为了解决这个问题， Elasticsearch使用了名为`软删除`[soft deletes](https://www.amazingkoala.com.cn/Lucene/Index/2020/0616/软删除softDeletes（一）)的功能来保留Lucene索引上最近的删除信息，使得可以用于replay。

&emsp;&emsp;Elasticsearch只保留索引中一些最近删除的文档，是因为软删除的文档仍然占用一些空间。Elasticsearch最终会完全的丢弃这些软删除的文档来释放空间使得索引不会随着时间一直增长。Elasticsearch不需要replay在分片上执行的每一个操作，因为总是有可能在remote cluster执行完整的分片拷贝。然而，复制整个分片可能比replay一些丢失的操作要花费更多的时间，所以Elasticsearch会保留期望在未来用于replay的所有操作。

&emsp;&emsp;Elasticsearch使用了`shard history retention leases`的机制来追踪期望在未来将用于replay的操作。每一个可能需要用于replay的操作的分片拷贝（shard copy）必须首先为自己创建一个shard history retention leases。例如，this shard copy might be a replica of a shard或者在使用cross-cluster replication时的follower index。每一个retention lease会追踪对应的分片拷贝第一个没有收到的操作的序号，随着分片拷贝收到新的操作，增加retention lease中的序号值，表示不会在未来replay这些操作。一旦soft-deleted operations没有被任何的retention lease占用（hold），Elasticsearch就会丢弃它们。

&emsp;&emsp;如果分片拷贝发生了故障，那它会停止更新它的shard history retention lease。意味着Elasticsearch会保留所有的新的操作，使得在分片拷贝恢复后执行replay操作。然而，retention lease只持续一个有限的时间。如果分片拷贝没有及时的恢复，那retention lease会到期。这可以保护Elasticsearch，防止出现某个分片拷贝永久性的故障后，retention history会永久保留的问题，一旦某个retention lease到期后，Elasticsearch可以开始丢失这些历史信息。如果分片拷贝在retention lease到期后才恢复，那Elasticsearch会回退到拷贝整个索引，因为这时候再也没法简单的replay丢失的操作了。retention lease的过期时间默认是`12h`，这个时间已经足够长了，适用于大多数的场景。

#### History retention settings

##### index.soft_deletes.enabled

&emsp;&emsp;索引是否开启soft deletes（Deprecated in 7.6.0）。Soft deletes只能在创建索引时配置并且是Elasticsearch6.5及以后的版本中的索引创建。默认值为`true`。

##### index.soft_deletes.retention_lease.period

&emsp;&emsp;在retention lease过期前保留shard history retention lease的最大时间。Shard history retention leases 能保证Lucene在执行段的合并时仍然会[保留](https://www.amazingkoala.com.cn/Lucene/Index/2020/0616/软删除softDeletes（一）)Soft delete对应的信息。如果soft delete在复制（replicate）到follower之前，在段的合并期间被合并了，那么follower中接下来的操作会因为leader上不完整的历史操作而失败。默认值`12h`。


### Index Sorting
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-modules-index-sorting.html)

&emsp;&emsp;在Elasticsearch中创建一个新的索引时，可以配置每一个分片中的段内文档的排序方式。默认情况下，Lucene不会使用任何的排序。`index.sort.*`这些设置定义了使用哪些字段用于段中文档的排序。

> WARNING：nested fields are not compatible with index sorting。because they rely on the assumption that nested documents are stored in contiguous doc ids, which can be broken by index sorting。An error will be thrown if index sorting is activated on an index that contains nested fields.

&emsp;&emsp;下面的例子定义如何在单个字段上定义一个排序

```text
PUT my-index-000001
{
  "settings": {
    "index": {
      "sort.field": "date", 
      "sort.order": "desc"  
    }
  },
  "mappings": {
    "properties": {
      "date": {
        "type": "date"
      }
    }
  }
}
```

&emsp;&emsp;第5行，根据`date`字段排序
&emsp;&emsp;第6行，降序

&emsp;&emsp;也可以使用多个字段进行排序：

```text
PUT my-index-000001
{
  "settings": {
    "index": {
      "sort.field": [ "username", "date" ], 
      "sort.order": [ "asc", "desc" ]       
    }
  },
  "mappings": {
    "properties": {
      "username": {
        "type": "keyword",
        "doc_values": true
      },
      "date": {
        "type": "date"
      }
    }
  }
}
```

&emsp;&emsp;第5行，先根据`username`字段排序，再根据`date`字段排序 
&emsp;&emsp;第6行，根据`username`字段使用升序，根据`date`使用降序


&emsp;&emsp;Index Sorting支持下面的设置：

`index.sort.field`

&emsp;&emsp;指定用于排序的一个或者多个字段。只允许开启`doc_values`的`boolean` 、`numeric`、 `date`、 `keyword`类型的字段。

`index.sort.order`

&emsp;&emsp;每一个排序字段的排序规则：

- `asc`: 升序
- `desc`：降序

`index.sort.mode`

&emsp;&emsp;Elasticsearch支持使用mutil-values的字段用于排序。这个参数描述的是使用哪个值用于文档排序。两个参数值可选：

- `min`：选择最小的值
- `max`：选择最大的值

`index.sort.missing`

&emsp;&emsp;这个参数描述的是，如果文档中缺失用于排序的字段该如何对这篇文档进行排序。两个参数值可选：

- `_last`：没有用于排序的字段的文档排在最后面
- `_first`：没有用于排序的字段的文档排在最前面

> WARNING：Index Sorting只有定义在创建索引时。不能对现有的索引添加或者更新排序。Index Sorting在索引期间有一定的开销，因为文档必须在flush和merge时进行排序。你应该在启动这个功能前测试下对你的应用的影响

##### Early termination of search request

&emsp;&emsp;默认情况下，Elasticsearch中的一个查询请求需要访问匹配查询的所有文档才能获取到某个排序条件下的TopN的文档。然而当Index sort和查询中指定的排序（Search sort）一致时，每个段中的查询TopN的操作就有可能限制需要访问的文档数量。例如，如果索引中的文档根据`timestamp`字段排序：

```text
PUT events
{
  "settings": {
    "index": {
      "sort.field": "timestamp",
      "sort.order": "desc" 
    }
  },
  "mappings": {
    "properties": {
      "timestamp": {
        "type": "date"
      }
    }
  }
}
```

&emsp;&emsp;第6行，索引按照`timestamp`字段降序排序（most recent first）

&emsp;&emsp;你可以搜索最新的10条事件：

```text
GET /events/_search
{
  "size": 10,
  "sort": [
    { "timestamp": "desc" }
  ]
}
```

&emsp;&emsp;Elasticsearch会检测到每一个的top doc都已经排序过了，并且将只会对每个段中的TopN的文档进行比较。剩余的满足查询条件的文档用于统计结果的数量以及构造aggregation。

&emsp;&emsp;如果你只是想要查找最新的10条事件并且对于满足查询条件的文档的数量不感兴趣，那么你可以将`track_total_hits`设置为`false`：

```text
GET /events/_search
{
  "size": 10,
  "sort": [ 
      { "timestamp": "desc" }
  ],
  "track_total_hits": false
}
```

&emsp;&emsp;第4行，Index sort将用于top documents并且每个段都会在收集了前10篇文档后就提前退出。

&emsp;&emsp;这次Elasticsearch不会去统计满足查询条件的文档的数量，并且能在每一个段收集了N篇文档后就提前退出。

```text
{
  "_shards": ...
   "hits" : {  
      "max_score" : null,
      "hits" : []
  },
  "took": 20,
  "timed_out": false
}
```

&emsp;&emsp;第3行，由于提前退出了，所以不知道满足查询条件的文档数量。

> NOTE：聚合操作会收集所有满足查询条件的文档，无视`track_total_hits`的值。

#### Use index sorting to speed up conjunctions
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-modules-index-sorting-conjunctions.html)

&emsp;&emsp;Index sorting对organize Lucene doc id（不要跟`_id`混淆（conflate））非常有用，使得conjunction更加高效（a AND b AND ...）。如果任意一个clause没有匹配到，那么整个conjunction都不会匹配，conjunction可以变的非常高效。通过使用Index sorting，我们可以把没有匹配的文档放一起，有助于在范围很大的ID区间内进行跳跃，跳过那些不匹配的文档。

&emsp;&emsp;这个trick只能在low-cardinality的字段上工作。A rule of thumb，你首先应该将拥有low-cardinality以及经常用于过滤的字段作为排序字段。The sort order (asc or desc) does not matter as we only care about putting values that would match the same clauses close to each other。

&emsp;&emsp;例如，如果你正在索引汽车用于销售，你应该根据fuel type, body type, make, year of registration and finally mileage进行排序。

### Indexing pressure
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-modules-indexing-pressure.html)

&emsp;&emsp;索引文档到Elasticsearch会引入系统的内存和CPU的负载。每一个索引操作，包含了coordinating, primary和replica这几个阶段。这些阶段在集群中的多个节点上运行。

&emsp;&emsp;Indexing pressure可以由外部的索引请求或者内部的，例如恢复，跨集群副本分片（cross-cluster replication）引起。如果系统中有过多的索引工作，集群会达到饱和（saturated）。这会对其他操作造成不好的（adversely）影响，例如查询，cluster coordination以及background processing。

&emsp;&emsp;为了防止出现这个问题，Elasticsearch内部会监控索引负载。当负载超过一定的限制，新的索引操作会被reject。

#### Indexing stages

&emsp;&emsp;外部的索引操作（external indexing operation）会经历三个阶段：coordinating，primary以及replica。见[Basic write model](#Basic write model)。

#### Memory limits

&emsp;&emsp;节点设置`indexing_pressure.memory.limit`严格限制了未完成（outstanding）索引请求的可用的字节数量。这个设置默认值是堆内存的10%。

&emsp;&emsp;每一个索引阶段（indexing stage）的开头，Elasticsearch会统计一个索引请求占用的字节数。在索引阶段的结尾才会释放这个统计。意味着上游的阶段会统计请求开销直到所有的下游都完成。例如，coordinating请求会保留统计直到primary和replica阶段完成，primary请求会保留统计直到每一个同步的replica返回响应，如果有需要的话开启replica重试。

&emsp;&emsp;当未完成的coordinating, primary和 replica的索引占用字节超过配置的值后，节点会在coordinating或者primary阶段reject这个新的索引请求。

&emsp;&emsp;当未完成的replica阶段的索引字节超过配置限制的1.5倍后，节点会在replicate阶段reject这个索引请求。这种设计意味着由于indexing pressure应用于节点，它会天然的在发生未完成的replica工作时会停止coordinating和primary工作。

&emsp;&emsp;`indexing_pressure.memory.limit`的默认为`10%`。你应该深思熟虑后再去修改它。这个设置只应用于索引请求。这意味着其他的索引开销（buffers，listener，等等）同样需要堆内存。Elasticsearch的其他组件同样需要内存，这个值设置的太高会影响其他有内存操作（memory operation）的操作和组件。

#### Monitoring

&emsp;&emsp;你可以使用[node stats API](#Nodes stats API)查询indexing pressure metrics。

#### Indexing pressure settings

##### indexing_pressure.memory.limit

&emsp;&emsp;索引请求可能需要消费的字节数。当达到或者超过这个限制后，节点会reject新的coordinating和primary操作。当replica操作消费超过1.5倍，节点会reject新的replicate请求。默认值是堆内存的10%。

## Mapping
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mapping.html)

&emsp;&emsp;Mapping是定义一篇文档中的字段（field）如何存储以及索引的过程。

&emsp;&emsp;每篇文档都是字段的集合，每个字段都有自己的[date type](#Field data types)。在mapping你的数据时，你要创建一个mapping定义，定义中包含的字段的集合跟文档相关（pertinent）。mapping的定义同样包括了[metadata fields](#Metadata fields)。比如`_source`字段，它自定义了文档相关metadata的处理方式。

&emsp;&emsp;使用`dynamic mapping`和`explicit mapping`来定义你的数据。每一种方式在你使用数据过程中有不同的好处。例如，显示的对字段进行映射使得不按照默认方式进行处理，或者你想要获得对字段的创建有更多的控制，随后你可以允许Elasticsearch对其他的字段进行动态映射。

> NOTE：在7.0.0之前，mapping的定义包含一个类型名（type name）。Elasticsearch 7.0.0以及之后的版本再也不接受一个默认的mapping。见[Removal of mapping types](#Removal of mapping types)。
> 

**Experiment with mapping options**

&emsp;&emsp;使用[Define runtime fields in a search request](#Define runtime fields in a search request)来体验不同的mapping options，同时还可以在查询请求中通过覆盖mapping中字段值的方式来解决你的Index mapping中的错误。

#### Dynamic mapping(overview)

&emsp;&emsp;[Dynamic mapping](#Dynamic field mapping)适合你刚开始接触Elasticsearch/Mapping时体验数据的探索。Elasticsearch会自动的增加新的字段，你只需要索引文档就可以了。你可以添加字段到top-level mapping中，以及inner [object](#Object field type)和[nested](#Nested field type)字段中。

&emsp;&emsp;使用[dynamic templates](#Dynamic templates)自定义mappings，根据满足的条件动态的添加字段。

#### Explicit mapping(overview)

&emsp;&emsp;[Explicit mapping](#Explicit mapping)允许你精确的选择并定义mapping definition，例如：

- 哪些string字段应该作为full text field
- 哪些字段包含数值，日期或者地理位置
- 日期的[format](#format(mapping parameter))
- 为[dynamically added fields](#Dynamic mapping)自定义规则来控制mapping

&emsp;&emsp;在不需要reindex的前提下，使用[runtime fields](#Map a runtime field)作出策略变更。runtime field和indexed field配合使用来平衡资源使用和性能。索引文件体积会更小，但是会降低查询性能。

#### Settings to prevent mapping explosion

&emsp;&emsp;在一个索引中定义太多的字段会导致mapping explosion，引起OOM的错误以及难以恢复的情况。

&emsp;&emsp;想象这么一种场景，每一篇新的文档都会引入新的字段，比如使用了[dynamic mapping](#Dynamic mapping)。每一个字段添加到Index mapping中，随着mapping中字段的数量的增长便成长为了问题。

&emsp;&emsp;使用[mapping limit settings](#Mapping limit settings)限制mapping（手动或者动态创建）中字段的数量，防止因为文档导致mapping explosion。

### Dynamic mapping
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/dynamic-mapping.html)

&emsp;&emsp;Elasticsearch最重要的一个功能就是能够让你快速的去探索数据。索引一篇文档，你不需要首先创建一个索引，然后定义好mapping，接着定义每一个字段，你只需要直接对一篇文档进行索引，索引名、类型、字段都会自动的配置好。

```text
PUT data/_doc/1 
{ "count": 5 }
```

&emsp;&emsp;第1行，创建一个名为data的索引，以及`_doc`类型的mapping，和一个字段名为`count`，数据类型为`long`的字段。


&emsp;&emsp;这种自动检测并且添加（addition of）一个新字段的机制称为动态索引dynamic mapping。可以基于下列方法来自定义你的动态索引规则：

- [Dynamic field mappings](#Dynamic field mapping)：用于管理字段的检测的规则
- [Dynamic templates](#Dynamic templates)：配置自定义的规则实现自动的添加字段

> TIP：[Index templates](#Index templates)允许你为新的索引配置默认的mapping，settings 以及[aliases](#Aliases) ，不管是自动还是显示创建索引。

#### Dynamic field mapping
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/dynamic-field-mapping.html)

&emsp;&emsp;当Elasticsearch检测到文档中有一个新字段，会默认将它添加到mapping中。参数[dynamic](#dynamic(mapping parameter))控制了其行为方式。

&emsp;&emsp;你可以设置参数`dynamic`的值为`true`或者`runtime`来显示的指引Elasticsearch对于即将到来的文档动态的创建字段。当开启了dynamic field mapping ，Elasticsearch会根据下表中的规则决定如何映射为不同类型的字段。

> NOTE：Elasticsearch仅支持下表中字段的数据类型（[field data type](#Field data types)）的自动检测，你必须显式的指定其他数据类型。

|                      **JSON data type**                      |                **"dynamic":"true"**                |              **"dynamic":"runtime"**               |
| :----------------------------------------------------------: | :------------------------------------------------: | :------------------------------------------------: |
|                             null                             |                   No field added                   |                   No field added                   |
|                      `true` or `false`                       |                     `boolean`                      |                     `boolean`                      |
|                           `double`                           |                      `float`                       |                      `double`                      |
|                            `long`                            |                       `long`                       |                       `long`                       |
|                           `object`                           |                      `object`                      |                   No field added                   |
|                           `array`                            | Depends on the first non-`null` value in the array | Depends on the first non-`null` value in the array |
|  `string` that passes [date detection](#Date detection)  |                       `date`                       |                       `date`                       |
| `string` that passes [numeric detection](#Numeric detection) |                 `float` or `long`                  |                 `double` or `long`                 |
| `string` that doesn’t pass `date`detection or `numeric` detection |         `text` with a `.keyword`sub-field          |                     `keyword`                      |

&emsp;&emsp;你可以同时在文档层或者[object](#Object field type)层来关闭动态mapping。将参数`dynamic`设置为`false`来忽略新的字段，设置为`strict`来当Elasticsearch遇到一个未知的字段时就reject这个文档。

> TIP：可以通过[update mapping API](#Update mapping API)来对现有的字段的参数`dynamic`的值进行更新。

&emsp;&emsp;你可以为[Date detection](#Date detection)和[Numeric detection](#Numeric detection)自定义dynamic field mapping的规则。你可以使用[dynamic_templates](#Dynamic templates)来自定义个性化的mapping规则并应用到额外的dynamic fields。

##### Date detection

&emsp;&emsp;如果开启了`date_detection`（默认开启），新来的string类型的字段会被检测是否其字段值能够被`dynamic_date_formats`中指定的日期模板匹配到。如果匹配到了，就添加对应的新的[date](#Date field type)字段。

&emsp;&emsp;`dynamic_date_formats`的默认值是：
- [ "strict_date_optional_time","yyyy/MM/dd HH:mm:ss Z||yyyy/MM/dd Z"]

&emsp;&emsp;例如：

```text
PUT my-index-000001/_doc/1
{
  "create_date": "2015/09/02"
}

GET my-index-000001/_mapping 
```

&emsp;&emsp;第6行，`create_date`字段以`"yyyy/MM/dd HH:mm:ss Z||yyyy/MM/dd Z"`这种[format](#format(mapping parameter))作为[date](#Date field type)被添加。

##### Disabling date detection

&emsp;&emsp;可以将`date_detection`的值设置为`false`来关闭date detection。

```text
PUT my-index-000001
{
  "mappings": {
    "date_detection": false
  }
}

PUT my-index-000001/_doc/1 
{
  "create_date": "2015/09/02"
}
```

&emsp;&emsp;第8行，`create_date`被添加为[text](#Text type family)字段。

##### Customizing detected date formats

&emsp;&emsp;另外，`dynamic_date_formats`也能定制为你自己的[formats](#format(mapping parameter)))：

```text
PUT my-index-000001
{
  "mappings": {
    "dynamic_date_formats": ["MM/dd/yyyy"]
  }
}

PUT my-index-000001/_doc/1
{
  "create_date": "09/25/2015"
}
```

##### Numeric detection

&emsp;&emsp;JSON原生支持浮点型以及整数的数据类型，但是有些应用或者语言有时会将数值类型作为（render）字符串类型。通常正确的解决办法是显示的映射这些字段。但是numeric detection能使得自动实现（默认关闭）：

```text
PUT my-index-000001
{
  "mappings": {
    "numeric_detection": true
  }
}

PUT my-index-000001/_doc/1
{
  "my_float":   "1.0", 
  "my_integer": "1" 
}
```

&emsp;&emsp;第10行，`my_float`被添加为[float](#Numeric field types)字段
&emsp;&emsp;第11行，`my_integer`被添加为[integer](#Numeric field types)字段

#### Dynamic templates
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/dynamic-templates.html)

&emsp;&emsp;Dynamic template能让你在默认的[dynamic field mapping rules](#Dynamic field mapping)之外，更好的控制Elasticsearch如何映射你的数据。通过设置[dynamic](#dynamic(mapping parameter))参数的值为`true`或者`runtime`来开启dynamic mapping。然后你就可以基于下面的匹配条件使用dynamic template来自定义mapping，动态的应用（apply）到字段：

- [match_mapping_type](#match_mapping_type)对Elasticsearch检测到的数据类型进行操作
- [match and unmatch ](#match and unmatch)使用一个pattern对字段名进行匹配
- [path_match and path_unmatch](#path_match and path_unmatch)对full dotted path的字段名进行操作
- 如果dynamic template没有‘定义`match_mapping_type`，`match`或者`path_match`，那它不会匹配任何字段。 it won’t match any field. You can still refer to the template by name in dynamic_templates section of a [bulk request](#Bulk indexing usage)。

&emsp;&emsp;使用`{name}`和`{dynamic_type}`这些[template variables](#Template variables)作为mapping中的占位符。

> IMPORTANT：只有当某个字段包含一个具体的值才能添加Dynamic field mappings。当字段中包含`null`或者空的数组时，Elasticsearch不会添加一个dynamic field mapping。如果在`dynamic_template`中使用了`null_value`选项，只有在一篇有具体的值的文档索引后才能应用这个选项。 

&emsp;&emsp;Dynamic template由一组带名称的object数组组成：

```text
  "dynamic_templates": [
    {
      "my_template_name": { 
        ... match conditions ... 
        "mapping": { ... } 
      }
    },
    ...
  ]
```

&emsp;&emsp;第3行，模板的名称可以是任意的string value
&emsp;&emsp;第4行，匹配条件可以包含: `match_mapping_type`, `match`, `match_pattern`, `unmatch`, `path_match`, `path_unmatch`
&emsp;&emsp;第5行，匹配到的字段应该使用的mapping

##### Validating dynamic templates

&emsp;&emsp;如果提供的mapping中包含了一个非法的mapping 片段（snippet），则返回一个validation error。Validation会在索引期间应用dynamic template时发生，并且在大多数情况下发生在dynamic template更新后。提供一个非法的mapping片段可能会因为下面的一些条件导致更新或者dynamic template的校验失败：

- 如果你创建了一个动态模板（dynamic template）但没有指定match_mapping_type，这个模板仍然可以是有效的，前提是它至少对一个预定义的映射类型（如字符串）有效。然而，如果匹配了模板的某个字段索引为不同的类型，那么会返回validation error。例如，配置一个没有`match_mapping_type`的dynamic template在索引期间认为string类型是合法的，但是匹配了模板的某个字段索引为了long类型，那么会返回一个validation error。建议在mapping片段中配置`mathc_mapping_type`为期望的JSON类型或者想要的`type`
- 如果在mapping片段中使用`{name}`占位符，那当更新dynamic template时会跳过validation。这是因为在那个时间点字段名是未知的。在索引期间应用模版时才进行validation。

&emsp;&emsp;模版是有序处理的，使用匹配到的第一个模板。当通过[update mapping](#Update mapping API) API更新新的dynamic template后，所有现有的模板会被覆盖。这使得最开始添加的模板可以被重新排序或者被删除。

##### Mapping runtime fields in a dynamic template

&emsp;&emsp;如果你想要Elasticsearch动态映射某些类型的新字段作为runtime field，那么在索引mapping中设置`"dynamic":"runtime"`。这些字段不会被索引，在查询期间从`_source`中获取。

&emsp;&emsp;或者你可以使用默认的动态mapping规则，然后创建dynamic模版将指定的字段映射为runtime field。你可以在索引mapping中设置`"dynamic":"true"`然后创建一个dynamic template将某些类型的新字段映射为runtime field。

&emsp;&emsp;比如说你的数据中的每一个字段的字段名都是以`ip_`开始的。基于[dynamic mapping rules](#match_mapping_type)，Elasticsearch将通过了`numeric`检测的字段映射为`float`或者`long`，然而你可以创建一个dynamic template将string类型的值映射为runtime filed并且字段的类型为`ip`。

&emsp;&emsp;下面的请求中定义了一个名为`strings_as_ip`的dynamic template。当Elasticsearch检测到新的字段值为字符串字段匹配到了`ip*`这个pattern，它就会将这些字段映射为runtime field并且字段的类型为[ip](#ip)。因为`ip`字段没有动态映射，你可以使用带`"dynamic":"true"`或者`"dynamic":"runtime"`的模版。

```text
PUT my-index-000001/
{
  "mappings": {
    "dynamic_templates": [
      {
        "strings_as_ip": {
          "match_mapping_type": "string",
          "match": "ip*",
          "runtime": {
            "type": "ip"
          }
        }
      }
    ]
  }
}
```

&emsp;&emsp;见[this template](#text-only mappings for strings)来了解如何使用dynamic template将字符串字段映射为indexed filed或者runtime field。

##### match_mapping_type

&emsp;&emsp;`match_mapping_type`是数据类型，用于JSON parser。由于JSON不能区分`integer`和`long`，`double`和`float`，浮点型数值都被认为是`double`的JSON数据类型，`integer`数值都被认为是`long`。

> NOTE：在dynamic mappings中，Elasticsearch总是选择wider data type。唯一特例是`float`，它需要比`double`更少的存储空间并且对于大多数应用来说精度是足够的。Runtime field不支持`float`，这就是为什么`"dynamic":"runtime"`使用的是`double`。

&emsp;&emsp;Elasticsearch会自动检测下面的数据类型：

| JSON data type | "dynamic":"true" | "dynamic":"runtime" |
| :------------: | :--------------: | :-----------------: |
| null           | 不增加字段           |      不增加字段               |
| true/false | boolean | Boolean |
| double | float | double |
| long | long | long |
| object | object | 不增加字段 |
| array | 取决于数组中第一个不是null的值 | 取决于数组中第一个不是null的值 |
| string（通过了[Numeric detection](#Numeric detection)的解析） | float或者long | double或者long |
| string（通过了[date detection](#Date detection)的解析） | date | date |
| string没有通过date或者numeric的解析 | text以及.keyword的子字段 | keyword |


&emsp;&emsp;使用通配符(`*`)匹配所有的数据类型。

&emsp;&emsp;例如，如果想要将数值类型的字段都映射为`integer`而不是`long`，并且所有的字符串字段映射为`text`和`keyword`，我们可以使用下面的模板：

```text
PUT my-index-000001
{
  "mappings": {
    "dynamic_templates": [
      {
        "integers": {
          "match_mapping_type": "long",
          "mapping": {
            "type": "integer"
          }
        }
      },
      {
        "strings": {
          "match_mapping_type": "string",
          "mapping": {
            "type": "text",
            "fields": {
              "raw": {
                "type":  "keyword",
                "ignore_above": 256
              }
            }
          }
        }
      }
    ]
  }
}

PUT my-index-000001/_doc/1
{
  "my_integer": 5, 
  "my_string": "Some string" 
}
```

&emsp;&emsp;第33行，`my_integer`字段映射为一个`integer`
&emsp;&emsp;第34行，`my_string`字段映射为`text`以及一个`keyword`（见[multi-field](#fields)）

##### match and unmatch

&emsp;&emsp;`match`参数使用pattern来匹配字段名（field name），同时`unmatch`使用pattern排除`match`参数匹配到的字段。

&emsp;&emsp;`match_pattern`参数调整了`match`参数的行为，支持Java regular expression来匹配字段名而不是使用简单的通配符，例如：

```text
  "match_pattern": "regex",
  "match": "^profit_\d+$"
```

&emsp;&emsp;下面的例子匹配了所有字段名以`long_`开头的字段，但是排除了以`_text`结尾的字段，并将它们的字段值映射为`long`：

```text
PUT my-index-000001
{
  "mappings": {
    "dynamic_templates": [
      {
        "longs_as_strings": {
          "match_mapping_type": "string",
          "match":   "long_*",
          "unmatch": "*_text",
          "mapping": {
            "type": "long"
          }
        }
      }
    ]
  }
}

PUT my-index-000001/_doc/1
{
  "long_num": "5", 
  "long_text": "foo" 
}
```

&emsp;&emsp;第21行，`long_num`字段映射为`long`
&emsp;&emsp;第22行，`long_text`使用了默认的`string`映射


##### path_match and path_unmatch

&emsp;&emsp;`path_match`和`path_unmatch`参数的工作方式跟`match`和`unmatch`是一样的，不同的是，它们是应用在full dotted path上，而不是final name，比如：`some_object.*.some_filed`。

&emsp;&emsp;下面的例子将`object`类型的字段`name`中的除了`middle`的所有字段都`copy to`顶层的full_name中：

```text
PUT my-index-000001
{
  "mappings": {
    "dynamic_templates": [
      {
        "full_name": {
          "path_match":   "name.*",
          "path_unmatch": "*.middle",
          "mapping": {
            "type":       "text",
            "copy_to":    "full_name"
          }
        }
      }
    ]
  }
}

PUT my-index-000001/_doc/1
{
  "name": {
    "first":  "John",
    "middle": "Winston",
    "last":   "Lennon"
  }
}
```

&emsp;&emsp;注意的是`path_match`和`pathc_match`除了匹配leaf filed（上图中的`first`、`middle`、`last`就是leaf field）还会匹配object path（下图中的`name.tittle`就是object path）。例如，索引下面的文档会报错因为`path_match`这个设置同样会匹配到`name.tille`，这个字段是个object不能映射为text：

```text
PUT my-index-000001/_doc/2
{
  "name": {
    "first":  "Paul",
    "last":   "McCartney",
    "title": {
      "value": "Sir",
      "category": "order of chivalry"
    }
  }
}
```

##### Template variables

&emsp;&emsp;`{name}`和`{dynamic_type}`是mapping中的占位符，它们分别用字段名和检测到的动态类型（detected dynamic type）进行值的替换。下面的例子中将所有字段值为字符串的字段名作为[analyzer](#analyzer)的值，并且关闭所有不是字符串类型的字段的[doc_values](#doc_values)：

```text
PUT my-index-000001
{
  "mappings": {
    "dynamic_templates": [
      {
        "named_analyzers": {
          "match_mapping_type": "string",
          "match": "*",
          "mapping": {
            "type": "text",
            "analyzer": "{name}"
          }
        }
      },
      {
        "no_doc_values": {
          "match_mapping_type":"*",
          "mapping": {
            "type": "{dynamic_type}",
            "doc_values": false
          }
        }
      }
    ]
  }
}

PUT my-index-000001/_doc/1
{
  "english": "Some English text", 
  "count":   5 
}
```

&emsp;&emsp;第30行，因为字段名为`english`的字段值是`string`类型，那么它的字段名将作为anlyzer参数的值
&emsp;&emsp;第31行，因为`count`的字段值是个数值，所以它会被动态映射为`long`类型并且关闭`doc_values`

##### Dynamic template examples

&emsp;&emsp;下面是一些可能比较有用的dynamic template：

###### Structured search

&emsp;&emsp;当你设置了`"dynamic":"true"`，Elasticsearch会将字符串字段映射为`text`字段以及`keyword`的子字段。如果你只要索引结构化的内容并且对不需要全文检索，你可以让Elasticsearch只映射为`keyword`字段。然而，你在查询那些被索引的字段时，你必须提供精确的关键字。

```text
PUT my-index-000001
{
  "mappings": {
    "dynamic_templates": [
      {
        "strings_as_keywords": {
          "match_mapping_type": "string",
          "mapping": {
            "type": "keyword"
          }
        }
      }
    ]
  }
}
```

###### text-only mappings for strings

&emsp;&emsp;与上一个例子相反的（contrary）是，如果你只关心字符串字段上的全文检索并且不计划使用聚合，排序或者精确（exact ）查询，你可以让Elasticsearch映射为`text`了：

```text
PUT my-index-000001
{
  "mappings": {
    "dynamic_templates": [
      {
        "strings_as_text": {
          "match_mapping_type": "string",
          "mapping": {
            "type": "text"
          }
        }
      }
    ]
  }
}
```

&emsp;&emsp;或者你可以创建一个dynamic template将你的字符串字段在mapping的runtime块映射为`keyword`字段。当Elasticsearch检测到`string`类型的字段，会将这些字段创建为runtime field并且字段的类型为`keyword`。

&emsp;&emsp;尽管你的字符串字段不会被索引，但是它们的值会存储在`_source`中并且可以用于查询请求，聚合，过滤和排序。

&emsp;&emsp;例如下面的请求创建了一个dynamic template将字符串字段映射为runtime field并且字段的类型为`keyword`，尽管`runtime`的定义是空白的。但是基于Elasticsearch用于添加字段的类型到mapping的[dynamic mapping rules](#Dynamic field mapping)，新的字符串字段会被映射为runtime field并且字段的类型为`keyword`。所有没有通过date detection和numeric detection的`string`都会被自动的映射为`keyword`：

```text
PUT my-index-000001
{
  "mappings": {
    "dynamic_templates": [
      {
        "strings_as_keywords": {
          "match_mapping_type": "string",
          "runtime": {}
        }
      }
    ]
  }
}
```

&emsp;&emsp;你可以索引一篇简单的文档：

```text
PUT my-index-000001/_doc/1
{
  "english": "Some English text",
  "count":   5
}
```

&emsp;&emsp;当你查看mapping时，你会看到`english`字段是一个runtime field并且字段的类型为`keyword`。

```text
GET my-index-000001/_mapping
```

```text
{
  "my-index-000001" : {
    "mappings" : {
      "dynamic_templates" : [
        {
          "strings_as_keywords" : {
            "match_mapping_type" : "string",
            "runtime" : { }
          }
        }
      ],
      "runtime" : {
        "english" : {
          "type" : "keyword"
        }
      },
      "properties" : {
        "count" : {
          "type" : "long"
        }
      }
    }
  }
}
```

###### Disabled norms

&emsp;&emsp;Norms是索引期间（index-time）的打分因子。如果你不关心打分，那你就不能根据文档分数对文档进行排序，你可以禁用索引中的打分因子的存储来节省一些空间。

```text
PUT my-index-000001
{
  "mappings": {
    "dynamic_templates": [
      {
        "strings_as_keywords": {
          "match_mapping_type": "string",
          "mapping": {
            "type": "text",
            "norms": false,
            "fields": {
              "keyword": {
                "type": "keyword",
                "ignore_above": 256
              }
            }
          }
        }
      }
    ]
  }
}
```

&emsp;&emsp;模版中出现的`keyword`字段是dynamic mappings中默认存在的。当然如果你不需要在这个字段上执行精确查询或者聚合，你可以不需要它们，你可以正如上文中描述的那样来移除它们。

###### Time series

&emsp;&emsp;当在Elasticsearch中处理时序数据时，通常有很多的数值类型的字段，你会经常使用它们用于聚合但是从来不用于过滤。在这种场景下，你可以不索引这些字段来节省磁盘空间并且能获得一些索引速度的提升：

```text
PUT my-index-000001
{
  "mappings": {
    "dynamic_templates": [
      {
        "unindexed_longs": {
          "match_mapping_type": "long",
          "mapping": {
            "type": "long",
            "index": false
          }
        }
      },
      {
        "unindexed_doubles": {
          "match_mapping_type": "double",
          "mapping": {
            "type": "float", 
            "index": false
          }
        }
      }
    ]
  }
}
```

&emsp;&emsp;第18行，正如默认的dynamic mapping rules，double会被映射为`float`，因为通常来说精确是足够的，并且只要一半的磁盘空间。

### Explicit mapping
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/explicit-mapping.html)

&emsp;&emsp;相较于Elasticsearch猜测你的数据类型，如果你更了解数据类型，你可能想要指定自己的显示的mapping（explicit mapping）。

&emsp;&emsp;你可以在[create an index ](#Create an index with an explicit mapping)或者[add fields to an existing index](#Add a field to an existing mapping)时创建字段的mapping（field mapping）。

#### Create an index with an explicit mapping

&emsp;&emsp;你可以使用[create index](#Create index API) API创建一个explicit mapping的新的索引。

```text
PUT /my-index-000001
{
  "mappings": {
    "properties": {
      "age":    { "type": "integer" },  
      "email":  { "type": "keyword"  }, 
      "name":   { "type": "text"  }     
    }
  }
}
```

&emsp;&emsp;第5行，创建一个名为`age`，类型为[integer](#Numeric field types)的字段
&emsp;&emsp;第6行，创建一个名为`email`，类型为[keyword](#Keyword type family)的字段
&emsp;&emsp;第7行，创建一个名为`name`，类型为[text](#Text type family)的字段

#### Add a field to an existing mapping

&emsp;&emsp;你可以使用[update mapping](#Update mapping API) API在一个现有的（existing）索引中增加一个或者多个字段。

&emsp;&emsp;下面的例子中创建了一个名为`employee-id`，类型为`keyword`，mapping参数为[index](#index(mapping parameter))的字段。这意味着`employee-id`字段的字段值会被存储但是不会被索引，即无法用于查询。

```text
PUT /my-index-000001/_mapping
{
  "properties": {
    "employee-id": {
      "type": "keyword",
      "index": false
    }
  }
}
```

#### Update the mapping of a field

&emsp;&emsp;除了支持更新[mapping parameters](#Mapping parameters)，你不能对现有的字段更改mapping或者字段的类型。更改一个现有的字段会invalidate已经写入到索引文件中的数据。

&emsp;&emsp;如果你更改data stream的backing index中字段的类型，见[Change mappings and settings for a data stream](#Change mappings and settings for a data stream)。

&emsp;&emsp;如果你需要在其他索引中更改字段的mapping，那么使用正确的mapping创建一个新的索引，然后[reindex](#Reindex API)你的数据到那个索引中。

&emsp;&emsp;重命名一个字段会invalidate使用旧的字段名索引的数据，你可以增加一个[alias](#Alias field type)字段创建一个替换的字段名。

#### View the mapping of an index

&emsp;&emsp;你可以使用[get mapping](#Get mapping API) API查看现有的索引的mapping。

```text
GET /my-index-000001/_mapping
```

&emsp;&emsp;下面的API返回下面的响应：

```text
{
  "my-index-000001" : {
    "mappings" : {
      "properties" : {
        "age" : {
          "type" : "integer"
        },
        "email" : {
          "type" : "keyword"
        },
        "employee-id" : {
          "type" : "keyword",
          "index" : false
        },
        "name" : {
          "type" : "text"
        }
      }
    }
  }
}
```

#### View the mapping of specific fields

&emsp;&emsp;如果你只想指定一个或者多个字段并查看它们的mapping，你可以使用[get field mapping](#Get field mapping API) API。

&emsp;&emsp;如果你的索引包含很多的字段或者你不需要完整的索引的mapping，这个接口是很有用的。

&emsp;&emsp;下面的请求展示了`employee-id`字段的mapping。

```text
GET /my-index-000001/_mapping/field/employee-id
```

&emsp;&emsp;这个API返回下面的响应：

```text
{
  "my-index-000001" : {
    "mappings" : {
      "employee-id" : {
        "full_name" : "employee-id",
        "mapping" : {
          "employee-id" : {
            "type" : "keyword",
            "index" : false
          }
        }
      }
    }
  }
}
```

### Runtime fields
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/runtime.html)

&emsp;&emsp;`runtime field`是在查询阶段计算出来的字段，可以让你用于：

- 添加新的字段到现有的文档并且不需要重新索引你的数据
- 不需要你理解是如何结构化（ how it’s structured）的就可以开始处理你的数据
- 在查询期间覆盖索引中字段的值
- 在不用修改现有策略情况下，定义用于特殊用途的字段

&emsp;&emsp;你可以通过search API访问`runtime field`，就像访问其他字段一样，Elasticsearch不会对`runtime field`区别对待。你可以在[index mapping](#Map a runtime field)或者[search request](#Define runtime fields in a search request)中定义`runtime field`，提供不同的方式体现了`runtime field`内部的灵活性。

&emsp;&emsp;在`_search` API上使用[fields](#Retrieve selected fields from a search)参数来 [retrieve the values of runtime fields](#Retrieve a runtime field)，runtime filed不会在`_source`中展示，但是`fields` API可以在所有字段上使用，即使这些字段不在`_source`中。

&emsp;&emsp;Runtime fields用于处理日志数据（log data）时候特别好用（见[例子](#Explore your data with runtime fields)），特别是当你不确定数据结构时。尽管查询性能会降低，但是你能更快的索引日志数据，并且索引体积较小。

##### Benefits

&emsp;&emsp;由于runtime fiels没有被索引，所以新增的runtime field不会增加索引大小。你直接在索引mapping中定义runtime fields来节省存储开销以及提高提取（ingestion）速度。你能够更快的把数据提取到Elastic Stack并可以正确的访问。当你定义了一个runtime field，你可以在在查询请求中使用它用于聚合、过滤、和排序。

&emsp;&emsp;如果你让一个runtime field成为一个索引字段（indexed field），你不需要修改任何请求来指定runtime filed。甚至你可以指定某个字段在某些索引中是runtime field，同时在某个索引中是一个索引字段。你可以灵活的选择哪些字段作为索引字段还是runtime field。

&emsp;&emsp;Runtime fileds最核心、最重要的好处就是它提供了在提取（ingest）文档后可以在这篇文档中添加字段的能力。这种能力简化了mapping的设计，因为你不需要提前决定用哪种数据类型进行解析，可以在任何时候修改。使用runtime field使得有更小的索引和更快的提取时间，这将使用更少的资源并降低你的操作成本。

##### Incentives

&emsp;&emsp;使用脚本的`_search` API中的很多方法可以用runtime field替代。runtime field的使用会受到runtime field中定义的脚本需要处理的文档数量的影响。例如，如果你在`_search` API上使用`fields`参数来[retrieve the values of a runtime field](#Retrieve a runtime field)，脚本跟[script fields ](#Script fields)一样，都只会对top hits进行处理。

&emsp;&emsp;你可以使用script fields访问`_source`中的值并且返回基于脚本计算出的值。runtime fields有相同的能力，但是能提供更好的灵活性，因为在查询请求中可以对runtime fields进行查询以及聚合。Script field只能获取数据。

&emsp;&emsp;同样的，你可以在查询请求中基于脚本写一个[script query](#Script query)来过滤文档。runtime field提供一个类似的功能并且更加灵活。你编写一个脚本来创建字段值，它们能在任何地方都可见，比如[fields](#Retrieve selected fields from a search)，[all queries](#Query DSL)以及[aggregation](#Aggregations)。

&emsp;&emsp;你可以使用脚本来[sort search results](#Sort search results)，但使用runtime field可以完成完全相同的工作。

&emsp;&emsp;如果你将查询请求中所有的脚本替换为runtime field，性能是差不多的，因为它们计算的文档数量是一样的。这个功能的性能很大程度上取决于脚本需要计算的文档数量。

##### Compromises

&emsp;&emsp;Runtime fields使用更少的磁盘空间并且在查询你的数据上提供灵活性，但是会因为runtime脚本中定义的计算而影响查询性能。

&emsp;&emsp;若要平衡性能和灵活性，将那些你常用于查询，过滤，例如timestamp这类字段，对这些字段进行索引。在执行查询时，Elasticsearch会自动的使用这些索引字段（indexed field）在很快的响应时间内获得结果。随后你可以使用runtime fields来限制Elasticsearch需要用于计算的字段的数量。索引字段和runtime field的配合使用，可以提供数据弹性以及如何为其他的字段定义查询。

&emsp;&emsp;使用[asynchronous search API](#Async search)用于执行包含runtime field的查询。这种查询方法可以抵偿计算每一篇包含runtime field的文档的性能影响。如果query不能同步返回数据集，你将在这些结果变得可见后异步获取到。

> IMPORTANT：对runtime field的查询是昂贵的，如果[search.allow_expensive_queries](#Query DSL)设置为`false`，Elasticsearch不允许昂贵的查询并且reject所有对runtime field的查询。

#### Map a runtime field
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/runtime-mapping-fields.html)

&emsp;&emsp;通过在mapping中定义一个`runtime` section以及一个[Painless script](#How to write scripts)就可以映射到runtime fileds。这个脚本可以访问所在文档中所有的内容，包括可以通过`params._source`来读取到`_source`的内容以及任何字段加上字段值。在查询时，查询条件中包含了这个scripted field（下文中的day_of_week），脚本会运行并且生成结果给scripted field。

>当定义了一个脚本用于runtime fields时，脚本中必须使用 [emit method](https://www.elastic.co/guide/en/elasticsearch/painless/8.2/painless-runtime-fields-context.html)来输出计算的值。

&emsp;&emsp;下面例子的脚本中，将根据`@timestamp`的字段值计算出的结果赋值给`day of the week`，`day of the week`是一个`data`类型：

```text
PUT my-index-000001/
{
  "mappings": {
    "runtime": {
      "day_of_week": {
        "type": "keyword",
        "script": {
          "source": "emit(doc['@timestamp'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ROOT))"
        }
      }
    },
    "properties": {
      "@timestamp": {"type": "date"}
    }
  }
}
```

&emsp;&emsp;`runtime`中可以是以下的类型：

- boolean
- composite
- date
- double
- geo_point
- ip
- keyword
- long
- [lookup](#Retrieve a runtime field)

&emsp;&emsp;`date`类型的runtime fileds的字段值格式跟`date`字段接受的[format](#format(mapping parameter))是一致的。

&emsp;&emsp;`lookup`类型的runtime fileds允许从关联的索引中检索字段的信息，见[retrieve fields from related indices](#Retrieve a runtime field)。

&emsp;&emsp;如果开启了[dynamic field mapping](#Dynamic field mapping)，并且`dynamic`参数设置了参数值`runtime`，新的字段会作为runtime自动添加到索引mapping中。

```text
PUT my-index-000001
{
  "mappings": {
    "dynamic": "runtime",
    "properties": {
      "@timestamp": {
        "type": "date"
      }
    }
  }
}
```

##### Define runtime fields without a script

&emsp;&emsp;Runtime fileds通常使用脚本来管理数据，然后有些情况下可以不使用脚本来使用runtime fileds。比如你想从`_source`中检索出某个字段的字段值并不作任何的改变，那你不需要提供脚本，你只需要创建一个不带脚本的runtime fileds：

```text
PUT my-index-000001/
{
  "mappings": {
    "runtime": {
      "day_of_week": {
        "type": "keyword"
      }
    }
  }
}
```

&emsp;&emsp;当没有提供脚本时，在查询期间，Elasticsearch会从`_source`中查找到跟runtime fields字段名一样的字段，如果存在的话就返回该字段值。如果不存在，那么在response中不会包含runtime fields的任何值。

&emsp;&emsp;在大多数情况下，会优先从[doc_values](#doc_values)中读取。由于在Lucene中不同的存储方式，相较于从`_source`中检索，通过[doc_values](#doc_values)的方式读取速度更快。

&emsp;&emsp;但是有些场景下需要从`_source`中检索字段的信息。比如说由于`text`类型默认情况下不会有`doc_values`，所以不得不从`_source`中读取，在其他情况下，可以选择禁用特定字段上的doc_values。

>NOTE：你可以在`params._source`中添加前缀来检索想要的数据（比如`params._source.day_of_week`）,为了简单起见，定义runtime fields时，建议尽可能在mapping定义中不使用脚本。

##### Updating and removing runtime fields

&emsp;&emsp;你可以在任何时候更新或者移除runtime fileds。添加一个相同名称的runtime files就可以覆盖现在有的runtime files，通过设置为null来移除现在有的runtime files：

```text
PUT my-index-000001/_mapping
{
 "runtime": {
   "day_of_week": null
 }
}
```

###### Downstream impacts

&emsp;&emsp;更新或者移除runtime files可能会使得正在查询的请求返回的数据不一致。由于mapping的变更可能会影响到每一个分片会访问到不同版本的脚本。

>WARNING：如果你对runtime fileds进行变更或者移除，依赖runtime fileds的Existing queries or visualizations in Kibana可能会出现失败的情况，比如可视化的图表使用runtime field类型的`ip`被改为`boolean`，或者被移除后会导致出错。

#### Define runtime fields in a search request
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/runtime-search-request.html)

&emsp;&emsp;你可以在查询请求中指定一个`runtime_mappings`区域/块（section）来创建runtime fields，它只属于这次的请求。你指定了一个脚本作为`runtime_mappings`的一部分跟[adding a runtime field to the mappings](#Map a runtime field)是一样的。

&emsp;&emsp;在查询请求中定义一个runtime field跟在index mapping中使用的格式是一样的，所以只需要把查询请求中`runtime_mappings`区域中的内容直接拷贝到index mapping的`runtime`区域中。

&emsp;&emsp;下面的查询请求添加了`day_of_week`字段到`runtime_mappings`区域中，这个字段的字段值将被自动的计算出来并且只处于这次查询请求的上下文中（only within the context of this search request）。

```text
GET my-index-000001/_search
{
  "runtime_mappings": {
    "day_of_week": {
      "type": "keyword",
      "script": {
        "source": "emit(doc['@timestamp'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ROOT))"
      }
    }
  },
  "aggs": {
    "day_of_week": {
      "terms": {
        "field": "day_of_week"
      }
    }
  }
}
```

##### Create runtime fields that use other runtime fields

&emsp;&emsp;你甚至可以在查询请求中定义runtime field，并且它的字段值可以从其他的runtime field中计算出来。比如你批量索引了传感器的数据：

```text
POST my-index-000001/_bulk?refresh=true
{"index":{}}
{"@timestamp":1516729294000,"model_number":"QVKC92Q","measures":{"voltage":"5.2","start": "300","end":"8675309"}}
{"index":{}}
{"@timestamp":1516642894000,"model_number":"QVKC92Q","measures":{"voltage":"5.8","start": "300","end":"8675309"}}
{"index":{}}
{"@timestamp":1516556494000,"model_number":"QVKC92Q","measures":{"voltage":"5.1","start": "300","end":"8675309"}}
{"index":{}}
{"@timestamp":1516470094000,"model_number":"QVKC92Q","measures":{"voltage":"5.6","start": "300","end":"8675309"}}
{"index":{}}
{"@timestamp":1516383694000,"model_number":"HG537PU","measures":{"voltage":"4.2","start": "400","end":"8625309"}}
{"index":{}}
{"@timestamp":1516297294000,"model_number":"HG537PU","measures":{"voltage":"4.0","start": "400","end":"8625309"}}
```

&emsp;&emsp;你意识到你将数值类型numeric type索引成了`text`类型，你想要在`measures.start`以及`measures.end`字段上进行聚合操作却无法实现，因为你不能在`text`类型上进行聚合。Runtime fields to the rescue!。你可以添加runtime fileds，跟索引字段index field（即measures.start和measures.end）的字段名保持一致并且修改字段的类型：

```text
PUT my-index-000001/_mapping
{
  "runtime": {
    "measures.start": {
      "type": "long"
    },
    "measures.end": {
      "type": "long"
    }
  }
}
```

&emsp;&emsp;Runtime field的优先级（take precedence over）比相同字段名的索引字段高。这种灵活性使得你可以投影（shadow）现有的字段（existing fields）并且计算出新值而不用修改字段本身的信息。如果你在索引数据的时候犯错了，那么你可以使用runtime field在查询阶段重新计算，将获得的值进行[override values](#Override field values at query time)。

&emsp;&emsp;现在你可以很容易的在`measures.start`和`measures.end`字段上执行[average aggregation](#Avg aggregation)：

```text
GET my-index-000001/_search
{
  "aggs": {
    "avg_start": {
      "avg": {
        "field": "measures.start"
      }
    },
    "avg_end": {
      "avg": {
        "field": "measures.end"
      }
    }
  }
}
```

&emsp;&emsp;响应中返回了聚合结果并且没有改变 the underlying data（指的是measures.start跟measures.end）：

```text
{
  "aggregations" : {
    "avg_start" : {
      "value" : 333.3333333333333
    },
    "avg_end" : {
      "value" : 8658642.333333334
    }
  }
}
```

&emsp;&emsp;另外你还可以定义一个runtime field作为查询请求中的一部分来计算出一个值，在相同的query（基于上文中介绍的query）中对这个runtime field进行[stats aggregation](#Metrics aggregations)。

&emsp;&emsp;`duration`这个runtime field不在 index mapping中，当我们仍然可以使用这个字段进行查询和聚合计算。下面的请求将返回经过计算的`duration`的值并且从用于聚合的文档中基于数值类型的数据执行stats aggregation来计算统计值（即count、min、max、avg、sum）：

```text
GET my-index-000001/_search
{
  "runtime_mappings": {
    "duration": {
      "type": "long",
      "script": {
        "source": """
          emit(doc['measures.end'].value - doc['measures.start'].value);
          """
      }
    }
  },
  "aggs": {
    "duration_stats": {
      "stats": {
        "field": "duration"
      }
    }
  }
}
```

&emsp;&emsp;尽管 `duration`这个runtime field只在这次查询请求的上下文中存在，你也可以对该字段进行搜索和聚合。灵活性十分的强大，它可以弥补（rectify）你在index mapping中犯的错误并且在单个查询请求中动态的完成计算。

```text
{
  "aggregations" : {
    "duration_stats" : {
      "count" : 6,
      "min" : 8624909.0,
      "max" : 8675009.0,
      "avg" : 8658309.0,
      "sum" : 5.1949854E7
    }
  }
}
```



#### Override field values at query time
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/runtime-override-values.html)

&emsp;&emsp;如果您创建的runtime field与mapping已经存在的字段具有相同的名称，则runtime field会将对mapping filed进行投影（shadow）。在查询时，Elasticsearch计算runtime field，根据脚本计算一个值，并将该值作为查询的一部分返回。因为runtime field会对mapping field进行投影（shadow），所以你可以覆盖搜索中返回的值，而不需要修改mapping field。

&emsp;&emsp;例如你在索引`my-index-000001`中添加了以下的文档：

```text
POST my-index-000001/_bulk?refresh=true
{"index":{}}
{"@timestamp":1516729294000,"model_number":"QVKC92Q","measures":{"voltage":5.2}}
{"index":{}}
{"@timestamp":1516642894000,"model_number":"QVKC92Q","measures":{"voltage":5.8}}
{"index":{}}
{"@timestamp":1516556494000,"model_number":"QVKC92Q","measures":{"voltage":5.1}}
{"index":{}}
{"@timestamp":1516470094000,"model_number":"QVKC92Q","measures":{"voltage":5.6}}
{"index":{}}
{"@timestamp":1516383694000,"model_number":"HG537PU","measures":{"voltage":4.2}}
{"index":{}}
{"@timestamp":1516297294000,"model_number":"HG537PU","measures":{"voltage":4.0}}
```

&emsp;&emsp;你后来意识到`HG537PU`传感器并没有报告它们的真实电压。索引值应该是报告值的1.7倍。你可以在`_search`请求的`runtime_mappings`部分定义一个脚本，来投影（shadow）电压场，并在查询时计算一个新值，而不是重新索引数据。

&emsp;&emsp;如果你想要查询 model number是`HG537PU`的文档：

```text
GET my-index-000001/_search
{
  "query": {
    "match": {
      "model_number": "HG537PU"
    }
  }
}
```

&emsp;&emsp;相应中包含了model number是`HG537PU`的索引值：

```text
{
  ...
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 1.0296195,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_id" : "F1BeSXYBg_szTodcYCmk",
        "_score" : 1.0296195,
        "_source" : {
          "@timestamp" : 1516383694000,
          "model_number" : "HG537PU",
          "measures" : {
            "voltage" : 4.2
          }
        }
      },
      {
        "_index" : "my-index-000001",
        "_id" : "l02aSXYBkpNf6QRDO62Q",
        "_score" : 1.0296195,
        "_source" : {
          "@timestamp" : 1516297294000,
          "model_number" : "HG537PU",
          "measures" : {
            "voltage" : 4.0
          }
        }
      }
    ]
  }
}
```

&emsp;&emsp;下面的请求中定义了一个runtime field，它的脚本中将估算字段值为`HG537PU`的`model_number`字段，会对于每一个满足条件的`voltage`的字段值乘以1.7。

&emsp;&emsp;通过在`_serach` API中使用[field](#Retrieve selected fields from a search)参数，你可以检索`measures.voltage field`，满足查询请求中的文档会根据脚本对该字段值进行计算。

```text
POST my-index-000001/_search
{
  "runtime_mappings": {
    "measures.voltage": {
      "type": "double",
      "script": {
        "source":
        """if (doc['model_number.keyword'].value.equals('HG537PU'))
        {emit(1.7 * params._source['measures']['voltage']);}
        else{emit(params._source['measures']['voltage']);}"""
      }
    }
  },
  "query": {
    "match": {
      "model_number": "HG537PU"
    }
  },
  "fields": ["measures.voltage"]
}
```

&emsp;&emsp;我们从结果中可以看到，经过计算的`measures.voltage`的字段值分别是`7.14`和`6.8`。That’s more like it！。runtime field作为查询请求的一部分进行字段值计算，而不修改索引值，索引值仍然在响应中返回：

```text
{
  ...
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 1.0296195,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_id" : "F1BeSXYBg_szTodcYCmk",
        "_score" : 1.0296195,
        "_source" : {
          "@timestamp" : 1516383694000,
          "model_number" : "HG537PU",
          "measures" : {
            "voltage" : 4.2
          }
        },
        "fields" : {
          "measures.voltage" : [
            7.14
          ]
        }
      },
      {
        "_index" : "my-index-000001",
        "_id" : "l02aSXYBkpNf6QRDO62Q",
        "_score" : 1.0296195,
        "_source" : {
          "@timestamp" : 1516297294000,
          "model_number" : "HG537PU",
          "measures" : {
            "voltage" : 4.0
          }
        },
        "fields" : {
          "measures.voltage" : [
            6.8
          ]
        }
      }
    ]
  }
}
```

#### Retrieve a runtime field
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/runtime-retrieving-fields.html)

&emsp;&emsp;在`_search` API中使用参数[fields](#Retrieve selected fields from a search)来检索runtime fields的字段值。`_source`中不会展示runtime fields的信息，但是`fields`可以展示所有字段的信息，即使有些字段不是`_source`中的一部分。

##### Define a runtime field to calculate the day of week

&emsp;&emsp;例如下面的请求中增加了一个名为`day_of_week`的runtime filed。这个runtime filed中包含了一个根据`@timestamp`字段计算星期的脚本。请求中定义了**"[dynamic](#dynamic(mapping parameter))":"runtime"**使得新添加的字段在mapping中将作为runtime fileds。

```text
PUT my-index-000001/
{
  "mappings": {
    "dynamic": "runtime",
    "runtime": {
      "day_of_week": {
        "type": "keyword",
        "script": {
          "source": "emit(doc['@timestamp'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ROOT))"
        }
      }
    },
    "properties": {
      "@timestamp": {"type": "date"}
    }
  }
}
```

##### Ingest some data

&emsp;&emsp;我们ingest一些样本数据，有两个索引字段`@timestamp`和`message`：

```text
POST /my-index-000001/_bulk?refresh
{ "index": {}}
{ "@timestamp": "2020-06-21T15:00:01-05:00", "message" : "211.11.9.0 - - [2020-06-21T15:00:01-05:00] \"GET /english/index.html HTTP/1.0\" 304 0"}
{ "index": {}}
{ "@timestamp": "2020-06-21T15:00:01-05:00", "message" : "211.11.9.0 - - [2020-06-21T15:00:01-05:00] \"GET /english/index.html HTTP/1.0\" 304 0"}
{ "index": {}}
{ "@timestamp": "2020-04-30T14:30:17-05:00", "message" : "40.135.0.0 - - [2020-04-30T14:30:17-05:00] \"GET /images/hm_bg.jpg HTTP/1.0\" 200 24736"}
{ "index": {}}
{ "@timestamp": "2020-04-30T14:30:53-05:00", "message" : "232.0.0.0 - - [2020-04-30T14:30:53-05:00] \"GET /images/hm_bg.jpg HTTP/1.0\" 200 24736"}
{ "index": {}}
{ "@timestamp": "2020-04-30T14:31:12-05:00", "message" : "26.1.0.0 - - [2020-04-30T14:31:12-05:00] \"GET /images/hm_bg.jpg HTTP/1.0\" 200 24736"}
{ "index": {}}
{ "@timestamp": "2020-04-30T14:31:19-05:00", "message" : "247.37.0.0 - - [2020-04-30T14:31:19-05:00] \"GET /french/splash_inet.html HTTP/1.0\" 200 3781"}
{ "index": {}}
{ "@timestamp": "2020-04-30T14:31:27-05:00", "message" : "252.0.0.0 - - [2020-04-30T14:31:27-05:00] \"GET /images/hm_bg.jpg HTTP/1.0\" 200 24736"}
{ "index": {}}
{ "@timestamp": "2020-04-30T14:31:29-05:00", "message" : "247.37.0.0 - - [2020-04-30T14:31:29-05:00] \"GET /images/hm_brdl.gif HTTP/1.0\" 304 0"}
{ "index": {}}
{ "@timestamp": "2020-04-30T14:31:29-05:00", "message" : "247.37.0.0 - - [2020-04-30T14:31:29-05:00] \"GET /images/hm_arw.gif HTTP/1.0\" 304 0"}
{ "index": {}}
{ "@timestamp": "2020-04-30T14:31:32-05:00", "message" : "247.37.0.0 - - [2020-04-30T14:31:32-05:00] \"GET /images/nav_bg_top.gif HTTP/1.0\" 200 929"}
{ "index": {}}
{ "@timestamp": "2020-04-30T14:31:43-05:00", "message" : "247.37.0.0 - - [2020-04-30T14:31:43-05:00] \"GET /french/images/nav_venue_off.gif HTTP/1.0\" 304 0"}
```

##### Search for the calculated day of week

&emsp;&emsp;下面的请求使用 search API检索`day_of_week`字段，这个字段在mapping定义为runtime field。这个字段的字段值在查询期间动态的生成，并不需要重新对数据索引，也不需要索引这个字段。这种灵活性使得不需要更改索引字段的情况下改变映射：

```text
GET my-index-000001/_search
{
  "fields": [
    "@timestamp",
    "day_of_week"
  ],
  "_source": false
}
```

&emsp;&emsp;上述请求会为每一个匹配的文档返回一个`day_of_week`字段。我们可以定义另一个名为`client_ip`的runtime filed，对`message`字段进行操作来获取它的字段值来完善接下来的查询。

```text
PUT /my-index-000001/_mapping
{
  "runtime": {
    "client_ip": {
      "type": "ip",
      "script" : {
      "source" : "String m = doc[\"message\"].value; int end = m.indexOf(\" \"); emit(m.substring(0, end));"
      }
    }
  }
}
```

&emsp;&emsp;使用字段名为`client_ip`的runtime filed，指定一个字段值来进行查询：

```text
GET my-index-000001/_search
{
  "size": 1,
  "query": {
    "match": {
      "client_ip": "211.11.9.0"
    }
  },
  "fields" : ["*"]
}
```

&emsp;&emsp;这次查询的响应中命中2条，`day_of_week`（`sunday`）的值会在查询中使用mapping中定义的脚本计算获得。结果中包含了IP address的值为`211.11.9.0`的文档：

```text
{
  ...
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_id" : "oWs5KXYB-XyJbifr9mrz",
        "_score" : 1.0,
        "_source" : {
          "@timestamp" : "2020-06-21T15:00:01-05:00",
          "message" : "211.11.9.0 - - [2020-06-21T15:00:01-05:00] \"GET /english/index.html HTTP/1.0\" 304 0"
        },
        "fields" : {
          "@timestamp" : [
            "2020-06-21T20:00:01.000Z"
          ],
          "client_ip" : [
            "211.11.9.0"
          ],
          "message" : [
            "211.11.9.0 - - [2020-06-21T15:00:01-05:00] \"GET /english/index.html HTTP/1.0\" 304 0"
          ],
          "day_of_week" : [
            "Sunday"
          ]
        }
      }
    ]
  }
}
```

##### Retrieve fields from related indices

>WARNING：这个功能属于技术预览（technical preview ），可能在未来的发行版中变更或者移除。

&emsp;&emsp;`_serach` API中的[fields](#Retrieve selected fields from a search)参数可以用来检索类型为`lookup`的runtime fileds，它的字段值可以从其他索引中获取。

```text
POST ip_location/_doc?refresh
{
  "ip": "192.168.1.1",
  "country": "Canada",
  "city": "Montreal"
}

PUT logs/_doc/1?refresh
{
  "host": "192.168.1.1",
  "message": "the first message"
}

PUT logs/_doc/2?refresh
{
  "host": "192.168.1.2",
  "message": "the second message"
}

POST logs/_search
{
  "runtime_mappings": {
    "location": {
        "type": "lookup", 
        "target_index": "ip_location", 
        "input_field": "host", 
        "target_field": "ip", 
        "fetch_fields": ["country", "city"] 
    }
  },
  "fields": [
    "host",
    "message",
    "location"
  ],
  "_source": false
}
```

&emsp;&emsp;第24行，定义了类型为`lookup`的runtime field，从目标索引中使用[term](#Term query)查询获得的结果作为runtime field的字段值
&emsp;&emsp;第25行，ip_location即目标索引
&emsp;&emsp;第26行，host字段的字段值将作为term查询条件中的字段值
&emsp;&emsp;第27行，ip字段的字段名将作为term查询条件中的字段名
&emsp;&emsp;第28行，要求从目标索引ip_location中返回的字段，见[fields](#Retrieve selected fields from a search)
&emsp;&emsp;上述的查询将从ip_location索引中为返回的搜索命中的每个ip地址返回country和city。

```text
{
  "took": 3,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 2,
      "relation": "eq"
    },
    "max_score": 1.0,
    "hits": [
      {
        "_index": "logs",
        "_id": "1",
        "_score": 1.0,
        "fields": {
          "host": [ "192.168.1.1" ],
          "location": [
            {
              "city": [ "Montreal" ],
              "country": [ "Canada" ]
            }
          ],
          "message": [ "the first message" ]
        }
      },
      {
        "_index": "logs",
        "_id": "2",
        "_score": 1.0,
        "fields": {
          "host": [ "192.168.1.2" ],
          "message": [ "the second message" ]
        }
      }
    ]
  }
}
```

&emsp;&emsp;在返回的结果中，每一个结果中单独的包含了一个从目标索引中获取的数据，这些数据分组展示（JSON中的array）。每一个结果中最多展示一条从目标索引中获取的数据，如果上文中的term查询获取了多个结果，那么就随机返回一条。

#### Index a runtime field
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/runtime-indexed.html)

&emsp;&emsp;Runtime fields在运行时的上下文中定义，你可以在[search query](#Define runtime fields in a search request) 的上下文定义runtime field或者在index mapping中定义一个[runtime](#Map a runtime field)区域（section）。如果你决定将runtime field写入到索引来获得较好的性能，你只要把完整的runtime filed的定义（包括脚本）都移动到 index mapping的上下文中。Elasticsearch会自动的使用这些索引字段来驱动查询，获得更快的响应时间。这种能力使得只需要你写一次脚本并应用到任何支持runtime field的上文中。

>NOTE：目前不支持索引composite runtime field。

&emsp;&emsp;你可以使用runtime filed来限制Elasticsearch需要计算的字段的数量。将索引字段与runtime filed结合使用可以为你索引的数据以及如何为其他字段定义查询方式提供灵活性。

>IMPORTANT：当你把runtime field写入到索引中，你不能更新其包含的脚本。如果你需要更新脚本，那么使用这个更新后的脚本并且创建一个新的字段。

&emsp;&emsp;比如你的公司想要替换一些旧的压力值。已经连接的传感器只能报告真实读数的一小部分。相较于使用新的传感器来得到新的压力值，你决定基于现有的读数进行计算。基于现有的报告值，你可以为在索引`my-index-000001`中定义下列的mapping：

```text
PUT my-index-000001/
{
  "mappings": {
    "properties": {
      "timestamp": {
        "type": "date"
      },
      "temperature": {
        "type": "long"
      },
      "voltage": {
        "type": "double"
      },
      "node": {
        "type": "keyword"
      }
    }
  }
}
```

&emsp;&emsp;你批量写入了传感器的一些样本数据，包括了从每一个传感器中的voltage的读数：

```text
POST my-index-000001/_bulk?refresh=true
{"index":{}}
{"timestamp": 1516729294000, "temperature": 200, "voltage": 5.2, "node": "a"}
{"index":{}}
{"timestamp": 1516642894000, "temperature": 201, "voltage": 5.8, "node": "b"}
{"index":{}}
{"timestamp": 1516556494000, "temperature": 202, "voltage": 5.1, "node": "a"}
{"index":{}}
{"timestamp": 1516470094000, "temperature": 198, "voltage": 5.6, "node": "b"}
{"index":{}}
{"timestamp": 1516383694000, "temperature": 200, "voltage": 4.2, "node": "c"}
{"index":{}}
{"timestamp": 1516297294000, "temperature": 202, "voltage": 4.0, "node": "c"}
```

&emsp;&emsp;在跟一些网页工程师沟通后，你们意识到传感器应该报告2倍于现在的压力值，并且有可能更高。你创建了一个名为`voltage_corrected`的runtime filed，然后检索当前的`voltage`的值并且乘以2：

```text
PUT my-index-000001/_mapping
{
  "runtime": {
    "voltage_corrected": {
      "type": "double",
      "script": {
        "source": """
        emit(doc['voltage'].value * params['multiplier'])
        """,
        "params": {
          "multiplier": 2
        }
      }
    }
  }
}
```

&emsp;&emsp;在`_search` API上使用[fields](#Retrieve selected fields from a search)参数来检索计算后的值：

```text
GET my-index-000001/_search
{
  "fields": [
    "voltage_corrected",
    "node"
  ],
  "size": 2
}
```

&emsp;&emsp;在审查了传感器数据以及运行了一些测试后，对于报告的传感器数据，`multiplier`的值应该是`4`。为了获得更高的性能，你决定把`voltage_corrected`字段写入到索引中，并且使用新的`multiplier`参数：

&emsp;&emsp;只要在一个新的名为`my-index-000001`索引中，把名为`voltage_corrected`的runtime field中的所有定义直接复制到新索引的mappings中，就是这么简单。你可以额外定义一个参数`on_script_error`来控制在索引期间当脚本抛出异常时是否要reject整个文档。

```text
PUT my-index-000001/
{
  "mappings": {
    "properties": {
      "timestamp": {
        "type": "date"
      },
      "temperature": {
        "type": "long"
      },
      "voltage": {
        "type": "double"
      },
      "node": {
        "type": "keyword"
      },
      "voltage_corrected": {
        "type": "double",
        "on_script_error": "fail", 
        "script": {
          "source": """
        emit(doc['voltage'].value * params['multiplier'])
        """,
          "params": {
            "multiplier": 4
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;第19行，在索引期间当脚本抛出异常时文档会被reject，将`on_script_error`的值置为`ignore`使得将`voltage_corrected`注册到[\_ignored](#\_ignored field)的metadata filed中然后继续索引（索引当前文档的其他字段）。

&emsp;&emsp;批量写入传感器的数据到`my-index-000001`中：

```text
POST my-index-000001/_bulk?refresh=true
{ "index": {}}
{ "timestamp": 1516729294000, "temperature": 200, "voltage": 5.2, "node": "a"}
{ "index": {}}
{ "timestamp": 1516642894000, "temperature": 201, "voltage": 5.8, "node": "b"}
{ "index": {}}
{ "timestamp": 1516556494000, "temperature": 202, "voltage": 5.1, "node": "a"}
{ "index": {}}
{ "timestamp": 1516470094000, "temperature": 198, "voltage": 5.6, "node": "b"}
{ "index": {}}
{ "timestamp": 1516383694000, "temperature": 200, "voltage": 4.2, "node": "c"}
{ "index": {}}
{ "timestamp": 1516297294000, "temperature": 202, "voltage": 4.0, "node": "c"}
```

&emsp;&emsp;你现在可以搜索查询中检索计算后的值，并且找到精确数值的文档。下面的范围查询返回`voltage_corrected`的值大于等于`16`并且小于等于`20`的所有文档。这里重申一遍，在`_search` API上使用[fields](#Retrieve selected fields from a search)参数来检索你想要查询的字段：

```text
POST my-index-000001/_search
{
  "query": {
    "range": {
      "voltage_corrected": {
        "gte": 16,
        "lte": 20,
        "boost": 1.0
      }
    }
  },
  "fields": ["voltage_corrected", "node"]
}
```

&emsp;&emsp;响应中返回了满足范围查询的包含`voltage_corrected`字段的文档，`voltage_corrected`的值基于脚本中计算出的值：

```text
{
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_id" : "yoSLrHgBdg9xpPrUZz_P",
        "_score" : 1.0,
        "_source" : {
          "timestamp" : 1516383694000,
          "temperature" : 200,
          "voltage" : 4.2,
          "node" : "c"
        },
        "fields" : {
          "voltage_corrected" : [
            16.8
          ],
          "node" : [
            "c"
          ]
        }
      },
      {
        "_index" : "my-index-000001",
        "_id" : "y4SLrHgBdg9xpPrUZz_P",
        "_score" : 1.0,
        "_source" : {
          "timestamp" : 1516297294000,
          "temperature" : 202,
          "voltage" : 4.0,
          "node" : "c"
        },
        "fields" : {
          "voltage_corrected" : [
            16.0
          ],
          "node" : [
            "c"
          ]
        }
      }
    ]
  }
}
```

#### Explore your data with runtime fields
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/runtime-examples.html)

&emsp;&emsp;假设你从一个数据量很大的日志数据中提取字段。数据的索引非常的耗时，并且使用大量的磁盘空间，并且你想要在不提前给出一个策略（mapping）前就能探索其数据结构（Date structure）。

&emsp;&emsp;已知日志数据中包含了你想要提取的字段（field）。在这个例子中，我们集中关注`@timestamp`和`message`字段。通过使用running fields，你可以定义脚本，并在查询期间计算这些字段的值。

##### Define indexed fields as a starting point

&emsp;&emsp;你可以先从一个简单的例子开始，将`@timestamp`和`message`添加到名为`my-index-000001`的索引的mapping中，作为indexed filed。To remain flexible，使用[wildcard](#Wildcard field type)作为`message`的字段类型：

```text
PUT /my-index-000001/
{
  "mappings": {
    "properties": {
      "@timestamp": {
        "format": "strict_date_optional_time||epoch_second",
        "type": "date"
      },
      "message": {
        "type": "wildcard"
      }
    }
  }
}
```

##### Ingest some data

&emsp;&emsp;在添加了用于检索的字段的mapping后，开始将你的日志数据索引到Elasticsearch中。下面的请求使用了[bulk API](#Bulk API)将原始日志数据索引到`my-index-000001`中。你可以先用一小部分样例数据来体验runtime field而不是使用所有的日志数据。

&emsp;&emsp;The final document is not a valid Apache log format, but we can account for that scenario in our script。

```text
POST /my-index-000001/_bulk?refresh
{"index":{}}
{"timestamp":"2020-04-30T14:30:17-05:00","message":"40.135.0.0 - - [30/Apr/2020:14:30:17 -0500] \"GET /images/hm_bg.jpg HTTP/1.0\" 200 24736"}
{"index":{}}
{"timestamp":"2020-04-30T14:30:53-05:00","message":"232.0.0.0 - - [30/Apr/2020:14:30:53 -0500] \"GET /images/hm_bg.jpg HTTP/1.0\" 200 24736"}
{"index":{}}
{"timestamp":"2020-04-30T14:31:12-05:00","message":"26.1.0.0 - - [30/Apr/2020:14:31:12 -0500] \"GET /images/hm_bg.jpg HTTP/1.0\" 200 24736"}
{"index":{}}
{"timestamp":"2020-04-30T14:31:19-05:00","message":"247.37.0.0 - - [30/Apr/2020:14:31:19 -0500] \"GET /french/splash_inet.html HTTP/1.0\" 200 3781"}
{"index":{}}
{"timestamp":"2020-04-30T14:31:22-05:00","message":"247.37.0.0 - - [30/Apr/2020:14:31:22 -0500] \"GET /images/hm_nbg.jpg HTTP/1.0\" 304 0"}
{"index":{}}
{"timestamp":"2020-04-30T14:31:27-05:00","message":"252.0.0.0 - - [30/Apr/2020:14:31:27 -0500] \"GET /images/hm_bg.jpg HTTP/1.0\" 200 24736"}
{"index":{}}
{"timestamp":"2020-04-30T14:31:28-05:00","message":"not a valid apache log"}
```

&emsp;&emsp;现在你可以查看Elasticsearch是如何存储你的原始数据的。

```text
GET /my-index-000001
```

&emsp;&emsp;下面的mapping包含了两个字段：`@timestamp`和`message`。

```text
{
  "my-index-000001" : {
    "aliases" : { },
    "mappings" : {
      "properties" : {
        "@timestamp" : {
          "type" : "date",
          "format" : "strict_date_optional_time||epoch_second"
        },
        "message" : {
          "type" : "wildcard"
        },
        "timestamp" : {
          "type" : "date"
        }
      }
    },
    ...
  }
}
```

##### Define a runtime field with a grok pattern

&emsp;&emsp;如果你想要检索结果中包含`clientip`，你可以在mapping中添加这个字段并且作为runtime field。下面的runtime script定义了一个[grok pattern](#Grokking-grok)，它从单个文本（single text）的文档中提取出结构化的字段。grok pattern类似与正则表达式支持aliased expressions。

&emsp;&emsp;脚本会匹配`%{COMMONAPACHELOG}` log pattern，它能处理Apache日志结构。如果pattern匹配到(`clientip != null`)，脚本会emit匹配到的IP地址。如果没有匹配到，脚本直接退出而不是报错。

```text
PUT my-index-000001/_mappings
{
  "runtime": {
    "http.client_ip": {
      "type": "ip",
      "script": """
        String clientip=grok('%{COMMONAPACHELOG}').extract(doc["message"].value)?.clientip;
        if (clientip != null) emit(clientip); 
      """
    }
  }
}
```

&emsp;&emsp;第8行，这个条件保证pattern没有匹配到也不会报错。

&emsp;&emsp;或者你可以在查询请求时定义相同的runtime field。runtime的定义以及脚本和之前在index mapping中的定义是完全一样。只要将定义拷贝到查询请求中的`runtime_mappings`块中以及一个匹配runtime field的query。这个query返回的结果跟你在index mapping中定义`http.clientip` runtime field返回的结果是一样的。前提是你在查询中指定了这个runtime field：

```text
GET my-index-000001/_search
{
  "runtime_mappings": {
    "http.clientip": {
      "type": "ip",
      "script": """
        String clientip=grok('%{COMMONAPACHELOG}').extract(doc["message"].value)?.clientip;
        if (clientip != null) emit(clientip);
      """
    }
  },
  "query": {
    "match": {
      "http.clientip": "40.135.0.0"
    }
  },
  "fields" : ["http.clientip"]
}
```

##### Define a composite runtime field

&emsp;&emsp;你可以定义一个`composite` runtime filed从单个脚本中emit多个字段。你可以定义类型化的子字段（typed subfields）集合然后emit多个值。在查询期间，每一个子字段会检索出跟它们名称相关的值。意味着你只需要指定一次grok pattern然后就能返回多个值：

```text
PUT my-index-000001/_mappings
{
  "runtime": {
    "http": {
      "type": "composite",
      "script": "emit(grok(\"%{COMMONAPACHELOG}\").extract(doc[\"message\"].value))",
      "fields": {
        "clientip": {
          "type": "ip"
        },
        "verb": {
          "type": "keyword"
        },
        "response": {
          "type": "long"
        }
      }
    }
  }
}
```

###### Search for a specific IP address

&emsp;&emsp;使用`http.clientip` runtime field，你可以定义单个query用于查询指定的IP地址并返回所有相关的字段。

```text
GET my-index-000001/_search
{
  "query": {
    "match": {
      "http.clientip": "40.135.0.0"
    }
  },
  "fields" : ["*"]
}
```

&emsp;&emsp;上面的API会返回下面的结果。由于`http`是一个`composite` runtime field，响应中的`fields`会包括每一个子字段以及匹配到这个query相关的结果。不用提前构建你的数据结构，你就可以查询并以某种方式展示你的数据来体验以及决定索引哪些字段。

```text
{
  ...
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_id" : "sRVHBnwBB-qjgFni7h_O",
        "_score" : 1.0,
        "_source" : {
          "timestamp" : "2020-04-30T14:30:17-05:00",
          "message" : "40.135.0.0 - - [30/Apr/2020:14:30:17 -0500] \"GET /images/hm_bg.jpg HTTP/1.0\" 200 24736"
        },
        "fields" : {
          "http.verb" : [
            "GET"
          ],
          "http.clientip" : [
            "40.135.0.0"
          ],
          "http.response" : [
            200
          ],
          "message" : [
            "40.135.0.0 - - [30/Apr/2020:14:30:17 -0500] \"GET /images/hm_bg.jpg HTTP/1.0\" 200 24736"
          ],
          "http.client_ip" : [
            "40.135.0.0"
          ],
          "timestamp" : [
            "2020-04-30T19:30:17.000Z"
          ]
        }
      }
    ]
  }
}
```

&emsp;&emsp;还记得脚本中的`if`声明吗？

```text
if (clientip != null) emit(clientip);
```

&emsp;&emsp;如果脚本没有包含这个条件，当某个分片上没有匹配到pattern，query会失败。在包含这个条件后，query会跳过没有匹配到grok pattern的数据。

###### Search for documents in a specific range

&emsp;&emsp;你也可以在`timestamp`字段上执行一个[range query](#Range query)。下面的请求会返回大于等于`2020-04-30T14:31:27-05:00`的文档。

```text
GET my-index-000001/_search
{
  "query": {
    "range": {
      "timestamp": {
        "gte": "2020-04-30T14:31:27-05:00"
      }
    }
  }
}
```

&emsp;&emsp;响应中会包含日志格式不匹配的文档，但是timestamp在定义的范围中。

```text
{
  ...
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_id" : "hdEhyncBRSB6iD-PoBqe",
        "_score" : 1.0,
        "_source" : {
          "timestamp" : "2020-04-30T14:31:27-05:00",
          "message" : "252.0.0.0 - - [30/Apr/2020:14:31:27 -0500] \"GET /images/hm_bg.jpg HTTP/1.0\" 200 24736"
        }
      },
      {
        "_index" : "my-index-000001",
        "_id" : "htEhyncBRSB6iD-PoBqe",
        "_score" : 1.0,
        "_source" : {
          "timestamp" : "2020-04-30T14:31:28-05:00",
          "message" : "not a valid apache log"
        }
      }
    ]
  }
}
```

##### Define a runtime field with a dissect pattern

&emsp;&emsp;如果你不需要正则表达式的强大功能，你也可以使用[dissect patterns](#Dissect processor)而不是grok patterns。Dissect patterns匹配固定的分隔符，但通常来说比grok快。

&emsp;&emsp;你可以使用dissect解析Apache log并且达到跟grok pattern一样的结果。相较于log pattern，它会包含你想要丢弃的string。特别的注意下你想要丢弃的string，能帮助你成功构建dissect patterns。

```text
PUT my-index-000001/_mappings
{
  "runtime": {
    "http.client.ip": {
      "type": "ip",
      "script": """
        String clientip=dissect('%{clientip} %{ident} %{auth} [%{@timestamp}] "%{verb} %{request} HTTP/%{httpversion}" %{status} %{size}').extract(doc["message"].value)?.clientip;
        if (clientip != null) emit(clientip);
      """
    }
  }
}
```

&emsp;&emsp;同样的，你可以定义一个dissect pattern来提取[HTTP response code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status)。

```text
PUT my-index-000001/_mappings
{
  "runtime": {
    "http.responses": {
      "type": "long",
      "script": """
        String response=dissect('%{clientip} %{ident} %{auth} [%{@timestamp}] "%{verb} %{request} HTTP/%{httpversion}" %{response} %{size}').extract(doc["message"].value)?.response;
        if (response != null) emit(Integer.parseInt(response));
      """
    }
  }
}
```

&emsp;&emsp;随后你可以使用`http.responses` runtime field执行一个query来检索指定的HTTP response。在`_search`请求中使用`fields`参数指明想要检索的字段：

```text
GET my-index-000001/_search
{
  "query": {
    "match": {
      "http.responses": "304"
    }
  },
  "fields" : ["http.client_ip","timestamp","http.verb"]
}
```

&emsp;&emsp;下面的响应包含了HTTP response为`304`的文档。

```text
{
  ...
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_id" : "A2qDy3cBWRMvVAuI7F8M",
        "_score" : 1.0,
        "_source" : {
          "timestamp" : "2020-04-30T14:31:22-05:00",
          "message" : "247.37.0.0 - - [30/Apr/2020:14:31:22 -0500] \"GET /images/hm_nbg.jpg HTTP/1.0\" 304 0"
        },
        "fields" : {
          "http.verb" : [
            "GET"
          ],
          "http.client_ip" : [
            "247.37.0.0"
          ],
          "timestamp" : [
            "2020-04-30T19:31:22.000Z"
          ]
        }
      }
    ]
  }
}
```

### Field data types
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mapping-types.html)

&emsp;&emsp;每个字段都有一个`field data type`或者`field type`。这个类型指明（indicate）了这个字段中包含的数据种类（kind），比如一个string或者boolean值以及预期用途（intended use）。例如，你可以同时将String索引为`text`和`keyword`字段。然而`text`字段会被[analyzed](#Text analysis)用于全文检索，同时`keyword`用于过滤和排序。

&emsp;&emsp;字段的类型（Field type）根据家族（family）分组。同一个家族的类型有完全一样的查询行为，但是在空间使用（space usage）或者性能属性（performance characteristic）上有差别。

&emsp;&emsp;当前有两个类型家族（type family），`keyword`和`text`。其他的type family只有单个字段的类型。例如`boolean`类型家族由一个字段的类型: `boolean`组成。

##### Common types

|    类型    |               描述               |
| :--------: | :------------------------------: |
| [binary](#Binary field type) | Binary值编码为一个Base64的字符串 |
| [boolean](#Boolean field type) | `true`以及`false` |
| [Keywords](#Keyword type family) | keyword家族，包括`kewword`、`constant_keyword`以及`wildcard` |
| [Numbers](#Numeric field types) | 数值类型，例如`long`和`double`，用来表示为数量 |
| Dates | 时间类型，包括[date](#Date field type)和[date_nanos](#Date nanoseconds field type) |
| [alias](#Alias field type) | 为现有的字段定义一个别名 |


##### Objects and relational types

|     类型      |                     描述                     |
| :-----------: | :------------------------------------------: |
|  [object](#Object field type)   |                 一个JSON对象                 |
| [flattened](#Flattened field type) |           整个JSON对象作为单个字段值           |
|  [nested](#Nested field type)   | 一个JSON对象，保留了子字段（subfield）间的关系 |
|   [join](#Join field type)    |      在同一个索引中定义文档间的父子关系      |

##### Structured data types

|                             类型                             |                             描述                             |
| :----------------------------------------------------------: | :----------------------------------------------------------: |
|                [Range](#Range field types)                | 范围类型，例如`long_range`, `double_range`, `date_range`, and `ip_range` |
|                   [ip](#IP field type)                    |                        IPv4和IPv6地址                        |
|              [version](#Version field type)               | 软件版本。支持[[Semantic Versioning precedence rules](https://semver.org) |
| [murmur3](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/mapper-murmur3.html) |                      计算以及存储哈希值                      |

##### Aggregate data types

|                             类型                             |                 描述                 |
| :----------------------------------------------------------: | :----------------------------------: |
| [aggregate_metric_double](#Aggregate metric field type) | 预先聚合（pre-aggregated）的指标数据 |
|                        [histogram](#Histogram field type)                         |    预先聚合的histogram格式的数值     |

##### Text search tpyes

|          类型          |                             描述                             |
| :--------------------: | :----------------------------------------------------------: |
|    [text_fields](#Text type family)     | text家族，包括`text`、`match_only_text`。分词的，无结构的文本 |
|   [annotated-text](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/mapper-annotated-text.html)   | Text containing special markup. Used for identifying named entities |
|     [completion](#Suggesters)     |                 用于auto-complete suggestion                 |
| [search_as_you_type](#Search-as-you-type field type) |         `text`-like type for as-you-type completion          |
|    [token_count](#Token count field type)     |                      文本中token的统计                       |


##### Document ranking types

|                    类型                     |                          描述                          |
| :-----------------------------------------: | :----------------------------------------------------: |
| [dense_vector](#Dense vector field type) |         Records dense vectors of float values          |
| [rank_feature](#Rank feature field type) | Records a numeric feature to boost hits at query time. |
|            [rank_features](#Rank features field type)            |  Records numeric features to boost hits at query time  |

##### Spatial data types

|     类型      | 描述                                         |
| :-----------: | :------------------------------------------: |
| [geo_point](#Geopoint field type) | 经纬度点数据（point）                        |
| [geo_shape](#Geoshape field type) | 复杂的形状，例如多边形（polygon）            |
|   [point](#Point field type)   | 笛卡尔坐标（Arbitrary cartesian points）     |
|   [shape](#Shape field type)   | 笛卡尔几何（Arbitrary cartesian geometries） |

##### Other types

|      类型      | 描述                             |
| :------------: | -------------------------------- |
| [percolator](#Percolator field type) | 对[Query DSL](#Query DSL)中的query进行索引 |

##### Arrays

&emsp;&emsp;在Elasticsearch中，数组不要求一个专用的字段的数据类型。默认情况下每一个字段可能包含0个或多个值。然而数组中所有的值必须是相同的字段类型。见[Arrays](#Arrays（field Type）)。

##### Multi-fields

&emsp;&emsp;通常来说对同一个字段使用不同方式索引时很有用的。例如，一个字符串字段可以映射为`text`字段用于全文检索，映射为`keyword`用于排序或者聚合。你还可以索引一个text字段时，使用[standard analyzer](#Standard analyzer)、[english analyzer](#Standard analyzer)以及[french analyzer](#Standard analyzer)。

&emsp;&emsp;这就是multi-fields的目的。大多数的字段类型通过[fields](#fields)参数来支持multi-fields。

#### Aggregate metric field type
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/aggregate-metric-double.html)

&emsp;&emsp;为[metric aggregations](#Metrics aggregations)存储预先聚合（pre-aggregated）的数值类型的值。`aggregate_metric_double`类型的字段包含了一个或多个这些指标子字段（metric sub-field）：`min`、`max`、`sum`、`value_count`。

&emsp;&emsp;当你在`aggregate_metric_double`字段上运行指标聚合时，会使用到对应的指标字段值。例如，在`aggregate_metric_double`字段执行[min aggregation](#Min aggregation)时会返回所有`aggregate_metric_double`的子字段`min`中的最小值。

> IMPORTANT：`aggregate_metric_double`字段中为每一个指标子字段使用[doc value](#doc_values)存储一个数值类型的字段值。不支持Array。`min`、`max`、`sum`的值是`doulbe`类型。`value_count`是`long`类型的正数。



```text
PUT my-index
{
  "mappings": {
    "properties": {
      "my-agg-metric-field": {
        "type": "aggregate_metric_double",
        "metrics": [ "min", "max", "sum", "value_count" ],
        "default_metric": "max"
      }
    }
  }
}
```

##### Parameters for aggregate_metric_double fields

###### metrics

&emsp;&emsp;（Required, array of strings）存储指标子字段的数组。每一个数组元素都对应于[metric aggregation](#Metrics aggregations)。合法的数组元素是[min](#Min aggregation)，[max](#Max aggregation)，[sum](#Sum aggregation)以及[value_count](#Value count aggregation)。你必须至少指定一个值。

###### default_metric

&emsp;&emsp;（Required, string）默认用于query，script以及聚合的指标子字段，该字段必须是`metrics`数组中的某个元素。

###### time_series_metric（预览功能）

&emsp;&emsp;该字段只用于Elastic内部使用。将字段作为一个Time series metric。这个值是指标类型（metric type）。默认是`null`（不是一个时间序列指标）。

&emsp;&emsp;对于`aggregate_metric_double`字段，这个参数只可以是`counter`、`gauge`以及`summary`。不能对现有的字段更新这个参数。

##### Uses

&emsp;&emsp;设计`aggregate_metric_double`字段是为了用于下面的聚合：

- [min aggregation](#Min aggregation) 返回所有`min`子字段中的最小值
- [max aggregation](#Max aggregation) 返回所有`max`子字段中的最大值
- [sum aggregation](#Sum aggregation) 返回所有`sum`子字段中的和
- [value_count aggregation](#Value count  aggregation) 返回所有`Value count`子字段中的和
- [avg aggregation](#Avg aggregation)。`aggregate_metric_double`字段中并没有指标子字段`avg`；`avg` aggregation是通过`sum`和`value_count`两个指标子字段计算得来的，因此`metric`数组中必须同时包含`sum`和`value_count`两个指标子字段。

&emsp;&emsp;如果将`aggregate_metric_double`字段用于其他的聚合操作，那么将会使用`default_metric`，相当于一个`doulbe`类型的字段。`default_metric`同样可以用于Script以及下面的query中：

- [existes](#Exists query)
- [range](#Range query)
- [term](#Term query)
- [terms](#Terms query)

##### Example

&emsp;&emsp;下面的[create index](#Create index API)API创建了一个名为`agg_metric`的`aggregate_metric_double`字段。并且`max`作为`default_metric`。：

```text
PUT stats-index
{
  "mappings": {
    "properties": {
      "agg_metric": {
        "type": "aggregate_metric_double",
        "metrics": [ "min", "max", "sum", "value_count" ],
        "default_metric": "max"
      }
    }
  }
}
```

&emsp;&emsp;下面的[index](#Index API) API请求在`agg_metric`字段中添加了预先聚合的数据。

```text
PUT stats-index/_doc/1
{
  "agg_metric": {
    "min": -302.50,
    "max": 702.30,
    "sum": 200.0,
    "value_count": 25
  }
}

PUT stats-index/_doc/2
{
  "agg_metric": {
    "min": -93.00,
    "max": 1702.30,
    "sum": 300.00,
    "value_count": 25
  }
}
```

&emsp;&emsp;你可以在`agg_metric`字段上运行`min`、`max`、`sum`、`value_count`以及`avg`聚合。

```text
POST stats-index/_search?size=0
{
  "aggs": {
    "metric_min": { "min": { "field": "agg_metric" } },
    "metric_max": { "max": { "field": "agg_metric" } },
    "metric_value_count": { "value_count": { "field": "agg_metric" } },
    "metric_sum": { "sum": { "field": "agg_metric" } },
    "metric_avg": { "avg": { "field": "agg_metric" } }
  }
}
```

&emsp;&emsp;聚合结果基于对应的指标子字段的字段值：

```text
{
...
  "aggregations": {
    "metric_min": {
      "value": -302.5
    },
    "metric_max": {
      "value": 1702.3
    },
    "metric_value_count": {
      "value": 50
    },
    "metric_sum": {
      "value": 500.0
    },
    "metric_avg": {
      "value": 10.0
    }
  }
}
```

&emsp;&emsp;在`aggregate_metric_double`查询会默认使用`default_metric`中定义的字段。

```text
GET stats-index/_search
{
  "query": {
    "term": {
      "agg_metric": {
        "value": 702.30
      }
    }
  }
}
```

&emsp;&emsp;上述查询返回下列的命中结果。`default_metric`字段的字段值匹配请求中的值（702.30）。

```text
{
  ...
    "hits": {
    "total": {
      "value": 1,
      "relation": "eq"
    },
    "max_score": 1.0,
    "hits": [
      {
        "_index": "stats-index",
        "_id": "1",
        "_score": 1.0,
        "_source": {
          "agg_metric": {
            "min": -302.5,
            "max": 702.3,
            "sum": 200.0,
            "value_count": 25
          }
        }
      }
    ]
  }
}
```

#### Alias field type
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/field-alias.html)

&emsp;&emsp;`alias`可以给索引中某个字段定义一个别名（alternate）。在[search](#Search APIs)请求以及像 [field capabilities API](#Field capabilities API)中可以用来替代为目标字段（target filed）。

```text
PUT trips
{
  "mappings": {
    "properties": {
      "distance": {
        "type": "long"
      },
      "route_length_miles": {
        "type": "alias",
        "path": "distance" 
      },
      "transit_mode": {
        "type": "keyword"
      }
    }
  }
}

GET _search
{
  "query": {
    "range" : {
      "route_length_miles" : {
        "gte" : 39
      }
    }
  }
}
```

&emsp;&emsp;第10行，目标字段的路径。注意的是必须是全路径，包括所有的parent objects（比如：object1.object2.field）。

&emsp;&emsp;几乎所有的查询请求都可以使用alias field。特别是可以用于查询、聚合、以及排序字段，同时还有`docvalue_fields`, `stored_fields`, suggestions, and highlights。脚本同样支持alias来访问它的字段值。见[unsupported APIs](#Unsupported APIs)了解一些不支持的情况。

&emsp;&emsp;有些[search](#Search APIs)请求以及像 [field capabilities API](#Field capabilities API)中可以使用field wildcard patterns，这个wildcard patterns可以匹配到具体的filed alias：

```text
GET trips/_field_caps?fields=route_*,transit_mode
```

##### Alias targets

&emsp;&emsp;对于alias的目标字段有一些要求限制：

- 目标字段必须是一个具体的字段（concrete field），不能是一个对象或者另一个alias
- 在alias创建时目标字段必须已经存在
- 如果定义了nested objects，field alias必须要跟目标字段有相同的nested scope

&emsp;&emsp;另外，alias只能对应一个目标字段。意味着不可能使用一个alias在single clause（一个query属于一个clause）中查询多个目标字段。

&emsp;&emsp;可以通过更新mapping来更改alias指向（refer to）的目标字段。已知的一个限制是如果任何已经存储的percolator query包含了alias，它们仍然指向原先的目标。更多信息见[percolator documentation](#Percolator field type)。

##### Unsupported APIs

&emsp;&emsp;不支持往alias中写入字段值：尝试在索引或者更新alias字段会产生一个错误。同样的，alias不能作为多个字段的[copy_to](#copy_to)的目标。

&emsp;&emsp;因为alias的名称没有在输入文档中呈现，所以不能通过用于source filtering。例如下面的请求会返回一个空结果：

```text
GET /_search
{
  "query" : {
    "match_all": {}
  },
  "_source": "route_length_miles"
}
```

&emsp;&emsp;目前只有[search](#Search APIs)请求以及像 [field capabilities API](#Field capabilities API)中可以接受alias。像[term vectors](#Term vectors API)就不能使用alias。

&emsp;&emsp;Finally, some queries, such as terms, geo_shape, and more_like_this, allow for fetching query information from an indexed document. Because field aliases aren’t supported when fetching documents, the part of the query that specifies the lookup path cannot refer to a field by its alias。

#### Arrays（field Type）
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/array.html)

&emsp;&emsp;Elasticsearch中没有专门的`array data type`。默认情况下任意一个字段可以包含零个或者多个值 。然而，在数组中的字段值必须是相同的数据类型。例如：

- string数组：["one", "two"]
- integer数组：[1, 2]
- 数组的数组：[1, [2, ,3]]，这种相当于[1, ,2 ,3]
- object数组：[{ "name": "Mary", "age": 12 }, { "name": "John", "age": 10 }]

> NOTE：Arrays of objects对象数组
> 对象数组可能无法按照你的期望工作：不能独立于数组中的其他对象来查询每个对象。如果你需要能够这样做，那么应该使用嵌套（[nested](#Nested field type)）数据类型，而不是对象（[object](#Object field type)）数据类型

&emsp;&emsp;当动态的添加一个字段时，数组中的第一个值决定了数组元素的类型。随后所有的值都必须是相同的数据类型或者可以强转（[coerce](#coerce)）为相同的数据类型。

&emsp;&emsp;数组中有不同的数据类型是不支持的: [ 10, "some string" ]。

&emsp;&emsp;数组中可以包含`null`，可以被[null_value](#null_value)(如果配置的话)替换或者跳过。空的数组`[]`被认为是缺失字段（missing field）-某个字段没有字段值。

&emsp;&emsp;不需要在文档中有任何预先配置就可以使用数组，这个开箱支持的功能：

```text
PUT my-index-000001/_doc/1
{
  "message": "some arrays in this document...",
  "tags":  [ "elasticsearch", "wow" ], 
  "lists": [ 
    {
      "name": "prog_list",
      "description": "programming list"
    },
    {
      "name": "cool_list",
      "description": "cool stuff list"
    }
  ]
}

PUT my-index-000001/_doc/2 
{
  "message": "no arrays in this document...",
  "tags":  "elasticsearch",
  "lists": {
    "name": "prog_list",
    "description": "programming list"
  }
}

GET my-index-000001/_search
{
  "query": {
    "match": {
      "tags": "elasticsearch" 
    }
  }
}
```

&emsp;&emsp;第4行，`tags`字段自动添加为字符串字段（string field）。
&emsp;&emsp;第5行，`lists`字段自动添加为`object`字段。
&emsp;&emsp;第17行，第二篇文档没有包含数组，但是会被索引成相同的字段。
&emsp;&emsp;第31行，这个query在`tags`字段中查找`elasticsearch`，并且匹配到这两篇文档。

#### Binary field type
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/binary.html)

&emsp;&emsp;`binary`类型用于存储使用[Base64](https://en.wikipedia.org/wiki/Base64)编码的二进制值，默认情况下，这个字段不会存储并且无法用于搜索：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "name": {
        "type": "text"
      },
      "blob": {
        "type": "binary"
      }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "name": "Some binary blob",
  "blob": "U29tZSBiaW5hcnkgYmxvYg==" 
}
```

&emsp;&emsp;第18行，二进制值中不能有换行符`\n`

##### Parameters for binary fields

&emsp;&emsp;`binary`字段可以有以下参数：

- [doc_values](#doc_values) : 该字段是否用基于列式存储于磁盘中，使得可以用于排序、聚合、或者Script。参数值为`true`或者`false`（默认值）
- [store](#store)：该字段是否独立于[\_source](#_source field)字段并且可以用于检索。参数值为`true`或者`false`（默认值）

#### Boolean field type
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/boolean.html)

&emsp;&emsp;`boolean`字段的值可以是JSON种的`true`和`false`，也可以是可以被解释为`true`或者`false`的字符串。

- False values：`false`，`"false"`, ""（空的字符串）
- True values：`true`，`"true"`

&emsp;&emsp;例如

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "is_published": {
        "type": "boolean"
      }
    }
  }
}

POST my-index-000001/_doc/1?refresh
{
  "is_published": "true" 
}

GET my-index-000001/_search
{
  "query": {
    "term": {
      "is_published": true 
    }
  }
}
```

&emsp;&emsp;第14行，索引了一篇文档，is_published字段的字段值是一个字符串的true，它会被解释为`true`
&emsp;&emsp;第21行，使用JSON种的`true`查询文档

&emsp;&emsp;[terms aggregation](#Terms aggregation)中使用`1`和`0`用于`key`的值，并且`"true"`和`"false"`用于`key_as_string`的值。在Script中使用Boolean字段时，会返回`true`和
`false`：

```text
POST my-index-000001/_doc/1?refresh
{
  "is_published": true
}

POST my-index-000001/_doc/2?refresh
{
  "is_published": false
}

GET my-index-000001/_search
{
  "aggs": {
    "publish_state": {
      "terms": {
        "field": "is_published"
      }
    }
  },
  "sort": [ "is_published" ],
  "fields": [
    {"field": "weight"}
  ],
  "runtime_mappings": {
    "weight": {
      "type": "long",
      "script": "emit(doc['is_published'].value ? 10 : 0)"
    }
  }
}
```

##### Parameters for boolean fields

&emsp;&emsp;`boolean`字段有以下参数：

- [doc_values](#doc_values) : 该字段是否用基于列式存储于磁盘中，使得可以用于排序、聚合、或者Script。参数值为`true`或者`false`（默认值）
- [index](#index(mapping parameter))：该字段是否需要被快速的检索？参数值为`true`或者`false`（默认值）。
- [null_value](#null_value)：参数值可以是上文中列出的各种true或者false的值（JSON，字符串）。默认值是`null`，意味着缺失值。注意的是如果使用了`script`参数，则不能设置这个参数
- on_script_error：该参数定义了当Script在索引期间抛出错误后，该如何处理。默认值为`fail`，会导致整个文档被reject；如果参数值为`continue`，将在[\_ignored](#\_ignored field)字段注册这个字段然后继续索引。这个参数只有在`script`设置后才能被设置。
- script：如果设置了这个参数，这个字段的字段值会通过script生成，而不是直接从源数据中读取。如果文档中设置了该字段的值，则会reject并且抛出错误。script跟[runtime](#Map a runtime field)中有相同的format。
- [store](#store)：该字段是否独立于[\_source](#_source field)字段并且可以用于检索。参数值为`true`或者`false`（默认值）
- [meta](#meta(mapping parameter))：字段的元数据信息

#### Date field type
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/date.html)

&emsp;&emsp;JSON没有日期（dates）类型，所以在Elasticsearch中日期字段可以是：

- 格式化后的字符串，比如`2015-01-01` 或者 `2015/01/01 12:10:30`
- 数值类型：milliseconds-since-the-epoch
- 数值类型：seconds-since-the-epoch（[配置](#Epoch seconds)）

>NOTE：milliseconds-since-the-epoch的值必须是非负的，使用格式化后的值来表示1970前的日期

&emsp;&emsp;源码中，日期会被转化为UTC（如果指定了时区），存储为一个long类型的数值来表示milliseconds-since-the-epoch。

&emsp;&emsp;日期类型的查询会被表示为一个long类型的范围查询，聚合的结果和stored field会根据该字段对应的date format被转回到一个字符串。

>NOTE：日期总是会被表示为字符串，即使在JSON中是long类型

&emsp;&emsp;时间格式data format可以自定义，如果`format`没有指定，会使用下面默认值：

```text
"strict_date_optional_time||epoch_millis"
```

&emsp;&emsp;上述默认值意味着接受的可选时间戳有milliseconds-since-the-epoch或者[strict_date_optional_time](#format(mapping parameter))支持的格式。

&emsp;&emsp;例如

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "date": {
        "type": "date" 
      }
    }
  }
}

PUT my-index-000001/_doc/1
{ "date": "2015-01-01" } 

PUT my-index-000001/_doc/2
{ "date": "2015-01-01T12:10:30Z" } 

PUT my-index-000001/_doc/3
{ "date": 1420070400001 } 

GET my-index-000001/_search
{
  "sort": { "date": "asc"} 
}
```

&emsp;&emsp;第6行将会使用默认的`format`
&emsp;&emsp;第13行使用纯日期
&emsp;&emsp;第16行包含时间
&emsp;&emsp;第19行使用milliseconds-since-the-epoch
&emsp;&emsp;第23行，返回的`sort`的值将是milliseconds-since-the-epoch

>WARNING：日期将会接受类似`{"date": 1618249875.123456}`的带小数点的数值，但目前我们会丢失一定的精度，见这个[issue](https://github.com/elastic/elasticsearch/issues/70085) 。

##### Multiple date formats

&emsp;&emsp;可以使用`||`作为分隔符来指定multiple formats，这些`format`都被依次进行匹配直到找到对应的format。下面第一个format会将`milliseconds-since-the-epoch`的值转化为一个字符串。

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "date": {
        "type":   "date",
        "format": "yyyy-MM-dd HH:mm:ss||yyyy-MM-dd||epoch_millis"
      }
    }
  }
}
```

##### Parameters for date fields

&emsp;&emsp;下面列出的参数可以用于`date`字段

|       [doc_values](#doc_values)       | 是否以列式存储在磁盘上，使得可以用于排序、聚合、脚本处理，该值可以是true（默认值）或者false |
| :--------------------------------------: | ------------------------------------------------------------ |
|          [format](#format(mapping parameter) )           | 用于对日期进行解析，默认值`strict_date_optional_time||epoch_millis` |
|                  locale                  | 解析日期时使用的区域设置，因为在所有语言中，月份没有相同的名称和/或缩写。默认是 [ROOT locale](https://docs.oracle.com/javase/8/docs/api/java/util/Locale.html#ROOT) |
| [ignore_malformed](#ignore_malformed) | 如果是true，格式错误的数字会被忽略。如果是false，格式错误的数字会抛出异常并且整个文档会被丢弃。注意的是如果使用了参数`script`，当前参数不会被设置。 |
|            [index](#index(mapping parameter))            | 是否需要被快速的搜索到？该值可以是true（默认值）或者false。日期字段date field只有开启[doc_values](#doc_values)才能进行查询，尽管查询速度较慢 |
|       [null_value](#null_value)       | 可以设置一个满足[format](#format(mapping parameter) )的值，用来替代空值。默认值是null，意味这是一个缺失值。注意的是如果使用了参数`script`，当前参数不会被设置。 |
|             on_script_error              | 该值描述了通过参数`script`定义的脚本在索引期间发生错误后的行为。可以设置为false（默认值），那么整个文档会被reject。或者设置为`continue`，会通过[ignored field](#\_ignored field)来进行索引并继续进行索引。这个参数只有参数`script`被设置后才能被设置 |
|                  script                  | 如果设置了该值，将会索引这个脚本生成的值，而不是直接读取source（输入文档中这个字段的字段值）。如果输入的文档中已经设置了这个字段的值，那么这个脚本会被reject并报错。脚本的格式跟[runtime equivalent](#Map a runtime field)一致，并且应该输出long类型的时间戳。 |
|            [store](#store)            | 是否要额外存储字段值并且可以被检索，不仅仅存储在[\_source](#\_source field) 字段中，该值可以是true或者false（默认值） |
|             [meta](#meta)             | 这个字段的Metadata                                             |



##### Epoch seconds

&emsp;&emsp;如果你需要传输`seconds-since-the-epoch`，那么保证在`format`中列出`epoch_second`：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "date": {
        "type":   "date",
        "format": "strict_date_optional_time||epoch_second"
      }
    }
  }
}

PUT my-index-000001/_doc/example?refresh
{ "date": 1618321898 }

POST my-index-000001/_search
{
  "fields": [ {"field": "date"}],
  "_source": false
}
```

&emsp;&emsp;会收到下面的日期数据：

```text
{
  "hits": {
    "hits": [
      {
        "_id": "example",
        "_index": "my-index-000001",
        "_score": 1.0,
        "fields": {
          "date": ["2021-04-13T13:51:38.000Z"]
        }
      }
    ]
  }
}
```

#### Date nanoseconds field type
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/date_nanos.html)

&emsp;&emsp;这个时间类型是`date`数据类型的补充。然而它们之间有一个很重要的不同。现有的`date`数据类型存储毫秒级分辨率的时间，而`date_nanos`则是纳秒级分辨率，严格限制了从1970到2262这个时间范围，因为日期仍然以代表自纪元（epoch）以来纳秒数的long类型存储。

&emsp;&emsp;内部处理纳秒时会将其转化为long类型的数值，聚合和存储的结果会被转回成一个字符串，字符串的值区间于Date format。

&emsp;&emsp;可以自定义Date format，如果不指定则会使用默认的format：

```text
"strict_date_optional_time_nanos||epoch_millis"
```

&emsp;&emsp;例如：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "date": {
        "type": "date_nanos" 
      }
    }
  }
}

PUT my-index-000001/_bulk?refresh
{ "index" : { "_id" : "1" } }
{ "date": "2015-01-01" } 
{ "index" : { "_id" : "2" } }
{ "date": "2015-01-01T12:10:30.123456789Z" } 
{ "index" : { "_id" : "3" } }
{ "date": 1420070400000 } 

GET my-index-000001/_search
{
  "sort": { "date": "asc"}, 
  "runtime_mappings": {
    "date_has_nanos": {
      "type": "boolean",
      "script": "emit(doc['date'].value.nano != 0)" 
    }
  },
  "fields": [
    {
      "field": "date",
      "format": "strict_date_optional_time_nanos" 
    },
    {
      "field": "date_has_nanos"
    }
  ]
}
```

&emsp;&emsp;第6行，`date`字段使用默认的date format
&emsp;&emsp;第14行，这篇文档使用一个plain Date
&emsp;&emsp;第16行，这篇文档包含一个时间
&emsp;&emsp;第18行，这个文档使用了milliseconds-since-the-epoch
&emsp;&emsp;第22行，注意的是`sort`的值在返回的结果中使用milliseconds-since-the-epoch
&emsp;&emsp;第26行，在脚本中使用了`.nano`返回纳米级别的时间
&emsp;&emsp;第32行，在获取数据时，你可以使用[fields parameter](#The fields option)来指定Date format。如果不使用[strict_date_optional_time_nanos](#strict_date_optional_time_nanos) 你会获取一个四舍五入的结果。

&emsp;&emsp;你也可以通过`||`指定多个Date format。Date nanoseconds field type可以跟Date field type有相同的可用的参数。

> WARNING：Date nanoseconds可以是一个类似`{"date": 1618249875.123456}`的带有小数点的数字。在某些情况下（[#70085](https://github.com/elastic/elasticsearch/issues/70085)），我们会丢失这些日期的精度，因此应该避免使用这种格式

##### Limitations

&emsp;&emsp;即使使用了`date_nanos`字段，也只能在毫秒级分辨率上聚合。这个局限性同样影响[transforms](#Transforming data)。

#### Dense vector field type
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/dense-vector.html)

&emsp;&emsp;

#### Flattened field type
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/flattened.html)

&emsp;&emsp;默认情况下，对象中每一个子字段（sub-field）都单独映射和索引。如果子字段的类型或者名称无法提前知晓，那么它们会被自动映射（[mapped dynamically](#Dynamic mapping)）。

&emsp;&emsp;`flattened`提供另外一种方法，它将整个对象映射为单个字段。`flattened`会解析出leaf value并且将它们作为keyword索引到一个字段中。对象的内容随后可以通过query和aggregation查询。

&emsp;&emsp;对于很大的或者有未知数量键的对象，使用`flattened`就很有用。只要创建一个字段的类型来处理整个JSON对象，有助于防止因太多不同的字段的类型导致[mappings explosion](#Settings to prevent mapping explosion)。

&emsp;&emsp;另外，`flattened`在搜索功能存在一种折衷。只允许一些基本的查询，不支持数值类型的范围查询以及高亮。更多的一些局限性见[Supported operations](#Supported operations)。

> NOTE：`flattened`不应该用于索引整个文档的内容，因为它会将所有的值作为keyword，不能提供全文检索功能。默认每一个子字段在mapping中有自己的entry，大多数的用例中工作都正常。

&emsp;&emsp;`flattened`字段可以通过下面的方式创建：

```text
PUT bug_reports
{
  "mappings": {
    "properties": {
      "title": {
        "type": "text"
      },
      "labels": {
        "type": "flattened"
      }
    }
  }
}

POST bug_reports/_doc/1
{
  "title": "Results are not sorted correctly.",
  "labels": {
    "priority": "urgent",
    "release": ["v1.2.5", "v1.3.0"],
    "timestamp": {
      "created": 1541458026,
      "closed": 1541457010
    }
  }
}
```

&emsp;&emsp;在索引期间，JSON对象中的每一个leaf value都会创建token。索引为keywords类型，不会分词，也不会为数值或者日期进行特别的处理。

&emsp;&emsp;查询最顶层`flattened`字段（mapping中定义的那个字段）会搜索所有的leaf values：

```text
POST bug_reports/_search
{
  "query": {
    "term": {"labels": "urgent"}
  }
}
```

&emsp;&emsp;若要查询flattened object中某个key，则通过点路径：

```text
POST bug_reports/_search
{
  "query": {
    "term": {"labels.release": "v1.3.0"}
  }
}
```

##### Supported operations

&emsp;&emsp;由于索引的方式很类似，所以`flattened`字段跟[keyword](#Keyword type family)字段一样有许多相同的mapping和查询功能。

&emsp;&emsp;目前，`flattened`字段可以用于下面的query类型：

- `term`, `terms`, and `terms_set`
- `prefix`
- `range`
- `match` and `multi_match`
- `query_string` and `simple_query_strin`g
- `exists`

&emsp;&emsp;查询时，不能使用wildcards，比如：`{ "term": {"labels.time*": 1541457010}}`。注意的事，所有的查询，包括`range`，都是将值当成string类型。`flattened`字段不支持高亮。

&emsp;&emsp;可以将`flattened`字段用于排序，就是类似`terms`中执行简单keyword风格的聚合。与查询一样，这里没有对数字的特殊支持——JSON 对象中的所有值都被视为keyword。在排序时，这意味着值会按字典顺序进行比较。

&emsp;&emsp;目前不能存储`flattened`字段。不能在mapping中指定[store](#store(mapping parameter))。

##### Retrieving flattened fields

&emsp;&emsp;字段值以及固定子字段可以通过[fields parameter](#The fields option).content检索。由于`flattened`将整个对象（以及对象中的子字段）映射为一个字段，响应中的`_source`包含了未更改的结构。

&emsp;&emsp;获取单个子字段可以在请求中显示的指定。只能对有固定路径的生效，但不能使用wildcards：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "flattened_field": {
        "type": "flattened"
      }
    }
  }
}

PUT my-index-000001/_doc/1?refresh=true
{
  "flattened_field" : {
    "subfield" : "value"
  }
}

POST my-index-000001/_search
{
  "fields": ["flattened_field.subfield"],
  "_source": false
}
```

```text
{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 1,
      "relation": "eq"
    },
    "max_score": 1.0,
    "hits": [{
      "_index": "my-index-000001",
      "_id": "1",
      "_score": 1.0,
      "fields": {
        "flattened_field.subfield" : [ "value" ]
      }
    }]
  }
}
```

&emsp;&emsp;你可以使用[Painless script](#Painless scripting language)读取flattened字段的子字段的字段值。相比较`doc['<field_name>'].value`。在你的Script中，使用`doc['<field_name>.<sub-field_name>'].value`。例如如果你有一个名为`label`的flattened字段以及一个`release`的子字段。你的Painless Script中应该是`doc['labels.release'].value`。

&emsp;&emsp;例如，比如说你的mapping中包含两个字段，其中一个是`flattened`类型：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "title": {
        "type": "text"
      },
      "labels": {
        "type": "flattened"
      }
    }
  }
}
```

&emsp;&emsp;索引一些包含你mapping中的字段的文档。`labels`字段有三个子字段：

```text
POST /my-index-000001/_bulk?refresh
{"index":{}}
{"title":"Something really urgent","labels":{"priority":"urgent","release":["v1.2.5","v1.3.0"],"timestamp":{"created":1541458026,"closed":1541457010}}}
{"index":{}}
{"title":"Somewhat less urgent","labels":{"priority":"high","release":["v1.3.0"],"timestamp":{"created":1541458026,"closed":1541457010}}}
{"index":{}}
{"title":"Not urgent","labels":{"priority":"low","release":["v1.2.0"],"timestamp":{"created":1541458026,"closed":1541457010}}}
```

&emsp;&emsp;因为`labels`是一个`flattened`字段，所以整个对象映射到一个字段中。若要在一个Painless Script中检索子字段，那么使用`doc['<field_name>.<sub-field_name>'].value`。

```text
"script": {
  "source": """
    if (doc['labels.release'].value.equals('v1.3.0'))
    {emit(doc['labels.release'].value)}
    else{emit('Version mismatch')}
  """
```

##### Parameters for flattened object fields

&emsp;&emsp;可以使用下面的参数：

- depth_limit：flattened字段允许嵌套的最大深度（nested Inner object）。如果超过限制，则会抛出错误。默认值为`20`。注意的是可以通过[update mapping](#Update mapping API)更新`depth_limit`。
- [doc_values](#doc_values)：- 该字段值是否在磁盘上使用列式存储，使得可以用来进行聚合、排序或者脚本。可选值`true`或者`false`
- [eager_global_ordinals](#eager_global_ordinals) ：是否在refresh尽快的载入global ordinals？默认是`false`。如果经常用于terms aggregation，开启这个参数是很有必要的
- [ignore_above](#ignore_above)：leaf values的长度超过限制的话则不会被索引。默认情况喜爱没有限制并且都可以被索引。注意的是这个限制只应用与flattened的leaf values，而不是整个字段的长度
- [index](#index(mapping parameter))：是否该字段需要被快速的搜索到？可选值`true`或者`false`。
- [index_options](#index_options)：在索引中存储哪些信息用于打分目的。默认是`docs`但是可以设置为`freqs`，在打分时会将词频考虑进去。
- [null_value](#null_value)：flattened字段中`null`值会被替换为一个string value。默认是`null`。意味着null值被认为是缺失值
- [similarity](#similarity)：使用哪一个打分算法。默认值为`BM25`。
- split_queries_on_whitespace：在flattened字段上执行[full text queries](#Full text queries)时，是否根据空格对输入进行分割。可选值为`true`或`false`（default）

#### Geopoint field type
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/geo-point.html)

&emsp;&emsp;`geo_point`字段可以接受经度纬度对，可以用于：

- 在一个[bounding box](#Geo-bounding box query)中，相对于中心点某个距离([distance](#Geo-distance query))找到一个地理点（geopoints），或者从一个[polygon](#Geo-polygon query)中、或者一个[geo_shape query](#Geoshape query)中找到一个地理点。
- 根据地理位置（[geographically ](#Geohash grid aggregation)）或者离中心点的距离([distance](#Geo-distance aggregation))对文档进行聚合
- 将距离信息集成到文档的相关性打分[relevance score](#Function score query)中
- 根据距离对文档排序([sort](#Geo Distance Sorting))

&emsp;&emsp;有五种方式来指定地理点，如下所示：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "location": {
        "type": "geo_point"
      }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "text": "Geopoint as an object",
  "location": { 
    "lat": 41.12,
    "lon": -71.34
  }
}

PUT my-index-000001/_doc/2
{
  "text": "Geopoint as a string",
  "location": "41.12,-71.34" 
}

PUT my-index-000001/_doc/3
{
  "text": "Geopoint as a geohash",
  "location": "drm3btev3e86" 
}

PUT my-index-000001/_doc/4
{
  "text": "Geopoint as an array",
  "location": [ -71.34, 41.12 ] 
}

PUT my-index-000001/_doc/5
{
  "text": "Geopoint as a WKT POINT primitive",
  "location" : "POINT (-71.34 41.12)" 
}

GET my-index-000001/_search
{
  "query": {
    "geo_bounding_box": { 
      "location": {
        "top_left": {
          "lat": 42,
          "lon": -72
        },
        "bottom_right": {
          "lat": 40,
          "lon": -74
        }
      }
    }
  }
}
```

&emsp;&emsp;第15行，地理点用一个对象表示，对象中有`lat`跟`lon`两个key
&emsp;&emsp;第24行，地理点用一个字符串类型的format表示：`lat,lon`
&emsp;&emsp;第30行，地理点用GeoHash表示
&emsp;&emsp;第36行，地理点用数组类型的format表示：[`lon`, `lat`]
&emsp;&emsp;第42行，地理点用 [Well-Known Text POINT](https://docs.opengeospatial.org/is/12-063r5/12-063r5.html)的format表示：`"POINT(lon lat)"`
&emsp;&emsp;第48行，一个geo-bounding_box query，用来找到在地理位置范围内所有的地理点。

> IMPORTANT：Geopoints expressed as an array or string
> 注意的使string类型的地理点的经纬度先后顺序为 `lat` `lon`，然而数组类型的地理点的经纬度先后顺序为`lon``lat`。
> 
> 原先这两种类型的地理点都是`lat` `lon`的先后顺序，但是数组类型的地理点为了符合GeoJSON的格式做出了更改

> NOTE：一个point可以用[geohash](https://en.wikipedia.org/wiki/Geohash)表示，是将纬度和经度的比特交错后，用 [base32](https://en.wikipedia.org/wiki/Base32) 编码成字符串。geohash的每一个字符增加额外的 5 个比特精度。因此，哈希越长，其精度就越高。为了索引目的，geohash被转换成纬度-经度对。在这个过程中，只使用前 12 个字符，所以在geohash指定超过 12 个字符并不会增加精度。这 12 个字符提供了 60 个比特，应该能将可能的误差减少到不到 2 厘米。

##### Parameters for geo_point fields

`geo_point`字段可以有下面的参数：

- [ignore_malformed](#ignore_malformed)：如果为`true`，格式错误的地理点会被忽略。如果为`false`（default），格式错误的地理点会抛出一个异常并且reject整个文档。如果其纬度超出了 -90 至 90 的范围，或者其经度超出了 -180 至 180 的范围，则认为地理点是格式错误的。注意的使如果使用了script参数，则无法设置这个参数。
- ignore_z_value：如果为`true`(default)，可以索引三维的point（点数据），但是只有经纬度的值会被索引；第三个维度的值被忽略。如果为`false`地理点的维度如果超过了经纬度这两个维度就会抛出异常并且reject文档。注意的使如果使用了script参数，则无法设置这个参数。
- [index](#index(mapping parameter))：是否该字段需要被快速的搜索到？可选值`true`或者`false`。只有[doc_values](#doc_values)参数的字段仍然可以被查询，尽管慢一些（albeit slower）
- [null_value](#null_value)：可是是一个地理点用来替换所有显示为`null`的值。默认是`null`，意味着当前字段被认为是一个缺失值。注意的是如果使用了下面的参数`script`，那么不能设置当前参数
- on_script_error：定义了当使用参数`script`并且在索引期间脚本抛出异常后的行为。默认值为`false`会导致整个文档会被reject，然后被注册到文档的 [\_ignored](#\_ignored field)这个[metadata field](#Metadata fields)中并且继续索引。注意的是如果使用了下面的参数`script`，那么不能设置当前参数
- script：如果设置了这个参数，字段值会使用脚本生成的值而不是文档中的原始数据。如果使用输入文档（input document）中的数据作为字段值，那么这篇文档会被reject并且返回错误。Script跟[runtime ](#Map a runtime field)的格式是一样的。并且应该抛出double类型的经纬度对值。

##### Using geopoints in scripts

&emsp;&emsp;在script中访问地理点时，地理点是一个`GeoPoint`对象，可以分别访问`lat`跟`lon`的值。

```text
def geopoint = doc['location'].value;
def lat      = geopoint.lat;
def lon      = geopoint.lon;
```

&emsp;&emsp;出于性能考虑，最好直接访问经纬度值：

```text
def lat      = doc['location'].lat;
def lon      = doc['location'].lon;
```

#### Geoshape field type
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/geo-shape.html)


#### Histogram field type
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/histogram.html)

&emsp;&emsp;用来存放预先聚合、数值类型的数据来代表一个直方图的字段。使用下面的成对的两个数组：

- 一个[double](#Numeric field types)类型的**值数组**(value array)，代表直方图的分桶。这些值在数组中必须有序并且是升序。
- 一个[interger](#Numeric field types)类型的**统计数组**(count array)，代表每个分桶中有多少个值。这些值必须是正数或者零。

&emsp;&emsp;因为value array中的元素对应在count array中相同数组下标的元素，所以这两个数组的长度必须要相同。

> IMPORTANT：每一篇文档中，一个`histogram`字段只能存储单个`values`和`count`的数组对。不支持nested arrays。
> `histogram`字段不支持排序。

##### Parameters

###### time_series_metric（预览功能）

&emsp;&emsp;将字段作为一个Time series metric。这个值是指标类型（metric type）。默认是`null`（不是一个时间序列指标）。

&emsp;&emsp;对于`histogram`字段，这个参数接收`histogram`。你不能对现有的字段更新这个参数

##### Uses

&emsp;&emsp;使用`histogram`字段的主要目的是聚合。为了使其更容易被聚合访问，直方图字段数据被存储为二进制文档值并且不被索引。其以字节为单位的大小最多为 13 * numValues，其中 numValues 是提供的数组的长度。

&emsp;&emsp;因为没有索引数据，你可以在下面的聚合和查询中使用`histogram`字段：

- [min](#Min aggregation) aggregation
- [max](#Max aggregation) aggregation
- [sum](#Sum aggregation) aggregation
- [value_count](#Value count aggregation) aggregation
- [avg](#Avg aggregation) aggregation
- [percentiles](#Percentiles aggregation) aggregation
- [percentile ranks](#Percentile ranks aggregation) aggregation
- [boxplot](#Boxplot aggregation) aggregation
- [histogram](#Histogram aggregation) aggregation
- [range](#Range aggregation) aggregation
- [exists](#Exists query) query

##### Building a histogram

&emsp;&emsp;当使用histogram作为聚合的一部分时，聚合结果的准确性取决于如何构建的。[percentiles aggregation](#Percentiles aggregation) mode中需要重点关注：

- For the [T-Digest](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-metrics-percentile-rank-aggregation.html#_hdr_histogram) mode, the values array represents the mean centroid positions and the counts array represents the number of values that are attributed to each centroid. If the algorithm has already started to approximate the percentiles, this inaccuracy is carried over in the histogram.
- For the [High Dynamic Range](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-metrics-percentile-rank-aggregation.html#_hdr_histogram) (HDR) histogram mode, the values array represents fixed upper limits of each bucket interval, and the counts array represents the number of values that are attributed to each interval. This implementation maintains a fixed worse-case percentage error (specified as a number of significant digits), therefore the value used when generating the histogram would be the maximum accuracy you can achieve at aggregation time.

&emsp;&emsp;The histogram field is "algorithm agnostic" and does not store data specific to either T-Digest or HDRHistogram. While this means the field can technically be aggregated with either algorithm, in practice the user should chose one algorithm and index data in that manner (e.g. centroids for T-Digest or intervals for HDRHistogram) to ensure best accuracy.

##### Examples

&emsp;&emsp;下面的[create index](#Create index API) API请求创建了一个新的索引，mapping中有两个字段：

- `my_histogram`是`histogram`字段用于存储百分值
- `my_text`是`keyword`字段用于存储直方图的标题

```text
PUT my-index-000001
{
  "mappings" : {
    "properties" : {
      "my_histogram" : {
        "type" : "histogram"
      },
      "my_text" : {
        "type" : "keyword"
      }
    }
  }
}
```

&emsp;&emsp;下面的[index](#Index-API) API请求为两个直方图：`histogram_1`和`histogram_2`存储预先聚合的数据。

```text
PUT my-index-000001/_doc/1
{
  "my_text" : "histogram_1",
  "my_histogram" : {
      "values" : [0.1, 0.2, 0.3, 0.4, 0.5], 
      "counts" : [3, 7, 23, 12, 6] 
   }
}

PUT my-index-000001/_doc/2
{
  "my_text" : "histogram_2",
  "my_histogram" : {
      "values" : [0.1, 0.25, 0.35, 0.4, 0.45, 0.5], 
      "counts" : [8, 17, 8, 7, 6, 2] 
   }
}
```

&emsp;&emsp;第5、14行：Values for each bucket. Values in the array are treated as doubles and must be given in increasing order. For [T-Digest](#Percentiles aggregation) histograms this value represents the mean value. In case of HDR histograms this represents the value iterated to.
&emsp;&emsp;第6、15行：Count for each bucket. Values in the arrays are treated as integers and must be positive or zero. Negative values will be rejected. The relation between a bucket and a count is given by the position in the array.

#### IP field type
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ip.html)

&emsp;&emsp;`ip`字段可以用来索引/存储[IPV4](https://en.wikipedia.org/wiki/IPv4)或者[IPV6](https://en.wikipedia.org/wiki/IPv6)地址：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "ip_addr": {
        "type": "ip"
      }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "ip_addr": "192.168.1.1"
}

GET my-index-000001/_search
{
  "query": {
    "term": {
      "ip_addr": "192.168.0.0/16"
    }
  }
}
```

>NOTE：你可以使用[ip_range data type](#Range field types)在单个字段中存储ip ranges。

##### Parameters for ip fields

&emsp;&emsp;`ip`字段接受下列的参数：

- [doc_values](#doc_values)
  - 该字段值是否在磁盘上使用列式存储，使得可以用来进行聚合、排序或者脚本。可选值`true`或者`false`

- [ignore_malformed](#ignore_malformed)
  - 如果设置为`true`，数据格式错误的IP地址被忽略。如果设置为`false`（默认值），数据格式错误的IP地址会导致抛出异常并reject整个文档。注意的是如果使用了下面的参数`script`，那么不能设置当前参数

- [index](#index(mapping parameter))
  - 是否该字段需要被快速的搜索到？可选值`true`或者`false`。只有[doc_values](#doc_values)参数的字段仍然可以使用term or range-based的查询，尽管慢一些（albeit slower）

- [null_value](#null_value)
  - `ip`字段接受一个IPV4或者IPV6的值，同时也接受显示（explicit）的`null`值，意味着当前字段被认为是一个缺失值。注意的是如果使用了下面的参数`script`，那么不能设置当前参数

- on_script_error
  - 定义了当使用参数`script`并且在索引期间脚本抛出异常后的行为。默认值为`reject`会导致整个文档会被reject，设置为`ignore`后，会被注册到文档的 [\_ignored](#\_ignored field)这个[metadata field](#Metadata fields)中并且继续索引。注意的是如果使用了下面的参数`script`，那么不能设置当前参数

- script
  - 如果设置了这个参数，字段值会使用脚本生成的值而不是文档中的原始数据。如果使用输入文档（input document）中的数据作为字段值，那么这篇文档会被reject并且返回错误。脚本跟[runtime equivalent](#Map a runtime field)采用一样的格式，并且应该返回IPV4或者IPV6格式的值

- [store](#store)
  - 当前字段是否要存储使得可以从`_source`检索该字段。可选值`true`或者`false`

- time_series_dimension
  - 预览功能， 可能会被更改或者移除，目前只在Elasticsearch代码内部使用，暂不介绍。


##### Querying ip fields

&emsp;&emsp;最常用查询IP的方式就是使用 [CIDR](https://en.wikipedia.org/wiki/Classless_Inter-Domain_Routing#CIDR_notation)：

```text
GET my-index-000001/_search
{
  "query": {
    "term": {
      "ip_addr": "192.168.0.0/16"
    }
  }
}
```

&emsp;&emsp;或者

```text
GET my-index-000001/_search
{
  "query": {
    "term": {
      "ip_addr": "2001:db8::/48"
    }
  }
}
```

&emsp;&emsp;注意的是，冒号是query_string查询的特殊字符，因此ipv6地址需要转义。最简单的方法是在搜索值两边加双引号:

```text
GET my-index-000001/_search
{
  "query": {
    "query_string" : {
      "query": "ip_addr:\"2001:db8::/48\""
    }
  }
}
```

#### Join field type
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/parent-join.html)

&emsp;&emsp;`join`数据类型是一个特殊的字段，用来创建同一个索引中文档间的父子关系。`relations`定义了文档之间的关系，它也定义了父文档的名称以及子文档的名称。

> WARNING：不要使用[多级父子关系](Multiple levels of parent join)来模拟关系型数据库模型。每增加一级关系，都会在查询时增加内存和计算的开销。为了获得更好的搜索性能，建议denormalize你的数据。

&emsp;&emsp;通过以下方式定义父子关系：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "my_id": {
        "type": "keyword"
      },
      "my_join_field": { 
        "type": "join",
        "relations": {
          "question": "answer" 
        }
      }
    }
  }
}
```

&emsp;&emsp;第8行，字段的名字
&emsp;&emsp;第11行，定义了单个关系，`question`是`answer`的父级（parent）

&emsp;&emsp;若要使用join索引一篇文档，需要在`_source`中提供关系的名称和父级文档（可选）。例如下面的例子创建了在`question`的上下文中定义了两个`parent`文档：

```text
PUT my-index-000001/_doc/1?refresh
{
  "my_id": "1",
  "text": "This is a question",
  "my_join_field": {
    "name": "question" 
  }
}

PUT my-index-000001/_doc/2?refresh
{
  "my_id": "2",
  "text": "This is another question",
  "my_join_field": {
    "name": "question"
  }
}
```

&emsp;&emsp;第6行，这篇文档时`question`文档。

&emsp;&emsp;当索引父级文档（parent document）时，你可以选择快捷的只指定关系的名称，而不是使用对象进行封装：

```text
PUT my-index-000001/_doc/1?refresh
{
  "my_id": "1",
  "text": "This is a question",
  "my_join_field": "question" 
}

PUT my-index-000001/_doc/2?refresh
{
  "my_id": "2",
  "text": "This is another question",
  "my_join_field": "question"
}
```

&emsp;&emsp;第5行，直接使用关系的名称。

&emsp;&emsp;当索引一篇子文档（child parent）时，必须在`_source`中添加关系的名称以及父文档的文档ID（`_id`）.

> WARNING：要求将父子关系索引到同一个分片上，因此你必须使用父级文档ID来路由


&emsp;&emsp;例如下面的例子中展示了如何索引两篇`child`文档：

```text
PUT my-index-000001/_doc/3?routing=1&refresh 
{
  "my_id": "3",
  "text": "This is an answer",
  "my_join_field": {
    "name": "answer", 
    "parent": "1" 
  }
}

PUT my-index-000001/_doc/4?routing=1&refresh
{
  "my_id": "4",
  "text": "This is another answer",
  "my_join_field": {
    "name": "answer",
    "parent": "1"
  }
}
```

&emsp;&emsp;第1行，路由值是强制要有的，因为父/子文档必须索引到同一个分片上
&emsp;&emsp;第6行，`answer`是这个文档的join 名称
&emsp;&emsp;第7行，这个子文档的父级文档的文档ID

##### Parent-join and performance

&emsp;&emsp;在Elasticsearch中，不应将join字段用作关系数据库中的连接操作。为了保持良好的性能，关键是将数据反规范化（de-normalize）成文档。每个join字段、`has_child`或`has_parent`查询都会显著增加查询性能的负担，并可能触发[global ordinals](#eager_global_ordinals)的构建。

&emsp;&emsp;只有在数据包含一对多关系，其中一个实体数量显著多于另一个实体的情况下，使用join字段才有意义。例如，产品和这些产品的优惠就是这样的用例，如果优惠的数量远多于产品的数量，那么将产品模型化为父文档，将优惠模型化为子文档是有意义的。

##### Parent-join restrictions

- 每一个索引的mapping中只能有一个`join`类型的字段
- 父/子文档必须索引到相同的分片上。意味着在[getting](#Get API)、[deleting](#Delete API)、[updating](#Update API)子文档时必须提供相同的`route`值
- 一个元素可以有多个子节点但只能有一个父节点
- 可以像现有的`join`字段中添加新的关系
- 如果元素已经是父节点，也可以向现有元素添加子节点

##### Searching with parent-join

&emsp;&emsp;在Elasticsearch中使用父子连接（parent-join）搜索时，该功能创建一个字段来索引文档内关系的名称（例如my_parent, my_child等）。

&emsp;&emsp;对于每个父/子关系，它还会创建一个字段，该字段名称由join字段的名称加上`#`和关系中父项的名称组成。例如，对于`my_parent` → `[my_child, another_child]`关系，join字段会创建一个额外的名为`my_join_field#my_parent`的字段。

&emsp;&emsp;如果文档是子文档（my_child或another_child），此字段包含文档链接到的父`_id`；如果是父文档（my_parent），则包含文档的`_id`。搜索包含join字段的索引时，这两个字段总是返回在搜索响应中。

&emsp;&emsp;当查询一个包含`join`于的索引时，这两个字段总是会返回：

```text
GET my-index-000001/_search
{
  "query": {
    "match_all": {}
  },
  "sort": ["my_id"]
}
```

&emsp;&emsp;返回：

```text
{
  ...,
  "hits": {
    "total": {
      "value": 4,
      "relation": "eq"
    },
    "max_score": null,
    "hits": [
      {
        "_index": "my-index-000001",
        "_id": "1",
        "_score": null,
        "_source": {
          "my_id": "1",
          "text": "This is a question",
          "my_join_field": "question"         
        },
        "sort": [
          "1"
        ]
      },
      {
        "_index": "my-index-000001",
        "_id": "2",
        "_score": null,
        "_source": {
          "my_id": "2",
          "text": "This is another question",
          "my_join_field": "question"          
        },
        "sort": [
          "2"
        ]
      },
      {
        "_index": "my-index-000001",
        "_id": "3",
        "_score": null,
        "_routing": "1",
        "_source": {
          "my_id": "3",
          "text": "This is an answer",
          "my_join_field": {
            "name": "answer",                 
            "parent": "1"                     
          }
        },
        "sort": [
          "3"
        ]
      },
      {
        "_index": "my-index-000001",
        "_id": "4",
        "_score": null,
        "_routing": "1",
        "_source": {
          "my_id": "4",
          "text": "This is another answer",
          "my_join_field": {
            "name": "answer",
            "parent": "1"
          }
        },
        "sort": [
          "4"
        ]
      }
    ]
  }
}
```

&emsp;&emsp;第17行，这个文档是`question`
&emsp;&emsp;第30行，这个文档是`question`
&emsp;&emsp;第45行，这个文档是`answer`
&emsp;&emsp;第46行，子文档链接的父文档ID

##### Parent-join queries and aggregations

&emsp;&emsp;查看[has_child](#Has child query)、[has_parent](#Has parent query) Query、[child aggregation](#Children aggregation)以及[inner hits](#Parent/child inner-hits)了解更多信息。

&emsp;&emsp`join`字段的字段值可以在聚合根脚本中访问，可以使用[parent_id query](#Parent ID query)查询。

```text
GET my-index-000001/_search
{
  "query": {
    "parent_id": { 
      "type": "answer",
      "id": "1"
    }
  },
  "aggs": {
    "parents": {
      "terms": {
        "field": "my_join_field#question", 
        "size": 10
      }
    }
  },
  "runtime_mappings": {
    "parent": {
      "type": "long",
      "script": """
        emit(Integer.parseInt(doc['my_join_field#question'].value)) 
      """
    }
  },
  "fields": [
    { "field": "parent" }
  ]
}
```

&emsp;&emsp;第4行，查询`parent_id`字段(见[has_parent query ](#Has parent query)和[has_child query](#Has child query))
&emsp;&emsp;第12行，在`parent_id`上聚合（见[children aggregation](#Children aggregation)）
&emsp;&emsp;第21行，在脚本中访问`parent_id`

##### Global ordinals

&emsp;&emsp;`join`字段使用全局序号（[global ordinals](#eager_global_ordinals)）来加速连接操作。每当分片发生变化后，全局序号需要重建。分片中存储的父ID值越多，重建join字段的全局序号所需时间就越长；

&emsp;&emsp;默认情况下，全局序号会积极（eagerly）构建：如果索引发生变化，`join`字段的全局序号会作为刷新的一部分被重建。这可能会显著增加刷新时间。然而，大多数情况下这是正确的权衡，否则在首次使用父子连接查询或聚合时，全局序号会被重建，这可能会为用户引入显著的延迟峰值。

&emsp;&emsp;当`join`字段使用不频繁且频繁写入时，可能有必要禁用积极加载。

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "my_join_field": {
        "type": "join",
        "relations": {
           "question": "answer"
        },
        "eager_global_ordinals": false
      }
    }
  }
}

```

&emsp;&emsp;可以通过下面的方式查看每一个parent relation对应的全局序号的内存使用量：

```text
# Per-index
GET _stats/fielddata?human&fields=my_join_field#question

# Per-node per-index
GET _nodes/stats/indices/fielddata?human&fields=my_join_field#question
```

##### Multiple children per parent

&emsp;&emsp;也可以为单个parent定义多个child：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "my_join_field": {
        "type": "join",
        "relations": {
          "question": ["answer", "comment"]  
        }
      }
    }
  }
}
```

&emsp;&emsp;第8行，`question`是`answer`和`comment`的parent

##### Multiple levels of parent join

> WARNING：不要使用多级父子关系来模拟关系型数据库模型。每增加一级关系，都会在查询时增加内存和计算的开销。为了获得更好的搜索性能，建议denormalize你的数据。

&emsp;&emsp;Multiple levels of parent/child：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "my_join_field": {
        "type": "join",
        "relations": {
          "question": ["answer", "comment"],  
          "answer": "vote" 
        }
      }
    }
  }
}
```

&emsp;&emsp;第7行，`question`是`answer`和`comment`的parent
&emsp;&emsp;第8行，`vote`是`answer`的parent

&emsp;&emsp;上面的mapping意思就是：

```text
  question
    /    \
   /      \
comment  answer
           |
           |
          vote
```

&emsp;&emsp;索引一个grandchild文档需要跟grand-parent一样的路由值（the greater parent of the lineage）

```text
PUT my-index-000001/_doc/3?routing=1&refresh 
{
  "text": "This is a vote",
  "my_join_field": {
    "name": "vote",
    "parent": "2" 
  }
}
```

&emsp;&emsp;第1行，必须跟parent、grand-parent在相同的分片上
&emsp;&emsp;第6行，这个文档的父级文档的文档ID（必须指向`answer`文档）

#### Keyword type family
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/keyword.html)

&emsp;&emsp;keyword家族包含下面的字段类型：

- [keyword](#Keyword field type)：用于ID，邮件地址，hostname，状态码，zip code，或者标签
- [constant_keyword](#Constant keyword field type)：keyword类型，并且所有文档中该字段的字段值都是相同的
- [wildcard](#Wildcard field type)：用于非结构化的机器自动生成的内容。`wildcard`类型对于large value或者high cardinality的字段值进行了优化

&emsp;&emsp;keyword常用于[sorting](#Sort search results)，[aggregation](#Aggregations)以及[term-level queries](#Term-level queries)，例如[term](#Term query)。

> TIP：若要全文检索，请不要使用keyword。而是使用[text](#Text type family)类型。

##### Keyword field type

&emsp;&emsp;下面是`keyword`字段的mapping：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "tags": {
        "type":  "keyword"
      }
    }
  }
}
```

> TIP：**Mapping numeric identifiers**
> 不是所有的数值类型的数据都应该映射为一个[numeric](#Numeric field types)数据类型。Elasticsearch会为数值类型的字段，比如`integer`、`long`针对[range](#Range query)查询优化。然而，[keyword](#Keyword type family)字段更适合于[term](#Term query)以及其他[term-level](#Term-level queries) Query。
> 比如一些标识符，ISBN或者产品ID，很少用于`range` query，但是常用于term-level query。
> 如果有以下的场景，可以考虑将数值类型的标识符映射为`keyword`：
>
> - 你不打算将标识符类型的数据用于[range](#Range query) query
> - 快速的检索对你来说很重要的话，那么相较于数值类型的字段，在`keyword`字段上进行`term` query通常更快。
>
> 如果你不确定该使用哪种类型，那么可以通过[multi-field](#fields)来同时使用`keyword`跟数值类型的字段。

##### Parameters for basic keyword fields

&emsp;&emsp;`keyword`字段可以使用以下的参数：

- [doc_values](#doc_values) : 该字段是否用基于列式存储于磁盘中，使得可以用于排序、聚合、或者Script。参数值为`true`或者`false`（默认值）
- [eager_global_ordinals](#eager_global_ordinals) ：是否在refresh尽快的载入global ordinals？默认是`false`。如果经常用于terms aggregation，开启这个参数是很有必要的
- [fields](#fields)： Multi-fields允许同一个将string value基于不同的目的用多种方式进行索引。比如用于查询或者用于排序聚合
- [ignore_above](#ignore_above)：不会索引长于该参数的值。默认是`2147483647`。然而注意的是默认的动态映射创建的子字段`keyword`会覆盖这个默认的值，并且将`ignore_above`设置为`256`
- [index](#index(mapping parameter))：该字段是否需要被快速的检索？参数值为`true`或者`false`（默认值）。
- [index_options](#index_options)：在索引中存储哪些信息用于打分目的。默认是`docs`但是可以设置为`freqs`，在打分时会将词频考虑进去。
- [meta](#meta(mapping parameter))：字段的元数据信息
- [norms](#norms(mapping parameter))：打分时是否考虑字段值的长度。默认false，可以设置为`true`
- [null_value](#null_value)：参数值可以是一个该字段的相同类型的数值。默认值是`null`，意味着缺失值。注意的是如果使用了`script`参数，则不能设置这个参数
- on_script_error：该参数定义了当Script在索引期间抛出错误后，该如何处理。默认值为`fail`，会导致整个文档被reject；如果参数值为`continue`，将在[\_ignored](#\_ignored field)字段注册这个字段然后继续索引。这个参数只有在`script`设置后才能被设置。
- script：如果设置了这个参数，这个字段的字段值会通过script生成，而不是直接从源数据中读取。如果文档中设置了该字段的值，则会reject并且抛出错误。script跟[runtime](#Map a runtime field)中有相同的format。
- [store](#store)：该字段是否独立于[\_source](#_source field)字段并且可以用于检索。参数值为`true`或者`false`（默认值）
- [similarity](#similarity)：使用哪一个打分算法。默认值为`BM25`。
- [normalizer](#normalizer(mapping parameter))：如何在索引之前预先处理keyword的值，默认为`false`，即使用keyword原有的字段值。
- split_queries_on_whitespace：执行[full text queries](#Full text queries)时，是否根据空格对输入进行分割。可选值为`true`或`false`（default）
- time_series_dimension
  - 预览功能， 可能会被更改或者移除，目前只在Elasticsearch代码内部使用，暂不介绍。

##### Constant keyword field type

&emsp;&emsp;Constant keyword是`keyword`的一种特例用于所有的文档中会索引相同的值。

```text
PUT logs-debug
{
  "mappings": {
    "properties": {
      "@timestamp": {
        "type": "date"
      },
      "message": {
        "type": "text"
      },
      "level": {
        "type": "constant_keyword",
        "value": "debug"
      }
    }
  }
}
```

&emsp;&emsp;`constant_keyword`支持跟`keyword`有相同的查询跟聚合，但是会利用所有文档拥有相同的值这个特点来提高查询效率。

&emsp;&emsp;同时还允许文档中不包含这个字段或者这个字段的字段值跟mapping中的值相同。下面两个索引请求是等价的：

```text
POST logs-debug/_doc
{
  "date": "2019-12-12",
  "message": "Starting up Elasticsearch",
  "level": "debug"
}

POST logs-debug/_doc
{
  "date": "2019-12-12",
  "message": "Starting up Elasticsearch"
}
```

&emsp;&emsp;然而如果文档中该字段的字段值跟mapping中的不一致是不允许。

&emsp;&emsp;如果mapping中没有配置`value`，那么该字段会基于第一篇文档中该字段的值自动的配置。注意的是这意味着如果第一篇文档的值是有问题的，会导致其他的合法的文档被reject。

&emsp;&emsp;在提供`value`之前（通过mapping或者第一篇文档），任何对该字段的查询都不会匹配任何文档。包括[exists](#Exists query) query。

&emsp;&emsp;该字段的`value`在设置后就不能变更。

##### Parameters for constant keyword fields

&emsp;&emsp;该类型可以有以下的参数：

- [meta](#meta(mapping parameter))：字段的元数据信息
- value：索引中所有文档的constant keyword fields的字段值。如果没有设置该参数，则是基于第一篇被索引的文档中该字段的字段值

##### Wildcard field type

&emsp;&emsp;`wildcard`字段是一种特殊的keyword类型，用于无结构的机器自动生成的内容，并且你计划使用[wildcard](#Wildcard query)跟[regexp](#Regexp query)这类Query进行查询。`wildcard`类型对于large value和high cardinality有专门的优化。

> **Mapping unstructured content**
> &emsp;&emsp;你可以将一个无结构的内容使用`text`或者keyword家族字段。使用哪一种取决于内容的特点以及你如何查询这个字段
> 如果有以下的场景，则使用`text`字段：
>
> - 内容是可读的（human-readable），比如邮件正文或者产品描述
> - 你想要使用独立的单词或者短语进行查询，例如`the brown fox jumped`，使用[full text queries](#Full text queries)，Elasticsearch会对`text`字段进行[分词](#Text analysis)并且返回相关性最高的结果。
>
> 如果有以下的场景，则使用keyword家族字段：
> - 内容是机器自动生成的，比如日志消息或者HTTP请求信息
> - 你想要使用精确完整的值进行查询，比如`org.foo.bar`，或者使用部分字符序列，比如使用[term-level queires](#Term-level queries)查询`org.foo.*`。
>
> **Choosing a keyword family field type**
> 如果你选择了一个keyword家族字段，使用`keyword`还是`wildcard`取决于字段值的长度和字段值的种类（cardinality）。如果你经常使用[wildcard](#Wildcard query)或者[regexp](#Regexp query)查询并且满足下面类别的一种，则使用`wildcard`：
> - 字段值种类包含超过one million个并且经常使用通配符在关键字开头的查询，例如`*foo`和`*baz`。
> - 字段值大小超过32KB并且经常使用wildcard pattern查询
>
> 否则就使用`keyword`类型，这样查询速度更快，索引更快并且占用更少的存储开销。对于深入的比较以及讨论，见related blog post。
>
> **Switching from a text field to a keyword field**
> 如果你之前使用`text`字段处理无结构的机器自动生成的内容。你可以[reindex to update the mapping](https://www.elastic.co/cn/blog/find-strings-within-strings-faster-with-the-new-elasticsearch-wildcard-field)到`keyword`或者`wildcard`字段中。我们还建议，将你们应用或者工作流中在这个字段上的[full text queries](#Full text queries)替换为等价的[term-level queries](#Term-level queries)。


&emsp;&emsp;在内部，通配符字段使用 ngram 对整个字段值进行索引，并存储完整字符串。这个索引被用作rough filter，以减少随后通过检索和检查完整值来检查的值数量。这个字段特别适合用于在日志行上运行类似 grep 的查询。存储成本通常低于`keyword`字段，但对完整词项的精确匹配的搜索速度较慢。如果字段值共享许多前缀，如同一个网站的 URL，`wildcard`字段的存储成本可能高于等效的`keyword`字段。

&emsp;&emsp;你可以通过下面的方式索引以及查询一个`wildcard`字段：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "my_wildcard": {
        "type": "wildcard"
      }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "my_wildcard" : "This string can be quite lengthy"
}

GET my-index-000001/_search
{
  "query": {
    "wildcard": {
      "my_wildcard": {
        "value": "*quite*lengthy"
      }
    }
  }
}
```

##### Parameters for wildcard fields

&emsp;&emsp;`wildcard`可以使用下面的参数：

- [null_value](#null_value)：参数值可以是一个字符串。默认值是`null`，意味着缺失值。注意的是如果使用了`script`参数，则不能设置这个参数
- [ignore_above](#ignore_above)：不会索引长于该参数的值。默认是`2147483647`。

##### Limitations

- `wildcard`字段跟keyword一样不会被分词。因此不支持依赖位置的查询，比如短语查询
- 在执行通配符查询时，任何设置的重写参数将被忽略。返回的文档打分总是一个常数值。

#### Nested field type
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/nested.html)

&emsp;&emsp;`nested`类型是[object](#Object field type)类型的特殊版本，它允许将对象数组以一种可以独立查询每个对象的方式进行索引。

> TIP：当你需要处理包含大量任意键的键值对时，你可能会考虑将每一对键值作为一个独立的嵌套文档进行建模，其中每个文档包含两个字段：一个用于键（key），另一个用于值（value）。
> - 例如产品A:color: red size: medium  产品B: color: blueweight: 1kg。希望通过color: red size: medium这两个条件才会找到产品A。
> - 若考虑使用[flattened](#Flattened field type)数据类型，则将整个对象作为一个字段的字段值，允许对整个对象的内容做简单的搜索。Nested document和query通常开销昂贵，所以在这种例子中使用`flattened`数据类型是个不错的选择。

##### How arrays of objects are flattened

&emsp;&emsp;Elasticsearch没有inner object的概念。因此，它是将其平铺为的字段名字段值列表。例如索引以下的文档：

```text
PUT my-index-000001/_doc/1
{
  "group" : "fans",
  "user" : [ 
    {
      "first" : "John",
      "last" :  "Smith"
    },
    {
      "first" : "Alice",
      "last" :  "White"
    }
  ]
}
```

&emsp;&emsp;第4行，`user`字段会动态的添加为`object`类型。

&emsp;&emsp;上面的文档在内部被转化为以下的内容：

```text
{
  "group" :        "fans",
  "user.first" : [ "alice", "john" ],
  "user.last" :  [ "smith", "white" ]
}
```

&emsp;&emsp;`user.first`和`user.last`平铺为多值的字段，同时`alice`和`white`的关联信息也丢失了（关联信息指的是：它们两个属于同一个对象）。下面`alice AND smith`的这种查询会错误的匹配到这篇文档：

```text
GET my-index-000001/_search
{
  "query": {
    "bool": {
      "must": [
        { "match": { "user.first": "Alice" }},
        { "match": { "user.last":  "Smith" }}
      ]
    }
  }
}
```

##### Using nested fields for arrays of objects

&emsp;&emsp;如果你需要索引对象数组并且还要维护数组中每一个对象相对独立，那么使用`nested`数据类型而不是[object](#Object field type)数据类型。

&emsp;&emsp;在内部，nested object会将数组中每一个对象分别索引到一个hidden document（即在Lucene层每一个对象对应一个文档），意思是每一个nested object可以使用[nested query](#Nested query)独立于其他对象进行查询

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "user": {
        "type": "nested" 
      }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "group" : "fans",
  "user" : [
    {
      "first" : "John",
      "last" :  "Smith"
    },
    {
      "first" : "Alice",
      "last" :  "White"
    }
  ]
}

GET my-index-000001/_search
{
  "query": {
    "nested": {
      "path": "user",
      "query": {
        "bool": {
          "must": [
            { "match": { "user.first": "Alice" }},
            { "match": { "user.last":  "Smith" }} 
          ]
        }
      }
    }
  }
}

GET my-index-000001/_search
{
  "query": {
    "nested": {
      "path": "user",
      "query": {
        "bool": {
          "must": [
            { "match": { "user.first": "Alice" }},
            { "match": { "user.last":  "White" }} 
          ]
        }
      },
      "inner_hits": { 
        "highlight": {
          "fields": {
            "user.first": {}
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;第6行，`user`字段使用`nested`而不是`object`。
&emsp;&emsp;第36行，因为`Alice`和`Smith`不是同一个nested object，所以这个query不会匹配到文档
&emsp;&emsp;第53行，因为`Alice`和`White`是同一个nested object，所以这个query会匹配到文档
&emsp;&emsp;第57行，`inner_hits`允许我们高亮匹配到的文档。

##### Interacting with nested documents

&emsp;&emsp;使用[nested](#Nested query) query查询

&emsp;&emsp;用[nested](#Nested aggregation)聚合以及[reverse_nested](#Reverse nested aggregation) aggregations.

&emsp;&emsp;用[nested sorting](#Nested sorting examples)排序

&emsp;&emsp;用[nested inner hits](#Nested inner hits)检索以及高亮

> IMPORTANT：因为nested documents索引为单独的文档（在lucene中是单独的文档，数组中的每一个对象都是lucene中的一篇文档，并且包含父文档的父文档ID，以及在主文档中的位置信息）。因此它们只能通过`nested` query， `nested/reverse_nested` aggregation，或者[nested inner hits](#Nested inner hits)访问。
> 
> 例如，如果nested document内的字符串字段的[index_options](#index_options)设置为`offsets`以便在高亮显示时使用倒排信息，那么在主要高亮（main highlighting）显示阶段这些偏移量将不可用。相反，需要通过[nested inner hits](#Nested inner hits)来执行高亮显示。在通过[docvalue_fields](#Doc value fields)或[stored_fields](#Stored fields)进行搜索时加载字段时，同样的考虑也适用。

##### Parameters for nested fields

&emsp;&emsp;可以有下面的参数：

- [dynamic](#dynamic(mapping parameter))：（Optional, string）是否将新的`properties`写入到已存在的nested object中，可选值为`true` (default)， `runtime`， `false` and `strict`

- [properties](#properties)：nested object中的所有字段（这些字段被称为properties），这些字段可以是任何数据类型[data type](#Field data types)，新的字段可能被添加现在有的nested object字段中

- include_in_parent：（Optional, Boolean） 如果为`true`，nested object中的所有字段同样添加到parent document 作为标准（flat）字段。默认是`false`。
  - 如果有多层嵌套，那么上一层就是parent。就可以使用普通的query，比如`match`根据上一层父级的字段进行搜索
- include_in_root：（Optional, Boolean）如果为`true`，nested object中的所有字段同样添加到root document 作为标准（flat）字段。默认是`false`。

&emsp;&emsp;这两个参数为true的好处相同点就是可以通过普通query就可以进行查询，而不需要专用的[nested query](#Nested query)。

#### Numeric field types
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/number.html)

&emsp;&emsp;支持下列的数值类型：

- long：有符号的64位整数，最小值`2^63`，最大值`2^63 -1`
- integer：有符号32位整数，最小值`2^31`，最大值`2^31 -1`
- short：有符号的16位整数，最小值`-32,768`，最大值`32,767`
- byte：有符号的8位整数，最小值`-128`，最大值`127`
- double：一个双精度的64位IEEE 754浮点数，限制为有限值（不包括infinity和NaN，Not a Number）
- float：一个单精度的32位IEEE 754浮点数，限制为有限值
- half_float：一个半精度的16位IEEE 754浮点数，限制为有限值。
- scaled_float：一个由长整数（long）支持，通过固定双精度缩放因子缩放的浮点数
- unsigned_long：无符号的64位整数，最小值为0，最大值为`2^64-1`。

&emsp;&emsp;下面是在mapping中配置数值字段的例子：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "number_of_bytes": {
        "type": "integer"
      },
      "time_in_seconds": {
        "type": "float"
      },
      "price": {
        "type": "scaled_float",
        "scaling_factor": 100
      }
    }
  }
}
```

> NOTE：`double`, `float` 和 `half_float`类型中`-0.0`和`+0.0`是不一样的值。因此，`term` query `-0.0`不会匹配到`+0.0`，反之亦然。对于range query也是一样的：如果上界是`-0.0`那么`+0.0`不会匹配，同样的，如果下界是`+0.0`那么`-0.0`不会匹配到。

##### Which type should I use?

&emsp;&emsp;对于整数类型（`byte`, `short`, `integer` 以及`long`），对于你的用例，你应该选择最小类型（smallest Type）。这将让索引和搜索更加高效。注意的是尽管存储时会基于实际的值进行优化，因此选择一种类型而不是另一种类型对存储需求没有影响。 

&emsp;&emsp;对于浮点型，通常使用`scaling factor`将浮点数据存储为整数会更有效率，这正是`scaled_float`类型在底层所做的。例如，一个price字段可以使用`scaling factor`为100的`scaled_float`来存储。API中像是将字段值作为一个double存储，但在Elasticsearch内部以"分"为单位存储，`price*100`，就可以将double变成一个整数。相较于浮点数，整数更易于压缩来节省磁盘开销。`scaled_float`属于精度跟磁盘开销之间的折中方式。例如，假设你在跟踪 CPU利用率，这是一个介于 0 和 1 之间的数字。CPU 利用率是 12.7% 还是 13% 通常并不重要，所以你可以使用一个 scaling_factor 为 100 的 scaled_float，以将 CPU 利用率四舍五入到最接近的百分比，以节省空间。（如果字段值是固定小数位，用scaled_float是个不错的选择）

&emsp;&emsp;如果`scaled_float`不适合你的使用场景，你应该在浮点型之间选择一个够用且最小的类型：`double`、`float`、`half_float`。这里提供了一个类型之间差异的表，帮助你选择适当的浮点型类型。

| Type         | Minimum value | Maximum value    | Significant bits / digits | Example precision loss                    |
| ------------ | ------------- | ---------------- | ------------------------- | ----------------------------------------- |
| `double`     | `2^-1074`      | `(2-2^-52)·2^1023` | `53` / `15.95`            | `1.2345678912345678`→ `1.234567891234568` |
| `float`      | `2^-149`       | `(2-2^-23)·2^127`  | `24` / `7.22`             | `1.23456789`→ `1.2345679`                 |
| `half_float` | `2^-24`        | `65504`          | `11` / `3.31`             | `1.2345`→ `1.234375`                      |


> TIP：Mapping numeric identifiers
> 不是所有的数值类型的数据都应该映射为一个[numeric](#Numeric field types)数据类型。Elasticsearch会为数值类型的字段，比如`integer`、`long`针对[range](#Range query)查询优化。然而，[keyword](#Keyword type family)字段更适合于[term](#Term query)以及其他[term-level](#Term-level queries) Query。
> 比如一些标识符，ISBN或者产品ID，很少用于`range` query，但是常用于term-level query。
> 如果有以下的场景，可以考虑将数值类型的标识符映射为`keyword`：
> - 你不打算将标识符类型的数据用于[range](#Range query) query
> - 快速的检索对你来说很重要的话，那么相较于数值类型的字段，在`keyword`字段上进行`term` query通常更快。
>
> 如果你不确定该使用哪种类型，那么可以通过[multi-field](#fields)来同时使用`keyword`跟数值类型的字段。

##### Parameters for numeric fields

&emsp;&emsp;数值类型的字段可以使用以下的参数：

- [doc_values](#doc_values) : 该字段是否用基于列式存储于磁盘中，使得可以用于排序、聚合、或者Script。参数值为`true`或者`false`（默认值）
- [ignore_malformed](#ignore_malformed)：如果为`true`，忽略格式错误的数值。如果为`false`（default），遇到格式错误的数值会抛出一个异常并且reject整个文档。注意的是如果使用了`script`参数，则不能设置这个参数
- [index](#index(mapping parameter))：该字段是否需要被快速的检索？参数值为`true`或者`false`（默认值）。
- [meta](#meta(mapping parameter))：字段的元数据信息
- [null_value](#null_value)：参数值可以是一个该字段的相同类型的数值。默认值是`null`，意味着缺失值。注意的是如果使用了`script`参数，则不能设置这个参数
- on_script_error：该参数定义了当Script在索引期间抛出错误后，该如何处理。默认值为`fail`，会导致整个文档被reject；如果参数值为`continue`，将在[\_ignored](#\_ignored field)字段注册这个字段然后继续索引。这个参数只有在`script`设置后才能被设置。
- script：如果设置了这个参数，这个字段的字段值会通过script生成，而不是直接从源数据中读取。如果文档中设置了该字段的值，则会reject并且抛出错误。script跟[runtime](#Map a runtime field)中有相同的format。
- [store](#store)：该字段是否独立于[\_source](#_source field)字段并且可以用于检索。参数值为`true`或者`false`（默认值）

- [store](#store)：该字段是否独立于[\_source](#_source field)字段并且可以用于检索。参数值为`true`或者`false`（默认值）
- time_series_dimension
  - 预览功能， 可能会被更改或者移除，目前只在Elasticsearch代码内部使用，暂不介绍。
- time_series_metric
  - 预览功能。。该字段只用于Elastic内部使用。将字段作为一个Time series metric。这个值是指标类型（metric type）。默认是`null`（不是一个时间序列指标）

##### Parameters for scaled_float

&emsp;&emsp;`scaled_float`可以使用下面额外的参数

- scaling_factor：该参数用来对数值进行编码。在索引期间会将数值于这个参数进行乘积运算，然后四舍五入到最近的一个long值。比如说`scaled_float`的值为`10`，那么在Elasticsearch内部会将`2.34`存储为`23`并且查询期间的操作（query、aggregation、sorting）会将这篇文档的该值视为`2.3`。`scaled_float`值越高代表更高的精度但增加磁盘使用。必须设置这个参数。

#### Object field type
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/object.html)

&emsp;&emsp;事实上JSON documents是有层次的。文档中可能包含inner objects，并且它们自身还包含inner objects：

```text
PUT my-index-000001/_doc/1
{ 
  "region": "US",
  "manager": { 
    "age":     30,
    "name": { 
      "first": "John",
      "last":  "Smith"
    }
  }
}
```

&emsp;&emsp;第2行，文档外部同样是JSON object
&emsp;&emsp;第4行，包含一个名为`manager`的inner object
&emsp;&emsp;第6行，`manager`接着还包含一个名为`name`的inner object

&emsp;&emsp;内部的实现是一个简单的，扁平的key-value链，如下所示：

```text
{
  "region":             "US",
  "manager.age":        30,
  "manager.name.first": "John",
  "manager.name.last":  "Smith"
}
```

&emsp;&emsp;上述文档对应的显示的mapping如下所示：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": { 
      "region": {
        "type": "keyword"
      },
      "manager": { 
        "properties": {
          "age":  { "type": "integer" },
          "name": { 
            "properties": {
              "first": { "type": "text" },
              "last":  { "type": "text" }
            }
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;第4行，mapping中最顶层的properties的定义
&emsp;&emsp;第8行，`manager`字段是一个inner object
&emsp;&emsp;第11行，`manager.name`字段是`manager`字段中的inner object

&emsp;&emsp;你不需要显示的指定`object`指定`type`

##### Parameters for object fields

&emsp;&emsp;下面的参数可以应用于`object`字段

- [dynamic](#dynamic(mapping parameter))：是否将新的字段写入到已存在的object字段中，可选值为`true` (default)， `runtime`， `false` and `strict`
- [enabled](#enabled)：object字段的字段值是否要进行解析并且写入到索引中（`true`），或者完全忽略(`false`)
- [properties](#properties)：object中的所有字段（这些字段被称为properties），这些字段可以是任何数据类型[data type](#Field data types)，新的字段可能被添加现在有的object字段中（比如dynamic参数置为true的情况下）

> IMPORTANT：如果不是要索引一个object，而是object数组，请先阅读[Nested](#Nested field type)。

#### Percolator field type
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/percolator.html)

#### Point field type
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/point.html)

#### Range field types
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/range.html)

#### Rank features field type
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rank-features.html)

#### Rank feature field type
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rank-feature.html)

#### Search-as-you-type field type
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-as-you-type.html)

&emsp;&emsp;

#### Shape field type
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/shape.html)

&emsp;&emsp;

#### Text type family
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/text.html)

&emsp;&emsp;text family包含的字段类型（field type）如下所示：

- [text](#Text field type)，用于全文内容 的传统的字段类型，比如email的body或者产品的描述
- [match_only_text](#Match-only text field type)，经过space-optimized优化的`text`字段的变种，它关闭了打分并且对于需要position的查询执行较慢。这个字段最适合索引log messages。

##### Text field type

&emsp;&emsp;索引full-text values的字段，例如email的body或者一个产品的描述。字段值会被分词，字段值会被[analyzer](#Text analysis)分词，将一个string值转化为一个一个独立的term，并将这些term进行索引。analysis process允许Elasticsearch在每一个full text filed中查询独立的词（individual Word）。text字段不用于排序或者很少用于聚合（尽管[significant text aggregation](#Significant text aggregation)是一个明显的例外）。

&emsp;&emsp;`text`字段最适合用于无结构的并且是human-readable的内容。如果你需要索引无结构的machine-generated的内容，见[Mapping unstructured content](#Wildcard field type)。

&emsp;&emsp;如果你需要索引例如email地址，hostnames，状态码，或者tags，你应该选择[keyword](#Keyword type family)字段。

&emsp;&emsp;下面是text字段的mapping例子：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "full_name": {
        "type":  "text"
      }
    }
  }
}
```

##### Use a field as both text and keyword

&emsp;&emsp;有时候同一个字段需要同时有full text(`text`)和keyword(`keyword`)。一个用来全文检索，另一个用于聚合和排序。可以通过 [multi-fields](#fields)实现。

##### Parameters for text fields

&emsp;&emsp;`text`字段接受下面的参数

| [analyzer](#analyzer) | 分词器[analyzer](#Text analysis)y应该同时用于index-time和search-time（除非被[search_analyzer](#search_analyzer)重写（override）），默认是index analyzer或者[standard analyzer](#Standard analyzer). |
| :----------------------------------------------------------: | :--: |
| [eager_global_ordinals](#eager_global_ordinals) | 是否在refresh尽快的载入global ordinals？默认是`false`。如果经常用于terms aggregation，开启这个参数是很有必要的 |
| [fielddata](#fielddata mapping parameter) | 这个字段是否用于排序、聚合、或在script中使用？默认是`false` |
| [fielddata_frequency_filter](#fielddata_frequency_filter mapping parameter) | （Expert Setting）在开启`fielddata`后，将哪些值载入到内存中，默认值是`false` |
| [fields](#fields) | Multi-fields允许同一个将string value基于不同的目的索引用多种方式进行索引。比如用于查询或者用于排序聚合，或者使用不同的分词器对string value进行分词 |
| [index](#index(mapping parameter)) | 这个字段是否可以被搜索？默认是`true` |
| [index_options](#index_options) | 哪些信息需要被存储到索引中用于查询和高亮，默认是`positions` |
| [index_prefixes](#index_prefixes) | 开启这个参数后，term的2到5个前缀字符会被索引到额外的字段（separate filed）。这使得前缀搜索更高效，代价就是索引大小会增加 |
| [index_phrases](#index_phrases) | 开启这个参数后，2个term组成的字段值会被索引到额外的字段（separate filed）中。这使得exact phrase（no slop） 查询可以更高效，代价就是索引大小会增加。注意的是只有移除了停用词后才能达到最佳性能，因为包含了停用词的短语将不会使用subsidiary field并且会回滚到标准的短语查询。默认是`false` |
| [norms](#norms) | 执行打分的查询时是否要考虑字段值的长度 |
| [position_increment_gap](#position_increment_gap) | The number of fake term position which should be inserted between each element of an array of strings. Defaults to the `position_increment_gap` configured on the analyzer which defaults to `100`. `100` was chosen because it prevents phrase queries with reasonably large slops (less than 100) from matching terms across field values. |
| [store](#store) | 是否存储字段值并且可以通过`_source`字段查询。默认是`false` |
| [search_analyzer](#search_analyzer) | 是否在search-time时使用上文中第一个参数中指定的分词器（默认值）？ |
| [search_quote_analyzer](#search_quote_analyzer) | 当遇到短语时，是否在search-time使用上文中第一个参数中指定的分词。默认值是上文中`search_analyzer`中指定的分词器 |
| [similarity](#similarity) | 使用哪种打分算法？默认值是`BM25` |
| [term_vector](#term_vector) | 是否为这个字段存储term vectors？默认值是`false` |
| [meta](#meta) | 这个字段的Metadata |


##### fielddata mapping parameter

&emsp;&emsp;`text`字段默认是可以用于搜索的，但是默认不能用于聚合、排序、或者scripting。如果你想要排序，聚合，或者从script中访问`text`字段，你将会遇到这个异常：`Fielddata is disabled on text fields by default`。 设置`your_field_name`为`fielddata=true`，通过uninverting倒排索引（inverted index）将fielddata导入到内存中。注意的是这将会使用大量的内存。

&emsp;&emsp;Field data是在排序、聚合、或者scripting中访问fulll text field的分词后的token的唯一方法。例如，比如说`New York`这个full text field将会分词为`new`和`york`两个token，对这些token进行聚合需要field data。

##### Before enabling fielddata

&emsp;&emsp;在text field上开启fielddata通常是没有意义的。Field data使用[field data cache](#Field data cache settings)并存储在堆上，因为计算开销很大。计算field data会导致latency spikes并且增加堆内存的使用，这也是导致集群性能问题的原因之一。

&emsp;&emsp;大多数的用户会使用[multi-field mappings](#fields)来同时获得`text`字段用于全文检索以及不分词的`keyword`字段用于聚合，如下所示：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "my_field": { 
        "type": "text",
        "fields": {
          "keyword": { 
            "type": "keyword"
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;第5行，`my_field`字段用于查询
&emsp;&emsp;第8行，`my_field.keyword`用于聚合、排序或者在script中使用

##### Enabling fielddata on text fields

&emsp;&emsp;你可以使用[update mapping API](#Update mapping API)对现有的`text`字段开启fielddata，如下所示：

```text
PUT my-index-000001/_mapping
{
  "properties": {
    "my_field": { 
      "type":     "text",
      "fielddata": true
    }
  }
}
```

&emsp;&emsp;第4行，`my_field`指定的mapping应该由这个字段现有的mapping加上`fielddata`参数组成。

##### fielddata_frequency_filter mapping parameter

&emsp;&emsp;FieldData filtering可以用于减少载入到内存的term的数量，因此能降低内存使用量。可以通过frequency来过滤term：

&emsp;&emsp;frequency filter能让你只将frequency在`min`和`max`之间的term载入到内存，`min`和`max`可以是一个确切的数值（absolute number， 数值大于1）或者是一个百分值（比如`0.01`就是`1%`并且`1.0`就是`100%`）。frequency在每个段中进行计算。百分值基于一个段中拥有这个字段的文档数量相对于所有文档的数量。

&emsp;&emsp;可以通过指定`min_segment_size`，即段中包含的文档数量最小值来排除一些小段：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "tag": {
        "type": "text",
        "fielddata": true,
        "fielddata_frequency_filter": {
          "min": 0.001,
          "max": 0.1,
          "min_segment_size": 500
        }
      }
    }
  }
}
```

##### Match-only text field type

&emsp;&emsp;[text](#Text field type)类型的一个变体是`match_only_text`。它是打分和与位置相关的query的一种折中类型。这个类型的字段跟只索引文档号（index_options: docs）的`text`字段有相同存储数据效率并且关闭了norms（norms: false）。在这个字段上执行term query跟在`text`类型的字段上更快，至少也是相等。然而如果在`match_only_text`上执行需要位置信息的query，例如[match_phrase](#Match phrase query)，那性能会降低，因为需要从`_source`中验证短语是否匹配。所有的query返回固定的打分值：1.0。

&emsp;&emsp;不能配置分词器：文本总是用默认的[分词器分词](#Specify the default analyzer for an index)（默认是[standard](#Standard analyzer)）。

&emsp;&emsp;[span queries](#Span queries)无法在这个类型的字段上使用，可以使用[interval queries](#Intervals query)，或者你真的需要使用span queries，那么使用[text](#Text field type)类型。

&emsp;&emsp;除此之外，`match_only_text`支持与文本（`text`）相同的查询。就像`text`一样，它不支持排序，并且对聚合的支持有限。

```text
PUT logs
{
  "mappings": {
    "properties": {
      "@timestamp": {
        "type": "date"
      },
      "message": {
        "type": "match_only_text"
      }
    }
  }
}
```

##### Parameters for match-only text fields

&emsp;&emsp;`match_only_text`接受下面的参数：

- [fields](#fields)： Multi-fields允许同一个将string value基于不同的目的索引用多种方式进行索引。比如用于查询或者用于排序聚合，或者使用不同的分词器对string value进行分词
- [meta](#meta(mapping parameter))：字段的元数据信息

#### Token count field type
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/token-count.html)

&emsp;&emsp;`token_count`字段是一个[integer](#Numeric field types)类型的字段，它用来接收字符串的值，然后分词最后记录字符串中token的数量。

&emsp;&emsp;例如：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "name": { 
        "type": "text",
        "fields": {
          "length": { 
            "type":     "token_count",
            "analyzer": "standard"
          }
        }
      }
    }
  }
}

PUT my-index-000001/_doc/1
{ "name": "John Smith" }

PUT my-index-000001/_doc/2
{ "name": "Rachel Alice Williams" }

GET my-index-000001/_search
{
  "query": {
    "term": {
      "name.length": 3 
    }
  }
}
```

&emsp;&emsp;第5行，名为`name`的字段是一个[text](#Text type family)字段，使用了默认的`standard`分词器。
&emsp;&emsp;第7行，`name.length`字段是一个[multi-field](#fields)的`token_count`字段，它会记录`name`字段中token的数量，
&emsp;&emsp;第28行，这个query只会匹配包含`Rachel Alice Williams`的文档，因为它会被分为三个token。

##### Parameters for token_count fields

&emsp;&emsp;`token_count`字段接收下列的参数：
- [analyzer](#analyzer(mapping parameter))：[analyzer](#Text analysis)用来对字符串的值分词，必须要指定。处于性能考虑，使用不带token filter的分词器。
- enable_position_increments：是否要统计position increments。如果你不想要统计被分词器过滤掉（比如[stop](#Stop token filter)）的token的position increments，那么设置为`false`。默认为`true`。
- [index](#index(mapping parameter))：该字段是否需要被快速的检索？参数值为`true`或者`false`（默认值）。
- [doc_values](#doc_values) : 该字段是否用基于列式存储于磁盘中，使得可以用于排序、聚合、或者Script。参数值为`true`或者`false`（默认值）
- [index](#index(mapping parameter))：该字段是否需要被快速的检索？参数值为`true`或者`false`（默认值）。
- [null_value](#null_value)：参数值是具有相同类型的数值用来替换为`null`的字段值。默认值是`null`，意味着缺失值。注意的是如果使用了`script`参数，则不能设置这个参数
- [store](#store)：该字段是否独立于[\_source](#_source field)字段并且可以用于检索。参数值为`true`或者`false`（默认值）

#### Unsigned long field type
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/unsigned-long.html)

&emsp;&emsp;Unsigned long是一个数值类型的字段，代表一个无符号的64bit的整数，最小值为0并且最大值为`2^64 - 1`（[0, 18446744073709551615]）

```text
PUT my_index
{
  "mappings": {
    "properties": {
      "my_counter": {
        "type": "unsigned_long"
      }
    }
  }
}
```

&emsp;&emsp;unsigned long可以用数值或者字符串的格式进行索引，代表了[0, 18446744073709551615]范围的整数。不能有小数部分。

```text
POST /my_index/_bulk?refresh
{"index":{"_id":1}}
{"my_counter": 0}
{"index":{"_id":2}}
{"my_counter": 9223372036854775808}
{"index":{"_id":3}}
{"my_counter": 18446744073709551614}
{"index":{"_id":4}}
{"my_counter": 18446744073709551615}
```

&emsp;&emsp;Term query可以使用数值或者字符串格式。

```text
GET /my_index/_search
{
    "query": {
        "term" : {
            "my_counter" : 18446744073709551615
        }
    }
}
```

&emsp;&emsp;range query可以包含带有小数的值。在这种情况下，Elasticsearch会将它们转化成整数：`gte`和`gt`会转化成最近的向上的整数（inclusive）；`lt`和`lte`会转化成最近的向下的整数（inclusive）。

&emsp;&emsp;建议使用字符串格式的值，就不会再解析时丢失精度。

```text
GET /my_index/_search
{
    "query": {
        "range" : {
            "my_counter" : {
                "gte" : "9223372036854775808",
                "lte" : "18446744073709551615"
            }
        }
    }
}
```

##### Sort values

&emsp;&emsp;对于使用`unsigned_long`字段排序的query，对于某个文档，如果字段值在long的范围内，则返回`long`类型的排序值，如果字段值超过了这个范围，则返回`BigInteger`。

> NOTE：客户端需要能够正确的处理JSON种的big integer

```text
GET /my_index/_search
{
    "query": {
        "match_all" : {}
    },
    "sort" : {"my_counter" : "desc"}
}
```

##### Stored fields

&emsp;&emsp;如果`unsigned_long`被设置为`stored`，那么会以字符串存储，并且返回的结果也是字符串。

##### Aggregation

&emsp;&emsp;对于term aggregattion，跟sort values一样，会使用`Long`或者`BigInteger`。对于其他聚合，会被转化成`double`类型。

##### Script values

&emsp;&emsp;默认情况下，脚本中`unsigned_long`字段返回了一个signed long，意味着大于`Long.MAX_VALUE`的值会显示为负数。你可以使用`Long.compareUnsigned(long, long)`, `Long.divideUnsigned(long, long)` 以及 `Long.remainderUnsigned(long, long)`来正确的处理这些值。

&emsp;&emsp;比如说，下面的脚本返回一个counter/10的值：

```text
GET /my_index/_search
{
    "query": {
        "match_all" : {}
    },
    "script_fields": {
        "count10" : {
          "script": {
            "source": "Long.divideUnsigned(doc['my_counter'].value, 10)"
          }
        }
    }
}
```

&emsp;&emsp;或者在脚本中你可以使用field API将unsigned long类型视为`BigInteger`。下面的脚本中将`my_counter`视为`BigInteger`，并且默认值为`BigInteger.ZERO`。

```text
"script": {
    "source": "field('my_counter').asBigInteger(BigInteger.ZERO)"
}
```

&emsp;&emsp;如果脚本中需要返回float或者double类型，你可以进一步将`BigInteger`转化为一个double或者float：

```text
GET /my_index/_search
{
    "query": {
        "script_score": {
          "query": {"match_all": {}},
          "script": {
            "source": "field('my_counter').asBigInteger(BigInteger.ZERO).floatValue()"
          }
        }
    }
}
```

##### Queries with mixed numeric types

&emsp;&emsp;支持包含`unsigned_long`在内的混合数值类型的搜索查询，但排序查询除外。因此，在两个索引中进行排序查询时，如果相同的字段名在一个索引中是`unsigned_long`类型，在另一个索引中是`long`类型，这将无法产生正确的结果，应该避免这种情况。如果确实需要这种排序，可以使用基于脚本的排序。

&emsp;&emsp;支持包含`unsigned_long`在内的多种数值类型的聚合操作。在这种情况下，值会被转换为`double`类型。

#### Version field type
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/version.html)

&emsp;&emsp;`version`字段是`keyword`字段的特例类型，用于处理软件版本号以及支持特殊的优先级规则（specialized precedence rule）。优先级遵循[Semantic Versioning](https://semver.org)。例如按照主要（major）、次要（minor）、补丁（patch）的数值排序（比如 "2.1.0" < 2.4.1"" < "2.11.2"）。预发布版本排在正式版之前（比如"1.0.0-alpha" < 1.0.0""）。

&emsp;&emsp;你可以按照下面的内容索引一个`version`字段：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "my_version": {
        "type": "version"
      }
    }
  }
}
```

&emsp;&emsp;该字段提供了跟常规的`keyword`字段相同的搜索能力。比如可以使用`match`或者`term` query来精确匹配并且支持前缀（prefix）和通配符（wildcard）查询。最主要的好处就是`range` query遵循Semver排序，所以"1.0.0"到"1.5.0"的`range`查询会包含"1.2.3"，但是不会包含"1.11.2"。注意的是如果使用keyword存储版本号则排序是不同的，因为排序规则是字典序。

&emsp;&emsp;软件版本号应该遵循[Semantic Versioning rules]的策略和优先级规则，不过有一个显著的例外就是允许主要的版本标识多于或少于三个（例如"1.2"或"1.2.3.4"是有效的（valid）版本号，但没有严格遵循Semver的规则）不满足Semver规则的版本号（例如，“1.2.alpha.4”）仍然会被索引并且可以精确匹配。但是这些版本号会按照字典序排序，并且排在有效版本号之后。空的`version`字段值被认为是无效的，并且排在所有有效版本号之后，其他无效版本号之前。

##### Parameters for version fields

- [meta](#meta(mapping parameter))：字段的元数据信息

### Metadata fields
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mapping-fields.html)

&emsp;&emsp;每一篇文档都有关联的metadata，例如`_index`和`_id` metadata field。这些metadata fields的行为可以在创建mapping时自定义。

##### Identity metadata fields

&emsp;&emsp;[\_index](#_index field)：文档所属索引。

&emsp;&emsp;[\_id](#\_id field)：文档的ID。

##### Document source metadata fields

&emsp;&emsp;[\_source](#_source field)：代表文档内容的原始JSON。

&emsp;&emsp;[\_size](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/mapper-size.html)：`_srouce`字段的大小（单位：字节），由[mapper-size](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/mapper-size.html)插件提供。

##### Doc count metadata field

&emsp;&emsp;[\_doc_count](#_doc_count field)：当文档作为pre-aggregation 数据时的一个自定义的用于存储文档计数的字段。

##### Indexing metadata fields

&emsp;&emsp;[\_field_names](#_field_names field)：文档中所有字段值不为null的字段。

&emsp;&emsp;[\_ignored](#_ignored field)：因为[ignore_malformed](#ignore_malformed)，在索引期间文档中所有被ignore的字段、

##### Routing metadata field

&emsp;&emsp;[\_routing](#_routing field)：自定义的routing value用于将一篇文档路由到指定的分片上。

##### Other metadata field

&emsp;&emsp;[\_meta](#_meta field)：应用指定的metadata。

&emsp;&emsp;[\_tier](#_tier field)：文档所属的索引所在的[data tier](#Data tiers)。

#### \_doc_count field
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mapping-doc-count-field.html)

&emsp;&emsp;Bucket aggregation总是返回一个名为`doc_count`的字段，它表示每一个bucket中参与集合的文档数量。计算`doc_count`的值是很简单的。在每一个bucket中，每收集到一篇文档，`doc_count`的值就加一。

&emsp;&emsp;在独立的文档中进行聚合计算时这种简单的方式是很高效的。但是统计那些存储pre-aggregation的文档数量就会丢失精度（比如说[histogram](#Histogram aggregation)或者`aggregate_metric_double`），因为一个summary field中代表了多篇文档（见[Histogram fields](#Histogram fields（Histogram aggregation）)）。

&emsp;&emsp;为了允许纠正统计pre-aggregation数据时的文档数量。可以使用一种名为`_doc_count`的metadata field类型。`_doc_count`必须总是用一个正整数来表示单个summary field对应的文档数量。

&emsp;&emsp;当文档中添加了`_doc_count`，所有的[bucket aggregation](#Bucket aggregations)必须遵照这个`_doc_count`的值，按照这个值来累加bucket中`doc_count`的值。如果一篇文档中不包含任何`_doc_count`，默认存在一个`_doc_count = 1`的信息。

>IMPORTANT：
>- `_doc_count`字段的字段值在一篇文档中只能是一个正整数，Nested arrays是不允许的如果一篇文档中不包含`_doc_count`，
>- 按照`_doc_count = 1`这种默认行为进行聚合统计
##### Example

&emsp;&emsp;下面的[create index API](#Create index API)使用下面的mapping创建了一个新的索引：

- 字段名`my_histogram`是一个[histogram](#Histogram field type)类型的字段存储了百分比数据（percentile data）
- 字段名`my_text`是一个[keyword](#Keyword type family)类型的字段为这个Histogram存储了标题信息

```text
PUT my_index
{
  "mappings" : {
    "properties" : {
      "my_histogram" : {
        "type" : "histogram"
      },
      "my_text" : {
        "type" : "keyword"
      }
    }
  }
}
```

&emsp;&emsp;下面的[index API](#Index API)请求存储2个histogram的pre-aggregated data：`histogram_1`and `histogram_2`

```text
PUT my_index/_doc/1
{
  "my_text" : "histogram_1",
  "my_histogram" : {
      "values" : [0.1, 0.2, 0.3, 0.4, 0.5],
      "counts" : [3, 7, 23, 12, 6]
   },
  "_doc_count": 45 
}

PUT my_index/_doc/2
{
  "my_text" : "histogram_2",
  "my_histogram" : {
      "values" : [0.1, 0.25, 0.35, 0.4, 0.45, 0.5],
      "counts" : [8, 17, 8, 7, 6, 2]
   },
  "_doc_count": 62 
}
```

&emsp;&emsp;第18行，`_doc_count`必须是一个正整数表示每一个histogram中的文档数量。

&emsp;&emsp;如果我们在`my_index`上运行下面的[terms aggregation](#Terms aggregation)：

```text
GET /_search
{
    "aggs" : {
        "histogram_titles" : {
            "terms" : { "field" : "my_text" }
        }
    }
}
```

&emsp;&emsp;我们会得到下面的response：

```text
{
    ...
    "aggregations" : {
        "histogram_titles" : {
            "doc_count_error_upper_bound": 0,
            "sum_other_doc_count": 0,
            "buckets" : [
                {
                    "key" : "histogram_2",
                    "doc_count" : 62
                },
                {
                    "key" : "histogram_1",
                    "doc_count" : 45
                }
            ]
        }
    }
}
```

#### \_field_names field
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mapping-field-names-field.html)

&emsp;&emsp;` _field_names`字段用来对一篇文档中每一个字段值不是null的字段的字段名进行索引。[exists query](#Term query)使用这个字段来查找具有或不具有特定字段的任何非空值的文档。

&emsp;&emsp;现在` _field_names`只会对有`doc_values`以及关闭`norms`的字段的字段名进行索引。对于有些只开启`doc_values`或者只开启`norms`的字段也可以使用[exists query](#Term query)，但是不是通过` _field_names`字段实现的。

##### Disabling \_field_names

&emsp;&emsp;不允许关闭`_field_names`。现在默认开启`_field_names`是因为它不再有曾经的索引开销。

> NOTE：关闭`_field_names`的功能已经被移除了。在新的索引上使用它会抛出一个错误。在8.0前的索引上关闭`_field_names`仍然是允许的，但是会报告出一个deprecation的警告。

#### \_ignored field
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mapping-ignored-field.html)

&emsp;&emsp;`_ignored` field 用来索引并存储在索引期间被忽略的文档的所有字段的名称。例如字段值是格式错误的并且开启了[ignore_malformed](#ignore_malformed)或者keyword字段值超过了[ignore_above](#ignore_above)中的设置。

&emsp;&emsp;这个字段可以使用 [term](#Term query), [terms](#Terms query), [exists](#Exists query)进行查询。

&emsp;&emsp;下面的例子描述了匹配的文档中包含一个或者多个被忽略的字段。

```text
GET _search
{
  "query": {
    "exists": {
      "field": "_ignored"
    }
  }
}
```

&emsp;&emsp;下面的例子描述了将匹配在索引期间，`@timestamp`字段被忽略的文档。

```text
GET _search
{
  "query": {
    "term": {
      "_ignored": "@timestamp"
    }
  }
}
```

#### \_id field
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mapping-id-field.html)

&emsp;&emsp;每个文档都有一个`_id`用于唯一识别（unique identification）。该值会被索引使得可以通过[GET API](#Get API)或者[ids query](#IDs)用于查找。这个`_id`要么在索引期间被赋值（assigned）、要么通过Elasticsearch生成一个唯一的值。这个字段在mappings中是不可以配置的。

&emsp;&emsp;`_id`字段的值可以在`term`、`terms`、`match`、`query_string`查询中可用（accessible）。

```text
# Example documents
PUT my-index-000001/_doc/1
{
  "text": "Document with ID 1"
}

PUT my-index-000001/_doc/2?refresh=true
{
  "text": "Document with ID 2"
}

GET my-index-000001/_search
{
  "query": {
    "terms": {
      "_id": [ "1", "2" ] 
    }
  }
}

```

&emsp;&emsp;第16行即根据`_id`进行查询，见[ids query](#IDs)。

&emsp;&emsp;`_id`字段在聚合（aggregation）、排序、脚本中是被限制使用的。万一（in case）需要使用`_id`字段来进行排序或者聚合，建议将`_id`字段的值复制（duplicate）到其他字段并且这个字段开启`doc_valies`。

>\_id字段的值的大小被限制为512个字节并且larger values会被reject。

#### \_index field
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mapping-routing-field.html)

&emsp;&emsp;当在多个索引上执行查询时，有时候希望增加一个关联相关索引的子query（query clause）。`_index`字段允许你只在相关的索引上匹配文档。`_index`的值可以用于某些查询、聚合或者排序、Script：

```text
PUT index_1/_doc/1
{
  "text": "Document in index 1"
}

PUT index_2/_doc/2?refresh=true
{
  "text": "Document in index 2"
}

GET index_1,index_2/_search
{
  "query": {
    "terms": {
      "_index": ["index_1", "index_2"] 
    }
  },
  "aggs": {
    "indices": {
      "terms": {
        "field": "_index", 
        "size": 10
      }
    }
  },
  "sort": [
    {
      "_index": { 
        "order": "asc"
      }
    }
  ],
  "script_fields": {
    "index_name": {
      "script": {
        "lang": "painless",
        "source": "doc['_index']" 
      }
    }
  }
}
```

&emsp;&emsp;第15行，根据`_index`的字段值查询
&emsp;&emsp;第21行，在`_index`上执行聚合
&emsp;&emsp;第28行，基于`_index`字段排序
&emsp;&emsp;第37行，在脚本中访问`_index`的值

&emsp;&emsp;`_index` field exposed virtually，它不会作为一个真实的字段添加到Lucene的索引中。这意味着你可以在`term`或 `terms`查询中使用` _index` 字段（或任何被重写为 `term` 查询的查询，如 `match`、`query_string` 或 `simple_query_string` 查询），以及 `prefix` 和 `wildcard` 查询。但是，它不支持`regexp` 和 `fuzzy` 查询。

&emsp;&emsp;对` _index` 字段的查询除了接受具体的索引名称外，还接受索引别名。

> NOTE：在指定远程索引名称（如 `cluster_1:index_3`）时，查询必须包含分隔符 :。例如，对 `cluster_*:index_3` 的 `wildcard` 查询将匹配远程索引中的文档。然而，对 `cluster*index_1` 的查询只会匹配本地索引，因为没有分隔符。这种行为与远程索引名称的通常解析规则一致。

#### \_meta field
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mapping-meta-field.html)

&emsp;&emsp;一种可以使用自定义元数据（meta data）跟mapping进行关联的类型。Elasticsearch完全不会去使用它，可以用来存储指定应用（application-specific）的元数据，比如说这篇文档所属类别：

```text
PUT my-index-000001
{
  "mappings": {
    "_meta": { 
      "class": "MyApp::User",
      "version": {
        "min": "1.0",
        "max": "1.3"
      }
    }
  }
}
```

&emsp;&emsp;第4行，可以通过 [GET mapping API](#Get mapping API)检索`_meta`的信息

&emsp;&emsp;`_meta`字段可以通过[update mapping API](#Update mapping API)对已存在的类型进行变更。

```text
PUT my-index-000001/_mapping
{
  "_meta": {
    "class": "MyApp2::User3",
    "version": {
      "min": "1.3",
      "max": "1.5"
    }
  }
}
```

#### \_routing field
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mapping-routing-field.html)

&emsp;&emsp;使用下面的公式让索引中的一篇文档被路由到指定的分片中：

```text
routing_factor = num_routing_shards / num_primary_shards
shard_num = (hash(_routing) % num_routing_shards) / routing_factor
```

&emsp;&emsp;num_routing_shards即索引设置中的[index.number_of_routing_shards](#index.number_of_routing_shards)。

&emsp;&emsp;num_primary_shards即索引设置中的[index.number_of_shards](#index.number_of_shards)。

&emsp;&emsp;\_routing的默认值是文档的[\_id](#\_id field)。自定义的路由模板（routing pattern）可以通过对某个文档指定一个自定义的`routing`值实现。例如：

```text
PUT my-index-000001/_doc/1?routing=user1&refresh=true 
{
  "title": "This is a document"
}

GET my-index-000001/_doc/1?routing=user1 
```

&emsp;&emsp;第1行中，这篇文档使用`user1`作为路由值替代ID。
&emsp;&emsp;第6行中，当[getting](#Get API)、[deleting](#Delete API)、[updating](#Update API)这篇文档时需要提供路由值。

&emsp;&emsp;`_routing`的值可以在查询中使用（accessible）:

```text
GET my-index-000001/_search
{
  "query": {
    "terms": {
      "_routing": [ "user1" ] 
    }
  }
}
```

&emsp;&emsp;第5行中，在`_routing`字段上进行查询，见[ids query](#IDs)。

>Data streams不支持自定义routing除非创建时，在模板中开启allow_custom_routing。

#####  Searching with custom routing

&emsp;&emsp;自定义的路由能减少（reduce）查询的压力。不需要将搜索请求分散（fan out）到索引的所有分片，而是只发送到匹配特定路由值的分片。

```text
GET my-index-000001/_search?routing=user1,user2 
{
  "query": {
    "match": {
      "title": "document"
    }
  }
}
```

&emsp;&emsp;第1行中，这次查询只会在路由值user1、user2相关的分片中执行。

#####  Making a routing value required

&emsp;&emsp;当使用自定义的路由时，无论是[Indexing](#Index)、[getting](#Get API)、[deleting](#Delete API)、[updating](#Update API)一篇文档时都是需要提供路由值的。

&emsp;&emsp;忘掉路由值能将一篇文档索引到一个或多个分片上。作为一个安全措施（as a safeguard），`_routing`字段可以被配置使得在所有的CRUD操作时都需要`routing`值。

```text
PUT my-index-000002
{
  "mappings": {
    "_routing": {
      "required": true 
    }
  }
}

PUT my-index-000002/_doc/1 
{
  "text": "No routing value provided"
}
```

&emsp;&emsp;第5行中，所有文档的操作都需要提供路由值。
&emsp;&emsp;第10行中，这个请求会抛出`routing_missing_exception`的异常。

#####  Routing to an index partition

&emsp;&emsp;使用自定义的路由值来配置索引时，可以路由到一个分片集合中而不是单个分片上。这可以降低集群不平衡导致关闭的风险，同时能降低查询压力。

&emsp;&emsp;在索引创建期间可以通过索引层的设置[index.routing_partition_size](#index.routing_partition_size)来实现。随着分区（partition）的大小的增长，数据将分布的越均匀（evenly），那么每次查询都需要搜索更多的分片。

&emsp;&emsp;当提供了这个设置后，计算分片的公式将变成：

```text
routing_value = hash(_routing) + hash(_id) % routing_partition_size
shard_num = (routing_value % num_routing_shards) / routing_factor
```

&emsp;&emsp;也就是说（that is）`routing`字段将用来计算出分片集合，然后`_id`用来从分片集合中选出一个分片。

&emsp;&emsp;要开启这个功能，`index.routing_partition_size`的值要大于1并且小于`index.number_of_shards`。

&emsp;&emsp;一旦开启这个功能，被分区（partitioned）的索引将有下列的限制：

- 不能创建Mapping中的[join filed](#Join field type)的关系
- 这个索引的所有mappings信息都需要指定`_routing`的require

#### \_source field
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mapping-source-field.html)

&emsp;&emsp;`_source`字段包含了在索引期间传给Elasticsearch的原始的JSON格式的文档内容。`_source`自身不会被索引（因此这个字段没法用于搜索），又因为它被存储了，所以可以通过`fetch requests`例如[get](#Get API)或者[search](#Search API)方式来获得。

##### Disabling the \_source field

&emsp;&emsp;`_source`字段会带来索引的存储成本，基于这个原因可以通过下面的方式关闭：

```text
PUT my-index-000001
{
  "mappings": {
    "_source": {
      "enabled": false
    }
  }
}
```

>  WARNING：Think before disabling the \_source field
>  &emsp;&emsp;用户经常不考虑关闭`_source`字段的后果然后会后悔这么做。如果`_source`字段不可见那么下面的功能就没法支持：
>
>  - [update](#Update API)、[update_by_query](#Update By Query API)以及[reindex](#Reindex API)这些API
>  - [On the fly highlighting](#Highlighting)
>  - 从一个索引重新索引到另一个的能力，或者是修改mapping、分词器或者升级一个索引到一个新的主要版本
>  - 索引期间通过观察原始文档调试查询或者聚合的能力
>  - 在未来的版本中可能可以自动修复受损（corruption）索引的能力

> TIP：如果需要关心磁盘空间，可以提高 [compression level](#index.codec)而不是关闭`_source`。

##### Including / Excluding fields from \_source

&emsp;&emsp;这是expert-only的功能使得在`_source`的内容在索引之后、存储之前进行字段的剪枝。

> WARNING：从`_source`中移除一些字段跟关闭`_source`有着类似的负面问题（downside），特别是你不能从一个Elasticsearch索引重新索引到另一个。可以考虑使用[source filtering](#Retrieve selected fields from a search) 来替代这种方式。

&emsp;&emsp;参数`includes/excludes`（支持wildcards）可以按照以下方式使用：

```text
PUT logs
{
  "mappings": {
    "_source": {
      "includes": [
        "*.count",
        "meta.*"
      ],
      "excludes": [
        "meta.description",
        "meta.other.*"
      ]
    }
  }
}

PUT logs/_doc/1
{
  "requests": {
    "count": 10,
    "foo": "bar" 
  },
  "meta": {
    "name": "Some metric",
    "description": "Some metric description", 
    "other": {
      "foo": "one", 
      "baz": "two" 
    }
  }
}

GET logs/_search
{
  "query": {
    "match": {
      "meta.other.foo": "one" 
    }
  }
}
```

&emsp;&emsp;第21、25、27、28行，`_source` 字段存储之前会移除这几个字段
&emsp;&emsp;第37行，我们仍然可以通过这个字段进行检索，尽管它没有被存储

#### \_tier field
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mapping-tier-field.html)

&emsp;&emsp;在多个索引上执行查询时，有时候希望在包含指定数据层（data tie）（`data_hot`, `data_warm`, `data_cold` or `data_frozen`）的索引上查询。`_tier`字段用于匹配索引设置`tier_preference`：

```text
PUT index_1/_doc/1
{
  "text": "Document in index 1"
}

PUT index_2/_doc/2?refresh=true
{
  "text": "Document in index 2"
}

GET index_1,index_2/_search
{
  "query": {
    "terms": {
      "_tier": ["data_hot", "data_warm"] 
    }
  }
}
```

&emsp;&emsp;第15行，根据`_tier`的值查询

&emsp;&emsp;通常，查询会使用 terms 查询来列出感兴趣的层级，但你可以在任何被重写为 term 查询的查询中使用`_tier` 字段，比如 `match`、`query_string`、`term`、`terms` 或 `simple_query_string` 查询，以及 `prefix` 和 `wildcard` 查询。然而，它不支持 `regexp` 和 `fuzzy`查询。

&emsp;&emsp;索引的 tier_preference 设置是一个逗号分隔的层级名称列表，按优先顺序排列，即首选托管索引的层级排在最前，其后可能有许多备选选项。查询匹配只考虑第一优先级（列表的第一个值）。

### Mapping parameters
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mapping-params.html)

&emsp;&emsp;这一章节提供了不同的mapping 参数（mapping parameters）的详细的介绍，mapping参数用于[filed mappings](#Field data types)：

&emsp;&emsp;下面的mapping参数对于某些或者所有的字段的数据类型是通用的：

- [analyzer](#analyzer(mapping parameter))
- [coerce](#coerce)
- [copy_to](#copy_to)
- [doc_values](#doc_values)
- [dynamic](#dynamic(mapping parameter))
- [eager_global_ordinals](#eager_global_ordinals)
- [enabled](#enabled(mapping parameter))
- [fielddata](#fielddata mapping parameter-1)
- [fields](#fields)
- [format](#format(mapping parameter))
- [ignore_above](#ignore_above)
- [ignore_malformed](#ignore_malformed)
- [index_options](#index_options)
- [index_phrases](#index_phrases)
- [index_prefixes](#index_prefixes)
- [index](#index(mapping parameter))
- [meta](#meta(mapping parameter))
- [normalizer](#normalizer(mapping parameter))
- [norms](#norms(mapping parameter))
- [null_value](#null_value)
- [position_increment_gap](#position_increment_gap)
- [properties](#properties)
- [search_analyzer](#search_analyzer)
- [similarity](#similarity)
- [store](#store(mapping parameter))
- [term_vector](#term_vector)

#### analyzer(mapping parameter)
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analyzer.html#analyzer)

> IMPORTANT：只有[text](#Text type family)类型支持`analyzer`参数。

&emsp;&emsp;`analyzer`参数用来指定分词器[analyzer](#Anatomy of an analyzer) 并且用于在索引和查询[text](#Text type family)字段时进行文本分词[text analysis](#Text analysis)。

&emsp;&emsp;除非用mapping参数[search_analyzer](#search_analyzer)进行覆盖，分词器将同时用于[index and search analysis](#Index and search analysis)，见[Specify an analyzer](#Specify an analyzer)

> TIP：建议在生产中使用前先试用下分词器，见[Test an analyzer](#Test an analyzer)。

> TIP：`analyzer`参数不能通过[update mapping API](#Update mapping API)对已存在的字段进行变更。

##### search_quote_analyzer

&emsp;&emsp;`search_quote_analyzer`允许你指定一个分词器用于match_phrase和query_string这类phrase query，处理这类query并且不使用停用词时特别有用。

&emsp;&emsp;若要为phrase query不启动停用词可以利用三个分词器：

1. 使用设置`analyzer`用于索引所有的term，包括停用词
2. 使用设置`search_analyzer`用于non-phrase query，它将移除停用词
3. 使用设置`search_quote_analyzer`用于phrase query，它将不会移除停用词

```text
PUT my-index-000001
{
   "settings":{
      "analysis":{
         "analyzer":{
            "my_analyzer":{ 
               "type":"custom",
               "tokenizer":"standard",
               "filter":[
                  "lowercase"
               ]
            },
            "my_stop_analyzer":{ 
               "type":"custom",
               "tokenizer":"standard",
               "filter":[
                  "lowercase",
                  "english_stop"
               ]
            }
         },
         "filter":{
            "english_stop":{
               "type":"stop",
               "stopwords":"_english_"
            }
         }
      }
   },
   "mappings":{
       "properties":{
          "title": {
             "type":"text",
             "analyzer":"my_analyzer", 
             "search_analyzer":"my_stop_analyzer", 
             "search_quote_analyzer":"my_analyzer" 
         }
      }
   }
}

PUT my-index-000001/_doc/1
{
   "title":"The Quick Brown Fox"
}

PUT my-index-000001/_doc/2
{
   "title":"A Quick Brown Fox"
}

GET my-index-000001/_search
{
   "query":{
      "query_string":{
         "query":"\"the quick brown fox\"" 
      }
   }
}
```

> TIP：可以通过[update mapping API](#Update mapping API)对现有的字段更新`search_quote_analyzer`设置。

&emsp;&emsp;第1行，`my_analyzer`对所有的字符串进行分词，包括停用词
&emsp;&emsp;第2行，`my_stop_analyzer`会移除停用词
&emsp;&emsp;第3行，`analyzer`会在索引期间使用`my_analyzer`这个分词器
&emsp;&emsp;第4行，`search_analyzer`会为non-phrase query使用`my_stop_analyzer`这个分词器并且移除停用词
&emsp;&emsp;第5行，`search_quote_analyzer`会使用`my_analyzer`分词器并且确保使用phrase query时停用词不会被移除
&emsp;&emsp;第6行，由于query语句作为一个phrase query使用引号包裹，因此触发`search_quote_analyzer`并且保证停用词不会从这个query中移除。`my_analyzer`分词器会返回下面的token： [the, quick, brown, fox] 用来匹配文档。同时term query会使用`my_stop_analyzer`分词器并且过滤掉停用词。因此查询语句`quick brown fox`或者`A quick brown fox`都会返回两篇文档(the是停用词)，因为这两篇文档都包含下面的token： [quick, brown, fox]。没有`search_quote_analyzer`的话，就无法为phrase query执行精确匹配（exactly match），是因为停用词会被移除，使得两篇文档都会被匹配。


#### coerce
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/coerce.html#coerce)

&emsp;&emsp;Data is not always clean，在JSON中，5这个数字可以用字符串"5"表示，也可以用数值表示，或者应该是整数5，但是在JSON中是一个浮点数，比如5.0，甚至是字符串"5.0"。

&emsp;&emsp;`coerce`会尝试将dirty value转化为某个合适的字段的数据类型。比如：

- 字符串会被强制转化为数值
- 浮点型会被截断为整数

&emsp;&emsp;例如：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "number_one": {
        "type": "integer"
      },
      "number_two": {
        "type": "integer",
        "coerce": false
      }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "number_one": "10" 
}

PUT my-index-000001/_doc/2
{
  "number_two": "10" 
}
```

&emsp;&emsp;第18行，number_one这个字段将包含数值类型的10
&emsp;&emsp;第23行，由于`coerce`被禁用了，所以这篇文档会被reject

> TIP：`coerce`参数可以通过[update mapping API](#Update mapping API)更新。

##### Index-level default

&emsp;&emsp;`index.mapping.coerce`这个设置能够在索引层全局的关闭所有字段的`coerce`的设置

``` text
PUT my-index-000001
{
  "settings": {
    "index.mapping.coerce": false
  },
  "mappings": {
    "properties": {
      "number_one": {
        "type": "integer",
        "coerce": true
      },
      "number_two": {
        "type": "integer"
      }
    }
  }
}

PUT my-index-000001/_doc/1
{ "number_one": "10" } 

PUT my-index-000001/_doc/2
{ "number_two": "10" } 
```

&emsp;&emsp;第20行，`number_one`这个字段中的设置覆盖了索引层的设置，即开启了`coerce`
&emsp;&emsp;第23行，这个文档会被reject，因为`number_two`继承了索引层的设置，即关闭了`coerce`

#### copy_to
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/copy-to.html)

&emsp;&emsp;`copy_to`参数允许你将多个字段的字段值写入到group field中，这个group filed可以被当成一个字段进行查询。

> TIP：如果你经常使用多个字段进行查询，你可以使用copy_to写入到较少的字段来提高查询速度。

&emsp;&emsp;见[Tune for search speed](#Tune for search speed)。

&emsp;&emsp;下图中，`first_name`和`last_name`的字段值能被拷贝到`full_name`中：

``` text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "first_name": {
        "type": "text",
        "copy_to": "full_name" 
      },
      "last_name": {
        "type": "text",
        "copy_to": "full_name" 
      },
      "full_name": {
        "type": "text"
      }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "first_name": "John",
  "last_name": "Smith"
}

GET my-index-000001/_search
{
  "query": {
    "match": {
      "full_name": { 
        "query": "John Smith",
        "operator": "and"
      }
    }
  }
}
```

&emsp;&emsp;第7、11行，`first_name`和`last_name`的字段值被拷贝到`full_name`。
&emsp;&emsp;第30行，`first_name`和`last_name`这两个字段依然能分别用来查询frist_name以及last_name，而`full_name`可以用来同时查询frist_name以及last_name。

&emsp;&emsp;一些重要点：

- 字段值被拷贝，而不是terms被拷贝（字段值在进行分词处理后生成的一个或多个最小分词单元成为term）
- `_source`字段中的值不会显示被拷贝的字段值
- 同一个字段值可以被拷贝到多个字段中：`"copy_to": [ "field_1", "field_2" ]`
- 你不能递归的方式进行字段值拷贝，比如先field_1拷贝到field_2，然后field_2拷贝到field_3，并且期望通过这种方式将field_1的字段值拷贝到field_3中，这是不可行的。你应该使用将一个字段值直接拷贝到多个字段的方式


>NOTE：字段值类型是objects的字段不支持copy_to，例如date_range。

#### doc_values
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/doc-values.html)

&emsp;&emsp;大部分的字段默认情况下都会被[索引](#index)，使得可以被用于搜索。倒排索引使得可以在有序的term集合中找到该term，并且能马上获取到包含 term的文档列表。

&emsp;&emsp;排序、聚合以及脚本中访问字段值需要一种不同的数据访问模式（data access pattern）。我们需要能够根据文档（文档号）找到这个文档中包含的某个字段的所有字段值，而不是根据字段值找到文档。

&emsp;&emsp;DocValues一种 on-disk的数据结构。这种基于文档的索引，使得上述的数据访问模式的成为可能。DocValues使用列式存储column-oriented 存储了跟\_source字段相同的值，使得排序跟聚合更高效。DocValues几乎支持所有的字段类型，除了[text](#Text type family)跟annotated_text这两类字段。

##### Doc-value-only fields

&emsp;&emsp;[Numeric types](#Numeric field types), [date types](#Date field type),  [boolean type](#Boolean field type), [ip type](#IP field type), [geo_point type](#Geopoint field type) 和[keyword type](#Keyword type family)这些字段只开启了docValues（在Lucene中只用正排的数据结构存储）而没有用倒排索引存储，这些字段仍旧能够实现搜索，但是查询性能较差。这种索引方式是一种disk usage和查询性能的折中（tradeoff），适合用于那些不用于查询或者查询性能不重要的字段。这使得只包含docValues的字段非常适合通常不用于过滤的字段，例如metric data上的计量表（gauges）或计数器。

&emsp;&emsp;Doc-value-only字段可以通过下面方式配置：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "status_code": { 
        "type":  "long"
      },
      "session_id": { 
        "type":  "long",
        "index": false
      }
    }
  }
}
```

&emsp;&emsp;第6行，`status_code`是一个普通的`long`类型的字段。
&emsp;&emsp;第10行，`session_id`关闭了`index`，使得该字段是`Doc-value-only`字段，并且只开启DocValues。

##### Disabling doc values

&emsp;&emsp;所有支持DocValues的字段都会默认开启DocValues，如果你确定这个字段不需要进行聚合、排序，或者在脚本中访问这个字段的字段值，那么你可以关闭DocValues来节省磁盘空间：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "status_code": { 
        "type":       "keyword"
      },
      "session_id": { 
        "type":       "keyword",
        "doc_values": false
      }
    }
  }
}
```

&emsp;&emsp;`status_code`默认开启`doc_values`

&emsp;&emsp;`status_code`关闭了`doc_values`，但是仍然能被用于查询。

> NOTE：你不能关闭[wildcard ](#Keyword type family)字段的`doc_values`。

#### dynamic(mapping parameter)
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/dynamic.html)

&emsp;&emsp;当你索引的一篇文档中包含一个新的字段时，Elasticsearch会[动态的添加字段](#Dynamic mapping)，添加到文档中，或者文档中某个对象字段（object field）中，下面的例子在文档中添加了一个String类型的字段`username`，以及对象字段`name`，在这个对象字段中添加了两个字段：

```text
PUT my-index-000001/_doc/1
{
  "username": "johnsmith",
  "name": { 
    "first": "John",
    "last": "Smith"
  }
}

GET my-index-000001/_mapping 
```

&emsp;&emsp;第4行，`name`对象字段下的两个字段是`name.first`和`name.last`
&emsp;&emsp;第10行，查看这个索引的mapping

&emsp;&emsp;下面这篇文档增加两个字符串字段： `email`和`name.middle`: 

```text
PUT my-index-000001/_doc/2
{
  "username": "marywhite",
  "email": "mary@white.com",
  "name": {
    "first": "Mary",
    "middle": "Alice",
    "last": "White"
  }
}

GET my-index-000001/_mapping
```

##### Setting dynamic on inner objects

&emsp;&emsp;[inner object](#Object field type)会继承parent object或者mapping type中的`dynamic`参数，在下面的例子，在mapping顶层中关闭了dynamic，所以顶层中新的字段不会被动态添加。

&emsp;&emsp;然而，`user.social_networks`这个object字段开启了动态mapping，所以这一层的新的字段会被动态添加：

```text
PUT my-index-000001
{
  "mappings": {
    "dynamic": false, 
    "properties": {
      "user": { 
        "properties": {
          "name": {
            "type": "text"
          },
          "social_networks": {
            "dynamic": true, 
            "properties": {}
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;第4行，mapping类型那一层关闭了动态mapping
&emsp;&emsp;第6行，`user`这个object字段继承了mapping类型一层的`dynamic`的参数
&emsp;&emsp;第12行，`social_networks`这个inner object开启了动态mapping

##### Parameters for dynamic

&emsp;&emsp;`dynamic`的不同值控制了是否动态的添加新的字段，可选值如下：

- true：新的字段会被自动添加（默认值）
- runtime：新的字段会被作为[runtime fields](#Runtime fields)进行动态添加，这些字段不会被索引，查询阶段可以从`_source`中加载出来
- false：新的字段会被忽略，这些字段不会被索引也无法用于搜索，但仍然存在与`_source`中，这些字段不会被添加到mapping中，必须要显示添加
- strict：如果发现有新的字段出现，那么异常会被抛出并且这篇文档会被reject，新的字段必须在mapping中显示添加

#### eager_global_ordinals
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/eager-global-ordinals.html)

##### What are global ordinals?

&emsp;&emsp;为了能支持在聚合和其他操作中可以从文档中找到字段值，Elasticsearch使用了一个成为[doc values](#doc_values)的数据结构。例如`keyword`这种Term-based的字段类型（field type）会使用ordinal mapping这种更紧凑的数据结构来存储doc values。ordinal mapping的[工作原理](https://www.amazingkoala.com.cn/Lucene/DocValues/2019/0219/SortedDocValues)是给每一个term分配一个基于字典序，递增有序的整数。会为每篇文档存储这个字段的doc values的ordinal（数字），而不是字段值对应的原始term，通过一个lookup Structure实现ordinal和term之间的转化。

&emsp;&emsp;在聚合中使用ordinal能极大的提高性能。例如在分片级别（shard-level）`terms aggregation`中使用ordinal来收集文档号并放入到分桶中，并且在跨分片对查询结果合并（combining）前将ordinal转化为对应的原始的term值。

&emsp;&emsp;每一个index segment定义了自己的ordinal mapping。但是聚合是需要收集分片上的所有数据。所以为了能在分片级别使用ordinal用于聚合，Elasticsearch创建了一个名为`global ordinals`的统一映射（ordinal mapping）。这个global ordinal mapping是一个segment original，为每一个段构建并维护了global ordinal 到local ordinal的映射关系。

&emsp;&emsp;如果查询中包含了下列的component就会使用Global ordinals：

- 一些使用`keyword`，`ip`， `flattened`字段的bucket aggregation。包含了上文中提到的`terms aggregation`，同样的还有`composite`，`diversified_sampler`以及`significant_terms`。
- 使用`text`字段的bucket aggregation需要这个字段开启[fielddata](#fielddata mapping parameter)。
- Operations on parent and child documents from a `join`  field, including `has_child` queries and `parent` aggregations.

> NOTE: global ordinal mapping会占用heap memory，属于[field data cache](#Field data cache settings)的一部分。
> 对high cardinality fields（不同的字段值数量很多）进行聚合会使用很多的内存并且会触发[field data circuit breaker](#Field data circuit breaker)。

##### Loading global ordinals

&emsp;&emsp;查询期间，在ordinals可以被使用前必须先构建global ordinal mapping。默认情况下，在需要使用global ordinal时并且只在这类查询第一次发起后才会构建这个mapping。如果你想要优化索引速度（Indexing speed），这是一种正确的方式。但如果查询性能的优先级最高，建议尽快生成将要执行聚合的字段的global ordinals：

```text
PUT my-index-000001/_mapping
{
  "properties": {
    "tags": {
      "type": "keyword",
      "eager_global_ordinals": true
    }
  }
}
```

&emsp;&emsp;开启`eager_global_ordinals`后，在分片[refresh](#Refresh API)后就会构建global ordinals。Elasticsearch总是在展示（expose）变更的索引内容前先载入它们，将构建global ordinal的开销从查询期间转移到索引期间。Elasticsearch将总是在创建一个新的副本分片时急切的（eagerly）构建global ordinal，同样的会发生在增加副本数量或者在新的节点重新分配一个分片时。

&emsp;&emsp;可以通过更新`eager_global_ordinals`这个设置来关闭Eager loading：

```text
PUT my-index-000001/_mapping
{
  "properties": {
    "tags": {
      "type": "keyword",
      "eager_global_ordinals": false
    }
  }
}
```

##### Avoiding global ordinal loading

&emsp;&emsp;通常来说，global ordinal不会在载入时间跟内存占用方面有很大的开销。然而在大的分片上或者字段中包含了很多唯一term上载入global ordinal的开销会比较昂贵。因为global ordinal提供了分片上所有段的统一的mapping，当出现一个新段后需要重新完整的构建global ordinal：

&emsp;&emsp;在某些情况下，可以完全避免加载global ordinal：

- `terms`，`sampler`，`significant_terms`支持[execution_hint](#Execution hint)参数来控制分桶的收集。默认是值`global_ordinals`，但可以设置为`map`直接使用term值。
- 如果一个分片被[force-merged](#Force merge API)到单个段中，那么它的segment ordinal就已经相当于global ordinal。在这种情况下，Elasticsearch不需要构建一个global ordinal mapping并且不会有因为使用global ordinals产生的开销。注意的是，处于性能原因，你应该只force-merge那些你不再往里写入的索引。

#### enabled(mapping parameter)
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/enabled.html)

&emsp;&emsp;Elasticsearch会尝试索引你提供的所有的字段，但有时候你只是想保存某些字段而不想索引它们。例如你想要使用Elasticsearch来存储web session，你也许要索引session ID和 last update time，但是你不想对session data进行查询或者聚合操作。

&emsp;&emsp;`enabled`参数只能在mapping的顶层top-level定义并且只能应用[object](#Object field type)类型的字段，这会使得Elasticsearch不再解析object字段下所有内容，JSON内容仍然可以从[\_source](#\_source field)中检索到，无法通过其他方式查询或者进行存储：

```text
UT my-index-000001
{
  "mappings": {
    "properties": {
      "user_id": {
        "type":  "keyword"
      },
      "last_updated": {
        "type": "date"
      },
      "session_data": { 
        "type": "object",
        "enabled": false
      }
    }
  }
}

PUT my-index-000001/_doc/session_1
{
  "user_id": "kimchy",
  "session_data": { 
    "arbitrary_object": {
      "some_array": [ "foo", "bar", { "baz": 2 } ]
    }
  },
  "last_updated": "2015-12-06T18:20:22"
}

PUT my-index-000001/_doc/session_2
{
  "user_id": "jpountz",
  "session_data": "none", 
  "last_updated": "2015-12-06T18:22:13"
}
```

&emsp;&emsp;第11行，`session_data`字段的`enable`参数为false
&emsp;&emsp;第22行，任意类型的数据都可以赋值给`session_data`字段，Elasticsearch不会解析这些数据（不解析意味着可以给一个object字段赋非JSON格式的数据）
&emsp;&emsp;第33行，`session_data`字段同样会忽略不是JSON类型的数据

&emsp;&emsp;可以在mapping中直接定义`enable`参数为false，这样使得所有的内容可以在`_source`字段中检索到但是不会被索引。

```text
PUT my-index-000001
{
  "mappings": {
    "enabled": false 
  }
}

PUT my-index-000001/_doc/session_1
{
  "user_id": "kimchy",
  "session_data": {
    "arbitrary_object": {
      "some_array": [ "foo", "bar", { "baz": 2 } ]
    }
  },
  "last_updated": "2015-12-06T18:20:22"
}

GET my-index-000001/_doc/session_1 

GET my-index-000001/_mapping 
```

&emsp;&emsp;第4行，`enable`参数将应用mapping中的所有字段
&emsp;&emsp;第19行，文档可以被检索到
&emsp;&emsp;第21行，这个索引的mapping中没有任何的字段的信息

&emsp;&emsp;The `enabled` setting for existing fields and the top-level mapping definition cannot be updated。

&emsp;&emsp;由于Elasticsearch完全不会去解析字段值，所以可以把非object格式(JSON格式)的值赋值给`object`字段：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "session_data": {
        "type": "object",
        "enabled": false
      }
    }
  }
}

PUT my-index-000001/_doc/session_1
{
  "session_data": "foo bar" 
}
```

&emsp;&emsp;成功添加了文档，并且`session_data`字段的字段值不是JSON类型。

#### format(mapping parameter)
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mapping-date-format.html#strict-date-time)

&emsp;&emsp;在JSON类型的文档中，日期用字符串表示。Elasticsearch使用预先配置好的一些format集合来识别并且将这些字符串解析为long类型的milliseconds-since-the-epoch UTC。

&emsp;&emsp;除了[build-in formats](#Built In Formats)，你可以指定[custom formats](#Custom date formats)，例如使用熟悉的`yyyy/MM/dd`：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "date": {
        "type":   "date",
        "format": "yyyy-MM-dd"
      }
    }
  }
}
```

&emsp;&emsp;许多API支持date value，也支持[date math expression](#Date Math)，例如`now-1m/d`，即当前时间减去一个月，四舍五入到最近的那天。

##### Custom date formats

&emsp;&emsp;支持自定义的date format。参考[DateTimeFormatter docs](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html)。

##### Built In Formats

&emsp;&emsp;大多数下面提到的格式都有一个严格对应的格式，这意味着年、月和日的部分必须分别严格使用4位、2位和2位数字，可能需要前面补零。例如，像 5/11/1 这样的日期将被视为无效，需要重写为 2005/11/01 才能被日期解析器（Date format）接受。

&emsp;&emsp;要使用它们，您需要在日期格式的名称前加上 strict\_ ，例如使用 strict_date_optional_time 而不是 date_optional_time。

&emsp;&emsp;这些严格的日期格式在动态映射日期（[date fields are dynamically mapped](#Date detection)）字段时特别有用，以确保不会意外地将无关的字符串映射为日期。

&emsp;&emsp;下表列出了所有支持的默认 ISO 格式（国际标准化组织（International Organization for Standardization, ISO）制定的标准格式）：

###### epoch_millis

&emsp;&emsp;一个用于自epoch以来毫秒数的format。请注意，这个时间戳受限于 Java 的 Long.MIN_VALUE 和 Long.MAX_VALUE。

###### epoch_second

&emsp;&emsp;一个用于自epoch以来毫秒数的format。请注意，这个时间戳受限于 Java 的 Long.MIN_VALUE 和 Long.MAX_VALUE。epoch_millis除以1000即epoch_second。

###### date_optional_time or strict_date_optional_time

&emsp;&emsp;一个通用的 ISO 日期时间format，其中日期至少必须包括年份，而时间（由 T 分隔）是可选的。示例：`yyyy-MM-dd'T'HH:mm:ss.SSSZ` 或`yyyy-MM-dd`。`.sss`表示毫秒，`Z`表示时区，它等同于`+0000`。

###### strict_date_optional_time_nanos

&emsp;&emsp;一个通用的 ISO 日期时间format，其中日期至少必须包括年份，而时间（由 T 分隔）是可选的。秒的小数部分具有纳秒级的分辨率。示例：`yyyy-MM-dd'T'HH:mm:ss.SSSSSSZ` 或`yyyy-MM-dd`。

###### basic_date

&emsp;&emsp;一个基本的完整的日期format：4位数的年，2位数的月以及2位数的天。示例：`yyyyMMdd`。

###### basic_date_time

&emsp;&emsp;一个基本的format由日期和时间组成，用`T`分隔：`yyyyMMdd'T'HHmmssZ`。

###### basic_date_time_no_millis

&emsp;&emsp;一个基本的format由日期和时间组成，用`T`分隔，没有毫秒：`yyyyMMdd'T'HHmmssZ`。

###### basic_ordinal_date

&emsp;&emsp;完整的序数日期（ordinal date）的format，使用4位数的年以及3位数的天：`yyyyDDD`。比如2023年的1月1日表示为 2023001，而12月31日表示为 2023365（或在闰年中为 2023366）。。

###### basic_ordinal_date_time

&emsp;&emsp;一个用于完整序数日期和时间的格式化器，使用4位数年份和3位数一年中的天数：`yyyyDDD'T'HHmmss.SSSZ`。比如`2023301'T'153045.678+0100`

- 2023 代表年份。
- 301 表示一年中的第301天，这通常对应于10月28日。
- 'T' 是日期和时间之间的分隔符。
- 153045.678 表示时间为15点30分45秒和678毫秒。
- +0100 表示时区偏移量为UTC+1小时。

###### basic_ordinal_date_time_no_millis

&emsp;&emsp;一个用于完整序数日期和不带毫秒的时间的格式化器，使用4位数年份和3位数一年中的天数：`yyyyDDD'T'HHmmssZ`。

###### basic_time

&emsp;&emsp;一个基本的format，使用2位数的小时，2位数的分钟，2位数的秒，3位数的毫秒以及时区偏移：`HHmmss.SSSZ`。这里的Z表示UTC时区，例如`153045.678Z`表示0时区，而`153045.678+0800`表示东八区，同样的`153045.678Z`跟`153045.678+0000`是一样的。

###### basic_time_no_millis

&emsp;&emsp;一个基本的format，使用2位数的小时，2位数的分钟，2位数的秒以及时区偏移：`HHmmssZ`。

###### basic_t_time

&emsp;&emsp;一个基本的format，使用2位数的小时，2位数的分钟，2位数的秒，3位数的毫秒以及时区偏移，`T`作为前缀：`'T'HHmmss.SSSZ`。

###### basic_t_time_no_millis

&emsp;&emsp;一个基本的format，使用2位数的小时，2位数的分钟，2位数的秒以及时区偏移，`T`作为前缀：`'T'HHmmssZ`。

###### basic_week_date or strict_basic_week_date

&emsp;&emsp;一个基本完整日期，使用4位数的年，2位数的周以及1位数的天：`xxxx'W'wwe`。例如2023年的第一周的周三可以表示为 `2023W013`

- xxxx 表示周年（weekyear），即包含该周的年份，通常是四位数字。
- 'W' 是一个字面字符，用作周数前的标识符。
- ww 表示周年中的周数（01 到 53）。
- e 表示周中的天数（1 到 7），其中 1 代表周一，7 代表周日。

###### basic_week_date_time or strict_basic_week_date_time

&emsp;&emsp;相较于basic_week_date or strict_basic_week_date，额外增加了时间，用`T`分隔：`xxxx'W'wwe'T'HHmmss.SSSZ`。

###### basic_week_date_time_no_millis or strict_basic_week_date_time_no_millis

&emsp;&emsp;相较于basic_week_date_time or strict_basic_week_date_time，少了毫秒：`xxxx'W'wwe'T'HHmmssZ`。

###### date or strict_date

&emsp;&emsp;一个完整的日期，使用4位数的年，2位数的月，2位数的天：`yyyy-MM-dd`。

###### date_hour or strict_date_hour

&emsp;&emsp;相较于date or strict_date，额外增加了时间，用`T`分隔：`yyyy-MM-dd'T'HH`。

###### date_hour_minute or strict_date_hour_minute

&emsp;&emsp;相较于date_hour or strict_date_hour，额外增加了分钟：`yyyy-MM-dd'T'HH:mm`。

###### date_hour_minute_second or strict_date_hour_minute_second

&emsp;&emsp;相较于date_hour_minute or strict_date_hour_minute，额外增加了秒：`yyyy-MM-dd'T'HH:mm:ss`。

###### date_hour_minute_second_fraction or strict_date_hour_minute_second_fraction

&emsp;&emsp;相较于date_hour_minute_second or strict_date_hour_minute_second，额外增加了秒的小数部分（也即就是毫秒），描述000~999范围：`yyyy-MM-dd'T'HH:mm:ss.SSS`。

###### date_hour_minute_second_millis or strict_date_hour_minute_second_millis

&emsp;&emsp;相较于date_hour_minute_second or strict_date_hour_minute_second，额外增加了毫秒，描述000~999范围：`yyyy-MM-dd'T'HH:mm:ss.SSS`。

&emsp;&emsp;date_hour_minute_second_fraction or strict_date_hour_minute_second_fraction跟date_hour_minute_second_millis or strict_date_hour_minute_second_millis是相同的。

###### date_time or strict_date_time

&emsp;&emsp;一种基本的完整的日期跟时间的format，用`T`分隔日期跟时间：`yyyy-MM-dd'T'HH:mm:ss.SSSZ`。

###### date_time_no_millis or strict_date_time_no_millis

&emsp;&emsp;相较于date_time or strict_date_time，没有毫秒：`yyyy-MM-dd'T'HH:mm:ssZ`。

###### hour or strict_hour

&emsp;&emsp;2位数的小时：`HH`。

###### hour_minute or strict_hour_minute

&emsp;&emsp;相较于hour or strict_hour，额外增加了分钟：`HH:mm`。

###### hour_minute_second or strict_hour_minute_second

&emsp;&emsp;相较于hour_minute or strict_hour_minute，额外增加了秒：`HH:mm:ss`。

###### hour_minute_second_fraction or strict_hour_minute_second_fraction

&emsp;&emsp;相较于hour_minute_second or strict_hour_minute_second，额外增加了秒的小数（也就是毫秒）：`HH:mm:ss.SSS`。

###### hour_minute_second_millis or strict_hour_minute_second_millis

&emsp;&emsp;相较于hour_minute_second or strict_hour_minute_second，额外增加了毫秒：`HH:mm:ss.SSS`。

###### ordinal_date or strict_ordinal_date

&emsp;&emsp;一个完整的序数日期，使用4位数的年和3位数的天：`yyyy-DDD`。例如，2023-001 表示 2023 年的第一天，即 1 月 1 日。而 2023-365 表示 2023 年的最后一天，假设它不是闰年。

###### ordinal_date_time or strict_ordinal_date_time

&emsp;&emsp;相比较ordinal_date or strict_ordinal_date，额外增加了时间：`yyyy-DDD'T'HH:mm:ss.SSSZ`。

###### ordinal_date_time_no_millis or strict_ordinal_date_time_no_millis

&emsp;&emsp;相比较ordinal_date_time or strict_ordinal_date_time，没有毫秒：`yyyy-DDD'T'HH:mm:ssZ`。

###### time or strict_time

&emsp;&emsp;一个使用2位数的小时，2位数的分钟，2位数的秒以及3位数的秒的小数（也即是毫秒）和时区偏移：`HH:mm:ss.SSSZ`。

###### time_no_millis or strict_time_no_millis

&emsp;&emsp;相比较 time or strict_time，没有毫秒：`HH:mm:ssZ`。

###### t_time or strict_t_time

&emsp;&emsp;相比较 time or strict_time，额外使用了`T`作为前缀：`'T'HH:mm:ss.SSSZ`。

###### t_time_no_millis or strict_t_time_no_millis

&emsp;&emsp;相比较t_time or strict_t_time，没有毫秒：`'T'HH:mm:ssZ`。

###### week_date or strict_week_date

&emsp;&emsp;一个完整的日期，使用4位数的年，2位数的周以及1位数的天：`xxxx-'W'ww-e`。 例如2023年的第一周的周三可以表示为 2023-W01-3。

- xxxx 表示周年（weekyear），即包含该周的年份，通常是四位数字。
- 'W' 是一个字面字符，表示周数的前缀。
- ww 表示周年中的周数，是两位数字（01 到 53）。
- e 表示周中的天数，是一位数字（1 到 7），其中 1 代表周一，7 代表周日。

###### week_date_time or strict_week_date_time

&emsp;&emsp;相较于week_date or strict_week_date，额外增加了时间，使用`T`分隔：`xxxx-'W'ww-e'T'HH:mm:ss.SSSZ`。

###### week_date_time_no_millis or strict_week_date_time_no_millis

&emsp;&emsp;相较于week_date_time or strict_week_date_time，没有毫秒：`xxxx-'W'ww-e'T'HH:mm:ssZ`。

###### weekyear or strict_weekyear

&emsp;&emsp;一个使用4位数的年的format：`xxxx`。

###### weekyear_week or strict_weekyear_week

&emsp;&emsp;相较于weekyear or strict_weekyear，额外增加了2位数的周：`xxxx-'W'ww`。

###### weekyear_week_day or strict_weekyear_week_day

&emsp;&emsp;相较于weekyear_week or strict_weekyear_week，额外增加了1位数的天：`xxxx-'W'ww-e`。

###### year or strict_year

&emsp;&emsp;一个使用4位数的年的format：`yyyy`。

###### year_month or strict_year_month

&emsp;&emsp;相较于year or strict_year，额外增加了2位数的月：`yyyy-MM`。

###### year_month_day or strict_year_month_day

&emsp;&emsp;相较于year_month or strict_year_month，额外增加了2位数的天：`yyyy-MM-dd`。

#### ignore_above
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ignore-above.html)

&emsp;&emsp;字符串长度超过`ignore_above`中设置的值不会被索引或者存储，对于字符串数组，`ignore_above`会应用（apply）到数组中每一个字符串，并且超过`ignore_above`中设置的值不会被索引或者存储。

> NOTE：All strings/array elements will still be present in the `_source` field, if the latter is enabled which is the default in Elasticsearch.

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "message": {
        "type": "keyword",
        "ignore_above": 20 
      }
    }
  }
}

PUT my-index-000001/_doc/1 
{
  "message": "Syntax error"
}

PUT my-index-000001/_doc/2 
{
  "message": "Syntax error with some long stacktrace"
}

GET my-index-000001/_search 
{
  "aggs": {
    "messages": {
      "terms": {
        "field": "message"
      }
    }
  }
}
```

&emsp;&emsp;第7行，这个字段会忽略字符串长度超过20的字段值
&emsp;&emsp;第13行，这篇文档能成功索引
&emsp;&emsp;第18行，这篇文档会被索引，但是`message`字段不会被索引
&emsp;&emsp;第23行，两篇文档都会被搜索到，但聚合中只有第一篇文档的message的值会被呈现present

> TIP：可以通过[update mapping API](#Update mapping API)更新`ignore_above`的值。

&emsp;&emsp;这个选项也可以用来处理Lucene中单个term的最大长度只支持到32766的问题。

> NOTE：`ignore_above`计算的字符的数量，但Lucene是按照字节计算的，如果你使用UTF-8那么长度应该限制到 32766/4 =8191，因为UTF-8最多用4个字节来描述一个字符。

#### ignore_malformed
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ignore-malformed.html)

&emsp;&emsp;有时候你不用过度控制待写入的数据，用户可能发送了[date](#Data)类型的`login`字段，但是又发送了一个email地址的`login`字段。

&emsp;&emsp;尝试索引一个数据类型错误的字段默认情况下会抛出异常，并且reject这个文档，`ignore_malformed`参数如果设置为true，允许忽略异常，数据格式异常的字段不会被索引，其他的字段会被正常处理。

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "number_one": {
        "type": "integer",
        "ignore_malformed": true
      },
      "number_two": {
        "type": "integer"
      }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "text":       "Some text value",
  "number_one": "foo" 
}

PUT my-index-000001/_doc/2
{
  "text":       "Some text value",
  "number_two": "foo" 
}
```

&emsp;&emsp;第19行，这篇文档的`text`字段会被索引，`number_one`字段不会被索引
&emsp;&emsp;第25行，这篇文档会被reject因为`number_two`字段不允许数据格式错误的字段值

&emsp;&emsp;下面的mapping类型允许设置`ignore_malformed`参数：

| [Numeric](#Numeric field types) | `long`, `integer`, `short`, `byte`, `double`, `float`, `half_float`, `scaled_float` |
| :----------------------------------------------------------: | ------------------------------------------------------------ |
| [Date](#Date field type) | `date`                                                       |
| [Date nanoseconds](#Date nanoseconds field type) | `date_nanos`                                                 |
| [Geopoint](#Geopoint field type) | `geo_point` for lat/lon points                               |
| [Geoshape](#Geoshape field type) | `geo_shape` for complex shapes like polygons                 |
| [IP](#IP field type) | `ip` for IPv4 and IPv6 addresses                             |

>  TIP：可以通过[update mapping API](#Update mapping API)更新`ignore_malformed`的值。

##### Index-level default

&emsp;&emsp;在索引层index level设置`index.mapping.ignore_malformed`会对mapping中所有字段生效，如果mapping中的某些类型不支持这个参数会忽略该设置。

```text
PUT my-index-000001
{
  "settings": {
    "index.mapping.ignore_malformed": true 
  },
  "mappings": {
    "properties": {
      "number_one": { 
        "type": "byte"
      },
      "number_two": {
        "type": "integer",
        "ignore_malformed": false 
      }
    }
  }
}
```

&emsp;&emsp;第8行，`number_one`字段会继承索引层的设置
&emsp;&emsp;第13行，`number_two`字段的`ignore_malformed`会覆盖索引层的设置

##### Dealing with malformed fields

&emsp;&emsp;`ignore_malformed`开启后，数据格式错误的字段在索引期间会被忽略，无论如何建议保留那些包含数据格式错误字段的文档的数量，否则查询[\_ignored](#\_ignored field)字段就没有意义了。Elasticsearch可以使用`_ignored`字段结合`exists`,`term` or `terms`来查询哪些文档中有数据格式错误的字段。

##### Limits for JSON Objects

&emsp;&emsp;下面的数据类型不能使用`ignore_malformed`：

- [Nested data type](#Nested field type)
- [Object data type](#Object field type)
- [Range data types](#Range field types)

&emsp;&emsp;`ignore_malformed`参数不能用于JSON objects。JSON object是使用`{}`包住的类型，它包含的数据映射了nested, object, and range数据类型。

&emsp;&emsp;如果你提交了数据格式错误的JSON object，Elasticsearch会无视`ignore_malformed`参数的设置并返回错误并且reject这个文档。

#### index(mapping parameter)
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mapping-index.html)

&emsp;&emsp;`index`这个参数用来控制字段值是否要索引，该值可以是true或者false，默认值为true，没有进行索引的字段通常是无法用于查询的。

#### index_options
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-options.html)

&emsp;&emsp;`index_options`参数用来控制哪些信息被添加到倒排索引[inverted index](https://www.amazingkoala.com.cn/Lucene/Index/2019/0222/倒排表（上）)中，用于查询或者高亮。只有term-based的字段比如[text](#Text type family)、[keyword](#Keyword type family)才支持该配置。

&emsp;&emsp;`index_options`参数的可选值如下所示，注意的是每个参数值包含上一个参数值中添加的参数，比如`freq`包含`docs`；`positions`同时包含`freq`和`docs`：

- docs：只有文档号被添加到倒排索引中，可以解决这个问题：term是否在这个字段中？
- freqs：文档号跟词频会被添加到倒排索引中，词频会用于打分，词频大意味着文档能获得较高的分数
- positions（默认值）：文档号跟词频、位置会被添加到倒排索引中，位置信息可以用于[proximity or phrase queries](#Match phrase query)
- offsets：文档号跟词频、位置。term的开始跟结束的偏移信息（可以用于定位term在字符串中的位置）会被添加到倒排索引中，offsets可以用于提高[highlighting](#Highlighting)的性能。

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "text": {
        "type": "text",
        "index_options": "offsets"
      }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "text": "Quick brown fox"
}

GET my-index-000001/_search
{
  "query": {
    "match": {
      "text": "brown fox"
    }
  },
  "highlight": {
    "fields": {
      "text": {} 
    }
  }
}
```

&emsp;&emsp;第27行，因为第7行设置了`offsets`参数，`text`字段将默认使用倒排信息（posting）来实现高亮。

#### index_phrases
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-phrases.html)

&emsp;&emsp;开启这个参数后，2个term组成的字段值会被索引到额外的字段（separate filed）中。这使得exact phrase（no slop） 查询可以更高效，代价就是索引大小会增加。注意的是只有移除了停用词后才能达到最佳性能，因为包含了停用词的短语将不会使用subsidiary field并且会回滚到标准的短语查询。默认是false。

#### index_prefixes
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-prefixes.html)

&emsp;&emsp;`index_prefixes`参数对term的前缀进行索引来加速前缀查询。接收下面的可选设置：

- `min_chars`

&emsp;&emsp;用于索引的前缀长度最小值。该值必须大于0，默认值是2（inclusive）

- `max_chars`

&emsp;&emsp;用于索引的前缀长度最大值。该值必须小于20，默认值是5（inclusive）

&emsp;&emsp;下面的例子使用默认的前缀长度设置创建了一个text字段：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "body_text": {
        "type": "text",
        "index_prefixes": { }    
      }
    }
  }
}
```

&emsp;&emsp;第7行，空的object即使用默认的`min_chars`和`max_chars`设置。

&emsp;&emsp;这个例子使用了自定义的前缀长度设置：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "full_name": {
        "type": "text",
        "index_prefixes": {
          "min_chars" : 1,
          "max_chars" : 10
        }
      }
    }
  }
}
```

#### meta(mapping parameter)
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mapping-field-meta.html)

&emsp;&emsp;字段的附加元信息（metadata）（key-value键值对信息），元数据对Elasticsearch是不透明的，他只是用来在多个应用间分享在同一个索引中某个字段的元信息，比如字段值的单位unit：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "latency": {
        "type": "long",
        "meta": {
          "unit": "ms"
        }
      }
    }
  }
}
```

>NOTE：字段的元数据最多可以设置5个，key的长度必须小于等于20，value的长度必须小于等于50

>NOTE：字段的元数据可以通过mapping更新进行变更，新的元信息会覆盖旧的元信息

&emsp;&emsp;Elastic产品会使用下面2个标准的字段的元数据，你可以遵循这些元数据约定来获得更好的数据开箱即用体验

##### unit

&emsp;&emsp;数值类型的单位，例如`percent`, `byte`或者 [time unit](#API conventions)，默认情况字段是没有单位的，除了数值类型的字段，百分比单位约定 `1`即`100%`。

##### metric_type

&emsp;&emsp;metric_type同样用于数值类型的字段，可选值是`gauge`跟`counter`，gauge是一种单值测量，可以随着时间的推移而上升或下降，例如温度。counter是一个单值累积计数器，它只上升，比如web服务器处理的请求数。默认情况下，字段不会使用metric_type参数。仅对数值类型的字段有效。

#### fields
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/multi-fields.html)

&emsp;&emsp;经常出于某些目的需要对同一个字段执行不同的索引方式，这就是`multi-fields`的设计初衷。例如，一个字符串的字段值可以作为`text`字段使得可以全文检索，也可以作为`keyword`字段使得可以用于排序、聚合：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "city": {
        "type": "text",
        "fields": {
          "raw": { 
            "type":  "keyword"
          }
        }
      }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "city": "New York"
}

PUT my-index-000001/_doc/2
{
  "city": "York"
}

GET my-index-000001/_search
{
  "query": {
    "match": {
      "city": "york" 
    }
  },
  "sort": {
    "city.raw": "asc" 
  },
  "aggs": {
    "Cities": {
      "terms": {
        "field": "city.raw" 
      }
    }
  }
}
```

&emsp;&emsp;第8行，`city.raw`字段是`city`字段的`keyword`版本
&emsp;&emsp;第31行，`city`字段可以用于全文检索
&emsp;&emsp;第35、40行，`city.raw`可以用于排序跟聚合

&emsp;&emsp;你可以通过[update mapping API](#Update mapping API)对现有的字段添加`multi-filed`

&emsp;&emsp;`multi-field`字段完全独立与parent filed（上面的例子中，`city.raw`的parent filed就是`city`），意味着它不会继承parent field的任何参数。当然了，Multi-fields不会更改`_source`中的内容。

##### Multi-fields with multiple analyzers

&emsp;&emsp;另一个multi-fields的用例就是对同一个字段的字段值采用不同的分词器来得到更好的关联（better relevance）。比如我们可以对字段值使用[standard analyzer](#Standard analyzer)，将这个字段值分为多个词（word），或者对这个字段值使用[english analyzer](#Language analyzers) ，将英文单词转为root format，并且写入到索引：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "text": { 
        "type": "text",
        "fields": {
          "english": { 
            "type":     "text",
            "analyzer": "english"
          }
        }
      }
    }
  }
}

PUT my-index-000001/_doc/1
{ "text": "quick brown fox" } 

PUT my-index-000001/_doc/2
{ "text": "quick brown foxes" } 

GET my-index-000001/_search
{
  "query": {
    "multi_match": {
      "query": "quick brown foxes",
      "fields": [ 
        "text",
        "text.english"
      ],
      "type": "most_fields" 
    }
  }
}
```

&emsp;&emsp;第5行，`text`字段使用`standard`（默认）分词器
&emsp;&emsp;第8行，`text.english`字段使用`english`分词器
&emsp;&emsp;第19、22行，索引了两篇文档，一篇文档中包含了`fox`而另一篇则包含了`foxes`
&emsp;&emsp;第33行，同时查询`text`跟`text.english`字段，这两个字段都会影响文档的打分

&emsp;&emsp;`text`字段在第一篇文档中包含了`fox`并且在第二篇文档中包含了`foxes`。`text.english`字段在两篇文档中都包含了`fox`，因为`foxes`被词根化stemmed为成了`fox`。

&emsp;&emsp;查询中的字符串同样会对于`text`字段使用`standard`分词器，并且对于`text.english`字段使用`english`分词器。词根化stemmed的字段允许如果是查询`foxes`，会去匹配包含`fox`的文档，使得我们可以尽可能的匹配更多的文档。当查询没有词根化unstemmed的`text`字段时，我们会提高那些同时匹配`foxes`的文档的分数。

#### normalizer(mapping parameter)
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/normalizer.html)

&emsp;&emsp;[keyword](#Keyword type family)的属性`normalizer`类似与[analyzer](#analyzer(mapping parameter))，只是它能保证分析链（analysis chain）只会生成单个token。

&emsp;&emsp;`normalizer`先应用到keyword然后再进行索引，同样的在查询期间，查询`kewword`字段时会执行一个query parser，例如[match](#Match query) query或者term-level query例如[term](#Term query) query。

&emsp;&emsp;可以使用一个简单的elasticsearch自带的（ship with）名为`lowercase`的normalizer，自定义的normalizer可以在analysis中定义：

```text
PUT index
{
  "settings": {
    "analysis": {
      "normalizer": {
        "my_normalizer": {
          "type": "custom",
          "char_filter": [],
          "filter": ["lowercase", "asciifolding"]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "foo": {
        "type": "keyword",
        "normalizer": "my_normalizer"
      }
    }
  }
}

PUT index/_doc/1
{
  "foo": "BÀR"
}

PUT index/_doc/2
{
  "foo": "bar"
}

PUT index/_doc/3
{
  "foo": "baz"
}

POST index/_refresh

GET index/_search
{
  "query": {
    "term": {
      "foo": "BAR"
    }
  }
}

GET index/_search
{
  "query": {
    "match": {
      "foo": "BAR"
    }
  }
}
```

&emsp;&emsp;上面的查询会匹配到1和2这两篇文档，因为`BAR`同时在索引和查询期间被转化为`bar`。

```text
{
  "took": $body.took,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped" : 0,
    "failed": 0
  },
  "hits": {
    "total" : {
        "value": 2,
        "relation": "eq"
    },
    "max_score": 0.4700036,
    "hits": [
      {
        "_index": "index",
        "_id": "1",
        "_score": 0.4700036,
        "_source": {
          "foo": "BÀR"
        }
      },
      {
        "_index": "index",
        "_id": "2",
        "_score": 0.4700036,
        "_source": {
          "foo": "bar"
        }
      }
    ]
  }
}
```

&emsp;&emsp;同样的，由于keyword的值在索引前就先被转化了，意味着聚合操作返回的是normalized values：

```text
GET index/_search
{
  "size": 0,
  "aggs": {
    "foo_terms": {
      "terms": {
        "field": "foo"
      }
    }
  }
}
```

&emsp;&emsp;返回：

```text
{
  "took": 43,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped" : 0,
    "failed": 0
  },
  "hits": {
    "total" : {
        "value": 3,
        "relation": "eq"
    },
    "max_score": null,
    "hits": []
  },
  "aggregations": {
    "foo_terms": {
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": [
        {
          "key": "bar",
          "doc_count": 2
        },
        {
          "key": "baz",
          "doc_count": 1
        }
      ]
    }
  }
}
```

#### norms(mapping parameter)
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/norms.html)

&emsp;&emsp;`norms`存储各种标准化因子（normalization），随后在查询期间用于文档的打分 。

&emsp;&emsp;`norms`对于打分是很有用的，但是它同时需要很多的磁盘空间（通常在每个文档每个字段上约占一个字节，即使是索引中没有这个特定字段的文档也是如此）。因此，如果你不需要在某个特定字段上进行得分计算，你应该禁用该字段上的规范。特别是对于那些仅用于过滤或聚合的字段。

> TIP：可以通过[update mapping API](#Update mapping API)对关闭现有的字段的`norm`。

&emsp;&emsp;Norms可以通过[update mapping API](#Update mapping API)按照下面的方式关闭：

```text
PUT my-index-000001/_mapping
{
  "properties": {
    "title": {
      "type": "text",
      "norms": false
    }
  }
}
```

> NOTE：Norms不会马上被移除，而是在你继续索引新的文档后，并且在旧的段合并到新段后才会移除。被移除了norm的字段将返回不一致的结果，因为有些文档不在有norm，而其他的文档仍然有norm。

#### null_value
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/null-value.html)

&emsp;&emsp;`null`不能被索引或者搜索，当一个字段被设置为`null`（空的数组或者数组元素都是`null`），该字段被认为是没有值的。

&emsp;&emsp;`null_value`允许将那些为null的字段值替换为你指定的值，使得可以被索引和搜索，例如：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "status_code": {
        "type":       "keyword",
        "null_value": "NULL" 
      }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "status_code": null
}

PUT my-index-000001/_doc/2
{
  "status_code": [] 
}

GET my-index-000001/_search
{
  "query": {
    "term": {
      "status_code": "NULL" 
    }
  }
}
```

&emsp;&emsp;第7行，使用字符串`NULL`替换那些为null的字段值。
&emsp;&emsp;第20行，空的数组不包含显示的null值，该字段值不会被替换为`NULL`
&emsp;&emsp;第27行，字段名status_code、字段值NULL的查询可以匹配文档1，不会匹配文档2

>IMPORTANT：`null_value`的字段值类型必须跟这个字段的类型一致，long类型的字段不能设置为字符串类型的`null_value`

>NOTE：`null_value`只会影响数据的索引和查询，它不会修改_source中文档的数据

#### position_increment_gap
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/position-increment-gap.html)

&emsp;&emsp;为了能支持[proximity or phrase queries](#Match phrase query)，text类型的字段被分词（[Analyzed](#index(mapping parameter))）后会统计term的[positions](#index_options)信息。当索引多值（[Arrays](#Arrays)）的text字段时，Elasticsearch会在这些值之间添加一个假的距离，来阻止phrase query能跨相邻两个值进行查询。这个假的距离使用`position_increment_gap`配置并且默认值是100。

&emsp;&emsp;例如
```text
PUT my-index-000001/_doc/1
{
  "names": [ "John Abraham", "Lincoln Smith"]
}

GET my-index-000001/_search
{
  "query": {
    "match_phrase": {
      "names": {
        "query": "Abraham Lincoln" 
      }
    }
  }
}

GET my-index-000001/_search
{
  "query": {
    "match_phrase": {
      "names": {
        "query": "Abraham Lincoln",
        "slop": 101 
      }
    }
  }
}
```

&emsp;&emsp;第11行，这个phrase query 不会匹配到文档，与我们的期望不符。
&emsp;&emsp;第23行，这个phrase query会匹配到文档，因为`slop`大于`position_increment_gap`，尽管`Abraham`和`Lincoln`是两个数组元素的值。

&emsp;&emsp;`position_increment_gap`可以通过下面的方式在mapping中指定：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "names": {
        "type": "text",
        "position_increment_gap": 0 
      }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "names": [ "John Abraham", "Lincoln Smith"]
}

GET my-index-000001/_search
{
  "query": {
    "match_phrase": {
      "names": "Abraham Lincoln" 
    }
  }
}
```

&emsp;&emsp;第7行，若`position_increment_gap`设置为0，那么数组中相邻的两个term的距离为0

- 例如[ "John Abraham", "Lincoln Smith"]，Abraham跟Lincoln之间的距离为0。

&emsp;&emsp;第22行，这个phrase query会匹配到文档看起来是有点奇怪的，但原因是我们在mapping要求这么做的。

##### 小知识

&emsp;&emsp;在es中定义了"names": [ "John Abraham", "Lincoln Smith"]，如果position_increment_gap的值为0，那么相当于定义了"names": "John Abraham Lincoln Smith"。在Lucene层，es的arrays类型会将每一个数组元素作为Lucene中同一篇文档的相同字段名的多个字段信息。以name为例，在Lucene中，会生成两个字段名为name，字段值分别为John Abraham和Lincoln Smith的字段。由于这两个字段有相同的字段名，那么在[倒排索引](https://amazingkoala.com.cn/Lucene/Index/2019/0222/倒排表（上）/)中，Abraham和Lincoln的位置是相邻的，当然前提是position_increment_gap的值为0。

#### properties
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/properties.html)

&emsp;&emsp;类型映射，[object fields](#Object field type)和包含子字段（sub-field）[nested fiedlds](#Nested field type)都称为`properties`。这些properties可能是任意的数据类型（[date type](#Field data types)），包括`object`和`nested`。可以通过下面的方式添加`properties`：

- 在[creating an index](#Create index API)时显示的定义
- 通过[update mapping API](#Update mapping API)显示的增加或更新mapping时显示的定义
- 对包含新的字段的文档[动态](#Dynamic mapping)的添加

&emsp;&emsp;下面是一个添加`properties`的例子，添加了一个`object`和`nested`字段：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": { 
      "manager": {
        "properties": { 
          "age":  { "type": "integer" },
          "name": { "type": "text"  }
        }
      },
      "employees": {
        "type": "nested",
        "properties": { 
          "age":  { "type": "integer" },
          "name": { "type": "text"  }
        }
      }
    }
  }
}

PUT my-index-000001/_doc/1 
{
  "region": "US",
  "manager": {
    "name": "Alice White",
    "age": 30
  },
  "employees": [
    {
      "name": "John Smith",
      "age": 34
    },
    {
      "name": "Peter Brown",
      "age": 26
    }
  ]
}
```

&emsp;&emsp;第4行，在顶层mapping中定义
&emsp;&emsp;第6行，在`manager`下定义
&emsp;&emsp;第13行，在`employees`下定义
&emsp;&emsp;第22行，索引一篇对应上面定义的mapping的文档

> TIP：`properties`这个配置运行在同一个索引中对同一个字段名有不同的设置。可以通过[update mapping API](#Update mapping API)添加或者更新现有字段的properties。

##### Dot notation

&emsp;&emsp;Inner field可以通过dot notation获取并在query、aggregation中使用：

```text
GET my-index-000001/_search
{
  "query": {
    "match": {
      "manager.name": "Alice White"
    }
  },
  "aggs": {
    "Employees": {
      "nested": {
        "path": "employees"
      },
      "aggs": {
        "Employee Ages": {
          "histogram": {
            "field": "employees.age",
            "interval": 5
          }
        }
      }
    }
  }
}
```

> IMPORTANT：必须要指定Inner field的完整路径

#### search_analyzer
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-analyzer.html)

&emsp;&emsp;通常来说，在索引和查询期间应该应用（apply）相同的[analyzer](#analyzer(mapping parameter))来保证query中的term跟倒排索引中的term有相同的格式（in the same format）。

&emsp;&emsp;但有的时候在查询期间使用不同的分词器是有意义的，比如使用[edge_ngram tokenizer](#Edge n-gram tokenizer) 用于autocomplete或者查询期间的同义词查询。

&emsp;&emsp;默认情况下，查询会使用mapping中定义的`analyzer`，但是可以用`search_analyzer`设置覆盖：

```text
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "filter": {
        "autocomplete_filter": {
          "type": "edge_ngram",
          "min_gram": 1,
          "max_gram": 20
        }
      },
      "analyzer": {
        "autocomplete": { 
          "type": "custom",
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "autocomplete_filter"
          ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "text": {
        "type": "text",
        "analyzer": "autocomplete", 
        "search_analyzer": "standard" 
      }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "text": "Quick Brown Fox" 
}

GET my-index-000001/_search
{
  "query": {
    "match": {
      "text": {
        "query": "Quick Br", 
        "operator": "and"
      }
    }
  }
}
```

&emsp;&emsp;第13行，Analysis设置中自定义的分词器`autocomplete`
&emsp;&emsp;第28，29行，`text`字段使用在索引期间使用`autocomplete`分词器，而在查询期间使用`standard`分词器
&emsp;&emsp;第37行，这个字段对应的这些term会被索引：[q, qu, qui, quic, quick, b, br, bro, brow, brown, f, fo, fox]
&emsp;&emsp;第45行，query中查询的term为：[quick, br]

&emsp;&emsp;见[Index time search-as-you- type ](https://www.elastic.co/guide/en/elasticsearch/guide/2.x/_index_time_search_as_you_type.html)查看这个例子的详细介绍。

> TIP：可以使用[update mapping API](#Update mapping API)对现有的字段上更新`search_analyzer`设置。注意的是，Note, that in order to do so, any existing "analyzer" setting and "type" need to be repeated in the updated field definition。

#### similarity
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/similarity.html)

&emsp;&emsp;Elasticsearch允许你为每一个字段配置一个文本打分算法（text scoring algorithm）或者`similarity`。参数`similarity`允许你提供一个简单文本打分算法而不是使用默认的`BM25`，例如`boolean`。

|  BM25   | 即[Okapi BM25 algorithm](https://en.wikipedia.org/wiki/Okapi_BM25)，Elasticsearch以及Lucene默认使用这个算法 |
| :-----: | :----------------------------------------------------------: |
| boolean | 一个简单的布尔算法，不需要全文本ranking并且打分值基于是否匹配到了查询中的term。Boolean算法给term打出的分数跟它们的query boost一样 |

&emsp;&emsp;参数`similarity`可以在字段最开始创建时设置在这个字段的层级：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "default_field": { 
        "type": "text"
      },
      "boolean_sim_field": {
        "type": "text",
        "similarity": "boolean" 
      }
    }
  }
}
```

&emsp;&emsp;第5行，这个字段名为`default_field`的字段使用`BM25`算法
&emsp;&emsp;第10行，这个字段名为`boolean_sim_field`的字段使用`boolean`算法


#### store(mapping parameter)
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mapping-store.html)

&emsp;&emsp;默认情况下，字段值被[indexed](#index)使得可以被搜索到，但是它们没有存储（store）。意味着该字段能用于查询，但是原始的字段值（origin source value）不能被检索到。

&emsp;&emsp;通常来说没有关系，因为字段值早已经是[\_source](#\_source field)字段的一部分，默认被存储了。如果你想检索单个字段或者一些字段的字段值而不是整个`_source`， 你可以使用[source filtering](#Retrieve selected fields from a search)来达到你的目的。

&emsp;&emsp;在有些情况下`store`一个字段的字段值是合理的（make sense）。比如说你有一篇文档有`title`、`date`以及一个字段值很大的`content`字段。你可能只想获取`title`、`date`的字段值而不用从字段值很大（因为\_source中包含所有字段的字段值）的`_source`字段中提取出来：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "title": {
        "type": "text",
        "store": true 
      },
      "date": {
        "type": "date",
        "store": true 
      },
      "content": {
        "type": "text"
      }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "title":   "Some short title",
  "date":    "2015-01-01",
  "content": "A very long content field..."
}

GET my-index-000001/_search
{
  "stored_fields": [ "title", "date" ] 
}
```

&emsp;&emsp;第7、11行，`title`跟`date`字段的字段值被存储了
&emsp;&emsp;第29行，请求将检索`title`跟`date`的字段值（不是从`_source`中提取）

> NOTE：
>
> Stored fields returned as arrays
>
> 为了一致性，stored fields总是以数组方式返回，因为没法知道原始的字段值是单值，多值还是一个空的数组。如果你想获取这个字段原始的字段值（输入文档中的值），你应该从`_source`中获取。

#### term_vector
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/term-vector.html)

&emsp;&emsp;Term vectors包含了经过[analysis](#Text analysis)处理后生成的terms的信息，包括：

- terms的列表
- 每个term的位置（先后顺序）
- term在原始的字符串中的开始跟结束的偏移位置
- 负载（payload，不是所有的term都会有payload）每个不同位置（在原始字符串中的位置）的term关联了一个二进制数据的payload

&emsp;&emsp;这些term vector会被存储所以可以针对某一篇文档进行检索。

&emsp;&emsp;`term_vector`可以接受下面的参数：

|               no                |         不会存储term vectors（默认）         |
| :-----------------------------: | :------------------------------------------: |
|               yes               | 只有term（位置、偏移、负载不会存储）会被存储 |
|         with_positions          |                存储term跟位置                |
|          with_offsets           |                存储term跟偏移                |
|     with_positions_offsets      |            存储term、位置以及偏移            |
|     with_positions_payloads     |            存储term、位置以及负载            |
| with_positions_offsets_payloads |         存储term、位置、偏移以及负载         |

&emsp;&emsp;fast vector highlighter需要参数`with_positions_offsets`。[term vectors API](#Term vectors API)用来检索存储的信息。

> WARNING：参数`with_positions_offsets`设置后会使得这个字段的索引大小翻倍。

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "text": {
        "type":        "text",
        "term_vector": "with_positions_offsets"
      }
    }
  }
}

PUT my-index-000001/_doc/1
{
  "text": "Quick brown fox"
}

GET my-index-000001/_search
{
  "query": {
    "match": {
      "text": "brown fox"
    }
  },
  "highlight": {
    "fields": {
      "text": {} 
    }
  }
}
```

&emsp;&emsp;term vectors开启后，`text`字段会默认使用fast vector highlighter。

### Mapping limit settings

（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mapping-settings-limit.html)
&emsp;&emsp;使用下面的设置限制mapping（手动或者动态创建）中字段的数量，防止因为文档（比如每一篇文档都会增加一些字段）导致mapping explosion。

#### index.mapping.total_fields.limit

&emsp;&emsp;索引中字段的数量最大值。Field and object mappings，以及field aliases都会纳入到统计中并受到限制。默认值为`1000`。

> IMPORTANT：这个限制是为了防止mappings和查询变的很大（too large）。数量越高会导致性能下降以及内存问题，特别是集群处于高负载或者低资源时。
> 如果你提高这个设置，我们建议你同时提高[indices.query.bool.max_clause_count](#Search settings)，这个设置限制了一个Query中clause数量的最大值。

> TIP：如果你mapping中包含 a large，arbitrary set of keys，考虑使用[flattened](#Flattened field type) 数据类型。

#### index.mapping.depth.limit

&emsp;&emsp;某个字段的最大深度，用来估量inner object的数量。例如，如果所有的字段都在root object level，那么深度为`1`。如果某个字段有一个object mapping，那么深度就是`2`。默认值为`20`。

#### index.mapping.nested_fields.limit

&emsp;&emsp;索引中[nested](#Nested field type)类型的字段的数量最大值。`nested`类型应该只用于特殊的用例中，数组中的object需要各自独立的进行查询。为了保护mapping的不当设计，这个设置会限制每一个索引中`nested`类型的字段的数量。默认值是`50`。

#### index.mapping.nested_objects.limit

&emsp;&emsp;一篇文档中所有`nested`类型的字段中JSON object的数量总和最大值。当一篇文档中包含太多的nested object时，防止出现OOM的错误。默认值是`10000`。

#### index.mapping.field_name_length.limit

&emsp;&emsp;字段的名称长度最大值。这个设置不能解决mappings explosion的问题，但是如果你想要限制filed length时仍然是有用的。通常不需要设置它。除非用户增加了大量的字段，并且每个字段的字段名长度非常的长，否则使用默认值就可以了。默认值是`Long.MAX_VALUE (no limit)`。

#### index.mapping.dimension_fields.limit

&emsp;&emsp;预览功能（[Dynamic](#Dynamic index settings)，integer）

&emsp;&emsp;Elastic内部使用。

&emsp;&emsp;索引中时序维度（time series dimension）数量的最大值。默认值为`16`。

&emsp;&emsp;你可以使用[time_series_dimension](#Parameters for ip fields) mapping参数将某个字段标记为一个dimension。

### Removal of mapping types
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/removal-of-types.html)

&emsp;&emsp;Elasticsearch 8.0.0不再支持mapping types。见7.x中的[removal of types](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/removal-of-types.html)了解如何不使用mapping Type并迁移你的集群。

## Text analysis
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis.html)

&emsp;&emsp;Text analysis是将无结构的文本（unstructured text），例如email的body或者产品描述，转化为优化查询的有结构的格式（structured format）的过程。

#### When to configure text analysis

&emsp;&emsp;Elasticsearch在索引和查询[text](#Text type family)字段时会执行text analysis。

&emsp;&emsp;如果你的索引中没有`text`字段，那你不需要任何设置，可以跳过本章节内容。

&emsp;&emsp;然而，如果你使用了`text`字段或者你的文本查询（text analysis）没有返回期望的结果，那[configuring text analysis](#Configure text analysis)可以提供一定的帮助。如果你正在使用Elasticsearch实现下面的功能，那你也应该要查看下analysis configuration：

- 构建一个搜索引擎
- 挖掘无结构的数据
- 对特定语言进行微调（Fine-tune）搜索
- 进行词典或语言研究（Perform lexicographic or linguistic research）

#### In this section

- [Overview](#Text analysis overview)
- [Concepts](#Text analysis concepts)
- [Configure text analysis](#Configure text analysis)
- [Built-in analyzer reference](#Built-in analyzer reference)
- [Tokenizer reference](#Tokenizer reference)
- [Token filter reference](#Token filter reference)
- [Character filters reference](#Character filters reference)
- [Normalizers](#Normalizers)

### Text analysis overview
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-overview.html)

&emsp;&emsp;文本分析（Text analysis）使得Elasticsearch可以执行全文检索（full-text search），查询结果将返回所有相关的结果而不是返回精确的匹配结果。

&emsp;&emsp;如果你查询`Quick fox jumps`，你可能希望返回的文档中包含`A quick brown fox jumps over the lazy dog`，并且你可能还想要文档中包含像`fast fox`或者` foxes leap`相关的词。

#### Tokenization

&emsp;&emsp;Analysis通过`tokenization`使得全文检索成为可能。将一个文本break down成较小的多个块（smaller chunks），称为tokens。大部分情况下，token是一个个独立的单词（individual word）。

&emsp;&emsp;如果你将一个短语`the quick brown fox jumps`作为单个string，然后用户查询`quick fox`，则不会匹配到这个短语。如果对这个短语进行tokenize，并且分别索引每一个词，这个query string中的term会被分别的查询。意味着这个短语会被`quick fox`，`fox brown`或者其他variations匹配到。

#### Normalization

&emsp;&emsp;Tokenization使得可以匹配到独立的term（individual term），但只是字符匹配（match literally），意思是：

- 查询`Quick`不会匹配到`quick`，尽管你想要查询它们中的一个且能匹配到另一个
- 尽管`fox`和`foxes`有相同的词根（root word），但查询`foxes`不会匹配到`fox`，反之亦然（vice versa）
- 查询`jums`不会匹配到`leaps`。它们没有相同的词根，它们是同义词（synonyms）并且有相同的意思

&emsp;&emsp;为了解决上述的问题，text analysis可以将这些token normalize到一个标准格式。这使得你可以匹配到一些token，尽管这些token跟查询的term不是完全一样，但是它们足够的相似使得可以产生关联，例如：

- `Quick`可以小写为`quick`
- `foxes`可以被stemmed，或者reduce成它的词根：`fox`
- `jump`和`leap`是同义词，可以索引为单个词：`jump`

&emsp;&emsp;为了查询时能如期（intend）的匹配到这些词，你可以对query string应用相同的Tokenization和Normalization。例如`Foxes leap`的查询可以被normalized 成`fox jump`的查询。

#### Customize text analysis

&emsp;&emsp;Text analysis通过分词器[analyzer](#Anatomy of an analyzer)实现，一系列的规则来管理整个的处理过程。

&emsp;&emsp;Elasticsearch包含一个默认的分词器（analyzer），名为[standard analyzer](#Standard analyzer)，这个分词器在大部分的开箱即用用例中都表现不错。

&emsp;&emsp;如果你想自定（tailor）查询的体验，你可以选择不同的[built-in analyzer](#Built-in analyzer reference)，甚至[configure a custom one](#Create a custom analyzer)。自定义的分词器可以控制分词处理过程中的一些步骤，包括：

- 在Tokenization前更改文本（change text）
- text如何转化为token
- 在索引和搜索前，Normalization对token进行更改

### Text analysis concepts
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-concepts.html)

&emsp;&emsp;这一章节介绍Elasticsearch中text analysis的一些基本内容：

- [Anatomy of an analyzer](#Anatomy of an analyzer)
- [Index and search analysis](#Index and search analysis)
- [Stemming](#Stemming)
- [Token graphs](#Token graphs)

#### Anatomy of an analyzer
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analyzer-anatomy.html)

&emsp;&emsp;分词器（analyzer）可以是内置或者自定义的，它是包含了三个lower-level building blocks的包：character filters, tokenizers, 和 token filters。

##### Character filters(concept)

&emsp;&emsp;原始的文本作为一个字符流（a stream of characters），character filter接受这个字符流，通过adding，removing，或者changing character的方式对字符流进行转化。例如character filter可以用来将Hindu-Arabic numerals (٠١٢٣٤٥٦٧٨٩) 转化为 Arabic-Latin equivalents (0123456789)或者从流中移除（strip）HTML的element，比如`<b>`。

&emsp;&emsp;一个分词器可以没有，或者多个[character filter](#Character filters reference)。有序的依次应用（apply）这些character filter。

##### Tokenizer(concept)

&emsp;&emsp;Tokenizer接受字符流，将其拆分（break up）为独立的token（individual token），token通常是独立的一个词（individual word），然后输出一个token流（a stream of tokens）。例如[whitespace](#Whitespace tokenizer)会将文本按照空格进行token的划分，它会将`Quick brown fox!`划分成`[Quick, brown, fox!]`。

&emsp;&emsp;tokenizer同样负责记录每一个term的先后顺序，位置，以及字符的开始跟结束的[偏移](https://www.amazingkoala.com.cn/Lucene/Index/2019/0428/倒排表（中）)信息。

&emsp;&emsp;一个分词器只能有一个[Tokenizer](#Tokenizer reference)。

##### Token filters(concept)

&emsp;&emsp;token filters接受token流，并且add，remove，或者更改token。例如，[lowercase](#Lowercase token filter)这个token filters会将所有的token转成小写，[stop](#Stop token filter) token filter会从token流中移除常见的词（common word，停用词），比如`the`。[synonym](#Synonym token filter) token filter会将同义词放到token流中。

&emsp;&emsp;Token filters不允许更改token的位置以及偏移信息。

&emsp;&emsp;一个分词器可以没有，或者多个[Token filters](#Token filter reference)，有序的依次应用（apply）这些token filters。

#### Index and search analysis
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-index-search-time.html)

&emsp;&emsp;Text analysis会在下面两个时间点发生：

##### Index time

&emsp;&emsp;当索引一篇文档时，所有的[text](#Text type family)的字段值都会被分词（analyzed）。

##### Search time

&emsp;&emsp;当在`text`字段上执行[full-text search](#Full text queries)时，query string（用户查询的内容）会被分词。

&emsp;&emsp;Search time也被称为query time。

&emsp;&emsp;analyzer或者一系列的analysis rules在不同的时间点分别称为`index analyzer`和`search analyzer`。

##### How the index and search analyzer work together

&emsp;&emsp;在大多数情况下，Index time和search time应该使用相同的analyzer。这样能保证字段值跟query string都被更改（change）成相同的tokens。这样的话在搜索时就能达到期望的token匹配。

###### example

&emsp;&emsp;下面的值作为`text`字段的字段值进行索引：

```text
The QUICK brown foxes jumped over the dog!
```

&emsp;&emsp;如果Index analyzer将上文的字段值转化为成token并进行了normalizes，在这个例子中，每一个token代表了一个词：

```text
[ quick, brown, fox, jump, over, dog ]
```

&emsp;&emsp;这些token都会被索引。

&emsp;&emsp;随后，一位用户会在相同的`text`字段上进行以下的查询：

```text
"Quick fox"
```

&emsp;&emsp;用户想要匹配刚刚上文中索引的句子：`The QUICK brown foxes jumped over the dog!`。

&emsp;&emsp;但是用户提供的query string中不存在精确的词（exact word）来匹配文档中的内容：

- `Quick` vs `QUICK`
- `fox` vs `foxes`

&emsp;&emsp;为了能正确的查询，query string要使用相同的analyzer。这个analyzer会产生下面的tokens：

```text
[ quick, fox ]
```

&emsp;&emsp;执行查询时，Elasticsearch会比较query string中tokens 和`text`字段索引的tokens：

| Token | Query string | `text` field |
| :---: | :----------: | :----------: |
| quick |      X       |      X       |
| brown |              |      X       |
|  fox  |      X       |      X       |
| jump  |              |      X       |
| over  |              |      X       |
|  dog  |              |      X       |

&emsp;&emsp;由于字段值跟query string使用相同方式进行分词，因此它们会生成相同的token。`quick`和`fox`是精确匹配，意味着查询会匹配到包含`"The QUICK brown foxes jumped over the dog!"`的文档，满足用户期望。

##### When to use a different search analyzer

&emsp;&emsp;有时候index time和search time需要使用不同的analyzer，不过这种场景不是很常见（while less common）。Elasticsearch允许你[specify a separate search analyzer](#How Elasticsearch determines the search analyzer)来处理这种场景。

&emsp;&emsp;通常来说，当字段值和query string使用相同格式的token（same form of tokens）后得到不在期望内或者不相关的匹配时才会考虑使用不同的（separate）search analyzer。

###### example

&emsp;&emsp;Elasticsearch用来创建一个搜索引擎来匹配那些以查询词为前缀的结果。例如，查询词`tr`应该匹配`tram`或者`trope`，而不会匹配`taxi`或者`bat`。

&emsp;&emsp;一篇文档被添加到搜索引擎的索引中，这篇文档包含一个`text`类型的字段的值：

```text
"Apple"
```

&emsp;&emsp;index analyzer会将这个字段值转化为tokens并且执行normalizes。在这个例子中，每一个token是这个词的各种前缀。

```text
[ a, ap, app, appl, apple]
```

&emsp;&emsp;这些token都会被索引。

&emsp;&emsp;随后，用户在相同的`text`字段上进行搜索：

```text
"appli"
```

&emsp;&emsp;用户期望匹配以`appli`为前缀的结果，比如`appliance`或者`application`。这个查询不应该匹配到`apply`。

&emsp;&emsp;然而如果使用index analyzer来处理这个query string，它会生成下面的tokens：

```text
[ a, ap, app, appl, appli ]
```

&emsp;&emsp;当Elasticsearch拿着query string中的tokens去匹配`apply`的tokens时，它会找到好几个匹配的结果：

| Token | appli | apple |
| :---: | :---: | :---: |
|   a   |   X   |   X   |
|  ap   |   X   |   X   |
|  app  |   X   |   X   |
| appl  |   X   |   X   |
| appli |       |   X   |

&emsp;&emsp;这意味着错误的（erroneous）匹配到了`apple`，而且不仅仅会匹配到`apply`，还会匹配到所有以`a`为前缀的结果。

&emsp;&emsp;要修复这种情况，可以为query string指定一个不同的search analyzer来处理`text`字段。

&emsp;&emsp;在这个例子中，你可以指定一个search analyzer来生成单个token而不是生成前缀集合：

```text
[ appli]
```

&emsp;&emsp;这个query string token只会匹配以`appli`为前缀的结果，更接近用户期望的查询结果。

#### Stemming
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/stemming.html)

&emsp;&emsp;Stemming（词根化）将一个词reduce到这个词的root form（词根）。使得这个词的各种变形都能在查询时匹配到。

&emsp;&emsp;例如，`walking`和`walked`都可以被stemmed到相同的root word：`walk`。一旦进行stemming，`walking`、`walked`、`walk`任意一个出现在查询中都会匹配到另外两个。

&emsp;&emsp;Stemming属于language-dependent，但通常就是移除词的前缀跟后缀。

&emsp;&emsp;有些情况下，经过stemming后的词可能就不是一个真实的词。例如`jumping`和`jumpiness`可以stemmed为`jumpi`。`jumpi`不是一个真实存在的词，但是这不会影响查询。如果所有词的变形都可以reduce为相同的词根，就可以正确的匹配到。

##### Stemmer token filters

&emsp;&emsp;在Elasticsearch中，在[stemmer token filters](#Stemmer token filter)中处理stemming。这些token filter根据它们不同词根化的方式可以分类为：

- [Algorithmic stemmers](#Algorithmic stemmers)，基于一系列的规则执行词根化
- [Dictionary stemmers](#Dictionary stemmers)，基于查找词典执行词根化

&emsp;&emsp;由于词根化会更改token，我们建议在[index and search analysis](#Index and search analysis)中使用相同的stemmer token filters。

##### Algorithmic stemmers

&emsp;&emsp;Algorithmic stemmers通过应用一系列的规则将每一个词reduce到它的词根（root form）。例如，一个用于英文的algorithmic stemmers会移除复数（plural）的`-s`跟`-es`后缀。

&emsp;&emsp;Algorithmic stemmers有以下的一些优点：

- 它们要求更少的步骤并且开箱即用
- 它们占用少量的内存
- 通常比[Dictionary stemmers](#Dictionary stemmers)的处理速度快

&emsp;&emsp;然而大多数的algorithmic stemmers只会改变词的文字（text of a word）。这意味着一些irregular word，它们本身不包含词根的内容，这类词就不能很好的词根化：

- `be`，`are`和`am`
- `mouse`和`mice`
- `foot`和`feet`

&emsp;&emsp;下面的token filters使用了algorithmic stemming：

- [stemmer](#Stemmer token filter)，为一些语言提供了algorithmic stemming，带有一些额外的变形（variants）
- [kstem](#KStem token filter)，用于英语的词根化，它结合了内建字典（built-in dictionary）的algorithmic stemming
- [porter_stem](#Porter stem token filter)，推荐使用的用于英语的词根化
- [snowball](#Snowball token filter)，使用[Snowball-based](https://snowballstem.org)，用于多种语言

##### Dictionary stemmers

&emsp;&emsp;Dictionary stemmers从提供的字典中查找词。将未词根化的word variants替换为字典中词根化的词。

&emsp;&emsp;理论上，dictionary stemmers适用于：

- irregular words的词根化
- 辨别出那些拼写类似但含义毫无相关的单词，比如：
  - `organ`和`organization`
  - `broker`和`broken`

&emsp;&emsp;实践中，algorithmic stemmers通常优于（outperform）dictionary stemmers。因为dictionary stemmers有以下的缺点：

- Dictionary quality
  - dictionary stemmer的效果取决于字典。为了能很好的工作，这些字典必须包含大量的词，需要定期更新，并且根据不同的语言进行变更。通常来说，当一个词典在开始使用后，它的不完整性以及一些条目就已经过时了。
- Size and performance
  - Dictionary stemmers必须将所有的词，前缀，后缀从字典中载入到内存。这会带来大量的RAM开销。Low-quality的字典在去除前缀和后缀方面的效率也可能较低，这会显著减缓词根提取过程

&emsp;&emsp;你可以使用[hunspell token filter](#Hunspell token filter)来执行 dictionary stemming。

> TIPs：如果可以的话，我们建议在使用 [hunspell token filter](#Hunspell token filter)前先使用algorithmic stemmer。

##### Control stemming

&emsp;&emsp;有时候stemming会将一些词共享同一个词根但是这些词之间只是拼写相似但是含义是没有任何关联的。例如，`skies`和`skiing`会被词根化为`ski`。

&emsp;&emsp;为了防止出现上述问题以及更好的控制stemming，你可以使用下面的token filters：

- [stemmer_override](#Stemmer override token filter)，可以定义指定tokens的词根化规则
- [keyword_marker](#Keyword marker token filter)，可以指定token作为一个keyword，随后的stemmer token filters不会对keyword token词根化
- [conditional](#Conditional token filter)，可以标记token为keyword word，类似`keyword_marker`

&emsp;&emsp;对于内置（built-in）的[language analyzers](#Language analyzers)，你可以使用[stem_exclusion](#Excluding words from stemming )参数来指定不需要词根化的词。

#### Token graphs
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/token-graphs.html)

&emsp;&emsp;当[tokenizer ](#Tokenizer(concept))将文本转化为tokens后，它同时会记录下面的信息：

- 流中每一个token的`position`
- `positionLength`，一个token跨越（span）的位置数量（number of position）

&emsp;&emsp;使用上述信息可以创建一个[directed acyclic graph](https://en.wikipedia.org/wiki/Directed_acyclic_graph)，称为`token graph`。在一个token graph中，每一个位置代表了一个节点（node）。每一个token代表了一个edge或者arc，指向下一个位置。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/token-graph-qbf-ex.svg">

##### Synonyms

&emsp;&emsp;一些[token filters](#Token filters(concept))会增加新的token到现有的token stream中，例如同义词。这些同义词通常跟现有的tokens拥有相同的position。

&emsp;&emsp;在下面的图中，`quick`和它的同义词`fast`有相同的位置：`0`。它们跨越相同的位置：

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/token-graph-qbf-synonym-ex.svg">

##### Multi-position tokens

&emsp;&emsp;有些token filters会增加跨越多个位置的token。这个token是multi-word的同义词，例如使用`atm`作为`automatic teller machine`的同义词。

&emsp;&emsp;然而只有一些token filters，比如`graph token filters`，会为multi-position的token记录准确的`positionLength`。这类token filter包括：

- [synonym_graph](#Synonym graph token filter)
- [word_delimiter_graph](#Word delimiter graph token filter)

&emsp;&emsp;Some tokenizers, such as the [nori_tokenizer](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/analysis-nori-tokenizer.html), also accurately decompose compound tokens into multi-position tokens。

&emsp;&emsp;下图中，`domain name system`和它的同义词`dns`，它们的position都是`0`。然而`dns`的`positionLength`为`3`。其他的token在图中的`positionLength`的值默认就是`1`。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/token-graph-dns-synonym-ex.svg">

###### Using token graphs for search

&emsp;&emsp;[Indexing](#Index and search analysis)会忽略`positionLength`的属性并且不支持包含multi-position tokens的token graph。

&emsp;&emsp;然而[match](#Match query)和[match_phrase](#Match phrase query) query会使用这些图从单个query string中生成多个sub-queries。

&emsp;&emsp;某个用户使用`match_phrase`查询下面的短语：

```text
domain name system is fragile
```

&emsp;&emsp;在[search analysis](#Index and search analysis)中，`dns`是`domain name system`的同义词，会被添加到query string对应的token stream中。`dns`这个token的`positionLength`的值为`3`。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/token-graph-dns-synonym-ex.svg">

&emsp;&emsp;`match_phrase`使用这个图来生成下面的sub-queries：

```text
dns is fragile
domain name system is fragile
```

&emsp;&emsp;意味着查询会匹配包含`dns is fragile`或者`domain name system is fragile`的文档。

###### Invalid token graphs

&emsp;&emsp;下面的token filter会添加token，这些token会跨越多个位置，但是只将`positionLength`记录为默认值：`1`。

- [synonym](#Synonym token filter)
- [word_delimiter](#Word delimiter token filter)

&emsp;&emsp;这意味着这些filters会为包含这些token的流生产出错误的（invalid）token graph。

&emsp;&emsp;在下图中，`dns`是multi-position，它是`domain name system`的同义词。然而`dns`的`positionLength`为`1`，导致一个错误的图。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/token-graph-dns-invalid-ex.svg">

&emsp;&emsp;避免使用错误的token图用于搜索。错误的图会导致不满足期望的查询结果。

### Configure text analysis
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/configure-text-analysis.html)

&emsp;&emsp;默认情况下，Elasticsearch会使用默认的[standard analyzer](#Standard analyzer)用于所有的文本分析（text analysis）。`standard`分词器（analyzer）作为开箱即用能满足大多数的语言和用例。如果你选择`standard`，那么不需要其他的任何配置。

&emsp;&emsp;如果`standard`满足不了你的需求，你可以查看以及测试Elasticsearch提供的其他内置的[built-in analyzers](#Built-in analyzer reference)。内置的分词器也不需要进行配置，但是可以使用一些参数来调节分词器的行为。例如你可以对`standard`配置一个自定义的停用词列表。

&emsp;&emsp;如果内置的的分词器都不能满足你的需求，你可以创建并测试你自定义的分词器。自定义的分词器包括选择和组合不同的[analyzer components](#Anatomy of an analyzer)，让你能控制分词的过程。

- [Test an analyzer](#Test an analyzer)
- [Configuring built-in analyzers](#Configuring built-in analyzers)
- [Create a custom analyzer](#Create a custom analyzer)
- [Specify an analyzer](#Specify an analyzer)

#### Test an analyzer
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/test-analyzer.html)

&emsp;&emsp;[analyze API ](#Analyze API)是一个非常有用（invaluable）的工具来查看分词器的分词效果。请求中可以指定内置的分词器：

```text
POST _analyze
{
  "analyzer": "whitespace",
  "text":     "The quick brown fox."
}
```

&emsp;&emsp;API会返回如下的结果：

```text
{
  "tokens": [
    {
      "token": "The",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0
    },
    {
      "token": "quick",
      "start_offset": 4,
      "end_offset": 9,
      "type": "word",
      "position": 1
    },
    {
      "token": "brown",
      "start_offset": 10,
      "end_offset": 15,
      "type": "word",
      "position": 2
    },
    {
      "token": "fox.",
      "start_offset": 16,
      "end_offset": 20,
      "type": "word",
      "position": 3
    }
  ]
}
```

&emsp;&emsp;你也可以测试下面的组合：

- 一个tokenizer
- 没有或者多个token filters
- 没有或者多个character filters

```text
POST _analyze
{
  "tokenizer": "standard",
  "filter":  [ "lowercase", "asciifolding" ],
  "text":      "Is this déja vu?"
}
```

&emsp;&emsp;API会返回如下的结果：

```text
{
  "tokens": [
    {
      "token": "is",
      "start_offset": 0,
      "end_offset": 2,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "this",
      "start_offset": 3,
      "end_offset": 7,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "deja",
      "start_offset": 8,
      "end_offset": 12,
      "type": "<ALPHANUM>",
      "position": 2
    },
    {
      "token": "vu",
      "start_offset": 13,
      "end_offset": 15,
      "type": "<ALPHANUM>",
      "position": 3
    }
  ]
}
```

> Positions and character offsets
> 从上文中的`analyze` API可以看出，分词器不仅仅会将词转化为term，并且还会有序的记录每一个term的相对位置（用于短语查询或者word proximity查询），以及每一个term在原始文本中的开始跟结束的偏移位置（用于高亮查询）

&emsp;&emsp;或者在指定的索引上运行`analyze` API时指定一个[custom analyzer](#Create a custom analyzer)：

```text
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "std_folded": { 
          "type": "custom",
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "asciifolding"
          ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "my_text": {
        "type": "text",
        "analyzer": "std_folded" 
      }
    }
  }
}

GET my-index-000001/_analyze 
{
  "analyzer": "std_folded", 
  "text":     "Is this déjà vu?"
}

GET my-index-000001/_analyze 
{
  "field": "my_text", 
  "text":  "Is this déjà vu?"
}
```
&emsp;&emsp;第6行，定义一个名为`std_folded`的分词器
&emsp;&emsp;第21行，`my_text`字段使用`std_folded`分词器进行分词
&emsp;&emsp;第27行，要想引用这个分词器，`analyze` API必须要指定索引名
&emsp;&emsp;第29行，通过分词器的名称来指定一个分词器
&emsp;&emsp;第35行，对`my_text`字段进行分词


&emsp;&emsp;API会返回如下的结果：

```text
{
  "tokens": [
    {
      "token": "is",
      "start_offset": 0,
      "end_offset": 2,
      "type": "<ALPHANUM>",
      "position": 0
    },
    {
      "token": "this",
      "start_offset": 3,
      "end_offset": 7,
      "type": "<ALPHANUM>",
      "position": 1
    },
    {
      "token": "deja",
      "start_offset": 8,
      "end_offset": 12,
      "type": "<ALPHANUM>",
      "position": 2
    },
    {
      "token": "vu",
      "start_offset": 13,
      "end_offset": 15,
      "type": "<ALPHANUM>",
      "position": 3
    }
  ]
}
```

#### Configuring built-in analyzers
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/configuring-analyzers.html)

&emsp;&emsp;内置的分词器（analyzer）可以直接使用而不需要进行任何的配置。而一些内置的分词器支持参数配置来改变分词器的行为。例如，[standard analyzer](#Standard analyzer)可以配置自定义的停用词集合：

```text
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "std_english": { 
          "type":      "standard",
          "stopwords": "_english_"
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "my_text": {
        "type":     "text",
        "analyzer": "standard", 
        "fields": {
          "english": {
            "type":     "text",
            "analyzer": "std_english" 
          }
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "field": "my_text", 
  "text": "The old brown cow"
}

POST my-index-000001/_analyze
{
  "field": "my_text.english", 
  "text": "The old brown cow"
}
```

&emsp;&emsp;第6行，我们定义了一个基于`standard`的分词器`std_english`，但是使用了英语的停用词
&emsp;&emsp;第17行，`my_text`字段直接使用了`standard`分词器，没有进行任何的配置。
&emsp;&emsp;第31行，不会移除停用词，所以分词结果为：[`the, old, brown, cow`]
&emsp;&emsp;第21行，`my_text.english`字段使用了`std_english`，所以英语停用词会被移除，所以第37行的分词结果为：[`old, brown, cow`]

#### Create a custom analyzer
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-custom-analyzer.html)

&emsp;&emsp;当内置的分词器（analyzer）无法满足你的需求时，你可以创建一个`custom`分词器，并使用合适的组合：

- 没有或者多个[character filters](#Character filters reference)
- 一个[tokenizer](#Tokenizer reference)
- 没有或者多个[token filters](#Token filter reference)

##### Configuration

&emsp;&emsp;`custom`分词器接受以下的参数：

|          type          | 分词器类型，可以是[built-in analyzer](#Built-in analyzer reference)类型。对于custom analyzer，使用`custom`或者omit这个参数 |
| :--------------------: | :----------------------------------------------------------: |
|       tokenizer        |     内置或者自定义的[tokenizer](#Tokenizer reference)      |
|      char_filter       | 可选参数，一个或者多个内置或者自定义[character filters](#Character filters reference) |
|         filter         | 可选参数，一个或者多个内置或者自定义[token filters](#Token filter reference) |
| position_increment_gap | When indexing an array of text values, Elasticsearch inserts a fake "gap" between the last term of one value and the first term of the next value to ensure that a phrase query doesn’t match two terms from different array elements. Defaults to `100`. See [position_increment_gap](#position_increment_gap) for more. |

##### Example configuration

&emsp;&emsp;这个例子使用了以下的组合：

###### Character Filter

- [HTML Strip Character Filter](#HTML strip character filter)

###### Tokenizer

- [Standard Tokenizer](#Standard tokenizer)

###### Token Filters

- [Lowercase Token Filter](#Lowercase token filter)
- [ASCII-Folding Token Filter](#ASCII folding token filter)

```text
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "type": "custom", 
          "tokenizer": "standard",
          "char_filter": [
            "html_strip"
          ],
          "filter": [
            "lowercase",
            "asciifolding"
          ]
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_custom_analyzer",
  "text": "Is this <b>déjà vu</b>?"
}
```

&emsp;&emsp;第7行，对于`custom`分词器，使用参数`type`的值为`custom`或者omit`type`这个参数。

&emsp;&emsp;例子：

```text
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": { 
          "char_filter": [
            "emoticons"
          ],
          "tokenizer": "punctuation",
          "filter": [
            "lowercase",
            "english_stop"
          ]
        }
      },
      "tokenizer": {
        "punctuation": { 
          "type": "pattern",
          "pattern": "[ .,!?]"
        }
      },
      "char_filter": {
        "emoticons": { 
          "type": "mapping",
          "mappings": [
            ":) => _happy_",
            ":( => _sad_"
          ]
        }
      },
      "filter": {
        "english_stop": { 
          "type": "stop",
          "stopwords": "_english_"
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_custom_analyzer",
  "text": "I'm a :) person, and you?"
}
```

&emsp;&emsp;第6行，分配给这个索引一个默认自定义的分词器，`my_custom_analyzer`。这个分词器使用了一个自定义的tokenizer，character filter以及token filter。这个分词器omit了`type`参数。
&emsp;&emsp;第18行，定义了一个自定义的名为`punctuation`的tokenizer。
&emsp;&emsp;第24行，定义了一个自定义的名为`emoticons`的character filter。
&emsp;&emsp;第33行，定义了一个自定义的名为`english_stop`的token filter。

&emsp;&emsp;上面的例子会生成下面的term：

```text
[ i'm, _happy_, person, you ]
```

#### Specify an analyzer
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/specify-analyzer.html#specify-index-time-analyzer)

&emsp;&emsp;Elasticsearch提供了多种方法（a variety of ways）来指定一个内置的或者自定义的分词器（analyzer）。

- By text field, index, or query
- [Index and search analysis](#Index and search analysis)

> TIP: Keep it simple
> 在不同的level（filed-level，index-level）和不同的时间点（索引或查询期间）能够灵活的指定分词器is good，但只有必要时才这么做。
> 
> 大多数情况下，简单的方式可以work well：为每一个`text`字段指定分词器。见[Specify the analyzer for a field](#Specify the analyzer for a field)。
> 
> 这个方式会按Elasticsearch的默认行为工作，让你在索引和查询阶段使用相同的分词器。也能让你快速的通过[get mapping API](#Get mapping API)了解哪个字段使用了哪个分词器。
> 
> 如果你不为你的索引创建mapping，你可以使用[index templates](#Index templates)来达到相同的效果。

##### How Elasticsearch determines the search analyzer

&emsp;&emsp;Elasticsearch会有序的检查下面的参数来检测使用了哪种index analyzer：

- 字段的参数[analyzer](#analyzer)，见[Specify the analyzer for a field](#Specify the analyzer for a field)。
- 索引设置：`analysis.analyzer.default`。见[Specify the default analyzer for an index](#Specify the analyzer for a field)

&emsp;&emsp;如果没有指定这些参数，那么默认使用[standard analyzer](#Standard analyzer)。

##### Specify the analyzer for a field

&emsp;&emsp;你可以使用mapping参数[analyzer](#analyzer(mapping parameter))为每一个`text`字段指定分词器。

&emsp;&emsp;下面的[create index API](#Create index API)要求为`title`字段指定`whitespace`分词器：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "title": {
        "type": "text",
        "analyzer": "whitespace"
      }
    }
  }
}
```

##### Specify the default analyzer for an index

&emsp;&emsp;除了在field-level指定分词器，你可以使用`analysis.analyzer.default`设置一个fallback analyzer。

&emsp;&emsp;下面的[create index API](#Create index API)为索引`my-index-000001`设置了一个名为`simple`的fallback analyzer。

```text
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "default": {
          "type": "simple"
        }
      }
    }
  }
}
```

##### Specify the search analyzer for a query

&emsp;&emsp;在执行[full-text query](#Full text queries)时，你可以使用`analyzer`参数来指定 search analyzer，它会覆盖任何其他的search analyzer。

&emsp;&emsp;下面的[search API](#Search API)在[match](#Match query) query中设置了一个`stop`的分词器。

```text
GET my-index-000001/_search
{
  "query": {
    "match": {
      "message": {
        "query": "Quick foxes",
        "analyzer": "stop"
      }
    }
  }
}
```

##### Specify the search analyzer for a field

&emsp;&emsp;你可以使用[search_analyzer](#analyzer)参数为每一个`text`字段指定一个search analyzer。

&emsp;&emsp;如果提供了search analyzer，必须使用`analyzer`参数来指定index analyzer。

&emsp;&emsp;下面的[create index API](#Create index API)为`title`字段指定了一个`simply`分词器作为search analyzer。

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "title": {
        "type": "text",
        "analyzer": "whitespace",
        "search_analyzer": "simple"
      }
    }
  }
}
```

##### Specify the default search analyzer for an index

&emsp;&emsp;当[creating an index](#Create index API)时，你可以使用`analysis.analyzer.default_search`设置一个默认的search analyzer。

&emsp;&emsp;如果设置了默认的search analyzer，你必须要使用`analysis.analyzer.default`设置默认的index analyzer。

&emsp;&emsp;下面的[creating an index](#Create index API)中，为名为`my-index-000001`的索引设置了一个默认的名为`whitespace`分词器作为默认的search analyzer。

```text
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "default": {
          "type": "simple"
        },
        "default_search": {
          "type": "whitespace"
        }
      }
    }
  }
}
```

### Built-in analyzer reference
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-analyzers.html)

&emsp;&emsp;Elasticsearch自带多种内置分析器，这些分析器可以在任何索引中使用，无需进一步配置：

- [Standard Analyzer](#Standard analyzer)：`standard`根据Unicode文本分割算法的定义，在单词边界处将文本划分为term。它移除大部分标点符号，将词项转换为小写，并支持移除停用词
- [Simple Analyzer](#Simple analyzer)：`simple`在遇到非字母字符时将文本划分为term。它将所有词项转换为小写
- [Whitespace Analyzer](#Whitespace analyzer)：`whitespace`在遇到任何空白字符时将文本划分为term。它不会将词项转换为小写
- [Stop Analyzer](#Stop analyzer)：`stop`类似于`simple`分词器，但也支持移除停用词
- [Keyword Analyzer](#Keyword analyzer)：`keyword`是一种“无操作”分析器，接受任何给定的文本，并将相同的文本作为单个term输出
- [Pattern Analyzer](#Pattern analyzer)：`pattern`使用正则表达式将文本分割为term。它支持小写转换和停用词。
- [Language Analyzers](#Language analyzers)：Elasticsearch提供了许多特定语言的分析器，如英语或法语。
- [Fingerprint Analyzer](#Fingerprint analyzer)：`fingerprint`是一种专门的分词器，用于创建可用于重复检测的指纹。

##### Custom analyzers

&emsp;&emsp;如果你没有找到你想要的分词器，那你可以[custom](#Create a custom analyzer)一个分词器，让这个分词器包含合适的[character filter](#Character filters reference)、[tokenizer](#Tokenizer reference)以及[token filters](#Token filter reference)。

#### Fingerprint analyzer
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-fingerprint-analyzer.html)

&emsp;&emsp;`fingerprint` 分词器实现了[fingerprinting algorithm ](https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth#fingerprint)用于在OpenRefine项目中帮助进行聚类。

&emsp;&emsp;输入的文本会被小写化、标准化来移除扩展的字符，排序、去重并且最后拼接成单个token。如果配置了stopword列表，停用词会被移除。

##### Example output

```text
POST _analyze
{
  "analyzer": "fingerprint",
  "text": "Yes yes, Gödel said this sentence is consistent and."
}
```

&emsp;&emsp;上面的句子生成下面的单个term：

```text
[ and consistent godel is said sentence this yes ]
```

##### Configuration

&emsp;&emsp;`fingerprint` 分词器接受以下的参数：

- separator：拼接时使用的字符。默认是空格
- max_output_size：token数量的最大值。默认是`255`。超过这个大小会被丢弃
- stopwords：预先定义的类似`_english_`的停用词列表或者包含通用词的数组。默认是`_none_`
- stopwords_path：包含的通用词的文件路径

&emsp;&emsp;见[Stop Token Filter](#Stop token filter)了解更多关于停用词配置的信息。

##### Example configuration

&emsp;&emsp;下面的例子中，我们为`fingerprint`分词器配置了一个预先定义好的English停用词：

```text
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_fingerprint_analyzer": {
          "type": "fingerprint",
          "stopwords": "_english_"
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_fingerprint_analyzer",
  "text": "Yes yes, Gödel said this sentence is consistent and."
}
```

&emsp;&emsp;上面的例子中生成以下的term：

```text
[ consistent godel said sentence yes ]
```

##### Definition

&emsp;&emsp;`fingerprint`包含下面的内容：

###### Tokenizer

- [Standard Tokenizer](#Standard tokenizer)

###### Token Filters (in order)

- [Lower Case Token Filter](#Lowercase token filter)
- [ASCII folding](#ASCII folding token filter)
- [Stop Token Filter](#Stop token filter) (disabled by default)
- [Fingerprint](#Fingerprint token filter)

&emsp;&emsp;如果你需要自定义`fingerprint`分词器，你可以重新创建一个名为`custom`分词器然后修改它，通常添加token filter。这将重建内置的`fingerprint`分词器，你可以使用它，将其作为进一步的自定义作为一个起始点：

```text
PUT /fingerprint_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "rebuilt_fingerprint": {
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "asciifolding",
            "fingerprint"
          ]
        }
      }
    }
  }
}
```

#### Keyword analyzer
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-keyword-analyzer.html)

&emsp;&emsp;`keyword`分词器是一个"noop"（没有操作）的分词器，它将整个输入的字符串作为单个token。

##### Example output

```text
POST _analyze
{
  "analyzer": "keyword",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
```

&emsp;&emsp;上面的句子将会生成以下单个term：

```text
[ The 2 QUICK Brown-Foxes jumped over the lazy dog's bone. ]
```

##### Configuration

&emsp;&emsp;`keyword`分词器没有配置项。

##### Definition

&emsp;&emsp;`keyword` 分词器包含以下内容：

###### Tokenizer

- [Keyword Tokenizer](#Keyword tokenizer)

&emsp;&emsp;如果你需要自定义`keyword`分词器，你可以重新创建一个名为`custom`分词器然后修改它，通常添加token filter。通常如果你不需要将字符串切分为token时应该选择[Keyword type ](#Keyword type family)类型，但如果你一定要这么勇，这将重建内置的`fingerprint`分词器，你可以使用它，将其作为进一步的自定义作为一个起始点：

```text
PUT /keyword_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "rebuilt_keyword": {
          "tokenizer": "keyword",
          "filter": [         
          ]
        }
      }
    }
  }
}
```

&emsp;&emsp;第8行，你可以在这里添加任意的token filter。

#### Language analyzers
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-lang-analyzer.html)

##### Configuring language analyzers

###### Excluding words from stemming 

#### Pattern analyzer
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-pattern-analyzer.html)

#### Simple analyzer
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-simple-analyzer.html)

&emsp;&emsp;`simple`分词器根据任意非字母（non-letter）的字符对文本进行划分。比如数字、空格、连字符和撇号，丢弃非字母字符，并将大写字母转换为小写字母。

##### Example

```text
POST _analyze
{
  "analyzer": "simple",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
```

&emsp;&emsp;`simple`分词器解析上面的句子并且生成下面的token：

```text
[ the, quick, brown, foxes, jumped, over, the, lazy, dog, s, bone ]
```

##### Definition

&emsp;&emsp;`simple`分词器包含以下内容：

##### Tokenizer：

- [Lowercase Tokenizer](#Lowercase tokenizer)

##### Customize

&emsp;&emsp;如果你需要自定义`simple`分词器，你可以重新创建一个名为`custom`分词器然后修改它，通常添加token filter：

```text
PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_custom_simple_analyzer": {
          "tokenizer": "lowercase",
          "filter": [                          
          ]
        }
      }
    }
  }
}
```

&emsp;&emsp;第8行，在这里添加token filter。


#### Standard analyzer
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-standard-analyzer.html)

&emsp;&emsp;`standard` 分词器（analyzer）是默认的分词器，如果没有指定分词器则使用`standard`。`standard`提供了基于语法的分词（基于Unicode Text Segmentation algorithm，见 [Unicode Standard Annex #29](https://unicode.org/reports/tr29/)）,它能很好的对大多数语言进行分词。

##### Example output

```text
POST _analyze
{
  "analyzer": "standard",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
```

&emsp;&emsp;上面的句子会生成下面的term：

```text
[ the, 2, quick, brown, foxes, jumped, over, the, lazy, dog's, bone ]
```

##### Configuration

&emsp;&emsp;`standard`分词器接收下面的参数：

- `max_token_length`：token的最大长度。如果一个token的长度超过了`max_token_length`，会按照`max_token_length`长度进行分割。默认值是`255`。
- stopwords：预设的停用词，例如`_english_`或者包含了停用词的数组。默认值为`_none_`。
- stopwords_path：包含停用词的文件的路径

&emsp;&emsp;见[Stop Token Filter](#Stop token filter)查看停用词的配置。

##### Example configuration

&emsp;&emsp;在这个例子中，我们将`standard`分词器的参数`max_token_length`设置为`5`（只是为了展示参数效果），然后使用预先定义的英语停用词：

```text
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_english_analyzer": {
          "type": "standard",
          "max_token_length": 5,
          "stopwords": "_english_"
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_english_analyzer",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
```

&emsp;&emsp;上面的例子能生成下面的term：

```text
[ 2, quick, brown, foxes, jumpe, d, over, lazy, dog's, bone ]
```

##### Definition

&emsp;&emsp;`standard`分词器包括下面的内容：

###### Tokenizer

- [Standard Tokenizer](#Standard tokenizer)

###### Token Filters

- [Lower Case Token Filter](#Lowercase token filter)
- [Stop Token Filter](#Stop token filter)（默认关闭）

&emsp;&emsp;如果你需要自定义配置参数外的`standard`分词器，你需要重建一个`custom`的分词器然后修改它。通常是添加token filter。这样将重建一个内置的`standard`分词器。

```text
PUT /standard_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "rebuilt_standard": {
          "tokenizer": "standard",
          "filter": [
            "lowercase"       
          ]
        }
      }
    }
  }
}
```

&emsp;&emsp;第9行，你可以在`lowercase`后添加其他的token filter。


#### Stop analyzer
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-stop-analyzer.html)

&emsp;&emsp;`stop` 分词器和[simple analyzer](#Simple analyzer)是一样的，但是增加了支持移除停用词。默认使用`_english_`停用词。

##### Example output

```text
POST _analyze
{
  "analyzer": "stop",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
```

&emsp;&emsp;上面的句子将生成以下term：

```text
[ quick, brown, foxes, jumped, over, lazy, dog, s, bone ]
```

##### Configuration

&emsp;&emsp;`stop`分词器接受以下的参数：

- stopwords：预先定义的类似`_english_`的停用词列表或者包含通用词的数组。默认是`_english_`
- stopwords_path：包含的通用词的文件路径

&emsp;&emsp;见[Stop Token Filter](#Stop token filter)了解更多关于停用词配置的信息。

##### Example configuration

&emsp;&emsp;在这个例子中，我们配置`stop`分词器来使用一个指定的停用词列表：

```text
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_stop_analyzer": {
          "type": "stop",
          "stopwords": ["the", "over"]
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_stop_analyzer",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
```

&emsp;&emsp;上面的例子生成下面的term：

```text
[ quick, brown, foxes, jumped, lazy, dog, s, bone ]
```

##### Definition

&emsp;&emsp;包含以下内容：

###### Tokenizer：

- [Lower Case Tokenizer](#Lowercase tokenizer)

###### Token filters

- [Stop Token Filter](#Stop token filter)

&emsp;&emsp;如果你需要自定义`stop`分词器，你可以重新创建一个名为`custom`分词器然后修改它，通常添加token filter，这将重建内置的`stop`分词器，你可以使用它，将其作为进一步的自定义作为一个起始点：

```text
PUT /stop_example
{
  "settings": {
    "analysis": {
      "filter": {
        "english_stop": {
          "type":       "stop",
          "stopwords":  "_english_" 
        }
      },
      "analyzer": {
        "rebuilt_stop": {
          "tokenizer": "lowercase",
          "filter": [
            "english_stop"          
          ]
        }
      }
    }
  }
}
```

&emsp;&emsp;第8行，默认的停用出可以用`stopwords`或者`stopwords_path`参数覆盖
&emsp;&emsp;第15行，你可以在`english_stop`后面添加任意的token filter

#### Whitespace analyzer
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-whitespace-analyzer.html)

&emsp;&emsp;`whitespace`分词器（analyzer）会在根据空格将文本划分出term。

##### Example output

```text
POST _analyze
{
  "analyzer": "whitespace",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
```

&emsp;&emsp;上面的句子会生成下面的term：

```text
[ The, 2, QUICK, Brown-Foxes, jumped, over, the, lazy, dog's, bone. ]
```

##### Configuration

&emsp;&emsp;`whitespace` 分词器不可以配置。

##### Definition

&emsp;&emsp;`whitespace`分词器包含下面的内容：

###### Tokenizer

- [Whitespace Tokenizer](#Whitespace tokenizer)

&emsp;&emsp;如果你需要自定义`whitespace`分词器，你可以重新创建一个名为`custom`分词器然后修改它，通常添加token filter。这将重建内置的`whitespace`分词器：

```text
PUT /whitespace_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "rebuilt_whitespace": {
          "tokenizer": "whitespace",
          "filter": [         
          ]
        }
      }
    }
  }
```

&emsp;&emsp;第8行，你可以在这里添加token filter。

### Tokenizer reference
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-tokenizers.html)

#### Character group tokenizer
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-chargroup-tokenizer.html)

&emsp;&emsp;`char_group` tokenizer定义了一组集合，集合中的元素作为文本切分标识。希望自定义一个简单的分词器或者或者无法接受[pattern tokenizer ](#Pattern tokenizer)的开销时，使用这个tokenizer非常有用。

##### Configuration

&emsp;&emsp;`char_group` tokenizer 接受以下参数：

- `tokenize_on_chars`：包含字符的列表。列表中的字符作为切分的标识。这个参数可以包含单个字符，比如`-`，也可以是字符组，比如`whitespace`, `letter`, `digit`, `punctuation`, `symbol`
- `max_token_length`：token的最大长度。如果一个token的长度超过了`max_token_length`，会按照`max_token_length`长度进行划分。默认值是`255`。

##### Example output

```text
POST _analyze
{
  "tokenizer": {
    "type": "char_group",
    "tokenize_on_chars": [
      "whitespace",
      "-",
      "\n"
    ]
  },
  "text": "The QUICK brown-fox"
}
```

&emsp;&emsp;输出：

```text
{
  "tokens": [
    {
      "token": "The",
      "start_offset": 0,
      "end_offset": 3,
      "type": "word",
      "position": 0
    },
    {
      "token": "QUICK",
      "start_offset": 4,
      "end_offset": 9,
      "type": "word",
      "position": 1
    },
    {
      "token": "brown",
      "start_offset": 10,
      "end_offset": 15,
      "type": "word",
      "position": 2
    },
    {
      "token": "fox",
      "start_offset": 16,
      "end_offset": 19,
      "type": "word",
      "position": 3
    }
  ]
}
```

#### Classic tokenizer
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-classic-tokenizer.html)

&emsp;&emsp;`classic` tokenizer是一个基于语法的分词器，适用于英文文档。这个分词器对首字母缩写词、公司名称、电子邮件地址和互联网主机名有特殊处理的启发式（tokenizer）规则。然而，这些规则并非总是有效，而且这个分词器对于英语以外的大多数语言效果不佳：

- 它在大多数标点符号处分割单词，移除标点。但是，如果点号后面没有空白，则认为它是令token的一部分。
- 它在连字符（`-`）处分割单词，除非token中有数字，在这种情况下，整个token被解释为产品号，不进行分割。
- 它将电子邮件地址和互联网主机名识别为一个token。

##### Example output

```text
POST _analyze
{
  "tokenizer": "classic",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
```

&emsp;&emsp;上面的句子将会生成下面的term：

```text
[ The, 2, QUICK, Brown, Foxes, jumped, over, the, lazy, dog's, bone ]
```

##### Configuration

&emsp;&emsp;`classic` tokenizer 接受以下参数：

- `max_token_length`：token的最大长度。如果一个token的长度超过了`max_token_length`，会按照`max_token_length`长度进行划分。默认值是`255`。

##### Example configuration

&emsp;&emsp;在这个例子中，我们将`max_token_length`的值设置为5（出于演示目的）：

```text
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "classic",
          "max_token_length": 5
        }
      }
    }
  }
}
```

&emsp;&emsp;上面的例子生成以下的term：

```text
[ The, 2, QUICK, Brown, Foxes, jumpe, d, over, the, lazy, dog's, bone ]
```

#### Edge n-gram tokenizer
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-edgengram-tokenizer.html)

&emsp;&emsp;

#### Keyword tokenizer
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-keyword-tokenizer.html)

&emsp;&emsp;`keyword` tokenizer是一个`noop`（没有任何操作） tokenizer。无论给定的文本是什么样子，总是将其作为单个term。他可以跟token filter结合来标准化输出，比如将email地址小写。

#### Letter tokenizer
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-letter-tokenizer.html)

&emsp;&emsp;`letter` tokenizer在遇到一个非字母时开始切分。对于大多数的欧洲语言，这种切分方式是合理的，但是对于一些亚洲语言就不行了，因为单词之间不是用空格分隔的。

##### Example output

```text
POST _analyze
{
  "tokenizer": "letter",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
```

&emsp;&emsp;上面的例子生成以下的term：

```text
[ The, QUICK, Brown, Foxes, jumped, over, the, lazy, dog, s, bone ]
```

##### Configuration

&emsp;&emsp;`letter` tokenizer没有配置项。

#### Lowercase tokenizer
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-lowercase-tokenizer.html)

&emsp;&emsp;`lowercase` tokenizer跟[letter tokenizer](#Letter tokenizer)是一样的，也就是遇到一个非字母时开始切分，区别就是同时将所有的字母小写化。这个tokenizer的功能就相当于组合了[letter tokenizer](#Letter tokenizer)和[lowercase token filter](#Lowercase token filter)，但是性能更好，因为它将这两个步骤同时处理了。

##### Example output

```text
POST _analyze
{
  "tokenizer": "lowercase",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
```

&emsp;&emsp;上面的例子生成以下的term：

```text
[ the, quick, brown, foxes, jumped, over, the, lazy, dog, s, bone ]
```

##### Configuration

&emsp;&emsp;`lowercase` tokenizer没有配置项。


#### N-gram tokenizer
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-ngram-tokenizer.html)

&emsp;&emsp;

#### Path hierarchy tokenizer
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-pathhierarchy-tokenizer.html)

&emsp;&emsp;`path_hierarchy` tokenizer 用来处理有层级的值，比如说文件系统路径，根据路径符号划分，然后输出不同层的term。


#### Pattern tokenizer
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-pattern-tokenizer.html)

&emsp;&emsp;`Pattern` tokenizer使用正则表达式将文本进行分词，将匹配到的词，或者匹配到一段文本作为term。

#### Simple pattern tokenizer
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-simplepattern-tokenizer.html)

&emsp;&emsp;

#### Simple pattern split tokenizer
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-simplepatternsplit-tokenizer.html)

&emsp;&emsp;

#### Standard tokenizer
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-standard-tokenizer.html)

&emsp;&emsp;`standard` tokenizer提供了基于语法的分词（基于Unicode Text Segmentation algorithm，见 [Unicode Standard Annex #29](https://unicode.org/reports/tr29/)）,它能很好的对大多数语言进行分词。

##### Example output

```text
POST _analyze
{
  "tokenizer": "standard",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
```

&emsp;&emsp;上面的句子会生成下面的term：

```text
[ The, 2, QUICK, Brown, Foxes, jumped, over, the, lazy, dog's, bone ]
```

##### Configuration

&emsp;&emsp;`standard` tokenizer接收下面的参数：

- max_token_length：token的最大长度。如果一个token的长度超过了`max_token_length`，会按照`max_token_length`长度进行划分。默认值是`255`。

##### Example configuration

&emsp;&emsp;在这个例子中，我们将`standard`tokenizer的参数`max_token_length`配置为`5`（至少为了展示参数效果)：

```text
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "standard",
          "max_token_length": 5
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
```

&emsp;&emsp;上面的例子会生成下面的term：

```text
[ The, 2, QUICK, Brown, Foxes, jumpe, d, over, the, lazy, dog's, bone ]
```

#### Thai tokenizer
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-thai-tokenizer.html)

&emsp;&emsp;

#### UAX URL email tokenizer
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-uaxurlemail-tokenizer.html)

&emsp;&emsp;`uax_url_email` tokenizer跟[standard tokenizer](#Standard tokenizer)很像，差别是它能识别URLs以及email，并且他们作为单个term。

##### Example output

```text
POST _analyze
{
  "tokenizer": "uax_url_email",
  "text": "Email me at john.smith@global-international.com"
}
```

&emsp;&emsp;上面的例子会生成下面的term：

```text
[ Email, me, at, john.smith@global-international.com ]
```

&emsp;&emsp;如果是`standard` tokenizer会生成：

```text
[ Email, me, at, john.smith, global, international.com ]
```

##### Configuration

&emsp;&emsp;`uax_url_email` tokenizer接受以下参数：

- `max_token_length`：token的最大长度。如果一个token的长度超过了`max_token_length`，会按照`max_token_length`长度进行划分。默认值是`255`。

##### Example configuration

&emsp;&emsp;在这个例子中，我们将`max_token_length`设置为5（仅演示目的）：

```text
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "my_tokenizer"
        }
      },
      "tokenizer": {
        "my_tokenizer": {
          "type": "uax_url_email",
          "max_token_length": 5
        }
      }
    }
  }
}

POST my-index-000001/_analyze
{
  "analyzer": "my_analyzer",
  "text": "john.smith@global-international.com"
}

```

&emsp;&emsp;上面的例子会生成下面的term：

```text
[ john, smith, globa, l, inter, natio, nal.c, om ]
```

#### Whitespace tokenizer
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-whitespace-tokenizer.html)

&emsp;&emsp;`whitespace` tokenizer会在根据空格将文本划分出term。

##### Example output

```text
 POST _analyze
{
  "tokenizer": "whitespace",
  "text": "The 2 QUICK Brown-Foxes jumped over the lazy dog's bone."
}
````

&emsp;&emsp;上面的句子将生成下面的term：

```text
[ The, 2, QUICK, Brown-Foxes, jumped, over, the, lazy, dog's, bone. ]
```

##### Configuration

&emsp;&emsp;`whitespace` tokenizer接收下面的参数：

- `max_token_length`：token的最大长度。如果一个token的长度超过了`max_token_length`，会按照`max_token_length`长度进行划分。默认值是`255`。

### Token filter reference
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-tokenfilters.html)

#### Apostrophe token filter
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-apostrophe-tokenfilter.html)

&emsp;&emsp;移除撇号后所有的字符，包括撇号自身。

&emsp;&emsp;这个filter被包含在Elasticsearch内置的[Turkish language analyzer](#turkish analyzer)。它使用的是Lucene中的[ApostropheFilter](https://lucene.apache.org/core/9_1_0/analysis/common/org/apache/lucene/analysis/tr/ApostropheFilter.html)，为Turkish语言构建的。

##### Example

&emsp;&emsp;下面的[analyze API](#Analyze API)请求中展示了apostrophe token filter是如何工作的。

```text
GET /_analyze
{
  "tokenizer" : "standard",
  "filter" : ["apostrophe"],
  "text" : "Istanbul'a veya Istanbul'dan"
}
```

&emsp;&emsp;这个filter生成下面的tokens：

```text
[ Istanbul, veya, Istanbul ]
```

##### Add to an analyzer

&emsp;&emsp;下面的[create index API](#Create index API)请求中使用这个filter来配置一个全新的[custom analyzer](#Create a custom analyzer)。

```text
PUT /apostrophe_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "standard_apostrophe": {
          "tokenizer": "standard",
          "filter": [ "apostrophe" ]
        }
      }
    }
  }
}
```

#### ASCII folding token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-asciifolding-tokenfilter.html)

&emsp;&emsp;ASCII folding token filter将不在Basic Latin Unicode block（基本拉丁字母Unicode块）（前127个ASCII字符）中的字母、数字和符号字符转换为它们的等价的ASCII（如果存在的话）。例如，这个过滤器会将`à`转换为`a`。

&emsp;&emsp;这个filter使用的是Lucene中的[ASCIIFoldingFilter](https://lucene.apache.org/core/9_1_0/analysis/common/org/apache/lucene/analysis/miscellaneous/ASCIIFoldingFilter.html)。

#### CJK bigram token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-cjk-bigram-tokenfilter.html)

&emsp;&emsp;

#### CJK width token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-cjk-width-tokenfilter.html)

&emsp;&emsp;

#### Classic token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-classic-tokenfilter.html)

#### Common grams token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-common-grams-tokenfilter.html#analysis-common-grams-tokenfilter)

&emsp;&emsp;

#### Conditional token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-condition-tokenfilter.html)

&emsp;&emsp;定义token filter集合，应用到满足脚本中条件的token上。

&emsp;&emsp;这个filter使用的是Lucene中的[ConditionalTokenFilter](https://lucene.apache.org/core/9_1_0/analysis/common/org/apache/lucene/analysis/miscellaneous/ConditionalTokenFilter.html)。

#### Decimal digit token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-decimal-digit-tokenfilter.html)

&emsp;&emsp;

#### Delimited payload token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-delimited-payload-tokenfilter.html)

&emsp;&emsp;

#### Dictionary decompounder token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-dict-decomp-tokenfilter.html)

&emsp;&emsp;

#### Edge n-gram token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-edgengram-tokenfilter.html)

&emsp;&emsp;

#### Elision token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-elision-tokenfilter.html)

&emsp;&emsp;

#### Fingerprint token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-fingerprint-tokenfilter.html)

&emsp;&emsp;对一个token stream进行排序和去重，让将这个stream中的token进行拼接，最后作为单个token输出。

&emsp;&emsp;例如，这个filter会对`[ the, fox, was, very, very, quick ]`这个token stream进行如下更改：

1. 根据字母排序：`[ fox, quick, the, very, very, was ]`
2. 移除重复的`very` 这个token
3. 将这个token stream拼接成单个token：`[fox quick the very was ]`

#### Flatten graph token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-flatten-graph-tokenfilter.html)

&emsp;&emsp;

#### Hunspell token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-hunspell-tokenfilter.html)

&emsp;&emsp;

#### Hyphenation decompounder token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-hyp-decomp-tokenfilter.html)

&emsp;&emsp;

#### Keep types token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-keep-types-tokenfilter.html)

&emsp;&emsp;保留或移除指定的类型。比如你可以使用这个filter通过只保留`<ALPHANUM>`将`3 quick foxes`修改为`quick foxes`。

#### Keep words token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-keep-words-tokenfilter.html)

&emsp;&emsp;指定一个word list，只保留在这个列表中的word。

#### Keyword marker token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-keyword-marker-tokenfilter.html)

&emsp;&emsp;

#### Keyword repeat token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-keyword-repeat-tokenfilter.html)

&emsp;&emsp;

#### KStem token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-kstem-tokenfilter.html)

&emsp;&emsp;

#### Length token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-length-tokenfilter.html)

&emsp;&emsp;指定token中字符数量的min跟max，小于min或者大于max的token会被移除。比如你可以使用这个filter过滤掉小于2个字符以及大于5个字符的token。

#### Limit token count token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-limit-token-count-tokenfilter.html)

&emsp;&emsp;限制输出的token数量。这个filter通常用于根据token的数量来限制文档字段值的大小。

&emsp;&emsp;默认情况下，这个filter只保留token stream中的第一个token。比如，这个filter可以将`[ one, two, three ]`这个token filter改为`[ one ]`。

#### Lowercase token filter
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-lowercase-tokenfilter.html)

&emsp;&emsp;将token text都转为小写的token。例如你可以使用`lowercase` filter将`THE Lazy DoG`转化为`the lazy dog`。

&emsp;&emsp;除了默认的filter，`lowercase` token filter还提供了访问Lucene中为Greek, Irish, and Turkish特别定制的lowercase filter。

##### Example

&emsp;&emsp;下面的[analyze API](#Analyze API)使用默认的`Lowercase` filter 将`THE Quick FoX JUMPs`转化为小写。

```text
GET _analyze
{
  "tokenizer" : "standard",
  "filter" : ["lowercase"],
  "text" : "THE Quick FoX JUMPs"
}
```

&emsp;&emsp;这个filter产生下面的token：

```text
[ the, quick, fox, jumps ]
```

##### Add to an analyzer

&emsp;&emsp;下面的[create index API ](#Analyze API)配置了一个custom analyzer，它使用了`lowercase` filter：

```text
PUT lowercase_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_lowercase": {
          "tokenizer": "whitespace",
          "filter": [ "lowercase" ]
        }
      }
    }
  }
}
```

##### Configurable parameters

###### language

&emsp;&emsp;（optional）特定的一些语言对应的lowercase token filter。可选值为：

- greek：使用Lucene中的[GreekLowerCaseFilter](https://lucene.apache.org/core/9_1_0/analysis/common/org/apache/lucene/analysis/el/GreekLowerCaseFilter.html)
- irish：使用Lucene中的[IrishLowerCaseFilter](https://lucene.apache.org/core/9_1_0/analysis/common/org/apache/lucene/analysis/ga/IrishLowerCaseFilter.html)
- turkish：使用Lucene中的[TurkishLowerCaseFilter](https://lucene.apache.org/core/9_1_0/analysis/common/org/apache/lucene/analysis/tr/TurkishLowerCaseFilter.html)

&emsp;&emsp;如果不指定该参数，则默认使用[LowerCaseFilter](https://lucene.apache.org/core/9_1_0/analysis/common/org/apache/lucene/analysis/core/LowerCaseFilter.html)。

##### Customize

&emsp;&emsp;若要自定义`lowercase` filter，为新的自定义的token filter创建所需要的基本要素（basis）。

&emsp;&emsp;下面的例子中自定义了用于Greek语言的`lowercase` filter：

```text
PUT custom_lowercase_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "greek_lowercase_example": {
          "type": "custom",
          "tokenizer": "standard",
          "filter": ["greek_lowercase"]
        }
      },
      "filter": {
        "greek_lowercase": {
          "type": "lowercase",
          "language": "greek"
        }
      }
    }
  }
}
```

#### MinHash token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-minhash-tokenfilter.html)

&emsp;&emsp;

#### Multiplexer token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-multiplexer-tokenfilter.html)

&emsp;&emsp;

#### N-gram token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-ngram-tokenfilter.html)

&emsp;&emsp;

#### Normalization token filters
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-normalization-tokenfilter.html)

&emsp;&emsp;

#### Pattern capture token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-pattern-capture-tokenfilter.html)

&emsp;&emsp;

#### Pattern replace token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-pattern_replace-tokenfilter.html)

&emsp;&emsp;

#### Phonetic token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-phonetic-tokenfilter.html)

&emsp;&emsp;

#### Porter stem token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-porterstem-tokenfilter.html)

&emsp;&emsp;

#### Predicate script token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-predicatefilter-tokenfilter.html)

&emsp;&emsp;移除不满足脚本条件的token。这个filter只支持内联的[Painless](https://www.elastic.co/guide/en/elasticsearch/painless/8.2/index.html)脚本。脚本在[analysis predicate context](https://www.elastic.co/guide/en/elasticsearch/painless/8.2/painless-analysis-predicate-context.html)中计算。

#### Remove duplicates token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-remove-duplicates-tokenfilter.html)

#### Reverse token filter
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-reverse-tokenfilter.html)

&emsp;&emsp;将stream中的每一个token进行颠倒（反转）。比如，你可以使用这个filter将`cat`改成`tac`。

&emsp;&emsp;基于后缀查询，比如找到以`-ion`为后缀的词或者根据扩展名查询的场景使用这个filter就非常有用。

&emsp;&emsp;这个filter使用了Lucene中的[ReverseStringFilter](https://lucene.apache.org/core/9_1_0/analysis/common/org/apache/lucene/analysis/reverse/ReverseStringFilter.html)。

##### Example

&emsp;&emsp;下面的[analyze API](#Analyze API)请求中使用了`reverse` filter将`quick for jumps`中的每一个token进行了颠倒：

```text
GET _analyze
{
  "tokenizer" : "standard",
  "filter" : ["reverse"],
  "text" : "quick fox jumps"
}
```

&emsp;&emsp;下面的filter生成了以下的token：

```text
[ kciuq, xof, spmuj ]
```

#####  Add to an analyzer

&emsp;&emsp;下面的[create index API](#Create index API)请求使用了这个filter来配置一个全新的[custom analyzer](#Create a custom analyzer)。

```text
PUT reverse_example
{
  "settings" : {
    "analysis" : {
      "analyzer" : {
        "whitespace_reverse" : {
          "tokenizer" : "whitespace",
          "filter" : ["reverse"]
        }
      }
    }
  }
}
```


#### Shingle token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-shingle-tokenfilter.html)

&emsp;&emsp;

#### Snowball token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-snowball-tokenfilter.html)

#### Stemmer token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-stemmer-tokenfilter.html)

#### Stemmer override token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-stemmer-override-tokenfilter.html)

#### Stop token filter
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-stop-tokenfilter.html)

&emsp;&emsp;从token stream中移除[stop words](https://en.wikipedia.org/wiki/Stop_word)。

&emsp;&emsp;如果没有自定义停用词，会默认移除下列的英文单词：

```text
a, an, and, are, as, at, be, but, by, for, if, in, into, is, it, no, not, of, on, or, such, that, the, their, then, there, these, they, this, to, was, will, with
```

&emsp;&emsp;除了英语的停用词，`stop` filter支持预先定义好的[stop word lists for several languages](#Stop words by language)。你也可以用数组或者文件来定义属于你的停用词。

&emsp;&emsp;`stop` filter使用的是Lucene的[StopFilter](https://lucene.apache.org/core/9_1_0/analysis/common/org/apache/lucene/analysis/core/StopFilter.html)。

##### Example

&emsp;&emsp;下面的analyzer API请求中使用`stop` filter将移除`a quick fox jumps over the lazy dog`中的`a`和`the`：

```text
GET /_analyze
{
  "tokenizer": "standard",
  "filter": [ "stop" ],
  "text": "a quick fox jumps over the lazy dog"
}
```

&emsp;&emsp;这个filter会生成下面的token：

```text
[ quick, fox, jumps, over, lazy, dog ]
```

##### Add to an analyzer

&emsp;&emsp;下面的[create index API](#Create index API)请求使用`stop` filter来配置一个新的自定义的[custom analyzer](#Create a custom analyzer)：

```text
PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "whitespace",
          "filter": [ "stop" ]
        }
      }
    }
  }
}
```

##### Configurable parameters

- stopwords：（Optional, string or array of strings）。用于指定语言类型，例如`_arabic_`或者`_thai_`。默认值是[\_english\_](#_english_)。
  - 每一种语言对应一个预设的停用词列表，在Lucene中提供。见[Stop words by language](#Stop words by language)查看支持的语言以及其停用词。
  - 也接受停用词数组。
  - 若要使用空的停用词，使用`_none_`。
- stopwords_path：（Optional, string) 文件路径，该文件中包含待移除的停用词。
  - 这个路径必须是绝对路径或者是`config`下的相对路径。文件的字符编码必须是UTF-8。文件中每一个停用词必须用换行符分隔。
- ignore_case：（Optional, Boolean) 如果为`true`，停用词的匹配是大小写不敏感的。例如，如果该参数为`true`，那么`the`这个停用词会会移除`the`，`THE`，或者`The`。默认值是`false`。
- remove_trailing：（Optional, Boolean) 如果为`true`，并且token stream的最后一个token是停用词，则移除这个token。默认值是`true`。
  - 当使用[completion suggester](#Completion Suggester)时，这个参数应该要设置为`false`。这样能保证`green a`这个query能匹配并且建议出`green apple`并且仍然能够移除其他的停用词。

##### Customize

&emsp;&emsp;要自定义`stop` filter，需要创建一个新的custom token filter所需要的基本要素。你可以使用可配置的参数来更改filter。

&emsp;&emsp;例如，下面的请求中创建了一个自定义的大小写不敏感的`stop` filter。这个filter会移除[\_english\_](#\_english\_)中的停用词。

```text
PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "default": {
          "tokenizer": "whitespace",
          "filter": [ "my_custom_stop_words_filter" ]
        }
      },
      "filter": {
        "my_custom_stop_words_filter": {
          "type": "stop",
          "ignore_case": true
        }
      }
    }
  }
}
```

&emsp;&emsp;你也可以指定自定义的停用词。例如下面的请求中创建了一个自定义的大小写敏感的`stop` filter，它只移除`and`，`is`和`the`：

```text
PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "default": {
          "tokenizer": "whitespace",
          "filter": [ "my_custom_stop_words_filter" ]
        }
      },
      "filter": {
        "my_custom_stop_words_filter": {
          "type": "stop",
          "ignore_case": true,
          "stopwords": [ "and", "is", "the" ]
        }
      }
    }
  }
}
```

##### Stop words by language

&emsp;&emsp;下列是参数[stopwords](#stopwords)支持的参数值。并且链接指向了Lucene中每一种语言对应的停用词：

###### \_arabic\_
[Arabic stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/ar/stopwords.txt)

###### \_armenian\_
[Armenian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/hy/stopwords.txt)

###### \_basque\_
[Basque stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/eu/stopwords.txt)

###### \_bengali\_
[Bengali stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/bn/stopwords.txt)

###### \_brazilian\_  (Brazilian Portuguese)
[Brazilian Portuguese stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/br/stopwords.txt)

###### \_bulgarian\_
[Bulgarian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/bg/stopwords.txt)

###### \_catalan\_
[Catalan stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/ca/stopwords.txt)

###### \_cjk\_  (Chinese, Japanese, and Korean)
[CJK stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/cjk/stopwords.txt)

###### \_czech\_
[Czech stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/cz/stopwords.txt)

###### \_danish\_
[Danish stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/danish_stop.txt)

###### \_dutch\_
[Dutch stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/dutch_stop.txt)

###### \_english\_
[English stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/java/org/apache/lucene/analysis/en/EnglishAnalyzer.java#L48)

###### \_estonian\_
[Estonian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/et/stopwords.txt)

###### \_finnish\_
[Finnish stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/finnish_stop.txt)

###### \_french\_
[French stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/french_stop.txt)

###### \_galician\_
[Galician stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/gl/stopwords.txt)

###### \_german\_
[German stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/german_stop.txt)

###### \_greek\_
[Greek stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/el/stopwords.txt)

###### \_hindi\_
[Hindi stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/hi/stopwords.txt)

###### \_hungarian\_
[Hungarian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/hungarian_stop.txt)

###### \_indonesian\_
[Indonesian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/id/stopwords.txt)

###### \_irish\_
[Irish stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/ga/stopwords.txt)

###### \_italian\_
[Italian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/italian_stop.txt)

###### \_latvian\_
[Latvian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/lv/stopwords.txt)

###### \_lithuanian\_
[Lithuanian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/lt/stopwords.txt)

###### \_norwegian\_
[Norwegian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/norwegian_stop.txt)

###### \_persian\_
[Persian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/fa/stopwords.txt)

###### \_portuguese\_
[Portuguese stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/portuguese_stop.txt)

###### \_romanian\_
[Romanian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/ro/stopwords.txt)

###### \_russian\_
[Russian stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/russian_stop.txt)

###### \_sorani\_
[Sorani stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/ckb/stopwords.txt)

###### \_spanish\_
[Spanish stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/spanish_stop.txt)

###### \_swedish\_
[Swedish stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/snowball/swedish_stop.txt)

######  \_thai\_
[Thai stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/th/stopwords.txt)

###### \_turkish\_
[Turkish stop words](https://github.com/apache/lucene/blob/main/lucene/analysis/common/src/resources/org/apache/lucene/analysis/tr/stopwords.txt)

#### Synonym token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-synonym-tokenfilter.html)

&emsp;&emsp;`synonym` token filter允许在解析过程（analysis process）中处理同义词。通过一个配置文件来配置同义词。例子：

```text
PUT /test_index
{
  "settings": {
    "index": {
      "analysis": {
        "analyzer": {
          "synonym": {
            "tokenizer": "whitespace",
            "filter": [ "synonym" ]
          }
        },
        "filter": {
          "synonym": {
            "type": "synonym",
            "synonyms_path": "analysis/synonym.txt"
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;上面的例子中配置了一个带有`analysis/synonym.txt`路径的`synonym` filter（相对于`config`目录）。`synonym`分词器然后使用这个filter。

&emsp;&emsp;（未完成）

#### Synonym token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-synonym-tokenfilter.html)

&emsp;&emsp;

#### Synonym graph token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-synonym-graph-tokenfilter.html)

&emsp;&emsp;

#### Trim token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-trim-tokenfilter.html)

&emsp;&emsp;

#### Truncate token filter
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-truncate-tokenfilter.html)

&emsp;&emsp;对超过字符数量限制的token进行截断。这个限制值默认是`10`，可以通过参数`length`参数配置。

&emsp;&emsp;例如，你可以使用这个filter将所有的token中的字符数量限制到3或者更少的值。`jumping fox`修改为`jum fox`。

&emsp;&emsp;这个filter使用的是Lucene中的[TruncateTokenFilter](https://lucene.apache.org/core/9_1_0/analysis/common/org/apache/lucene/analysis/miscellaneous/TruncateTokenFilter.html)。

#### Example

&emsp;&emsp;下面的[analyze API](#Analyze API)请求使用这个filter将`he quinquennial extravaganza carried on`中超过10个字符的token进行了截断：

```text
GET _analyze
{
  "tokenizer" : "whitespace",
  "filter" : ["truncate"],
  "text" : "the quinquennial extravaganza carried on"
}
```

&emsp;&emsp;下面的filter生成以下的tokens：

```text
[ the, quinquenni, extravagan, carried, on ]
```

##### Add to analyzer

&emsp;&emsp;下面的[create index API](#Create index API)请求使用这个filter用来配置一个全新的[custom analyzer](#Create a custom analyzer)。

```text
PUT custom_truncate_example
{
  "settings" : {
    "analysis" : {
      "analyzer" : {
        "standard_truncate" : {
        "tokenizer" : "standard",
        "filter" : ["truncate"]
        }
      }
    }
  }
}
```

##### Configurable parameters

- length：（Optional, integer）每一个token包含的字符数量限制。超过这个值会被截断。默认为`10`

##### Customize

&emsp;&emsp;要自定义这个filter，需要创建一个新的custom token filter所需要的基本要素。你可以使用可配置的参数来更改filter。

&emsp;&emsp;例如，下面的请求创建了一个名为`5_char_trunc`的`truncate` filter，会将token中的字符限制到5个或者5个以下。

```text
PUT 5_char_words_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "lowercase_5_char": {
          "tokenizer": "lowercase",
          "filter": [ "5_char_trunc" ]
        }
      },
      "filter": {
        "5_char_trunc": {
          "type": "truncate",
          "length": 5
        }
      }
    }
  }
}
```

#### Unique token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-unique-tokenfilter.html)


#### Uppercase token filter
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-uppercase-tokenfilter.html)

&emsp;&emsp;将token转化为大写。比如你可以使用这个filter将`the Lazy DoG`修改为`THE LAZY DOG`。

&emsp;&emsp;这个filter使用的是Lucene中的[UpperCaseFilter](https://lucene.apache.org/core/9_1_0/analysis/common/org/apache/lucene/analysis/core/UpperCaseFilter.html)。

> WARNING：某些语言中大写字符能映射多个小写字符。因此使用这个filter可能导致失去小写字符的信息。
> 若要避免这种损失但仍保持一致的字母大小写，可以改用[lowercase](#Lowercase token filter) filter。

##### Example

&emsp;&emsp;下面的[analyze API](#Analyze API)请求使用这个filter将`the Quick FoX JUMPs`修改为大写。

```text
GET _analyze
{
  "tokenizer" : "standard",
  "filter" : ["uppercase"],
  "text" : "the Quick FoX JUMPs"
}
```

&emsp;&emsp;这个filter生成以下的tokens：

```text
[ THE, QUICK, FOX, JUMPS ]
```

##### Add to an analyzer

&emsp;&emsp;下面的[create index API](#Create index API)请求使用这个filter来配置一个全新的[customer analyzer](#Create a custom analyzer)。

```text
PUT uppercase_example
{
  "settings": {
    "analysis": {
      "analyzer": {
        "whitespace_uppercase": {
          "tokenizer": "whitespace",
          "filter": [ "uppercase" ]
        }
      }
    }
  }
}
```

#### Word delimiter token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-word-delimiter-graph-tokenfilter.html)

&emsp;&emsp;

#### Word delimiter graph token filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-word-delimiter-graph-tokenfilter.html)

&emsp;&emsp;

### Character filters reference
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-charfilters.html)

#### HTML strip character filter
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-htmlstrip-charfilter.html)

&emsp;&emsp;将HTML元素从文本中移除并且将HTML实体替换成解码后的值（比如`&amp;`替换为`&`）。

&emsp;&emsp;`html_strip` filter使用的是Lucene中的[HTMLStripCharFilter](https://lucene.apache.org/core/9_1_0/analysis/common/org/apache/lucene/analysis/charfilter/HTMLStripCharFilter.html)。

##### Example

&emsp;&emsp;下面的[analyzer API](#Analyze API)使用`html_strip` filter 将文本`<p>I&apos;m so <b>happy</b>!</p>` 修改为`\nI'm so happy!\n`。

```text
GET /_analyze
{
  "tokenizer": "keyword",
  "char_filter": [
    "html_strip"
  ],
  "text": "<p>I&apos;m so <b>happy</b>!</p>"
}
```

&emsp;&emsp;这个filter生成下面的文本：

```text
[ \nI'm so happy!\n ]
```

##### Add to an analyzer

&emsp;&emsp;下面的[create index API](#Create index API)请求使用`html_strip` filter来配置一个全新的[custom analyzer](#Create a custom analyzer)。

```text
PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "keyword",
          "char_filter": [
            "html_strip"
          ]
        }
      }
    }
  }
}
```

##### Configurable parameters

- escaped_tags：（Optional, array of strings）没有尖括号（`< >`）修饰的HTML元素列表。这个filter从文本中移除HTML元素的过程中会跳过这些。比如说配置了`["p"]`则会跳过`<p>`这个HTML元素

##### Customize

&emsp;&emsp;若要自定义`html_strip` filter，则将其作为一个全新自定义的filter的组成部分，你可以使用可配置的参数来修改filter。

&emsp;&emsp;下面的[create index API](#Create index API)使用了一个名为`my_custom_html_strip_char_filter`的自定义`html_strip` filter，并且配置到[custom analyzer ](#Create a custom analyzer)中。

&emsp;&emsp;`my_custom_html_strip_char_filter`跳过了`<b>`这个HTML 元素。

```text
PUT my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "keyword",
          "char_filter": [
            "my_custom_html_strip_char_filter"
          ]
        }
      },
      "char_filter": {
        "my_custom_html_strip_char_filter": {
          "type": "html_strip",
          "escaped_tags": [
            "b"
          ]
        }
      }
    }
  }
}
```

#### Mapping character filter
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-mapping-charfilter.html)

&emsp;&emsp;`mapping` character filter接收一个key，value的map。在character stream中遇到map中的key时，则将其替换为map中对应的value。

&emsp;&emsp;这是一种greedy的匹配。按照最长模板进行匹配。替换值（replacement）可以是一个空的string。

&emsp;&emsp;`mapping` filter 使用了 Lucene中的[MappingCharFilter](https://lucene.apache.org/core/9_1_0/analysis/common/org/apache/lucene/analysis/charfilter/MappingCharFilter.html)。

##### Example

&emsp;&emsp;下面的[analyze API](#Analyze API)使用了`mapping` filter 将Hindu-Arabic numerals (٠١٢٣٤٥٦٧٨٩) 转化为Arabic-Latin equivalents (0123456789)。将文本内容`My license plate is ٢٥٠١٥`转化为了`My license plate is 25015`。

```text
GET /_analyze
{
  "tokenizer": "keyword",
  "char_filter": [
    {
      "type": "mapping",
      "mappings": [
        "٠ => 0",
        "١ => 1",
        "٢ => 2",
        "٣ => 3",
        "٤ => 4",
        "٥ => 5",
        "٦ => 6",
        "٧ => 7",
        "٨ => 8",
        "٩ => 9"
      ]
    }
  ],
  "text": "My license plate is ٢٥٠١٥"
}
```

&emsp;&emsp;上面的filter会生成下面的文本：

```text
[ My license plate is 25015 ]
```

##### Configurable parameters

- mappings：（Required\*, array of strings）数组类型的mappings，包含了key到value的映射。
  - 必须指定这个参数或者`mappings_path`中的一个。
- mappings_path：（Required\*, string）文件的路径，该文件中包含了key到value的映射。
  - 该路径必须是绝对路径或者是相对于`config`的相对路径。文件中的字符编码必须是UTF-8。文件中每一个key到value必须用换行符区分。
  - 必须指定这个参数或者`mappings`中的一个。

##### Customize and add to an analyzer

&emsp;&emsp;要自定义`mappings` filter，创建一个自定义的character filter所需要的基本要素，你可以使用可配置的参数来修改它。

&emsp;&emsp;下面的[create index API](#Create index API)请求配置了一个新的[custom analyzer](#Create a custom analyzer)，它使用了一个自定义的`mappings` filter，`my_mappings_char_filter`。

&emsp;&emsp;`my_mappings_char_filter` filter将表情符号（emoticons）`:)，:(`替换成相同意思的文本：

```text
PUT /my-index-000001
{
  "settings": {
    "analysis": {
      "analyzer": {
        "my_analyzer": {
          "tokenizer": "standard",
          "char_filter": [
            "my_mappings_char_filter"
          ]
        }
      },
      "char_filter": {
        "my_mappings_char_filter": {
          "type": "mapping",
          "mappings": [
            ":) => _happy_",
            ":( => _sad_"
          ]
        }
      }
    }
  }
}
```

&emsp;&emsp;上面的[analyze API](#Analyze API)请求使用自定义的`my_mappings_char_filter`将文本`I'm delighted about it :(`中的`:(`替换成了`_sad_`。

```text
GET /my-index-000001/_analyze
{
  "tokenizer": "keyword",
  "char_filter": [ "my_mappings_char_filter" ],
  "text": "I'm delighted about it :("
}
```

&emsp;&emsp;这个filter会生成下面的文本：

```text
[ I'm delighted about it _sad_ ]
```

#### Pattern replace character filter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-pattern-replace-charfilter.html)

&emsp;&emsp;

### Normalizers
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/analysis-normalizers.html)

&emsp;&emsp;Normalizers跟分词器类似，区别在于它只输出单个token。因此，它没有[tokenizer](#Tokenizer reference)并且只接收一些有效的character filter和token filter。只有对单个字符处理的filter是允许在Normalizers中使用的。例如lowercase filter是允许的，而stemming filter则不允许，因为它需要将关键字作为一个整体来处理。下面的filter可以在Normalizers中使用：`arabic_normalization`, `asciifolding`, `bengali_normalization`, `cjk_width`, `decimal_digit`, `elision`, `german_normalization`, `hindi_normalization`, `indic_normalization`, `lowercase`, `persian_normalization`, `scandinavian_folding`, `serbian_normalization`, `sorani_normalization`, `uppercase`。

&emsp;&emsp;Elasticsearch使用`lowercase`用于内置的normalizers。其他形式的normalizers要求进行配置。

#### Custom normalizers

&emsp;&emsp;自定义的normalizers可以定义[character filter](#Character filters reference)和[token filter](#Token filter reference)的集合。

```text
PUT index
{
  "settings": {
    "analysis": {
      "char_filter": {
        "quote": {
          "type": "mapping",
          "mappings": [
            "« => \"",
            "» => \""
          ]
        }
      },
      "normalizer": {
        "my_normalizer": {
          "type": "custom",
          "char_filter": ["quote"],
          "filter": ["lowercase", "asciifolding"]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "foo": {
        "type": "keyword",
        "normalizer": "my_normalizer"
      }
    }
  }
}
```

## Index templates
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-templates.html)

>这个主题介绍了在Elasticsearch 7.8引入的composable index template。关于之前版本的index template是如何工作的见[legacy template documentation](#Create or update index template API（legacy）)。
>

&emsp;&emsp;在创建一个索引时，index template告诉Elasticsearch如何进行创建。对于[data streams](#Data streams)，index template用于创建流的[backing](#Backing indices)索引。Template先于索引的创建。手动或者通过索引一篇文档创建一个索引后，template setting会作为创建索引的一个基本要素（ basis）。

&emsp;&emsp;一共有索引模板index template和组件模版[component template](#Create or update component template API)两种类型的模板。component template构建配置了mapping，settings，alias这几块并可以复用。你可以使用一个或多个component template来构造index template，但不能直接应用到（apply）索引集合（a set of indices）。index templates 可以包含一个component template集合，直接指定settings，mappings和aliases。

&emsp;&emsp;应用（apply）index template的几个条件：

- composable template比legacy template优先级高。如果composable template没有匹配到索引，legacy template可能会匹配到并且应用。
- 如果使用显示的设置（explicit settings）来创建索引并且同时匹配到了一个index template。那么在[create index](#Create index API)请求中的settings比index template和component template中的优先级高。
- 如果一个新的data stream或者索引匹配到了多个index template，则使用优先级最高的index template

##### Avoid index pattern collisions

&emsp;&emsp; Elasticsearch有内置的index template，每一个的优先级都是`100`，如下：

- logs-*-*
- metrics-*-*
- synthetics-*-*

&emsp;&emsp;[Elastic Agent](https://www.elastic.co/guide/en/fleet/8.2/fleet-overview.html)使用这些模板来创建data streams。Fleet integration创建的index template使用类似的overlapping index patterns并且优先级是`200`。

&emsp;&emsp;如果你使用Fleet或者Elastic Agent， 那么你的index template的优先级要小于`100`来防止覆盖掉这些模板。若要避免意外的应用了这些内置模板，采取下面一个或者多个方法：

- 若要关闭内置的index template和component template，使用[cluster update settings API](#Cluster update settings API)将[stack.templates.enabled](#stack.templates.enabled)设置为`false`。
- 使用一个不会被 覆盖的index pattern
- 给overlapping pattern的模板一个`priority`大于`200`的值。例如如果你不想要使用Fleet或者Elastic Agent并且想要为`logs-*`这个index pattern创建一个模板，那么将你的模板的priority的值设置为`500`。这能保证你的模板能被应用于`logs-*`而不是使用内置的模板。

#### Create index template

&emsp;&emsp;使用[index template](#Create or update index template API) 和[put component template](#Create or update component template API)APIs来创建和更新index templates。你也可以在Kibana用意Stack Management来[manage index templates](#Index management in Kibana)。

&emsp;&emsp;下面的请求创建了两个component templates。

```text
PUT _component_template/component_template1
{
  "template": {
    "mappings": {
      "properties": {
        "@timestamp": {
          "type": "date"
        }
      }
    }
  }
}

PUT _component_template/runtime_component_template
{
  "template": {
    "mappings": {
      "runtime": { 
        "day_of_week": {
          "type": "keyword",
          "script": {
            "source": "emit(doc['@timestamp'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ROOT))"
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;第18行，这个component template添加了一个名为`day_of_week`的[runtime field](#Map a runtime field)的字段，用于一个新的索引匹配到模板时。

&emsp;&emsp;下面的请求创建了一个由上面的component template组成的index template。

```text
PUT _index_template/template_1
{
  "index_patterns": ["te*", "bar*"],
  "template": {
    "settings": {
      "number_of_shards": 1
    },
    "mappings": {
      "_source": {
        "enabled": true
      },
      "properties": {
        "host_name": {
          "type": "keyword"
        },
        "created_at": {
          "type": "date",
          "format": "EEE MMM dd HH:mm:ss Z yyyy"
        }
      }
    },
    "aliases": {
      "mydata": { }
    }
  },
  "priority": 500,
  "composed_of": ["component_template1", "runtime_component_template"], 
  "version": 3,
  "_meta": {
    "description": "my custom"
  }
}
```

### Simulate multi-component templates
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/simulate-multi-component-templates.html)

&emsp;&emsp;模板不仅仅由多个component template进行组合，也可以由index template自身。有两个simulation APIs用来查看最终的索引设置（index settings）。（当你指定了一个新的模板后，由于现有的模板可能跟这个新的模板有相同的index patter，由于优先级问题导致新的模板无法应用到新创建的索引，那么可以通过simulate来查看某个索引对应的index template，或者查看某个模板设置，见[Simulate index template API](#Simulate index template API)和[ Simulate index API](#Simulate index API)）

&emsp;&emsp;若要模拟（[simulate](#Simulate index API)）出将要应用到某个指定的索引的索引设置，可以通过以下请求获取。 

```text
POST /_index_template/_simulate_index/my-index-000001
```

&emsp;&emsp;若要模拟（[simulate](#Simulate index template API)）出某个模板的模板设置，可以通过以下请求获取。

```text
POST /_index_template/_simulate/template_1
```
&emsp;&emsp;你也可以在simulate 请求body中指定一个模板定义（新的模板设置）。这可以让你在添加一个新的模板前先校验下settings。（校验的目的之一是查看新的模板会不会被现有的模板覆盖或者覆盖现有的模板）

```text
PUT /_component_template/ct1
{
  "template": {
    "settings": {
      "index.number_of_shards": 2
    }
  }
}

PUT /_component_template/ct2
{
  "template": {
    "settings": {
      "index.number_of_replicas": 0
    },
    "mappings": {
      "properties": {
        "@timestamp": {
          "type": "date"
        }
      }
    }
  }
}

POST /_index_template/_simulate
{
  "index_patterns": ["my*"],
  "template": {
    "settings" : {
        "index.number_of_shards" : 3
    }
  },
  "composed_of": ["ct1", "ct2"]
}
```

&emsp;&emsp;下面的响应中展示了将会应用到匹配到的索引配置中的settings，mappings，以及aliases。任何被覆盖的模板中的配置会被simulate 请求中指定模板定义或者优先级更高的现有的模板取代。

```text
{
  "template" : {
    "settings" : {
      "index" : {
        "number_of_shards" : "3",   
        "number_of_replicas" : "0",
        "routing" : {
          "allocation" : {
            "include" : {
              "_tier_preference" : "data_content"
            }
          }
        }
      }
    },
    "mappings" : {
      "properties" : {
        "@timestamp" : {
          "type" : "date"           
        }
      }
    },
    "aliases" : { }
  },
  "overlapping" : [
    {
      "name" : "template_1",        
      "index_patterns" : [
        "my*"
      ]
    }
  ]
}
```

&emsp;&emsp;第5行，分片的数量来自simulate请求中的值，替代了index template`ct1`中的值
&emsp;&emsp;第19行，`@timestap`字段的定义来自index template`ct2`
&emsp;&emsp;第 27行，被覆盖的模板信息，它们有较低的优先级



## Data streams
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl.html)

&emsp;&emsp;数据流（data stream）允许你跨多个索引append-only方式存储时间序列数据，同时为你提供单个命名资源用于请求。Data stream非常适合用于logs、events、metrics以及其他源源不断生成的数据。

&emsp;&emsp;你可以直接向一个数据流提交索引和查询请求。数据流可以自动的将请求路由到存储流数据（stream's data）的[backing indices](#Backing indices)中。你可以使用[index lifecycle management(ILM)](#ILM: Manage the index lifecycle)实现backing indeices的自动化管理。例如，你可以使用ILM自动将较老的backing indices移到成本更低的硬件上以及删除不再需要的数据。ILM可以帮助你降低随着不断增长的数据带来的开销和成本。

#### Backing indices

&emsp;&emsp;一个数据流由一个或者多个[hidden](#index.hidden)、自动生成的backing indices组成。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/data-streams-diagram.svg">

&emsp;&emsp;一个数据流要求有一个匹配的index template（[index template](#Index templates)）。这个模板包含了用于配置流的backing indices的mappings和settings。

&emsp;&emsp;每一个索引到数据流中的文档必须包含一个`@timestamp`字段，映射为[date](#Date field type)或者[date_nanos](#Date nanoseconds field type)字段类型。如果index template没有指定`@timestamp`字段。Elasticsearch会根据默认选项映射一个`date`类型的`@timestmap`字段。

&emsp;&emsp;同一个index template可以用于多个数据流。你不能删除一个正在被某个数据流使用的index template。

#### Read requests

&emsp;&emsp;当你向一个数据流提交一个读请求时，数据流会将这个请求路由到所有的backing indice上。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/data-streams-search-request.svg">

#### Write index

&emsp;&emsp;最近创建的backing index是数据流的write index。数据流只向这个索引中增加文档。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/data-streams-index-request.svg">

&emsp;&emsp;你不能往其他backing indices中添加新的文档，甚至不能直接向这些索引中发送请求。

&emsp;&emsp;你同样不能在write index上执行下面的操作，免得阻碍（hinder）索引写入：

- [Clone](#Clone index API)
- [Delete](#Delete index API)
- [Shrink](#Shrink index API)
- [Split](#Split index API)

#### Rollover

&emsp;&emsp;[rollover](#Rollover API)操作会创建一个新的backing index，这个索引成为数据流中新的write index。

&emsp;&emsp;我们建议使用[ILM](#ILM: Manage the index lifecycle)，它能在write index达到一个指定的寿命（age）或者大小后自动的滚动（roll over）数据流。如果有必要的话，你也可以[manually roll over](#Manually roll over a data stream)一个数据流。

#### Generation

&emsp;&emsp;每一个数据流会追踪它的generation：一个六位数字，0作为填充值的整数，用来累计（cumulative）数据流滚动次数，从`000001`开始。

&emsp;&emsp;当创建一个backing index后，这个索引按下面的方式约定命名

```text
.ds-<data-stream>-<yyyy.MM.dd>-<generation>
```

&emsp;&emsp;`<yyyy.MM.dd> `是backing index的创建时间。generation数值越大的backing index包含更多最近的数据。例如，`web-server-logs`这个数据流的generation的值为`34`。这个流在2099年3月7日创建，那么就命名为`.ds-web-server-logs-2099.03.07-000034`。

&emsp;&emsp;有些[shink](#Shrink index API)或者[restore](#Restore a snapshot)操作会更改backing index的名称。这些名称的变更不会数据流中移除backing index。

#### Append-only

&emsp;&emsp;数据流被设计为用于现有数据很少或者从不更新的场景。你不能向数据流中现有的文档发送更新或者删除的请求。与之替换使用[update by query](#Update documents in a data stream by query)和[delete by query](#Delete documents in a data stream by query) APIs。

&emsp;&emsp;如果有必要的话，你可以使用[update or delete documents ](#Update or delete documents in a backing index)向文档的backing index中提交请求。

> TIP：如果你要频繁的更新或删除时序数据，使用write index的index alias来替换数据流。见[Manage time series data without data streams](#Manage time series data without data streams)。

### Set up a data stream 
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/set-up-a-data-stream.html#set-up-a-data-stream)

&emsp;&emsp;按照下面几个步骤来设置一个数据流

1. [Create an index lifecycle policy](#Create an index lifecycle policy)
2. [Create component templates](#Create component templates(data stream))
3. [Create an index template](#Create an index template(data stream))
4. [Create the data stream](#Create the data stream(data stream))
5. [Secure the data stream](#Secure the data stream)

&emsp;&emsp;你也可以[convert an index alias to a data stream](#Convert an index alias to a data stream)。

>IMPORTANT：如果你使用Fleet或者Elastic Agent，可以跳过这个教程。Fleet和Elastic Agent会为你设置数据流。见Fleet的[data streams](https://www.elastic.co/guide/en/fleet/8.2/data-streams.html)文档。

#### Create an index lifecycle policy

&emsp;&emsp;尽管是可选的，我们建议你使用ILM来实现数据流的backing  indice的管理自动化。ILM需要一个索引生命周期策略。

&emsp;&emsp;在Kibana中创建一个索引生命周期策略，打开主菜单然后选择**Stack Management > Index Lifecycle Policies**，点击**Create policy**。

&emsp;&emsp;你也可以通过[create lifecycle policy API](#Create or update lifecycle policy API)创建。

```json
PUT _ilm/policy/my-lifecycle-policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_primary_shard_size": "50gb"
          }
        }
      },
      "warm": {
        "min_age": "30d",
        "actions": {
          "shrink": {
            "number_of_shards": 1
          },
          "forcemerge": {
            "max_num_segments": 1
          }
        }
      },
      "cold": {
        "min_age": "60d",
        "actions": {
          "searchable_snapshot": {
            "snapshot_repository": "found-snapshots"
          }
        }
      },
      "frozen": {
        "min_age": "90d",
        "actions": {
          "searchable_snapshot": {
            "snapshot_repository": "found-snapshots"
          }
        }
      },
      "delete": {
        "min_age": "735d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

#### Create component templates(data stream)

&emsp;&emsp;data stream需要匹配一个索引模板（Index template）。在大多数情况下，你会使用一个或者多个组件模版component templates来组合成一个索引模板。你通常会为mappings跟Index settingss使用各自的component templates。这可以让你在多个索引模板中复用component templates。

&emsp;&emsp;当你创建组件模版时，包括：

- 为`@timestamp`字段定义mapping类型，[date](#Date field type)或者[date_nanos](#Date nanoseconds field type)。如果你不指定，Elasticsearch会默认将这个字段作为`date`类型的字段
- 索引设置`index.lifecycle.name`中给定生命周期策略的名称

> TIP：使用[Elastic Common Schema (ECS)](https://www.elastic.co/guide/en/ecs/8.2/ecs-reference.html)来映射你的字段类型。ECS字段默认跟一些Elastic Stack features 集成
> 如果你不确定如何映射你的字段类型，你可以在查询期间使用[runtime fields](#Define runtime fields in a search request)从非结构化的内容[unstructured content](#Wildcard field type)中提取字段。例如你可以将log message索引到一个`wildcard`字段，随后在查询期间从这个字段中提取IP地址和其他数据。

&emsp;&emsp;若要创建在kibana中创建一个组件模版，打开主菜单然后跳转到**Stack Management > Index Management**。在**Index Templates**视图中，点击**Create component template**。

&emsp;&emsp;你也可以使用[create component template API](#Create or update component template API)创建组件模版。

```text
# Creates a component template for mappings
PUT _component_template/my-mappings
{
  "template": {
    "mappings": {
      "properties": {
        "@timestamp": {
          "type": "date",
          "format": "date_optional_time||epoch_millis"
        },
        "message": {
          "type": "wildcard"
        }
      }
    }
  },
  "_meta": {
    "description": "Mappings for @timestamp and message fields",
    "my-custom-meta-field": "More arbitrary metadata"
  }
}

# Creates a component template for index settings
PUT _component_template/my-settings
{
  "template": {
    "settings": {
      "index.lifecycle.name": "my-lifecycle-policy"
    }
  },
  "_meta": {
    "description": "Settings for ILM",
    "my-custom-meta-field": "More arbitrary metadata"
  }
}
```

#### Create an index template(data stream)

&emsp;&emsp;若要使用你的组件模版来创建一个索引模板，需要指定：

- 一个或者多个`index_patterns`来匹配data stream的名称。我们建议使用[data stream naming scheme](https://www.elastic.co/guide/en/fleet/8.2/data-streams.html#data-streams-naming-scheme)
- 这个模版是data stream可以使用的（用`data_stream`字段表示该模版可以用于数据流）
- 包含你mapping跟索引设置Index settings的组件模版
- 优先级大于`200`，以避免与内置模板发生冲突。见[Avoid index pattern collisions](#Avoid index pattern collisions))

&emsp;&emsp;若要在Kibana中创建一个index template，那么打开主菜单然后跳转到Stack Management > Index Management。在Index Templates视图中，点击Create template。

&emsp;&emsp;你也可以使用[create index template API](#Create or update index template API)。包含`data_stream` object使得模版可以用于data streams。

```text
PUT _index_template/my-index-template
{
  "index_patterns": ["my-data-stream*"],
  "data_stream": { },
  "composed_of": [ "my-mappings", "my-settings" ],
  "priority": 500,
  "_meta": {
    "description": "Template for my time series data",
    "my-custom-meta-field": "More arbitrary metadata"
  }
}
```

#### Create the data stream(data stream)

&emsp;&emsp;[Indxing requests](#Add documents to a data stream)将文档添加到data stream中。这些请求必须使用`create`类型中的`op_type`。文档中必须包含`@timestamp`字段。

&emsp;&emsp;若要自动创建你的data stream，提交一个索引请求并指定目标data stream的名称。名称必须能匹配到索引模板中的`index_patterns`。

```text
PUT my-data-stream/_bulk
{ "create":{ } }
{ "@timestamp": "2099-05-06T16:21:15.000Z", "message": "192.0.2.42 - - [06/May/2099:16:21:15 +0000] \"GET /images/bg.jpg HTTP/1.0\" 200 24736" }
{ "create":{ } }
{ "@timestamp": "2099-05-06T16:25:42.000Z", "message": "192.0.2.255 - - [06/May/2099:16:25:42 +0000] \"GET /favicon.ico HTTP/1.0\" 200 3638" }

POST my-data-stream/_doc
{
  "@timestamp": "2099-05-06T16:21:15.000Z",
  "message": "192.0.2.42 - - [06/May/2099:16:21:15 +0000] \"GET /images/bg.jpg HTTP/1.0\" 200 24736"
}
```

&emsp;&emsp;你也可以使用[create data stream API](#Create data stream API)手动创建时stream。这个stream的名称仍然必须能匹配到索引模板中的`index_patterns`。

```text
PUT _data_stream/my-data-stream
```

#### Secure the data stream

&emsp;&emsp;使用[index privilege](#Indices privileges)来控制对data stream的访问。保证data stream跟它的backing indices有相同的权限。

&emsp;&emsp;见[Data stream privileges](#Data stream privileges)中的例子。

#### Convert an index alias to a data stream

&emsp;&emsp;在Elasticsearch7.9之前，你通常使用[index alias with a write index](#Manage time series data without data streams)来管理时序数据。Data stream替代了这个功能，更低的维护成本以及自动跟[data tiers](#Data tiers)集成。

&emsp;&emsp;若要将带有writer Index的索引别名转化为一个data stream，并且使用相同的名称，请使用[migrate to data stream API](#Migrate to data stream API)。在传化期间，别名的索引变成了stream中的隐藏的backing indices。索引别名中的writer Index变成了stream的writer Index。这个stream依然要求一个能匹配的索引模板，并且这个索引模板能适用于data stream（模版中要有`data_stream`字段）。

```text
POST _data_stream/_migrate/my-time-series-data
```

#### Get information about a data stream

&emsp;&emsp;若要在Kibana中获取data stream的信息，打开主菜单然后跳转到**Stack Management > Index Management**，在**Data Streams**视图中，点击data stream的名称。

&emsp;&emsp;你可以使用[get data stream API](#Get data stream API)。

```text
GET _data_stream/my-data-stream
```

#### Delete a data stream

&emsp;&emsp;若要在kibana中删除一个data stream和它的backing indices。打开主菜单然后跳转到**Stack Management > Index Management**，在**Data Streams**视图中，点击trash icon。当你有这个data stream的`delete_index`的[security privilege ](#Security privileges)才能看到这个图标。

&emsp;&emsp;你也可以使用[delete data stream API](#Delete data stream API)实现

```text
DELETE _data_stream/my-data-stream
```

### Use a data stream
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/use-a-data-stream.html#manually-roll-over-a-data-stream)

&emsp;&emsp;在[set up a data stream](#Set up a data stream)之后，你可以：

- [Add documents to a data stream](#Add documents to a data stream)
- [Search a data stream](#Search a data stream)
- [Get statistics for a data stream](#Get statistics for a data stream)
- [Manually roll over a data stream](#Manually roll over a data stream)
- [Open closed backing indices](#Open closed backing indices)
- [Reindex with a data stream](#Reindex with a data stream)
- [Update documents in a data stream by query](#Update documents in a data stream by query)
- [Delete documents in a data stream by query](#Delete documents in a data stream by query)
- [Update or delete documents in a backing index](######## Update or delete documents in a backing index)

#### Add documents to a data stream

&emsp;&emsp;若要添加一篇独立的文档，使用[index API](#Index API)。支持[Ingest pipeline](#Ingest pipelines)。

```text
POST /my-data-stream/_doc/
{
  "@timestamp": "2099-03-08T11:06:07.000Z",
  "user": {
    "id": "8a4f500d"
  },
  "message": "Login successful"
}
```

&emsp;&emsp;你不能使用index API中的`PUT/<target>/_doc/<_id>`这种请求格式将一篇文档添加到data stream中。若要指定文档ID，使用`PUT/<target>/_create/<_id>`这种格式刷。只支持`create`类型中的[op_type](#Index API)。

&emsp;&emsp;若要一次请求添加多篇文档，使用[bulk API](#Bulk API)。只支持`create`选项。

```text
PUT /my-data-stream/_bulk?refresh
{"create":{ }}
{ "@timestamp": "2099-03-08T11:04:05.000Z", "user": { "id": "vlb44hny" }, "message": "Login attempt failed" }
{"create":{ }}
{ "@timestamp": "2099-03-08T11:06:07.000Z", "user": { "id": "8a4f500d" }, "message": "Login successful" }
{"create":{ }}
{ "@timestamp": "2099-03-09T11:07:08.000Z", "user": { "id": "l7gk7f82" }, "message": "Logout successful" }
```

#### Search a data stream

&emsp;&emsp;data stream支持以下查询：

- [Search](#Search API)
- [Async search](#Async search)
- [Multi search](#Multi search API)
- [Field capabilities](#Field capabilities API)
- [EQL search](#EQL search API)

#### Get statistics for a data stream

&emsp;&emsp;使用[data stream stats API](#Data stream stats API)获取一个或多个data stream的统计数据。

```text
GET /_data_stream/my-data-stream/_stats?human=true
```

#### Manually roll over a data stream

&emsp;&emsp;使用[rollover API](#Rollover API)手动[roll over](#Rollover)一个data stream：

```text
POST /my-data-stream/_rollover/
```

#### Open closed backing indices

&emsp;&emsp;你不能搜索一个[closed](#Close index API) backing index，即使是搜索对应的data stream也不行。你不能[update](#Update documents in a data stream by query)或[delete](#Delete documents in a data stream by query)关闭的索引中的文档。

&emsp;&emsp;若要重新打开一个关闭的backing index，向对应的索引直接提交一个[open index API request](#Open index API)：

```text
POST /.ds-my-data-stream-2099.03.07-000001/_open/
```

&emsp;&emsp;若要重新打开data stream中所有已关闭的索引，向这个stream提交一个open index API 请求：

```text
POST /my-data-stream/_open/
```

#### Reindex with a data stream

&emsp;&emsp;使用[reindex API](#Reindex API)从一个现有的索引、别名。或者data stream中拷贝文档到一个data stream中。因为data stream是[append-only](#Append-only)，因此reindex到一个data stream时必须使用`create`类型为`op_type`的值。reindex不能更新现有文档到一个data stream中。

```text
POST /_reindex
{
  "source": {
    "index": "archive"
  },
  "dest": {
    "index": "my-data-stream",
    "op_type": "create"
  }
}
```

#### Update documents in a data stream by query

&emsp;&emsp;使用[update by query API](#Update By Query API)将满足Query的文档更新到一个data stream中。

```text
POST /my-data-stream/_update_by_query
{
  "query": {
    "match": {
      "user.id": "l7gk7f82"
    }
  },
  "script": {
    "source": "ctx._source.user.id = params.new_id",
    "params": {
      "new_id": "XgdX0NoX"
    }
  }
}
```

#### Delete documents in a data stream by query

&emsp;&emsp;使用[delete by query API](#Delete by query API)将满足Query的文档从data stream中删除。

#### Update or delete documents in a backing index

&emsp;&emsp;如果有需要的话，你可以通过往包含文档的backing index中发送请求来更新/删除data stream中的文档。你需要：

- [Document ID](#\_id field)
- 包含文档的backing index的名称
- 如果是更新文档，需要[sequence number and primary term](#Optimistic concurrency control)

&emsp;&emsp;若要获取这些信息，使用一个[search request](#Search a data stream)：

```text
GET /my-data-stream/_search
{
  "seq_no_primary_term": true,
  "query": {
    "match": {
      "user.id": "yWIumJd7"
    }
  }
}
```

&emsp;&emsp;响应：

```text
{
  "took": 20,
  "timed_out": false,
  "_shards": {
    "total": 3,
    "successful": 3,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 1,
      "relation": "eq"
    },
    "max_score": 0.2876821,
    "hits": [
      {
        "_index": ".ds-my-data-stream-2099.03.08-000003",      
        "_id": "bfspvnIBr7VVZlfp2lqX",              
        "_seq_no": 0,                               
        "_primary_term": 1,                         
        "_score": 0.2876821,
        "_source": {
          "@timestamp": "2099-03-08T11:06:07.000Z",
          "user": {
            "id": "yWIumJd7"
          },
          "message": "Login successful"
        }
      }
    ]
  }
}
```

&emsp;&emsp;第18行，包含文档的backing index
&emsp;&emsp;第19行，文档的Document ID
&emsp;&emsp;第20行，文档目前的序号
&emsp;&emsp;第21行，文档的primary term

&emsp;&emsp;若要更新文档，使用[index API](#Index-API)并且携带合法的`if_seq_no`和`if_primary_term`参数  ：

```text
PUT /.ds-my-data-stream-2099-03-08-000003/_doc/bfspvnIBr7VVZlfp2lqX?if_seq_no=0&if_primary_term=1
{
  "@timestamp": "2099-03-08T11:06:07.000Z",
  "user": {
    "id": "8a4f500d"
  },
  "message": "Login successful"
}
```

&emsp;&emsp;若要删除文档，使用[delete API](#Delete API)：

```text
DELETE /.ds-my-data-stream-2099.03.08-000003/_doc/bfspvnIBr7VVZlfp2lqX
```

&emsp;&emsp;如要使用一个请求来删除/更新多篇文档，使用[buli API](#Bulk API)中的`delete`、`index`以及`update`动作。对于`index`，需要包含合法的[if_seq_no and if_primary_term ](#Optimistic concurrency control)参数。

```text
PUT /_bulk?refresh
{ "index": { "_index": ".ds-my-data-stream-2099.03.08-000003", "_id": "bfspvnIBr7VVZlfp2lqX", "if_seq_no": 0, "if_primary_term": 1 } }
{ "@timestamp": "2099-03-08T11:06:07.000Z", "user": { "id": "8a4f500d" }, "message": "Login successful" }
```

### Change mappings and settings for a data stream
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/data-streams-change-mappings-and-settings.html)

&emsp;&emsp;每一个data steam有一个[matching index template](#Create an index template(data stream))。模板中的mappings和index settings会应用到这个stream创建出来的新的backing index。也同时包括stream自动创建出来的的第一个backing index。

&emsp;&emsp;在创建一个data stream之前，我们建议你考虑好模板中的mappings和settings。

&emsp;&emsp;如果你随后需要更改某个data stream的mappings或settings，你有以下的选项：

- [Add a new field mapping to a data stream](#Add a new field mapping to a data stream)
- [Change an existing field mapping in a data stream](#Change an existing field mapping in a data stream)
- [Change a dynamic index setting for a data stream](#Change a dynamic index setting for a data stream)
- [Change a static index setting for a data stream](#Change a static index setting for a data stream)

> TIP：如果你的更改中包含对现有的field mappings或[static index settings](#Index modules)做更改，通常需要reindex来应用到data stream的backing index 中。如果你已经执行了reindex，你可以使用相同的过程来添加新的field mappings以及更改[dynamic index settings](#Index modules)。见[Use reindex to change mappings or settings](#Use reindex to change mappings or settings)。

#### Add a new field mapping to a data stream

&emsp;&emsp;若要向data stream中为一个新的字段添加一个mapping，按照下面的步骤执行：

1. 更新data steam使用的index template。保证新的field mapping添加到这个stream以后创建的backing index中。

&emsp;&emsp;例如，名为`my-data-stream`的data stream现在使用的是名为`my-data-steram-template`的模板。

&emsp;&emsp;下面的[create or update index template](#Index templates)请求向模板中为新的字段`message`添加了一个mapping。

```text
PUT /_index_template/my-data-stream-template
{
  "index_patterns": [ "my-data-stream*" ],
  "data_stream": { },
  "priority": 500,
  "template": {
    "mappings": {
      "properties": {
        "message": {                              
          "type": "text"
        }
      }
    }
  }
}
```

&emsp;&emsp;第7行，为新的字段`message`添加一个mapping。

2. 使用[update mapping API](#Update mapping API)添加新的字段的mapping到data stream中。默认情况下会添加到stream中现有的backing index中，包括write index。

&emsp;&emsp;下面的更新mapping API请求将新的字段`message`添加到名为`my-data-stream`的data stream中。

```text
PUT /my-data-stream/_mapping
{
  "properties": {
    "message": {
      "type": "text"
    }
  }
}
```

&emsp;&emsp;若只向stream中的write index添加mapping，在更新mapping API中添加请求参数`write_index_only`为`true`。

&emsp;&emsp;下面的更新mapping请求中将新的字段`message`的mapping只添加到write index中。没有添加到这个stream的其他backing index中。

```text
PUT /my-data-stream/_mapping?write_index_only=true
{
  "properties": {
    "message": {
      "type": "text"
    }
  }
}
```

#### Change an existing field mapping in a data stream

&emsp;&emsp;每一个[mapping parameter](#Mapping parameters)的文档中都会告知是否可以使用[update mapping API](#Update mapping API)更新现有的字段。若要为现有的字段更新这些参数，按照下面的步骤执行：

1. 更新data steam使用的index template。保证新的field mapping添加到这个stream以后创建的backing index中。

&emsp;&emsp;例如，名为`my-data-stream`的data stream现在使用的是名为`my-data-steram-template`的模板。

&emsp;&emsp;下面的[create or update index template](#Index templates)请求为`host.ip`修改了参数。字段的[ignore_malformed](#ignore_malformed)的值设置为`true`。

```text
PUT /_index_template/my-data-stream-template
{
  "index_patterns": [ "my-data-stream*" ],
  "data_stream": { },
  "priority": 500,
  "template": {
    "mappings": {
      "properties": {
        "host": {
          "properties": {
            "ip": {
              "type": "ip",
              "ignore_malformed": true            
            }
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;第13行，修改`host.ip`的`ignore_malformed`的值为`true`。

2. 使用[update mapping API](#Update mapping API)添加新的字段的mapping到data stream中。默认情况下会添加到stream中现有的backing index中，包括write index。

&emsp;&emsp;下面的[update mapping API](#Update mapping API)请求目标是`my-data-stream`。该请求将`host.ip`字段的`ignore_malformed`的值设置为`true`。

```text
PUT /my-data-stream/_mapping
{
  "properties": {
    "host": {
      "properties": {
        "ip": {
          "type": "ip",
          "ignore_malformed": true
        }
      }
    }
  }
}
```

&emsp;&emsp;若只向stream中的write index添加mapping，在更新mapping API中添加请求参数`write_index_only`为`true`。

&emsp;&emsp;下面的更新mapping请求只对`my-data-stream`的write index中的`host.ip`字段变更。这个更改不会应用到stream的其他backing index。

```text
PUT /my-data-stream/_mapping?write_index_only=true
{
  "properties": {
    "host": {
      "properties": {
        "ip": {
          "type": "ip",
          "ignore_malformed": true
        }
      }
    }
  }
}
```

&emsp;&emsp;除了支持更新mapping参数，我们不建议更改现有字段的mapping或者类型，即使是在data stream匹配到的index template或者它的backing index中。更改现有字段的mapping会invalidate已经索引的数据。

&emsp;&emsp;如果你需要修改现有字段的mapping，那么创建一个新的data stream然后reindex。见[Use reindex to change mappings or settings](#Use reindex to change mappings or settings)。

#### Change a dynamic index setting for a data stream

&emsp;&emsp;若要为data stream更改[dynamic index template](#Index Settings)，按照下面的步骤执行：

1. 更新data steam使用的index template。保证setting能应用到到这个stream以后创建的backing index中。

&emsp;&emsp;例如，名为`my-data-stream`的data stream现在使用的是名为`my-data-steram-template`的模板。

&emsp;&emsp;下面的[create or update index template](#Index templates)请求将模板的`index.refresh_interval`设置为`30s`（30秒）。

```text
PUT /_index_template/my-data-stream-template
{
  "index_patterns": [ "my-data-stream*" ],
  "data_stream": { },
  "priority": 500,
  "template": {
    "settings": {
      "index.refresh_interval": "30s"             
    }
  }
}
```

第8行，将`index.refresh_interval`设置为`30s`。

2. 使用[update mapping API](#Update mapping API)更新data stream的index setting。默认情况下会添加到stream中现有的backing index中，包括write index。

&emsp;&emsp;下面的更新index settings API请求为`my-data-stream`更新设置`index.refresh_interval`。

```text
PUT /my-data-stream/_settings
{
  "index": {
    "refresh_interval": "30s"
  }
}
```

> IMPORTANT：若要修改`index.lifecycle.name`，首先使用[remove policy API](#Remove policy from index API)移除现有的ILM策略。见[Switch lifecycle policies](#Switch lifecycle policies)。


#### Change a static index setting for a data stream

&emsp;&emsp;只有在backing index创建时才能设置[Static index settings](#Index modules)。你不能使用[update index settings API](#Update index settings API)更新static index settings。

&emsp;&emsp;若要将新的static setting应用到以后的backing index上，那么更新data steam使用的index template。更新之后会自动的应用到新创建的backing index上。

&emsp;&emsp;例如，`my-data-stream-template`是一个`my-data-stream`使用的现有的index template。

&emsp;&emsp;下面的[create or update index template API](#Index templates)请求向模板中添加了新的`sort.field`和`sort.order`两个设置。

```text
PUT /_index_template/my-data-stream-template
{
  "index_patterns": [ "my-data-stream*" ],
  "data_stream": { },
  "priority": 500,
  "template": {
    "settings": {
      "sort.field": [ "@timestamp"],             
      "sort.order": [ "desc"]                    
    }
  }
}
```

&emsp;&emsp;第8行， 增加index setting `sort.field`
&emsp;&emsp;第9行，增加index setting `sort.order`

&emsp;&emsp;如果需要的话，你可以[roll over the data stream](#Use a data stream)来马上将设置应用到data stream的write index上。应用到在rollover之后新添加到stream的数据上。然而不会影响data stream中现有的索引和现有的数据上。

&emsp;&emsp;若要应用static settings changes到现有的backing index上，你必须创建一个新的data stream然后reindex。见[Use reindex to change mappings or settings](#Use reindex to change mappings or settings)。

#### Use reindex to change mappings or settings

&emsp;&emsp;你可以使用reindex修改data stream的mapping或settings。这么做通常是因为要对现有的字段的类型做变更或者要为backing index更新static index settings。

&emsp;&emsp;若要reindex某个data stream。首先创建或者更新index template，这样才能包含你想要的修改后的mapping或settings。你随后就可以将现有的数据reindex到一个新的匹配到模板的data stream中。模板中变更的mapping和setting都会应用到添加到新的data stream中的每一篇文档和backing index。

&emsp;&emsp;按照下面的步骤执行：

1. 为新的data stream选择一个名称或者index pattern。这个新的data stream会包含你现有的stream中的数据。

&emsp;&emsp;你可以使用[ resolve index API](#Resolve index API)检查下 名称或者pattern会不会匹配到现有的索引，aliases或者data stream。如果有，你应该考虑使用其他的名称或者pattern。

&emsp;&emsp;下面的resolve index API请求会检查现有的索引，aliases或者data stream有没有以`new-data-stream`开头的。如果没有，index pattern `new-data-stream*` 可以用于创建新的data stream。

```text
GET /_resolve/index/new-data-stream*
```

&emsp;&emsp;API返回了下面的响应，指出现有的目标（existing target）中不匹配这个pattern。

```text
{
  "indices": [ ],
  "aliases": [ ],
  "data_streams": [ ]
}
```

2. 创建或者更新一个index template。这个模板引应该包含你想要应用到新的data stream中的backing index的mappings和settings。

&emsp;&emsp;这个index template必须满足[requirements for a data stream template](#Create an index template(data stream))。他应该在`index_patterns`属性中同样包含之前选择的名称或者index patter。

> TIP：如果你只增加或者修改小部分东西，我们建议你通过拷贝现有的模板，根据你的需要进行修改，然后用于创建一个新的模板。

&emsp;&emsp;例如，名为`my-data-stream`的data stream现在使用的是名为`my-data-steram-template`的模板。

&emsp;&emsp;下面的[create or update index template API ](#Index templates)请求创建了一个新的index template，名为`new-data-stream-template`，`new-data-stream-template`将`my-data-stream-template`作为基准，作出了下面的改动：

- `index_patterns`中匹配以`new-data-stream`开头 的索引或data stream
- `@timestamp`字段使用`date_nanos`字段而不是使用`date`
- 模板中包含了`sort.field`和`sort.order`的index settings，原来的`my-data-stream-template`没有这两个setting

```text
PUT /_index_template/new-data-stream-template
{
  "index_patterns": [ "new-data-stream*" ],
  "data_stream": { },
  "priority": 500,
  "template": {
    "mappings": {
      "properties": {
        "@timestamp": {
          "type": "date_nanos"                 
        }
      }
    },
    "settings": {
      "sort.field": [ "@timestamp"],          
      "sort.order": [ "desc"]                 
    }
  }
}
```

&emsp;&emsp;第10行，`@timestamp`字段的类型修改为`date_nanos`
&emsp;&emsp;第15行，增加index setting `sort.field`
&emsp;&emsp;第16行，增加index setting `sort.order`

3. 使用[create data stream API ](#Create data stream API)手动创建新的data stream。data stream的名称必须匹配定义在index template中的`index_patterns`属性。

&emsp;&emsp;我们不建议[indexing new data to create this data stream](#Create data stream API)。因为随后你将从现有的data stream中的旧数据reindex到这个data stream中。这会导致一个或者多个backing index会同时包含新旧数据。

> IMPORTANT：Mixing new and old data in a data stream
> 尽管新旧数据的混合是安全，但它会影响data retention。如果你要删除旧的索引，你可能会意外的删除一个同时包含新旧数据的索引。为了防止过早的（premature）数据丢失，你需要保留这样的backing index直到里面最新的数据可以被删除。

&emsp;&emsp;下面的create data stream API请求的目标是`new-data-stream`，它会匹配到index template中的`new-data-stream-template`。因为现有的索引或data stream没有使用这个名称，所以这个请求会创建名为`new-data-stream`的data stream。

```text
PUT /_data_stream/new-data-stream
```

4. 如果你不想在新的data stream中混合新旧数据，那么先暂停新文档的索引。尽管新旧数据的混合是安全，但它会影响data retention。如果你要删除旧的索引，你可能会意外的删除一个同时包含新旧数据的索引。为了防止过早的（premature）数据丢失，你需要保留这样的backing index直到里面最新的数据可以被删除。

5. 如果你使用ILM来[automate rollover](#Tutorial: Automate rollover with ILM)，那么降低ILM的poll interval。这能保证在下一次rollover检查之前，当前的write index不会增长的过大。默认情况下，ILM每10分钟检查下rollover的条件。

&emsp;&emsp;下面[cluster update settings API ](#Cluster update settings API)请求将`indices.lifecycle.poll_interval`的值降低为`1m`。

```text
PUT /_cluster/settings
{
  "persistent": {
    "indices.lifecycle.poll_interval": "1m"
  }
}
```

6. 使用`op_type` 中的`create`将你的数据reindex到新的data stream中

&emsp;&emsp;如果你想要按照原来索引的顺序来对数据进行划分，你可以允许多个reindex请求。这些reindex请求可以使用独立的backing index作为输入源。你可以使用[get data stream API](#Get data stream API)查看backing index列表。

&emsp;&emsp;例如，你计划将`my-data-stream` reindex到`new-data-steram`中，然而你想要为`my-data-steram`中的每一个backing index分别提交一个reindex请求，从最旧的backing index开始。这样就能保留原来索引的顺序。

&emsp;&emsp;下面的get data stream API请求查看了`my-data-stream`的信息，包含了一个backing index的列表。

```text
GET /_data_stream/my-data-stream
```

&emsp;&emsp;响应中的`indices`包含了stream中当前的backing index。数组中第一个元素就是最旧的backing index的信息。

```text
{
  "data_streams": [
    {
      "name": "my-data-stream",
      "timestamp_field": {
        "name": "@timestamp"
      },
      "indices": [
        {
          "index_name": ".ds-my-data-stream-2099.03.07-000001", 
          "index_uuid": "Gpdiyq8sRuK9WuthvAdFbw"
        },
        {
          "index_name": ".ds-my-data-stream-2099.03.08-000002",
          "index_uuid": "_eEfRrFHS9OyhqWntkgHAQ"
        }
      ],
      "generation": 2,
      "status": "GREEN",
      "template": "my-data-stream-template",
      "hidden": false,
      "system": false,
      "allow_custom_routing": false,
      "replicated": false
    }
  ]
}
```

&emsp;&emsp;第10行，`my-data-steram`的`indices`数组中第一个条目中包含了这个stream中最旧的名为`.ds-my-data-stream-2099.03.07-000001`的backing index信息。

&emsp;&emsp;下面的[reindex API](#Reindex API)请求将`.ds-my-data-stream-2099.03.07-000001`中的文档拷贝到`new-data-steam`中。这个请求的`op_type`是`create`。

```text
POST /_reindex
{
  "source": {
    "index": ".ds-my-data-stream-2099.03.07-000001"
  },
  "dest": {
    "index": "new-data-stream",
    "op_type": "create"
  }
}
```

&emsp;&emsp;你也可以在每次的请求中使用一个query对一个文档子集进行reindex。

&emsp;&emsp;下面的[reindex API](#Reindex API)请求将`my-data-steram`中的文档拷贝到`new-data-stream`中，请求中使用了[range query](#Range query)，只reindex了上周的文档。注意的是这个请求的`op_type`是`craete`。

```text
POST /_reindex
{
  "source": {
    "index": "my-data-stream",
    "query": {
      "range": {
        "@timestamp": {
          "gte": "now-7d/d",
          "lte": "now/d"
        }
      }
    }
  },
  "dest": {
    "index": "new-data-stream",
    "op_type": "create"
  }
}
```

7. 如果你之前修改了ILM的poll interval，那么在reindex完成后改回到原来的值。防止master node上没必要的负载。

&emsp;&emsp;下面的cluster update settings API请求将`indices.lifecycle.poll_interval`设置为默认值。

```text
PUT /_cluster/settings
{
  "persistent": {
    "indices.lifecycle.poll_interval": null
  }
}
```

8. 使用新的data stream恢复索引。在这个stream上的查询会使用新的数据以及reindex的数据。

9. 一旦你验证完所有的reindex数据在新的data stream是可见的，你就可以安全的移除旧的stream。

&emsp;&emsp;下面的[delete data stream API ](#Delete data stream API)请求删除了`my-data-steam`，这个请求同样会删除它包含的所有的backing index和其他任何数据。

```text
DELETE /_data_stream/my-data-stream
```

## Ingest pipelines
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ingest.html)

&emsp;&emsp;Ingest pipelines能让你在索引之前对你的数据执行常用的转换（common transformation）。例如你可以使用pipeline来移除字段，从文本中提取值，并且丰富你的数据（enrich your data）。

&emsp;&emsp;一个pipeline由一系列称为[processors](#Ingest processor reference)的可配置的任务组成。每一个processor顺序的运行，对incoming document做对应的更改。在运行完processor后，Elasticsearch将转换后的document添加到data stream或者索引中。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ingest-process.svg">

&emsp;&emsp;你可以使用Kibana的**Ingest Pipelines** 功能或者[ingest APIS](#Ingest APIs)来管理ingest pipeline。Elasticsearch将pipeline存储在[cluster state](#Cluster state API)中。

#### Prerequisites(ingest pipeline)

- 节点角色（node role）为[Ingest node](#Ingest node)负责pipeline的处理。若要使用ingest pipeline，你的集群中必须至少有一个节点角色为`ingest`的节点。对于繁重的ingest负载，我们建议你创建一个[dedicated ingest nodes](#Ingest node)
- 如果开启了Elasticsearch security feature，你必须有`manage_pipeline`的[cluster privilege](#Cluster privileges)才能管理ingest pipeline。若要使用Kibana的**Ingest Pipeline** 功能，你也需要有`cluster:monitor/nodes/info`的cluster privilege。

#### Create and manage pipelines

&emsp;&emsp;在Kibana中，打开主菜单并且点击**Stack Management > Ingest Pipelines**。从下拉菜单中，你可以：

- 查看你的pipeline列表以及drill down后查看详情
- 编辑或者克隆现有的pipeline
- 删除pipeline

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ingest-pipeline-list.png">


&emsp;&emsp;若要创建一个pipeline，点击**Create pipeline > New pipeline**。可以查看[Example: Parse logs](#Example: Parse logs in the Common Log Format)这个示例教程。

> NOTE：**New pipeline from CSV**可以让你使用一个CSV来创建一个ingest pipeline，它将自定义的数据映射到[Elastic Common Schema (ECS)](https://www.elastic.co/guide/en/ecs/8.2/index.html)。将你的数据映射到ECS使得数据更易于查询并且能让你从其他数据集中复用（reuse）可视化。见[Map custom data to ECS](https://www.elastic.co/guide/en/ecs/8.2/ecs-converting.html)。

&emsp;&emsp;你也可以使用[ingest APIs](#Ingest APIs)来创建和管理pipeline。下面的[create pipeline API](#Create or update pipeline API)请求中创建了一个包含2个[set ](#Set processor) processor ，以及一个[lowercase](#Lowercase processor) processor的pipeline。这三个processor会根据指定的顺序有序执行。

```text
PUT _ingest/pipeline/my-pipeline
{
  "description": "My optional pipeline description",
  "processors": [
    {
      "set": {
        "description": "My optional processor description",
        "field": "my-long-field",
        "value": 10
      }
    },
    {
      "set": {
        "description": "Set 'my-boolean-field' to true",
        "field": "my-boolean-field",
        "value": true
      }
    },
    {
      "lowercase": {
        "field": "my-keyword-field"
      }
    }
  ]
}
```

#### Manage pipeline versions

&emsp;&emsp;当你创建或者更新一个pipeline时，你可以指定一个可选的`version`的整数值。你可以使用version以及参数[if_version](#Create or update pipeline API)来有条件的更新pipeline。一次成功的更新操作会提高pipeline的版本号。

```text
PUT _ingest/pipeline/my-pipeline-id
{
  "version": 1,
  "processors": [ ... ]
}
```

&emsp;&emsp;若要通过API不设置`version`，可以在替换或者更新pipeline时不指定`versopm`参数。

#### Test a pipeline

&emsp;&emsp;在生产中使用pipeline前，我们建议你先用样例文档（sample document）进行测试。当在Kibana中创建或者编辑了一个pipeline，点击**Document** tab页的**Add documents**，提供样例文档并点击**Run the pipeline**。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/test-a-pipeline.png">

&emsp;&emsp;你也可以使用[simulate pipeline API](#Simulate pipeline API)来测试pipeline。你可以在请求路径中指定一个配置好的pipeline。例如下面的请求中测试了`my-pipeline`。

```text
POST _ingest/pipeline/my-pipeline/_simulate
{
  "docs": [
    {
      "_source": {
        "my-keyword-field": "FOO"
      }
    },
    {
      "_source": {
        "my-keyword-field": "BAR"
      }
    }
  ]
}s
```

&emsp;&emsp;或者你可以指定一个在请求body中指定pipeline和它的processor。

```text
POST _ingest/pipeline/_simulate
{
  "pipeline": {
    "processors": [
      {
        "lowercase": {
          "field": "my-keyword-field"
        }
      }
    ]
  },
  "docs": [
    {
      "_source": {
        "my-keyword-field": "FOO"
      }
    },
    {
      "_source": {
        "my-keyword-field": "BAR"
      }
    }
  ]
}
```

&emsp;&emsp;这个API会返回转换后的文档：

```text
{
  "docs": [
    {
      "doc": {
        "_index": "_index",
        "_id": "_id",
        "_source": {
          "my-keyword-field": "foo"
        },
        "_ingest": {
          "timestamp": "2099-03-07T11:04:03.000Z"
        }
      }
    },
    {
      "doc": {
        "_index": "_index",
        "_id": "_id",
        "_source": {
          "my-keyword-field": "bar"
        },
        "_ingest": {
          "timestamp": "2099-03-07T11:04:04.000Z"
        }
      }
    }
  ]
}
```

#### Add a pipeline to an indexing request

&emsp;&emsp;在请求参数中使用`pipeline`对单篇或者多篇文档应用一个pipeline

```text
POST my-data-stream/_doc?pipeline=my-pipeline
{
  "@timestamp": "2099-03-07T11:04:05.000Z",
  "my-keyword-field": "foo"
}

PUT my-data-stream/_bulk?pipeline=my-pipeline
{ "create":{ } }
{ "@timestamp": "2099-03-07T11:04:06.000Z", "my-keyword-field": "foo" }
{ "create":{ } }
{ "@timestamp": "2099-03-07T11:04:07.000Z", "my-keyword-field": "bar" }
```

&emsp;&emsp;你也可以在[update by query](#Update By Query API)或者 [reindex](#Reindex API) API中使用`pipeline`参数：

```text
POST my-data-stream/_update_by_query?pipeline=my-pipeline

POST _reindex
{
  "source": {
    "index": "my-data-stream"
  },
  "dest": {
    "index": "my-new-data-stream",
    "op_type": "create",
    "pipeline": "my-pipeline"
  }
}
```

#### Set a default pipeline

&emsp;&emsp;使用索引设置[index.default_pipeline](#index.default_pipeline)来设置一个默认的pipeline。如果没有指定`pipeline`参数，Elasticsearch则会应用（apply ）这个默认的pipeline。

#### Pipelines for Beats

&emsp;&emsp;如果为Elastic Beat添加一个ingest pipeline，可以在`<BEAT_NAME>.yml`中的`output.elasticsearch`下指定参数`pipeline`。

```text
output.elasticsearch:
  hosts: ["localhost:9200"]
  pipeline: my-pipeline
```

#### Pipelines for Fleet and Elastic Agent

&emsp;&emsp;[Fleet](https://www.elastic.co/guide/en/fleet/8.2/index.html)会为integration自动的添加ingest pipeline。Fleet会使用包含了[pipeline index settings](#Set a default pipeline)的[index templates](#Index templates)来 应用pipeline。Elasticsearch会基于[stream’s naming scheme](https://www.elastic.co/guide/en/fleet/8.2/data-streams.html#data-streams-naming-scheme)匹配到这些模板到你的Fleet data streams中。

> WARNING：不要修改Fleet的ingest pipeline或者对你的Fleet integration使用自定义的pipeline。这么做会破坏你的Fleet data streams。

&emsp;&emsp;Fleet不会为**Custom logs** integration提供一个ingest pipeline。你可以使用两种方法中的一种来为你的integration安全的指定一个pipeline：[index template](#Option 1: Index template) 或者一个[custom configuration](#Option 2: Custom configuration)。

##### Option 1: Index template

1. [Create](#Create and manage pipelines)并且[test](#Test a pipeline)你的ingest pipeline。将你的pipeline命名为`logs-<dataset-name>-default`。这样方便为你的integration进行追踪（track）。

```text
PUT _ingest/pipeline/logs-my_app-default
{
  "description": "Pipeline for `my_app` dataset",
  "processors": [ ... ]
}
```
2. 创建一个[index template](#Index templates)，包含在index setting中设置的pipeline的[index.default_pipeline](#index.default_pipeline)和[index.final_pipeline](#index.final_pipeline)。保证模板中开启了[data stream](#Create an index template(data stream))。这个模板的index pattern应该匹配`logs-<dataset-name>-*`。

&emsp;&emsp;你可以使用Kibana的[Index Management](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-mgmt.html#manage-index-templates)功能来[create index template API](#Create or update index template API)。

&emsp;&emsp;例如，下面的请求创建了一个模板来匹配`logs-my_app-*`。这个模板使用了component template，它包含了`index.default_pipeline`这个index setting。

```text
# Creates a component template for index settings
PUT _component_template/logs-my_app-settings
{
  "template": {
    "settings": {
      "index.default_pipeline": "logs-my_app-default",
      "index.lifecycle.name": "logs"
    }
  }
}

# Creates an index template matching `logs-my_app-*`
PUT _index_template/logs-my_app-template
{
  "index_patterns": ["logs-my_app-*"],
  "data_stream": { },
  "priority": 500,
  "composed_of": ["logs-my_app-settings", "logs-my_app-mappings"]
}
```

3. 在Fleet中增加或者编辑**Custom logs** integration时，点击**Configure integration > Custom log file > Advanced options**。
4. 在**Dataset name**中，指定你的数据集名称，Fleet会为integration添加新的数据并输出（resulting）`logs-<dataset-name>-default` data stream。

&emsp;&emsp;例如，如果你的数据集名称是`my_app`，Fleet将新的数据添加到`logs-my_app-default`数据流。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/custom-logs.png">

5. 使用[rollover API](#Rollover API)来roll over你的数据流。这将保证Elasticsearch为integration将index template和pipeline设置应用到新的数据上。

```text
POST logs-my_app-default/_rollover/
```

##### Option 2: Custom configuration

1. [Create](#Create and manage pipelines)和[test](#Test a pipeline)你的ingest pipeline。默认命名pipeline的名称为`logs-<dataset-name>-default`。这使得让你的integration更易于追踪（track）。

&emsp;&emsp;例如，下面的请求中为数据集`my-app`创建了一个pipeline。这个pipeline的名称是`logs-my_app-default`。

```text
PUT _ingest/pipeline/logs-my_app-default
{
  "description": "Pipeline for `my_app` dataset",
  "processors": [ ... ]
}
```

2. 当你在Fleet中添加或者编辑你的**Custom logs** integration。点击**Configure integration > Custom log file > Advanced options**。

3. 在**Dataset name**，指定你的数据集名称。Fleet将为integration添加新的数据并且输出到`logs-<dataset-name>-default` 数据流中。

&emsp;&emsp;例如，如果你的数据集名称是`my_app`，Fleet将添加新的数据到`logs-my_app-default` 数据流中。

4. 在**Custom Configurations**，在`pipeline`策略设置中指定你的pipeline。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/custom-logs-pipeline.png">

##### Elastic Agent standalone

&emsp;&emsp;如果你用standalone方式启动Elastic Agent。你可以使用包含[index.default_pipeline](#index.default_pipeline)和[index.final_pipeline](#index.final_pipeline) index setting的[index template](#Index templates)来应用pipeline。或者你可以在`elastic-agent.yml`中指定`pipeline`策略。见[Install standalone Elastic Agents](https://www.elastic.co/guide/en/fleet/8.2/install-standalone-elastic-agent.html)。

#### Access source fields in a processor

&emsp;&emsp;Processor对incoming document的source field有读写权限。若要在一个processor中访问一个field，使用其字段名。下面的process `set`访问了`my-long-field`。

```text
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "set": {
        "field": "my-long-field",
        "value": 10
      }
    }
  ]
}
```

&emsp;&emsp;你也可以前置（prepend）`_source`前缀。

```text
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "set": {
        "field": "_source.my-long-field",
        "value": 10
      }
    }
  ]
}
```

&emsp;&emsp;使用`.`（dot notation）来访问object fields。

> IMPORTANT：如果你的文档中包含flattened object，使用[dot_expander](#Dot expander processor)先进行expand。其他的ingest processor不能访问flattened object。


```text
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "dot_expander": {
        "description": "Expand 'my-object-field.my-property'",
        "field": "my-object-field.my-property"
      }
    },
    {
      "set": {
        "description": "Set 'my-object-field.my-property' to 10",
        "field": "my-object-field.my-property",
        "value": 10
      }
    }
  ]
}
```

&emsp;&emsp;多个processor参数支持[Mustache template snippets](https://mustache.github.io/)。为了能在template snippet中访问字段值，使用三个大括号（curly brackets）包住字段名。你可以使用template snippets动态的设置字段名。

```text
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "set": {
        "description": "Set dynamic '<service>' field to 'code' value",
        "field": "{{{service}}}",
        "value": "{{{code}}}"
      }
    }
  ]
}
```

#### Access metadata fields in a processor

&emsp;&emsp;Processor能通过名称访问下面的metadata filed：

- \_index
- \_id
- \_routing
- \_dynamic_templates

```text
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "set": {
        "description": "Set '_routing' to 'geoip.country_iso_code' value",
        "field": "_routing",
        "value": "{{{geoip.country_iso_code}}}"
      }
    }
  ]
}
```

&emsp;&emsp;使用一个Mustache template snippet来访问metadata filed的值。例如`{{{_routing}}}`将retrieve一篇文档的 routing value。

```text
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "set": {
        "description": "Use geo_point dynamic template for address field",
        "field": "_dynamic_templates",
        "value": {
          "address": "geo_point"
        }
      }
    }
  ]
}
```

&emsp;&emsp;上面的`set processor`告诉ES如果字段address在索引的映射中还未定义，则使用名为geo_point的动态模板。如果在批量请求中已经定义了address字段的动态模板，这个处理器会覆盖它，但对批量请求中定义的其他动态模板没有影响。

> WARNING： 如果你[automatically generate](#Create document IDs automatically) document id，你不能在processor中使用`{{{_id}}}`，因为Elasticsearch在ingest之后才会自动分配`_id`值。

#### Access ingest metadata in a processor

&emsp;&emsp;Ingest processors可以使用key `_ingest`添加并且访问ingest metadata。

&emsp;&emsp;不同于source和metadata field，Elasticsearch默认不会索引ingest metadata。Elasticsearch同样允许以`_ingest`开头的source fields，如果你的数据中包含了这种source fields，使用`_source._ingest`来访问它们。

&emsp;&emsp;pipeline默认只创建名为`_ingest.timestamp`的ingest metadata字段。这个字段包含的是Elasticsearch收到文档索引请求时的timestamp。若要索引`_ingest.timestamp`或者其他ingest metadata字段，使用`set`processor。

```text
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "set": {
        "description": "Index the ingest timestamp as 'event.ingested'",
        "field": "event.ingested",
        "value": "{{{_ingest.timestamp}}}"
      }
    }
  ]
}
```

#### Handling pipeline failures

&emsp;&emsp;pipeline的processor有序运行的。默认情况下，当其中一个processor允许失败或者遇到一个错误时，pipeline的处理就会停止下来。

&emsp;&emsp;若要忽略某个processor的失败并且运行剩余的 processor。将`ignore_failure`为`true`。

```text
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "rename": {
        "description": "Rename 'provider' to 'cloud.provider'",
        "field": "provider",
        "target_field": "cloud.provider",
        "ignore_failure": true
      }
    }
  ]
}
```

&emsp;&emsp;使用`on_failure`参数指定processor list，在一个processor失败后马上就运行它们。如果指定了`on_failure`，即使没有配置任何processor，Elasticsearch之后会运行pipeline中剩余的processor。

```text
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "rename": {
        "description": "Rename 'provider' to 'cloud.provider'",
        "field": "provider",
        "target_field": "cloud.provider",
        "on_failure": [
          {
            "set": {
              "description": "Set 'error.message'",
              "field": "error.message",
              "value": "Field 'provider' does not exist. Cannot rename to 'cloud.provider'",
              "override": false
            }
          }
        ]
      }
    }
  ]
}
```

&emsp;&emsp;`on_failure`中嵌套processor用于嵌套错误的处理

```text
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "rename": {
        "description": "Rename 'provider' to 'cloud.provider'",
        "field": "provider",
        "target_field": "cloud.provider",
        "on_failure": [
          {
            "set": {
              "description": "Set 'error.message'",
              "field": "error.message",
              "value": "Field 'provider' does not exist. Cannot rename to 'cloud.provider'",
              "override": false,
              "on_failure": [
                {
                  "set": {
                    "description": "Set 'error.message.multi'",
                    "field": "error.message.multi",
                    "value": "Document encountered multiple ingest errors",
                    "override": true
                  }
                }
              ]
            }
          }
        ]
      }
    }
  ]
}
```

&emsp;&emsp;你也可以为一个pipeline指定`on_failure`。如果某个没有设置`on_failure`的processor失败后，Elasticsearch会使用这个pipeline-level的参数作为一个fallback。Elasticsearch不会尝试运行pipeline中剩余的processor。

```text
PUT _ingest/pipeline/my-pipeline
{
  "processors": [ ... ],
  "on_failure": [
    {
      "set": {
        "description": "Index document to 'failed-<index>'",
        "field": "_index",
        "value": "failed-{{{ _index }}}"
      }
    }
  ]
}
```

&emsp;&emsp;pipeline失败后的额外信息可以在document的metadata filed中查看：`on_failure_message`, `on_failure_processor_type`, `on_failure_processor_tag`, and `on_failure_pipeline`。这些字段的信息只能在`on_failure`中才能被访问。

&emsp;&emsp;下面的例子中使用了document的metadata fields，它包含了pipeline 失败后的信息：

```text
PUT _ingest/pipeline/my-pipeline
{
  "processors": [ ... ],
  "on_failure": [
    {
      "set": {
        "description": "Record error information",
        "field": "error_information",
        "value": "Processor {{ _ingest.on_failure_processor_type }} with tag {{ _ingest.on_failure_processor_tag }} in pipeline {{ _ingest.on_failure_pipeline }} failed with message {{ _ingest.on_failure_message }}"
      }
    }
  ]
}
```

#### Conditionally run a processor

&emsp;&emsp;每一个processor都支持可选的`if`条件，使用[Painless script](https://www.elastic.co/guide/en/elasticsearch/painless/8.2/painless-guide.html)编写。如果提供了这个配置，processor只有在`if`条件为`ture`的情况下才会运行。

> IMPORTANT： `if`条件运行于Painless的[ingest processor context](https://www.elastic.co/guide/en/elasticsearch/painless/8.2/painless-ingest-processor-context.html)。在`if`条件中，`ctx`的值是只读的。

```text
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "drop": {
        "description": "Drop documents with 'network.name' of 'Guest'",
        "if": "ctx?.network?.name == 'Guest'"
      }
    }
  ]
}
```

&emsp;&emsp;如果开启了集群设置[script.painless.regex.enabled](#script.painless.regex.enabled)，你可以在`if`条件脚本中使用正则表达式。见[Painless regular expressions](https://www.elastic.co/guide/en/elasticsearch/painless/8.2/painless-regexes.html)了解支持的语法。

> TIP：如果可以的话，避免在`if`条件中使用复杂或者开销大的脚本。然而你可以使用[Kibana consloe](https://www.elastic.co/guide/en/kibana/8.2/console-kibana.html#configuring-console)的 triple quote syntax来编写以及调试larger script。

```text
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "drop": {
        "description": "Drop documents that don't contain 'prod' tag",
        "if": """
            Collection tags = ctx.tags;
            if(tags != null){
              for (String tag : tags) {
                if (tag.toLowerCase().contains('prod')) {
                  return false;
                }
              }
            }
            return true;
        """
      }
    }
  ]
}
```

&emsp;&emsp;你可以使用[stored script](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-scripting-stored-scripts.html)作为`if`条件中的脚本。

```text
PUT _scripts/my-prod-tag-script
{
  "script": {
    "lang": "painless",
    "source": """
      Collection tags = ctx.tags;
      if(tags != null){
        for (String tag : tags) {
          if (tag.toLowerCase().contains('prod')) {
            return false;
          }
        }
      }
      return true;
    """
  }
}

PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "drop": {
        "description": "Drop documents that don't contain 'prod' tag",
        "if": { "id": "my-prod-tag-script" }
      }
    }
  ]
}
```

&emsp;&emsp;Incoming document通常包含object field。如果一个processor script尝试访问一个字段，但是parent object不存在，Elasticsearch会返回一个`NullPointerException`。若要避免这些异常，使用[null safe operators](https://www.elastic.co/guide/en/elasticsearch/painless/8.2/painless-operators-reference.html#null-safe-operator)，例如`?.`，你的script就可以变的null safe。

&emsp;&emsp;例如，`ctx.network?.name.equalsIgnoreCase('Guest')` 不是null safe。`ctx.network?.name`可能返回null，将脚本重写为`'Guest'.equalsIgnoreCase(ctx.network?.name)`的话就是null safe，因为`Guest`总是non-null。

&emsp;&emsp;如果无法确保是null safe，就添加一个显示的null检查。

```text
PUT _ingest/pipeline/my-pipeline
{
  "processors": [
    {
      "drop": {
        "description": "Drop documents that contain 'network.name' of 'Guest'",
        "if": "ctx.network?.name != null && ctx.network.name.contains('Guest')"
      }
    }
  ]
}
```

#### Conditionally apply pipelines

&emsp;&emsp;基于你的规则，在[pipeline](#Pipeline processor)中使用`if` 条件来为你的文档应用pipeline。你可以在[index template](#Index templates)中使用这个pipeline作为[default pipeline](#Set a default pipeline)。

```text
PUT _ingest/pipeline/one-pipeline-to-rule-them-all
{
  "processors": [
    {
      "pipeline": {
        "description": "If 'service.name' is 'apache_httpd', use 'httpd_pipeline'",
        "if": "ctx.service?.name == 'apache_httpd'",
        "name": "httpd_pipeline"
      }
    },
    {
      "pipeline": {
        "description": "If 'service.name' is 'syslog', use 'syslog_pipeline'",
        "if": "ctx.service?.name == 'syslog'",
        "name": "syslog_pipeline"
      }
    },
    {
      "fail": {
        "description": "If 'service.name' is not 'apache_httpd' or 'syslog', return a failure message",
        "if": "ctx.service?.name != 'apache_httpd' && ctx.service?.name != 'syslog'",
        "message": "This pipeline requires service.name to be either `syslog` or `apache_httpd`"
      }
    }
  ]
}
```

#### Get pipeline usage statistics

&emsp;&emsp;使用[node stats](#Nodes stats API) API来获取全局的以及per-pipeline的ingest 统计数据。来检测出哪些pipeline运行频次最多或者处理时间耗时最多。

### Example: Parse logs in the Common Log Format
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/common-log-format-example.html)

&emsp;&emsp;在这个教程中，你将使用[ingest pipeline](#Ingest pipelines)以及[common log format](https://en.wikipedia.org/wiki/Common_Log_Format)，在索引前对服务器日志进行解析。在开始之前，先查看使用ingest pipeline前的[prerequisite](#Prerequisites(ingest pipeline))。

&emsp;&emsp;你要解析的日志如下所示：

```text
212.87.37.154 - - [30/May/2099:16:21:15 +0000] \"GET /favicon.ico HTTP/1.1\"
200 3638 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6)
AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\"
```

&emsp;&emsp;这些日志中包含了timestamp，IP address，以及user agent。你要让这三个信息在Elasticsearch有自己的字段，使得可以用于快速的查询以及可视化。你同样想知道这些请求的出处。

1. 在Kibana中，打开主菜单并且点击**Stack Management > Ingest Pipelines**。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ingest-pipeline-list.png">

2. 点击**Create pipeline > New pipeline**
3. 给这个pipeline提供一个名称以及描述
4. 添加一个[grok processor](#Grok processor)来解析日志消息。
   1. 点击**Add a processor**并且选择Grok processor类型
   2. 设置**Field**为`message`并且**Patterns**设置为下面的[grok pattern](#Grok basics)
   3. 点击**Add**保存processor
   4. 设置processor的描述信息为`Extract fields from 'message'`
```text
%{IPORHOST:source.ip} %{USER:user.id} %{USER:user.name} \\[%{HTTPDATE:@timestamp}\\] \"%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}\" %{NUMBER:http.response.status_code:int} (?:-|%{NUMBER:http.response.body.bytes:int}) %{QS:http.request.referrer} %{QS:user_agent}
```
5. 为timestamp，IP address，和user agent增加processor。如下配置processor：

|             Processor type             |   Field    |          Additional options           |                   Description                   |
| :------------------------------------: | :--------: | :-----------------------------------: | :---------------------------------------------: |
|       [Date](#Date processor)       | @timestamp | **Formats**: `dd/MMM/yyyy:HH:mm:ss Z` | Format '@timestamp' as 'dd/MMM/yyyy:HH:mm:ss Z' |
|      [GeoIP](#GeoIP processor)      | source.ip  |    **Target field**: `source.geo`     |   Add 'source.geo' GeoIP data for 'source.ip'   |
| [User agent](#User agent processor) | user_agent |                                       |        Extract fields from 'user_agent'         |

&emsp;&emsp;如下所示：

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ingest-pipeline-processor.png">

&emsp;&emsp;下面四个processor将会顺序执行：

&emsp;&emsp;Grok > Date > GeoIP > User agent

&emsp;&emsp;你也可以使用箭头图标重新排序

&emsp;&emsp;或者，你可以点击**Import processors**链接然后定义如下的JSON：

```text
{
  "processors": [
    {
      "grok": {
        "description": "Extract fields from 'message'",
        "field": "message",
        "patterns": ["%{IPORHOST:source.ip} %{USER:user.id} %{USER:user.name} \\[%{HTTPDATE:@timestamp}\\] \"%{WORD:http.request.method} %{DATA:url.original} HTTP/%{NUMBER:http.version}\" %{NUMBER:http.response.status_code:int} (?:-|%{NUMBER:http.response.body.bytes:int}) %{QS:http.request.referrer} %{QS:user_agent}"]
      }
    },
    {
      "date": {
        "description": "Format '@timestamp' as 'dd/MMM/yyyy:HH:mm:ss Z'",
        "field": "@timestamp",
        "formats": [ "dd/MMM/yyyy:HH:mm:ss Z" ]
      }
    },
    {
      "geoip": {
        "description": "Add 'source.geo' GeoIP data for 'source.ip'",
        "field": "source.ip",
        "target_field": "source.geo"
      }
    },
    {
      "user_agent": {
        "description": "Extract fields from 'user_agent'",
        "field": "user_agent"
      }
    }
  ]

}
```

6. 若要测试pipeline，点击**Add documents**。
7. 在**Document** tab页，提供了一个文档文档样例用于测试：

```text
[
  {
    "_source": {
      "message": "212.87.37.154 - - [05/May/2099:16:21:15 +0000] \"GET /favicon.ico HTTP/1.1\" 200 3638 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\""
    }
  }
]
```

8. 点击**Run the pipeline**然后验证pipeline是否按预期运行
9. 如果一切看着都可以，关闭面板，然后点击**Create pipeline**。

&emsp;&emsp;现在你已经准备就绪将日志数据索引到[data stream](#Data streams)中。

10. 创建一个[index template](#Index templates)并开启[data stream](#Create an index template(data stream))。

```text
PUT _index_template/my-data-stream-template
{
  "index_patterns": [ "my-data-stream*" ],
  "data_stream": { },
  "priority": 500
}
```

11. 使用你创建的pipeline索引文档。

```text
POST my-data-stream/_doc?pipeline=my-pipeline
{
  "message": "89.160.20.128 - - [05/May/2099:16:21:15 +0000] \"GET /favicon.ico HTTP/1.1\" 200 3638 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\""
}
```

12. 若要进行校验，查询data stream来检索文档。下面的查询使用[filter_path](#Response Filtering)来只返回[document source](#\_source field)。

```text
GET my-data-stream/_search?filter_path=hits.hits._source
```

&emsp;&emsp;这个API返回如下：

```text
{
  "hits": {
    "hits": [
      {
        "_source": {
          "@timestamp": "2099-05-05T16:21:15.000Z",
          "http": {
            "request": {
              "referrer": "\"-\"",
              "method": "GET"
            },
            "response": {
              "status_code": 200,
              "body": {
                "bytes": 3638
              }
            },
            "version": "1.1"
          },
          "source": {
            "ip": "89.160.20.128",
            "geo": {
              "continent_name" : "Europe",
              "country_name" : "Sweden",
              "country_iso_code" : "SE",
              "city_name" : "Linköping",
              "region_iso_code" : "SE-E",
              "region_name" : "Östergötland County",
              "location" : {
                "lon" : 15.6167,
                "lat" : 58.4167
              }
            }
          },
          "message": "89.160.20.128 - - [05/May/2099:16:21:15 +0000] \"GET /favicon.ico HTTP/1.1\" 200 3638 \"-\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\"",
          "url": {
            "original": "/favicon.ico"
          },
          "user": {
            "name": "-",
            "id": "-"
          },
          "user_agent": {
            "original": "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_11_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/52.0.2743.116 Safari/537.36\"",
            "os": {
              "name": "Mac OS X",
              "version": "10.11.6",
              "full": "Mac OS X 10.11.6"
            },
            "name": "Chrome",
            "device": {
              "name": "Mac"
            },
            "version": "52.0.2743.116"
          }
        }
      }
    ]
  }
}
```

### Enrich your data
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/processors.html)

&emsp;&emsp;你可以在ingest过程中使用[enrich processor](#Enrich processor)将现有的索引中的数据添加到incoming document（传入文档）中。

&emsp;&emsp;例如，你可以使用enrich processor来实现以下目标：

- 根据已知的IP地址识别网络服务或供应商
- 根据产品ID添加零售订单的产品信息
- 根据电子邮件地址补充联系信息
- 根据用户坐标添加邮政编码

##### How the enrich processor works

&emsp;&emsp;大多数processor是自包含的（self-contained，也就是processor都是独立的，它们不依赖于外部数据或其他processor来执行其任务），仅改变incoming document中的现有数据。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ingest-process.svg">

&emsp;&emsp;enrich processor则是添加新的数据到incoming document中并且要求一些特殊的组件：

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/enrich-process.svg">

###### enrich policy

&emsp;&emsp;使用一些配置选项将正确的enrich data添加到正确的incoming document中。

&emsp;&emsp;一个enrich policy中包含以下内如：

- 一个或多个source index的列表，他们以文档形式存储enrich data
- policy type决定了processor如何匹配enrich data到incoming document中
- source index中的match field用来与incoming document匹配
- enrich field包含了source index中的enrich data，将被添加到incoming document中

&emsp;&emsp;enrich policy可以被processor使用之前需要先被[executed](#Execute enrich policy API)。执行后，enrich policy使用策略中source index中的enrich data来创建一个名为enrich index的streamlined（简化） system index。processor使用这个索引匹配以及丰富incoming document。

###### source index

&emsp;&emsp;source index中包含了你想要添加到incoming document中的enrich data。你可以跟Elasticsearch中其他常规索引（regular index，系统索引就不属于常规索引）一样创建并且管理这些索引 。你可以在一个enrich policy中指定多个source index。你可以在多个enrich policy中使用相同的source index。

##### enrich Index

&emsp;&emsp;一种特殊的系统索引跟一个特定的enrich policy关联

&emsp;&emsp;直接通过文档（source index中的document）到文档（incoming document）的方式会很慢，并且属于资源密集型的操作。为了提高速度，enrich processor使用了enrich index。

&emsp;&emsp;enrich index中包含了来自source index中的丰富数据，但是有一些特殊的属性来帮助简化（streamline）处理过程：

- 他们都是系统索引，意味着由Elasticsearch管理并且只被enrich processor使用
- 他们都是以`.enrich-*`开头
- 他们是只读的，意味着你不能直接修改他们
- 他们都[force merged](#Force merge API)，使得能被快速检索


#### Set up an enrich processor
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/enrich-setup.html)

&emsp;&emsp;按照以下步骤创建一个enrich processor：

1. Check the [prerequisites](#Prerequisites（Set up an enrich processor）).
2. [Add enrich data](#Add enrich data).
3. [Create an enrich policy](#Create an enrich policy).
4. [Execute the enrich policy](#Execute the enrich policy).
5. [Add an enrich processor to an ingest pipeline](#Add an enrich processor to an ingest pipeline).
6. [Ingest and enrich documents](#Ingest and enrich documents).

&emsp;&emsp;一旦你设置好了一个enrich processor，你可以[update your enrich data](#Update an enrich index)和[update your enrich policies](#Update an enrich policy)。

> IMPORTANT：enrich processor会执行很多操作，可能会影响ingest Pipeline的速度
> 强烈建议在生产中部署进行测试和基准测试
> 我们不建议使用enrich processor处理实时数据。enrich processor最合适处理很少更改的数据

##### Prerequisites（Set up an enrich processor）

&emsp;&emsp;如果开启了Elasticsearch security features，你必须要有：

- 被使用到的索引的`read` index privilege
- [built-in role](#Built-in roles) `enrich_user`

##### Add enrich data

&emsp;&emsp;首先，添加文档到一个或多个source index中。这些文档应该包含你最终想要添加到incoming document中的丰富数据

&emsp;&emsp;你可以使用[document](#Document APIs)和[index](#Index APIs) API，跟Elasticsearch中其他常规索引一样，管理source index。

&emsp;&emsp;你也可以设置[Beats](https://www.elastic.co/guide/en/beats/libbeat/8.2/getting-started.html)，例如[Filebeat](https://www.elastic.co/guide/en/beats/filebeat/8.2/filebeat-installation-configuration.html)，来自动的发送/索引你的source Index。见[Getting started with Beats](https://www.elastic.co/guide/en/beats/libbeat/8.2/getting-started.html)。

##### Create an enrich policy

&emsp;&emsp;source index中添加完丰富数据后，然后使用[create enrich policy API](#Create enrich policy API)创建一个enrich policy。

> WARNING：enrich policy一旦创建结束，你不能更新/更改。见[Update an enrich policy](#Update an enrich policy)。

##### Execute the enrich policy

&emsp;&emsp;enrich policy创建后，你可以使用[execute enrich policy API](#Execute enrich policy API)执行该策略来创建[enrich index](#enrich index)。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/enrich-policy-index.svg">

&emsp;&emsp;enrich index中包含了策略中的source index。enrich index 的名称总是以`.enrich-*`开头，只读索引，并且[force merged](#Force merge API)。

> WARNING：enrich index应该只被[enrich processor](#Enrich processor)使用，避免使用enrich index用于其他目的。

##### Add an enrich processor to an ingest pipeline

&emsp;&emsp;一旦你拥有了source Index、enrich policy以及相关的enrich Index，你就可以设置一个包含带有策略的enrich processor的ingest pipeline。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/enrich-processor.svg">

&emsp;&emsp;定义一个[enrich processor](#Enrich processor)，然后使用[create or update pipeline API](#Create or update pipeline API)将它添加到一个ingest pipeline中。

&emsp;&emsp;在定义enrich processor时，你必须至少包含下面的内容：

- 需要使用的enrich policy
- 用来enrich Index与incoming document匹配的字段
- 添加到incoming document的target Field。enrich policy中指定的`match_field`和`enrich_fields`中所有的字段将作为`target_field`的字段

&emsp;&emsp;你可以使用`max_matches`选项设置一个incoming document可以匹配的丰富文档的数量。如果设置为默认值的`1`。那么`target_field`字段变成json object否则就是json array。

&emsp;&emsp;见[Enrich](#Enrich processor)了解全部的可配置的选项。

&emsp;&emsp;你也可以在ingest pipeline中添加其他的[processor](#Ingest processor reference)。

##### Ingest and enrich documents

&emsp;&emsp;你现在可以使用ingest pipeline丰富并索引文档了。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/enrich-process.svg">

&emsp;&emsp;在生产中使用pipeline之前，我们建议你先写入一些测试文档然后使用[get API](#Get API)验证enrich data是否被正确的添加。

##### Update an enrich index

&emsp;&emsp;创建之后，你不能向enrich Index中更新或者添加文档。而是更新你的source index然后再次[execute](#Execute enrich policy API) enrich policy。这样会从更新后的source Index中创建新的enrich index。之前的enrich index会使用一个maintenance job稍后删除。默认是每15分钟。

&emsp;&emsp;如果有必要的话，你可以使用ingest pipeline[reindex](#Reindex API)或者[update](#Update By Query API)已经提取的文档（ingested document）。

##### Update an enrich policy

&emsp;&emsp;enrich policy一旦创建完毕，你就不能更新或者修改，不过你可以：

- 创建并且[execute](#Execute enrich policy API)一个新的enrich policy
- 在使用中的enrich processor中用新的enrich policy替换之前的
- 使用[delete enrich policy](#Delete enrich policy API)删除之前的enrich policy

##### Enrich components

&emsp;&emsp;enrich coordinator是一个组件，负责管理和执行每个ingest node上所需的搜索以丰富文档。它将所有pipeline中所有enrich processor的搜索合并成批量[multi-searches](#Multi search API)。

&emsp;&emsp;enrich policy executor是一个组件，负责管理所有enrich policies的执行。当执行一个enrich policies时，此组件将创建一个新的enrich index并移除之前的enrich index。enrich policies的执行由elected master node管理。这些策略的执行发生在不同的节点上。

##### Node Settings

&emsp;&emsp;`enrich` processor有enrich coordinator和enrich policy executor相关的节点设置。

&emsp;&emsp;enrich coordinator支持下面的节点设置：

###### enrich.cache_size

&emsp;&emsp;用于enriching documents的搜索的最大缓存数量。默认值为1000。整个集群的所有enrich processors共用一个缓存。此设置确定该缓存的大小。


###### enrich.coordinator_proxy.max_concurrent_requests

&emsp;&emsp;enriching documents时运行的最大并发[multi-search requests](#Multi search API)请求数量。默认值为8。

###### enrich.coordinator_proxy.max_lookups_per_request

&emsp;&emsp;enriching documents时在一个[multi-search requests](#Multi search API)请求中包含的最大搜索数量。默认值为128。

&emsp;&emsp;enrich policy executor支持下面的节点设置：

###### enrich.fetch_size

&emsp;&emsp;将source index reindex到enrich index时的最大批量大小。默认值为10000。

###### enrich.max_force_merge_attempts

&emsp;&emsp;允许在enrich index上进行的最大[force merge](#Force merge API) 尝试次数。默认值为3。

###### enrich.cleanup_period

&emsp;&emsp;Elasticsearch检查是否可以删除未使用的enrich index的频率。默认值为15分钟。

###### enrich.max_concurrent_policy_executions

&emsp;&emsp;最大并发执行的enrich policy数量。默认值为50。

#### Example: Enrich your data based on geolocation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/geo-match-enrich-policy-type.html)

&emsp;&emsp;`geo_match`类型的[enrich policies](#enrich policy)基于一个geographic location，使用[gep_shape](#Geoshape query) query，将匹配到的丰富数据添加到incoming document中。

&emsp;&emsp;下面的例子中创建了一个`geo_match`类型的enrich policy，基于坐标集（a set of coordinates）将`postal code`的信息添加到incoming document中。然后将`geo_match`类型的enrich policy添加到一个ingest pipeline中。

&emsp;&emsp;使用[create index API](##Create index API)创建一个source index，索引中至少包含一个`geo_shape`字段。

```text
PUT /postal_codes
{
  "mappings": {
    "properties": {
      "location": {
        "type": "geo_shape"
      },
      "postal_code": {
        "type": "keyword"
      }
    }
  }
}
```

&emsp;&emsp;使用[index API](#Index API)将enrich data添加到source index中

```text
PUT /postal_codes/_doc/1?refresh=wait_for
{
  "location": {
    "type": "envelope",
    "coordinates": [ [ 13.0, 53.0 ], [ 14.0, 52.0 ] ]
  },
  "postal_code": "96598"
}
```

&emsp;&emsp;使用create enrich policy API创建一个`geo_match`类型的enrich policy。这个策略包含：

- 一个或多个source index
- 一个source index中的`match_field`以及`geo_shape`，用来跟incoming document进行匹配
- source Index中的enrich field，将追加到incoming document中。

```text
PUT /_enrich/policy/postal_policy
{
  "geo_match": {
    "indices": "postal_codes",
    "match_field": "location",
    "enrich_fields": [ "location", "postal_code" ]
  }
}
```

&emsp;&emsp;使用[create or update pipeline API](#Create or update pipeline API)创建一个ingest pipeline。在这个pipeline中，添加一个包含以下内容的[enrich processor](#Enrich processor)：

- enrich policy
- incoming document的`field`用来匹配enrich Index的文档中的geoshape
- `target_field`用来为incoming document存储追加的enrich data。这个字段中包含了enrich policy中指定的`match_field`和`enrich_fields`信息
  - target_field可以是一个新的字段名（如果是incoming document中已有的字段，会被覆盖）
- `shape_relation`用来告知processor如何匹配incoming document和enrich index中的geoshapes。见[Spatial Relations ](#Geoshape query)了解更多可选参数以及介绍。

```text
PUT /_ingest/pipeline/postal_lookup
{
  "processors": [
    {
      "enrich": {
        "description": "Add 'geo_data' based on 'geo_location'",
        "policy_name": "postal_policy",
        "field": "geo_location",
        "target_field": "geo_data",
        "shape_relation": "INTERSECTS"
      }
    }
  ]
}
```

&emsp;&emsp;使用ingest pipeline索引一篇文档。incoming document中应该包含enrich processor中指定的`field`：

```text
PUT /users/_doc/0?pipeline=postal_lookup
{
  "first_name": "Mardy",
  "last_name": "Brown",
  "geo_location": "POINT (13.5 52.5)"
}
```

&emsp;&emsp;若要验证enrich processor匹配到的、以及追加的字段的信息。可以使用[get API](#Get API)查看索引后的文档：

```text
GET /users/_doc/0
```

&emsp;&emsp;该接口返回以下响应：

```text
{
  "found": true,
  "_index": "users",
  "_id": "0",
  "_version": 1,
  "_seq_no": 55,
  "_primary_term": 1,
  "_source": {
    "geo_data": {
      "location": {
        "type": "envelope",
        "coordinates": [[13.0, 53.0], [14.0, 52.0]]
      },
      "postal_code": "96598"
    },
    "first_name": "Mardy",
    "last_name": "Brown",
    "geo_location": "POINT (13.5 52.5)"
  }
}

```

#### Example: Enrich your data based on exact values
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/match-enrich-policy-type.html)

&emsp;&emsp;匹配[enrich policies](#enrich policy)，将匹配到的enrich data基于精确值添加到incoming document中，比如email地址或者ID，使用的是[term query](#Term query)。

&emsp;&emsp;下面的例子中创建了一个`match`类型的enrich policy，基于一个email地址将用户名（user name）和联系方式（contact）添加到incoming document中。然后将这个`match` policy添加到ingest pipeline的processor中。

&emsp;&emsp;使用[create index API](#Create index API)和[index API](#Index API)创建一个source index。

&emsp;&emsp;下面的index API创建了一个source index，然后向这个索引中索引了一个新的文档。

```text
PUT /users/_doc/1?refresh=wait_for
{
  "email": "mardy.brown@asciidocsmith.com",
  "first_name": "Mardy",
  "last_name": "Brown",
  "city": "New Orleans",
  "county": "Orleans",
  "state": "LA",
  "zip": 70116,
  "web": "mardy.asciidocsmith.com"
}
```

&emsp;&emsp;使用create enrich policy API创建一个`match`类型的enrich policy。这个策略包含：

- 一个或多个source index
- 一个source index中的`match_field`，用来跟incoming document进行匹配
- source Index中的enrich field，将追加到incoming document中。

```text
PUT /_enrich/policy/users-policy
{
  "match": {
    "indices": "users",
    "match_field": "email",
    "enrich_fields": ["first_name", "last_name", "city", "zip", "state"]
  }
}
```

&emsp;&emsp;使用[execute enrich policy API](#Execute enrich policy API)为这个策略创建一个enrich Index。

```text
POST /_enrich/policy/users-policy/_execute
```

&emsp;&emsp;使用[create or update pipeline API](#Create or update pipeline API)创建一个ingest pipeline。在这个pipeline中，添加一个包含以下内容的[enrich processor](#Enrich processor)：

- enrich polic
- incoming document的`field`用来匹配enrich Index中的文档
- `target_field`用来为incoming document存储追加的enrich data。这个字段中包含了enrich policy中指定的`match_field`和`enrich_fields`信息

```text
PUT /_ingest/pipeline/user_lookup
{
  "processors" : [
    {
      "enrich" : {
        "description": "Add 'user' data based on 'email'",
        "policy_name": "users-policy",
        "field" : "email",
        "target_field": "user",
        "max_matches": "1"
      }
    }
  ]
}
```

&emsp;&emsp;使用ingest pipeline索引一篇文档。incoming document中应该包含enrich processor中指定的`field`：

```text
PUT /my-index-000001/_doc/my_id?pipeline=user_lookup
{
  "email": "mardy.brown@asciidocsmith.com"
}
```

&emsp;&emsp;若要验证enrich processor匹配到的、以及追加的字段的信息。可以使用[get API](#Get API)查看索引后的文档：

```text
GET /my-index-000001/_doc/my_id
```

&emsp;&emsp;该接口返回以下响应：

```text
{
  "found": true,
  "_index": "my-index-000001",
  "_id": "my_id",
  "_version": 1,
  "_seq_no": 55,
  "_primary_term": 1,
  "_source": {
    "user": {
      "email": "mardy.brown@asciidocsmith.com",
      "first_name": "Mardy",
      "last_name": "Brown",
      "zip": 70116,
      "city": "New Orleans",
      "state": "LA"
    },
    "email": "mardy.brown@asciidocsmith.com"
  }
}
```

#### Example: Enrich your data by matching a value to a range
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/range-enrich-policy-type.html)

&emsp;&emsp;一个`range`类型的[enrich policy](#enrich policy)使用一个[term query](#Term query)匹配incoming document中的数值、日期或者IP地址类型的字段，使用匹配到的字段值去enrich Index中执行同一个字段的范围查询。不支持range到range的匹配。

&emsp;&emsp;下面的例子中创建了一个`range`类型的enrich policy，该策略基于IP地址向incoming document中添加描述性网络名称（descriptive network name）和负责部门（responsible department）。然后，它将enrich policy添加到ingest pipeline的一个processor中。

&emsp;&emsp;使用[create index API](#Create index API)以及合适的mapping创建一个source index。

```text
PUT /networks
{
  "mappings": {
    "properties": {
      "range": { "type": "ip_range" },
      "name": { "type": "keyword" },
      "department": { "type": "keyword" }
    }
  }
}
```

&emsp;&emsp;下面的请求index API的请求向source Index中添加了一篇新文档。

```text
PUT /networks/_doc/1?refresh=wait_for
{
  "range": "10.100.0.0/16",
  "name": "production",
  "department": "OPS"
}
```

&emsp;&emsp;使用create enrich policy API创建一个`range`类型的enrich policy。这个策略必须包含：

- 一个或多个source index
- 一个source index中的`match_field`，用来跟incoming document进行匹配
- source Index中的enrich field，将追加到incoming document中。

&emsp;&emsp;由于我们计划基于IP地址来丰富文档，因此策略中的`match_field`必须是`ip_range`字段

```text
PUT /_enrich/policy/networks-policy
{
  "range": {
    "indices": "networks",
    "match_field": "range",
    "enrich_fields": ["name", "department"]
  }
}
```

&emsp;&emsp;使用[execute enrich policy API ](#Execute enrich policy API)创建一个enrich Index。

```text
POST /_enrich/policy/networks-policy/_execute
```

&emsp;&emsp;使用[create or update pipeline API](#Create or update pipeline API)创建一个ingest pipeline。在这个pipeline中，添加一个包含以下内容的[enrich processor](#Enrich processor)：

- enrich policy
- incoming document的`field`用来匹配enrich Index中的文档
- `target_field`用来为incoming document存储追加的enrich data。这个字段中包含了enrich policy中指定的`match_field`和`enrich_fields`信息

```text
PUT /_ingest/pipeline/networks_lookup
{
  "processors" : [
    {
      "enrich" : {
        "description": "Add 'network' data based on 'ip'",
        "policy_name": "networks-policy",
        "field" : "ip",
        "target_field": "network",
        "max_matches": "10"
      }
    }
  ]
}
```

&emsp;&emsp;使用ingest pipeline索引一篇文档。incoming document中应该包含enrich processor中指定的`field`：

```text
PUT /my-index-000001/_doc/my_id?pipeline=networks_lookup
{
  "ip": "10.100.34.1"
}
```

&emsp;&emsp;若要验证enrich processor匹配到的、以及追加的字段的信息。可以使用[get API](#Get API)查看索引后的文档：

```text
GET /my-index-000001/_doc/my_id
```
&emsp;&emsp;该接口返回以下响应：

```text
{
  "_index" : "my-index-000001",
  "_id" : "my_id",
  "_version" : 1,
  "_seq_no" : 0,
  "_primary_term" : 1,
  "found" : true,
  "_source" : {
    "ip" : "10.100.34.1",
    "network" : [
      {
        "name" : "production",
        "range" : "10.100.0.0/16",
        "department" : "OPS"
      }
    ]
  }
}
```

### Ingest processor reference
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/processors.html)

#### Append processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/append-processor.html)

&emsp;&emsp;如果待追加的字段存在，并且是字段值是数组类型，那么追加一个或多个值到现有的数组中。如果待追加的字段存在，并且字段值是标量（scalar）类型，那么将其转化为数组类型，然后追加到数组中。如果待追加的字段不存在，那么创建一个数组类型的字段值并且追加到这个数组中。可以追加单个值或者数组。

#### Bytes processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/append-processor.html)

&emsp;&emsp;将易于理解（human readable）的字节值（比如 1kb）转化为其字节值（比如  1024）。如果字段是string数组，那么数组中所有元素都会被转化。

&emsp;&emsp;支持的易于理解的单位有`b`、`kb`、`mb`、`gb`、`tb`、`pb`。不区分大小写。如果有不支持的格式或者转化后的值超过2^63则会发生错误。

#### Circle processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ingest-circle-processor.html)

&emsp;&emsp;将圆形定义转化为近似的正多边形。

#### Community ID processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/community-id-processor.html)

&emsp;&emsp;根据社区ID（[Community ID Specification](https://github.com/corelight/community-id-spec)）规范计算网络流数据的社区ID。您可以使用社区ID来关联与单个流相关的网络事件。

&emsp;&emsp;社区ID处理器默认从相关的[Elastic Common Schema（ECS）](https://www.elastic.co/guide/en/ecs/8.2/index.html)字段读取网络流数据。如果您使用ECS，则无需配置。

#### Convert processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/convert-processor.html)

&emsp;&emsp;将当前被提取的文档中的字段转化为不同的类型。比如将string转化为Integer。如果字段值是个数组，那么所有数组元素都被转化。

&emsp;&emsp;支持的类型有：`integer`, `long`, `float`, `double`, `string`, `boolean`, `ip`以及`auto`。

&emsp;&emsp;转化为`boolean`时，如果字符串的值为`true`（不区分大小）则转化为true，如果是`false`则转化为false。如果是其他值则抛出异常。

&emsp;&emsp;如果包含了一个有效的IPv4或者IPv6地址可以转化为`ip`类型，并且使用[IP](#IP field type)的mapping类型进行索引。

&emsp;&emsp;如果使用了`auto`，那么会将字符串类型的字段值转化为最接近的non-string、non-IP类型。比如，字段值为`true`时会转化为`boolean`类型。注意的转化为float的优先级高于double。比如`242.15`会自动转化为float类型。如果无法正确的自动转化，这个processor仍然算作处理成功并且字段值保持原样。这样，`target_field`将被更新为转化前的值。

#### CSV processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/csv-processor.html)

&emsp;&emsp;从文档中的单个文本字段提取CSV行的字段。CSV中的任何空字段都将被跳过。

#### Date processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/date-processor.html)

&emsp;&emsp;解析字段中的日期。然后使用日期或者时间戳作为文档的timestamp。默认情况下，`date processor`将解析出的日期作为一个名为`@timestamp`的新字段。你可以通过`target_field`指定一个不同的字段名。可以在同一个date processor中定义多个format。它们将在处理过程中按照定义中的顺序依次用于尝试解析日期。

#### Date index name processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/date-index-name-processor.html)

&emsp;&emsp;这个processor的目的是使用[date math index name support.](#Date math support in system and index alias names-1)，基于文档中日期或者时间字段，将文档指向正确的基于时间的索引。

&emsp;&emsp;processor根据提供的索引名称前缀、正在处理的文档中的日期或时间戳字段以及提供的日期舍入，设置带有日期数学索引名称表达式的`_index`元数据字段。

&emsp;&emsp;首先processor获取处理中的文档中的日期或者时间戳字段的信息。然后（可选）将字段中的值根据format进行解析。然后，将此日期、提供的索引名称前缀和提供的日期舍入格式化为一个日期数学索引名称表达式（date math index name expression）。此处也可以可选地指定日期格式化，以指定日期应如何格式化为日期数学索引名称表达式。

#### Dissect processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/dissect-processor.html)

&emsp;&emsp;跟[Grok processor](#Grok processor)类型，`dissect`同样从文档中单个文本字段中提取出结构化的字段。跟[Grok processor](#Grok processor)不同的是，它不使用[Regular Expressions](https://en.wikipedia.org/wiki/Regular_expression)。这使得dissect的语法更简单并且有些情况下性能快于[Grok processor](#Grok processor)。

&emsp;&emsp;Dissect根据定义的模式去匹配某个单文本的字段。

&emsp;&emsp;比如有下面的模式：

```text
%{clientip} %{ident} %{auth} [%{@timestamp}] \"%{verb} %{request} HTTP/%{httpversion}\" %{status} %{size}
```

&emsp;&emsp;将会匹配下面的一行日志：

```text
1.2.3.4 - - [30/Apr/1998:22:00:52 +0000] \"GET /english/venues/cities/images/montpellier/18.gif HTTP/1.0\" 200 3171
```

&emsp;&emsp;将会生成下面的字段：

```text
"doc": {
  "_index": "_index",
  "_type": "_type",
  "_id": "_id",
  "_source": {
    "request": "/english/venues/cities/images/montpellier/18.gif",
    "auth": "-",
    "ident": "-",
    "verb": "GET",
    "@timestamp": "30/Apr/1998:22:00:52 +0000",
    "size": "3171",
    "clientip": "1.2.3.4",
    "httpversion": "1.0",
    "status": "200"
  }
}
```

&emsp;&emsp;dissect pattern由字符串中部分被丢弃的值定义，比如上面的例子中，第一个被丢弃的部分就是单个空格。Dissect会从头开始找到这个空格，然后将这个空前的所有值设置为`clientio`。后面dissect又匹配了`[`以及`]`，然后将`[`跟`]`中所有的值设置为`@timestamp`。特别注意要丢弃的字符串部分将帮助构建成功的解析模式。

&emsp;&emsp;成功的匹配要求模式中的所有key都必须有一个value。如果模式中定义的任何`%{keyname}`没有值，则会抛出异常，并且可能通过[on_failure](#Handling pipeline failures)指令进行处理。可以使用空键`%{}`或[named skip key ](#Named skip key (?))匹配值，但从最终文档中排除该值。所有匹配的值都表示为字符串数据类型。可以使用[convert processor](#Convert processor)将其转换为预期的数据类型。

&emsp;&emsp;Dissect还支持可以改变dissect默认行为的[key modifiers](#Dissect key modifiers)。例如，你可以指示dissect忽略某些字段、追加字段、跳过填充等。更多信息请见下文

#### Dot expander processor
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/dot-expand-processor.html)

&emsp;&emsp;将用点`.`表示的字段名扩展成一个对象字段。这个processor使得可以让点字段（field with dot）能在pipeline中被其他processor访问。否则这些点字段无法被任何processor访问。

##### Table 14. Dot Expand Options

- field：（Required）待扩展的点字段。如果设置为`*`，那么top-level上所有的字段都会被扩展。
- path：（Optional）点字段中包含了需要展开的点字段。如果你需要展开的点字段是另一个对象字段的key需要将path指定为对象字段的key，因为`field`字段无法理解leaf fields
  
  ```text
  {
  "address": {
    "street": {
      "name": "Main St"
    }
  }
  }
  ```
  
- override：（Optional）（默认值：false）如果扩展后跟现有的字段冲突，是否进行覆盖。如果为`true`，则覆盖，否则生成一个数组同时保留这两个值

  ```text
  {
    "foo.bar" : "value2",
    "foo" : {
      "bar" : "value1"
    }
  }
  ```

- description：（Optional）processor的描述信息。用来描述配置或这个processor的目的
- if：（Optional）有条件的运行processor。见[Conditionally run a processor](#Conditionally run a processor)
- ignore_failure：（Optional）（默认值：false）忽略processor的报错。见[Handling pipeline failures](#Handling pipeline failures)
- on_failure：（Optional）处理processor的报错。[Handling pipeline failures](#Handling pipeline failures)
- tag：（Optional）processor的标识符。对debugging或作为指标有用

```text
{
  "dot_expander": {
    "field": "foo.bar"
  }
}
```

&emsp;&emsp;该processor会将下面的内容：

```text
{
  "foo.bar" : "value"
}
```

&emsp;&emsp;扩展为：

```text
{
  "foo" : {
    "bar" : "value"
  }
}
```

&emsp;&emsp;如果`bar`已经是`foo`的子字段，那么processor合并这两个值。如果这个字段属于标量字段（scalar field，即只包含单个值的字段），那么就将这两个字段值用数组表示

&emsp;&emsp;比如有以下的文档：

```text
{
  "foo.bar" : "value2",
  "foo" : {
    "bar" : "value1"
  }
}
```

&emsp;&emsp;会被`dot_expander`扩展为：

```text

  "foo" : {
    "bar" : ["value1", "value2"]
  }
}
```

&emsp;&emsp;如果将`override`选项设置为`true`：

```text
{
  "dot_expander": {
    "field": "foo.bar",
    "override": true
  }
}
```

&emsp;&emsp;这种情况下就进行覆盖：

```text
{
  "foo" : {
    "bar" : "value2"
  }
}
```

&emsp;&emsp;`field`选项可以设置为`*`将top-level的字段都进行扩展：

```text
{
  "dot_expander": {
    "field": "*"
  }
}
```

&emsp;&emsp;该processor会将下面这篇文档：

```text
{
  "foo.bar" : "value",
  "baz.qux" : "value"
}
```

&emsp;&emsp;变成：

```text
{
  "foo" : {
    "bar" : "value"
  },
  "baz" : {
    "qux" : "value"
  }
}
```

&emsp;&emsp;如果leaf field上一层的字段跟现有的字段冲突，那么需要先重命名。

&emsp;&emsp;如果有以下的文档：

```text
{
  "foo": "value1",
  "foo.bar": "value2"
}
```

&emsp;&emsp;`foo`需要在`dot_expander` processor处理之前进行重命名。这样`foo.bar`才能将`bar`扩展到`foo`下：

```text
{
  "processors" : [
    {
      "rename" : {
        "field" : "foo",
        "target_field" : "foo.bar"
      }
    },
    {
      "dot_expander": {
        "field": "foo.bar"
      }
    }
  ]
}
```

&emsp;&emsp;这么做的理由是ingest pipeline不知道如何将一个标量字段（scalar field）的字段转化为一个对象字段。

#### Drop processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/drop-processor.html)

&emsp;&emsp;丢弃文档并且不会抛出任何的错误。如果基于某些条件不索引某些文档时，这个processor就非常有用。

#### Enrich processor
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/enrich-processor.html)

&emsp;&emsp;`enrich` processor可以将一个索引中的数据丰富到文档中。见[enrich data](#Enrich your data)了解更多关于设置信息。

##### Table 16. Enrich Options

- policy_name：（Required）enrich policy的名称
- field：（Required）匹配了enrich policy中`match_field`的，被丰富的文档中的字段。用来获取丰富数据（enrichment data）支持[template snippets](#Access source fields in a processor)
- target_field：（Required）被丰富的文档（incoming document）的字段，enrich policy中指定的`match_field`和`enrich_fields`中所有的字段将作为`target_field`的字段
- ignore_missing：（Optional）（默认值：false）如果为`true`并且`field`不存在，那么不会修改文档
- override：（Optional）（默认值：true）如果将要更新已经存在的非null值的字段，并且设置为`false`，这个字段就不会被覆盖
- max_matches：（Optional）（默认值：1）匹配到的文档数量最大值。如果`max_matches` 大于1，那么`target_field`字段变成json array否则就是json object。为了避免文档过大。最大值只能到128
- shape_relation：（Optional）（默认值：INTERSECTS）一个空间关系操作符，用于将传入文档的[geoshape](#Geoshape field type)与富集索引（enrich index）中的文档进行匹配。这个选项仅用于`geo_match` enrich policy 类型。有关操作符和更多信息，请参见[Spatial Relations](#Spatial Relations)。
- description：（Optional）processor的描述信息。用来描述配置或这个processor的目的
- if：（Optional）有条件的运行processor。见[Conditionally run a processor](#Conditionally run a processor)
- ignore_failure：（Optional）（默认值：false）忽略processor的报错。见[Handling pipeline failures](#Handling pipeline failures)
- on_failure：（Optional）处理processor的报错。[Handling pipeline failures](#Handling pipeline failures)
- tag：（Optional）processor的标识符。对debugging或作为指标有用


#### Fail processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/fail-processor.html)

&emsp;&emsp;抛出一个异常，当你预期某个pipeline会失败并且将指定的消息回复给请求方。

#### Fingerprint processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/fingerprint-processor.html)

&emsp;&emsp;计算文档的hash值，你可以使用这个hash值用于[content fingerprinting](https://en.wikipedia.org/wiki/Fingerprint_(computing))。

#### Foreach processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/foreach-processor.html)

&emsp;&emsp;对数组或者对象中的每一个元素运行ingest processor。

&emsp;&emsp;所有的ingest processor可以运行在数组或对象的院上。然而如果元素的数量未知，用相同的方法逐一处理它们可能会变得繁琐。

&emsp;&emsp;`foreach` processor可以让你指定一个包含数组或者对象的字段，以及一个`processor`运行在字段中的每一个元素上。

#### GeoIP processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/geoip-processor.html)

&emsp;&emsp;

#### Grok processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/grok-processor.html)

&emsp;&emsp;

#### Gsub processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/gsub-processor.html)

&emsp;&emsp;对一个string类型的字段根据一个正则表达式以及替换值进行转化。如果该字段是数组类型，则数组中所有的元素会被转化。如果遇到了non-string的字段，则抛出一个异常。

#### HTML strip processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/htmlstrip-processor.html)

&emsp;&emsp;移除字段中的HTML标签。如果该字段是数组类型的，将移除数组中所有元素的HTML tags。

#### Inference processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/inference-processor.html)

&emsp;&emsp;在Pipeline中被提取的数据上使用预训练的数据框架分析模型或为自然语言处理任务部署的模型进行推断。

#### Join processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/join-processor.html)

&emsp;&emsp;将数组中每一个元素通过分隔符组成单个字符串。如果该字段不是数组类型则会抛出异常。

#### JSON processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/json-processor.html)

&emsp;&emsp;将一个JSON字符串转化为一个JSON对象。

#### KV processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/kv-processor.html)

&emsp;&emsp;这个processor用于自动解析`foo-bar`这类日志（或者特定时间类型）类型。

&emsp;&emsp;例如，如果log message中包含`ip=1.2.3.4 error=REFUSED`，你可以通过以下配置自动解析出这些字段：

```text
{
  "kv": {
    "field": "message",
    "field_split": " ",
    "value_split": "="
  }
}
```

#### Lowercase processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/network-direction-processor.html)

&emsp;&emsp;将字符串转化为小写。如果该字段是数组类型，则数组中所有的元素会被转化。

#### Network direction processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/lowercase-processor.html)

&emsp;&emsp;

#### Pipeline processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/pipeline-processor.html)

&emsp;&emsp;执行另一个pipeline（在pipeline中使用其他pipeline）。

#### Registered domain processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/registered-domain-processor.html)

&emsp;&emsp;

#### Remove processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/remove-processor.html)

&emsp;&emsp;移除现有的字段。如果待移除的字段不存在，则抛出一个异常。

#### Rename processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rename-processor.html)

&emsp;&emsp;重命名一个现有的字段。如果该字段不存在或者新的名字已被使用，则抛出一个异常。

#### Script processor
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/script-processor.html)

&emsp;&emsp;在incoming document上运行脚本，可以是内联脚本，也可以是存储的脚本（[stored script](#Stored script APIs)）。脚本运行在[ingest](https://www.elastic.co/guide/en/elasticsearch/painless/8.2/painless-ingest-processor-context.html)的上下文中。

&emsp;&emsp;该processor使用[script cache](#Scripts, caching, and search speed)来避免处理每一个文档时重复编译脚本内容。若要提高性能，确保在生产中在使用script processor时，script cache有一个合适的大小。

##### Table 35. Script options

- lang：（Optional）（默认值：Painless）[脚本语言](#Scripting)
- id：（Optional）存储的脚本的ID。如果未指定`source`，那这个参数必须指定
- source：（Optional），内联脚本，如果`id`未指定，那这个参数必须指定
- params：（Optional）脚本使用的参数
- description：（Optional）processor的描述信息。用来描述配置或这个processor的目的
- if：（Optional）有条件的运行processor。见[Conditionally run a processor](#Conditionally run a processor)
- ignore_failure：（Optional）（默认值：false）忽略processor的报错。见[Handling pipeline failures](#Handling pipeline failures)
- on_failure：（Optional）处理processor的报错。[Handling pipeline failures](#Handling pipeline failures)
- tag：（Optional）processor的标识符。对debugging或作为指标有用

##### Access source fields

&emsp;&emsp;script processor将每一篇JSON格式的文档解析为map集合,列表，原始类型（primitives）。若要使用Painless 脚本访问这些字段，使用[map access operator](https://www.elastic.co/guide/en/elasticsearch/painless/8.2/painless-operators-reference.html#map-access-operator)：`ctx['my-field']`。你也可以使用简化的`ctx.<my-field>`语法。

> NOTE：script processor不支持`ctx['_source']['my-field']`或`ctx._source.<my-field>`语法

&emsp;&emsp;下面的processor使用了Painless脚本从`evf`字段中提取出`tags`字段。

```text
POST _ingest/pipeline/_simulate
{
  "pipeline": {
    "processors": [
      {
        "script": {
          "description": "Extract 'tags' from 'env' field",
          "lang": "painless",
          "source": """
            String[] envSplit = ctx['env'].splitOnToken(params['delimiter']);
            ArrayList tags = new ArrayList();
            tags.add(envSplit[params['position']].trim());
            ctx['tags'] = tags;
          """,
          "params": {
            "delimiter": "-",
            "position": 1
          }
        }
      }
    ]
  },
  "docs": [
    {
      "_source": {
        "env": "es01-prod"
      }
    }
  ]
}
```

&emsp;&emsp;这个processor会生成：

```text
{
  "docs": [
    {
      "doc": {
        ...
        "_source": {
          "env": "es01-prod",
          "tags": [
            "prod"
          ]
        }
      }
    }
  ]
}
```

##### Access metadata fields

&emsp;&emsp;你也可以使用这个processor访问元数据字段（[metadata field](#Metadata fields)），下面的processor使用Painless脚本设置了incoming document的`_index`的值。

```text
POST _ingest/pipeline/_simulate
{
  "pipeline": {
    "processors": [
      {
        "script": {
          "description": "Set index based on `lang` field and `dataset` param",
          "lang": "painless",
          "source": """
            ctx['_index'] = ctx['lang'] + '-' + params['dataset'];
          """,
          "params": {
            "dataset": "catalog"
          }
        }
      }
    ]
  },
  "docs": [
    {
      "_index": "generic-index",
      "_source": {
        "lang": "fr"
      }
    }
  ]
}
```

&emsp;&emsp;processor将文档的`_index`字段从`generic-index`修改为`fr-catalog`。

```text
{
  "docs": [
    {
      "doc": {
        ...
        "_index": "fr-catalog",
        "_source": {
          "lang": "fr"
        }
      }
    }
  ]
}
```

#### Set processor
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/set-processor.html)

&emsp;&emsp;设置某个字段为指定的值。如果字段已经存在，那么字段值会被替换。

##### Table 36. Set Options

- field：（Required）待插入、插入更新（upsert）、更新的字段。支持[template snippets](#Access source fields in a processor)
- value：（Required,但是不能跟`copy_from`同时指定）用于设置字段值。支持[template snippets](#Access source fields in a processor)。字段值可以通过`value`或者`copy_from`指定
- copy_from：（Optional）从某个字段拷贝到`field`中，不能同时设置`value`。支持的数据类型有`boolean`, `number`, `array`, `object`, `string`, `date`等等
- override：（Optional）（默认值：true）如果将要更新已经存在的非null值的字段，并且设置为`false`，这个字段就不会被覆盖
- ignore_empty_value：（Optional）（默认值：false）如果为`true`并且`value`是[template snippets](#Access source fields in a processor)并计算出`null`值或者是空的字符串时，processor就不会更改这篇文档
- media_type：（Optional）（默认值：application/json）用来对`value`编码的media Type。只有当`value`是[template snippets](#Access source fields in a processor)时才应用。可能是`application/json`, `text/plain`, 或者 `application/x-www-form-urlencoded`其中一个
- description：（Optional）processor的描述信息。用来描述配置或这个processor的目的
- if：（Optional）有条件的运行processor。见[Conditionally run a processor](#Conditionally run a processor)
- ignore_failure：（Optional）（默认值：false）忽略processor的报错。见[Handling pipeline failures](#Handling pipeline failures)
- on_failure：（Optional）处理processor的报错。[Handling pipeline failures](#Handling pipeline failures)
- tag：（Optional）processor的标识符。对debugging或作为指标有用

```text
{
  "description" : "sets the value of count to 1",
  "set": {
    "field": "count",
    "value": 1
  }
}
```

&emsp;&emsp;这个processor也可以从一个字段拷贝字段值到另一个字段。例如：

```text
PUT _ingest/pipeline/set_os
{
  "description": "sets the value of host.os.name from the field os",
  "processors": [
    {
      "set": {
        "field": "host.os.name",
        "value": "{{{os}}}"
      }
    }
  ]
}

POST _ingest/pipeline/set_os/_simulate
{
  "docs": [
    {
      "_source": {
        "os": "Ubuntu"
      }
    }
  ]
}
```

&emsp;&emsp;响应：

```text
{
  "docs" : [
    {
      "doc" : {
        "_index" : "_index",
        "_id" : "_id",
        "_source" : {
          "host" : {
            "os" : {
              "name" : "Ubuntu"
            }
          },
          "os" : "Ubuntu"
        },
        "_ingest" : {
          "timestamp" : "2019-03-11T21:54:37.909224Z"
        }
      }
    }
  ]
}

```

&emsp;&emsp;包含复杂类型的值比如数组跟对象，也可以使用`copy_from`从一个字段拷贝到另一个字段：

```text
PUT _ingest/pipeline/set_bar
{
  "description": "sets the value of bar from the field foo",
  "processors": [
    {
      "set": {
        "field": "bar",
        "copy_from": "foo"
      }
    }
  ]
}

POST _ingest/pipeline/set_bar/_simulate
{
  "docs": [
    {
      "_source": {
        "foo": ["foo1", "foo2"]
      }
    }
  ]
}
```

&emsp;&emsp;响应：

```text
{
  "docs" : [
    {
      "doc" : {
        "_index" : "_index",
        "_id" : "_id",
        "_source" : {
          "bar": ["foo1", "foo2"],
          "foo": ["foo1", "foo2"]
        },
        "_ingest" : {
          "timestamp" : "2020-09-30T12:55:17.742795Z"
        }
      }
    }
  ]
}
```


#### Set security user processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ingest-node-set-security-user-processor.html)

&emsp;&emsp;

#### Sort processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/sort-processor.html)

&emsp;&emsp;对数组中的元素进行升序或降序排序。数字的同类（Homogeneous）数组将按数字顺序排序，而字符串数组或字符串+数字的异类（heterogeneous）数组将按字典顺序排序。当字段不是数组时抛出错误。

#### Split processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/split-processor.html)

&emsp;&emsp;使用一个分隔符对字段值进行分割，写入到一个数组中。只能对string类型的字段处理。

#### Trim processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/trim-processor.html)

&emsp;&emsp;移除字段值首尾的空格。。。。。如果是数值类型的字段，则数组中的所有元素都会被处理

#### Uppercase processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/uppercase-processor.html)

&emsp;&emsp;将字符串转化为大写。如果该字段是数组类型，则数组的元素都会被处理。

#### URL decode processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/urldecode-processor.html)

&emsp;&emsp;将编码后的URL解码为字符串。如果该字段是数组类型，则数组的元素都会被处理。

- 例如，如果一个字段的值被编码为 "Hello%20World"，使用URL解码处理器后，该值会被转换为 "Hello World"。

#### URI parts processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/uri-parts-processor.html)

&emsp;&emsp;URI部分处理器用于解析统一资源标识符（URI）字符串，并将其组成部分提取为一个对象。这个URI对象包含了URI的多个属性，如域（domain）、路径（path）、片段（fragment）、端口（port）、查询参数（query）、方案（scheme）、用户信息（user info）、用户名（username）和密码（password）。


#### User agent processor
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/user-agent-processor.html)

&emsp;&emsp;

## Aliases
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/aliases.html#write-index)

&emsp;&emsp;Alias是一组数据流（[data stream](#Data streams)）或者索引的别名（secondary name）。大多数的Elasticsearch API接受用一个alias来代替一个数据流或者索引。

&emsp;&emsp;你可以在任何时间修改alias对应的数据流或者索引。如果你使用alias在你的应用中进行查询，你可以在不需要修改你代码并且不需要停机（downtime）的情况下reindex data。

#### Alias types

&emsp;&emsp;目前有两种类型的alias：

- data stream alias指向一个或者多个数据流
- index alias 指向一个或者多个索引

&emsp;&emsp;alias不能同时指向数据流和索引。你不能添加一个数据流的[backing index](#Backing indices)作为index alias。

#### Add an alias

&emsp;&emsp;使用 [aliases API](#Aliases API)的`add`动作对一个现有的数据流或者索引起个别名。如果别名不存在，那么这个请求就会进行创建。

```text
POST _aliases
{
  "actions": [
    {
      "add": {
        "index": "logs-nginx.access-prod",
        "alias": "logs"
      }
    }
  ]
}
```

&emsp;&emsp;API的`index`和`indices`支持通配符（\*）。如果通配符模板同时匹配到数据流或者索引则会返回错误。

```text
POST _aliases
{
  "actions": [
    {
      "add": {
        "index": "logs-*",
        "alias": "logs"
      }
    }
  ]
}
```

#### Remove an alias

&emsp;&emsp;使用alias API的`remove`动作来移除别名。

```text
POST _aliases
{
  "actions": [
    {
      "remove": {
        "index": "logs-nginx.access-prod",
        "alias": "logs"
      }
    }
  ]
}
```

#### Multiple actions

&emsp;&emsp;你可以使用alias API在一个原子操作中执行多个动作。

&emsp;&emsp;例如，别名`logs`指向了一个数据流。下面的请求将交换别名（swap alias）。在这次交换期间，这个`logs`别名没有downtime（还可以用于查询）并且不会在同一时间指向多个流。

```text
POST _aliases
{
  "actions": [
    {
      "remove": {
        "index": "logs-nginx.access-prod",
        "alias": "logs"
      }
    },
    {
      "add": {
        "index": "logs-my_app-default",
        "alias": "logs"
      }
    }
  ]
}
```

#### Add an alias at index creation

&emsp;&emsp;你可以在创建索引或者数据流时使用[component](#Create or update component template API)或者[index template](#Create or update index template API)为它们添加别名。

```text
# Component template with index aliases
PUT _component_template/my-aliases
{
  "template": {
    "aliases": {
      "my-alias": {}
    }
  }
}

# Index template with index aliases
PUT _index_template/my-index-template
{
  "index_patterns": [
    "my-index-*"
  ],
  "composed_of": [
    "my-aliases",
    "my-mappings",
    "my-settings"
  ],
  "template": {
    "aliases": {
      "yet-another-alias": {}
    }
  }
}
```

&emsp;&emsp;你也可以在[create index API](#Create index API)请求中指定索引别名：

```text
# PUT <my-index-{now/d}-000001>
PUT %3Cmy-index-%7Bnow%2Fd%7D-000001%3E
{
  "aliases": {
    "my-alias": {}
  }
}
```

#### View aliases

&emsp;&emsp;可以使用不带参数的get alias API来获取你的集群的别名列表。

```text
GET _alias
```

&emsp;&emsp;在`_alias`前指定数据流或者索引来查看它们的别名。

```text
GET my-data-stream/_alias
```

&emsp;&emsp;在`_alias`后指定别名来查看对应的数据量或者索引。

```text
GET _alias/logs
```

#### Write index（Alias）

&emsp;&emsp;你可以为别名使用`is_write_index`来指定一个用于写入的索引或者数据流。Elasticsearch会将对这个别名的任何写请求路由到这个索引或者数据流。

```text
POST _aliases
{
  "actions": [
    {
      "add": {
        "index": "logs-nginx.access-prod",
        "alias": "logs"
      }
    },
    {
      "add": {
        "index": "logs-my_app-default",
        "alias": "logs",
        "is_write_index": true
      }
    }
  ]
}
```

&emsp;&emsp;如果一个别名指向多个索引或者多个数据流，并且没有设置`is_write_index`，这次写请求会被reject。如果一个别名指向一个索引并且没有设置`is_write_index`，那么这个索引默认作为write index。数据流的别名不会自动的设置为write data stream，及时这个别名只指向了一个数据流。

> TIP：我们建议使用数据流来存储append-only的时序数据（time series data）。如果你经常会更新或者删除现有的时序数据，转而使用index alias来指定write index。见[Manage time series data without data streams](#Manage time series data without data streams)。


#### Filter an alias

&emsp;&emsp;`filter`选项使用 [Query DSL](#Query DSL)来限制通过别名能查询到的文档。

```text
POST _aliases
{
  "actions": [
    {
      "add": {
        "index": "my-index-2099.05.06-000001",
        "alias": "my-alias",
        "filter": {
          "bool": {
            "filter": [
              {
                "range": {
                  "@timestamp": {
                    "gte": "now-1d/d",
                    "lt": "now/d"
                  }
                }
              },
              {
                "term": {
                  "user.id": "kimchy"
                }
              }
            ]
          }
        }
      }
    }
  ]
}
```

#### Routing

&emsp;&emsp;使用`routing`选项将对别名发起的请求[route](#\_routing field)到指定的分片。这样可以让你利用[shard cache](#Shard request cache settings)来加速查询。数据流不支持这个选项。

```text
POST _aliases
{
  "actions": [
    {
      "add": {
        "index": "my-index-2099.05.06-000001",
        "alias": "my-alias",
        "routing": "1"
      }
    }
  ]
}
```

&emsp;&emsp;使用`index_routing`和`search_routing`给索引跟查询指定不同的路由值。在指定后，这些选项会用覆盖`routing`的值并执行它们各自的操作。

```text
POST _aliases
{
  "actions": [
    {
      "add": {
        "index": "my-index-2099.05.06-000001",
        "alias": "my-alias",
        "search_routing": "1",
        "index_routing": "2"
      }
    }
  ]
}
```

## Search your data
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-your-data.html)

### Filter search results
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/filter-search-results.html#rescore)

&emsp;&emsp;你可以使用两种方式对查询结果进行过滤：

- 使用带有一个`filter` clause的boolean query。查询请求会将[boolean filters](#Boolean query)同时应用到查询命中（search hits）和[aggregation](#Aggregations)上。
- 使用search API的`post_filter`参数。查询请求会将[post filters](#Post filter)应用到查询命中上，但不会应用到aggregation上。你可以使用一个post filter基于一个更大的结果集来计算聚合，然后进一步收缩（narrow）结果。

&emsp;&emsp;你也可以在post filter后使用对命中的结果[recore](#Rescore filtered search results)来提高相关性以及重排结果（reorder filter）。

#### Post filter

&emsp;&emsp;当你使用`post_filter`过滤查询结果时，会在聚合完成后对查询命中过滤。post filter不会对聚合结果有影响。

&emsp;&emsp;比如说，你正在出售衬衫（shirt），并且有以下的属性：

```text
PUT /shirts
{
  "mappings": {
    "properties": {
      "brand": { "type": "keyword"},
      "color": { "type": "keyword"},
      "model": { "type": "keyword"}
    }
  }
}

PUT /shirts/_doc/1?refresh
{
  "brand": "gucci",
  "color": "red",
  "model": "slim"
}
```

&emsp;&emsp;比如某个用户指定了两个filter：

&emsp;&emsp;`color:red`以及`brand:gucci`。你只想要在查询结果中显示品牌为Gucci的红色衬衫。通常你会使用[bool query](#Boolean query)实现：

```text
GET /shirts/_search
{
  "query": {
    "bool": {
      "filter": [
        { "term": { "color": "red"   }},
        { "term": { "brand": "gucci" }}
      ]
    }
  }
}
```

&emsp;&emsp;然而，你还想要使用`faceted navigation`显示一些可以让用户点击的选项，可能你有一个样式（`model`）字段允许用户限制他们的查询结果为T恤衫（`t-shirts`）以及正装衬衫（`dress-shirts`）。

&emsp;&emsp;那么可以用[terms aggregation](#Terms aggregation)实现：

```text
GET /shirts/_search
{
  "query": {
    "bool": {
      "filter": [
        { "term": { "color": "red"   }},
        { "term": { "brand": "gucci" }}
      ]
    }
  },
  "aggs": {
    "models": {
      "terms": { "field": "model" } 
    }
  }
}
```

&emsp;&emsp;第13行，返回Gucci的最流行红色衬衫的样式。

&emsp;&emsp;但是你可能想要告诉用户**其他颜色**的Gucci衬衫。如果你在`color`字段上添加一个`terms` aggregation，你只会获取到红色的衬衫，因为你的请求只会返回红色的Gucci衬衫。

&emsp;&emsp;你可能想要在聚合中包含所有颜色的衬衫，然后只对查询结果应用`colors`过滤。那么你就可以使用`post_filter`：

```text
GET /shirts/_search
{
  "query": {
    "bool": {
      "filter": {
        "term": { "brand": "gucci" } 
      }
    }
  },
  "aggs": {
    "colors": {
      "terms": { "field": "color" } 
    },
    "color_red": {
      "filter": {
        "term": { "color": "red" } 
      },
      "aggs": {
        "models": {
          "terms": { "field": "model" } 
        }
      }
    }
  },
  "post_filter": { 
    "term": { "color": "red" }
  }
}
```

&emsp;&emsp;第6行，请求现在会找到所有颜色的Gucci衬衫
&emsp;&emsp;第12行，对`colors`的聚合返回Gucci的所有颜色
&emsp;&emsp;第16、20行，`color_red`聚合将对`models`的聚合限制在红色的Gucci衬衫中
&emsp;&emsp;第25行，最终， `post_filter`从查询命中里移除了红色以外的结果。

#### Rescore filtered search results

&emsp;&emsp;rescore可以对[query](#Search API)以及[post_filter](#Post filter)返回的Top文档使用一个通常有额外开销的次级算法（secondary algorithm）重新排序来提高精确度。而不是将这个次级算法应用到索引中所有文档上。

&emsp;&emsp;`rescore` 请求在每一个分片上执行，然后通过节点重新排序来处理所有的查询请求。

&emsp;&emsp;目前，rescore API只有一种实现方式：查询重评分器（[query rescorer](#Query rescorer)），它使用查询来微调评分。将来，可能会提供其他类型的重评分器，例如成对重评分器（pair-wise rescorer）。

> NOTE：如果在rescore query中提供了显式[sort](#Sort search results)（除了降序的\_score外），则会抛出错误。

> NOTE：当向用户展示分页时，你在浏览每一页时不应更改`window_size`（通过传递不同的`from`值），因为这可能会改变顶部命中，导致用户在翻页时结果出现令人困惑的变化。

##### Query rescorer

&emsp;&emsp;Query rescorer在[query](#Search API)以及[post_filter](#Post filter)返回的Top-K的结果上执行一个次级的query。可以通过参数`window_size`控制每一个分片上的计算的文档数量，默认值为`10`。

&emsp;&emsp;默认情况下，original query和rescore query的得分会线性组合以产生每个文档的最终\_score。original query和rescore query的相对重要性可以分别通过`query_weight`和`rescore_query_weight`来控制，这两个参数默认值都是1。

```text
POST /_search
{
   "query" : {
      "match" : {
         "message" : {
            "operator" : "or",
            "query" : "the quick brown"
         }
      }
   },
   "rescore" : {
      "window_size" : 50,
      "query" : {
         "rescore_query" : {
            "match_phrase" : {
               "message" : {
                  "query" : "the quick brown",
                  "slop" : 2
               }
            }
         },
         "query_weight" : 0.7,
         "rescore_query_weight" : 1.2
      }
   }
}
```

&emsp;&emsp;分数组合可以由`score_mode`控制：

| Score Mode | Description                                                  |
| ---------- | ------------------------------------------------------------ |
| `total`    | Add the original score and the rescore query score. The default. |
| `multiply` | Multiply the original score by the rescore query score. Useful for [`function query`](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-function-score-query.html) rescores. |
| `avg`      | Average the original score and the rescore query score.      |
| `max`      | Take the max of original score and the rescore query score.  |
| `min`      | Take the min of the original score and the rescore query score. |

##### Multiple rescores

&emsp;&emsp;也可以有序的执行多个rescore：

```text
POST /_search
{
   "query" : {
      "match" : {
         "message" : {
            "operator" : "or",
            "query" : "the quick brown"
         }
      }
   },
   "rescore" : [ {
      "window_size" : 100,
      "query" : {
         "rescore_query" : {
            "match_phrase" : {
               "message" : {
                  "query" : "the quick brown",
                  "slop" : 2
               }
            }
         },
         "query_weight" : 0.7,
         "rescore_query_weight" : 1.2
      }
   }, {
      "window_size" : 10,
      "query" : {
         "score_mode": "multiply",
         "rescore_query" : {
            "function_score" : {
               "script_score": {
                  "script": {
                    "source": "Math.log10(doc.count.value + 2)"
                  }
               }
            }
         }
      }
   } ]
}
```

&emsp;&emsp;第一个rescore基于query 的结果排序，第二个rescore根据第一个rescores排序后的结果再排序。第二个rescore可以看到第一个rescore的排序结果因此可以在第一个rescore中使用大的窗口然后再第二个rescore中使用较小的窗口。

### Highlighting
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/highlighting.html)

&emsp;&emsp;高亮器允许你从搜索结果中的一个或多个字段中获取高亮片段，以便可以向用户展示查询匹配的位置。当请求高亮显示时，响应中会为每个搜索命中包含一个额外的高亮元素，其中包含高亮字段和高亮片段（Highlighted  fragments）。

> NOTE：高亮器（highlighter）在提取待高亮的术语时，不会考虑查询的布尔逻辑。因此，对于复杂的布尔查询（如嵌套布尔查询、使用`minimum_should_match`等），可能会高亮显示与查询不匹配的文档部分。

&emsp;&emsp;高亮要求字段的真实内容。如果这个字段没有存储（即mapping中参数`store`设置为`false`），那么就会加载`_source`的内容，并从`_source`中提取出这个字段的内容。

&emsp;&emsp;例如使用默认的高亮器从`content`字段中获取高亮信息，那么需要再请求体中包含一个`Highlight`对象，在对象中指定`content`字段：

```text
GET /_search
{
  "query": {
    "match": { "content": "kimchy" }
  },
  "highlight": {
    "fields": {
      "content": {}
    }
  }
}
```

&emsp;&emsp;Elasticsearch支持三种高亮器：`unified`、`plain`、`fvh`（fast vector highlighter）。你可以为每一个字段指定不同的高亮器。

#### Unified highlighter

&emsp;&emsp;`Unified`高亮器使用Lucene Unified Highlighter，它将文本分解成句子，并使用BM25算法为每个句子打分，就像它们是语料库中的文档一样。此外，它还支持准确的短语和多项（模糊、前缀、正则）高亮显示。这是默认的高亮器。

#### Plain highlighter

&emsp;&emsp;`plain`高亮器使用标准的Lucene高亮器。它尝试在理解词语重要性和短语查询中的词语位置标准方面，反映查询匹配逻辑。
&emsp;&emsp;`plain`高亮器最适合在单个字段中高亮显示简单的查询匹配。为了准确反映查询逻辑，它创建一个小型内存索引，并通过Lucene的查询执行计划再次运行原始查询标准，以获取当前文档的底层匹配信息。这一过程需要为每个字段和每个需要高亮的文档重复。如果想在许多文档中高亮显示许多字段，并且查询复杂，我们推荐使用`Unified`高亮器处理`posting`或`term_vector`字段。

#### Fast vector highlighter

&emsp;&emsp;`fvh`高亮器使用Lucene快速向量高亮器，适用于在mapping中将`term_vector`设置为`with_positions_offsets`的字段。

- 它可以通过[boundary_scanner](#boundary_scanner)自定义
- 需要设置`term_vector`为`with_positions_offsets`，这会增加索引大小
- 它能将多个字段的匹配合并为一个结果。见`matched_fields`
- 可以为不同位置的匹配分配不同的权重，允许在高亮Boosting Query时，将短语匹配排序在term匹配之上

> WARNING：`fvh`高亮器不支持span query，如果需要span query支持，可以尝试其他高亮器，如`unified`高亮器。

#### Offsets strategy

&emsp;&emsp;若要从被查询的term中找到有意义的查询片段，高亮器需要知道在原始文本中每一个word中字符的开始跟结束偏移信息，这些偏移可以从以下方式获取：

- posting list（倒排信息）。如果在mapping中将`index_options`设置为`offsets`，那么`unified`高亮器就可以利用倒排信息来高亮文档，而不需要重新分析文本。他可以直接在倒排（posting）中重新运行原始的查询然后从索引中获取匹配到的偏移信息，限制了被高亮的文档集合。当你有large field时通过这种方式获取偏移就很重要了，因为它不需要对被高亮的文本重新分析。相比较使用`term_vectors`，它要求更少的磁盘空间
- term vector。如果通过在mapping中将`term_vector`设置为`with_positions_offsets`使得能提供`term_verctor`信息，那么`unified`高亮器会自动的使用`term_vector`来高亮字段（Highlight field）。当为large field（>  `1MB`）以及为例如`prefix`、`wildcard`这类multi-term query高亮时特别快。因为他能访问每一篇文档中term的字典。`fvh`高亮器总是使用term vectors。
- Plain Highlight。该模式是`unified`高亮器在没有其他选择时使用的模式。它创建一个小型内存索引，并通过Lucene的查询执行规划器重新运行原始查询标准，以获取当前文档的底层匹配信息。这个过程会对每个需要高亮的字段和文档重复执行。`plain`高亮器始终使用plain highlighting 。

> WARNING：Plain Highlight对于大型文本可能要求更多的内存和处理时间。因此文本的字符数量会被限制为1000000来应对这个问题。可以通过[index.highlight.max_analyzed_offset](#index.highlight.max_analyzed_offset)为特定的索引修改这个限制

#### Highlighting settings

&emsp;&emsp;高亮设置可以为全局（global-level）并且覆盖域层（field-level）的设置。

- boundary_chars：

- boundary_max_scan：

- boundary_scanner：

- boundary_scanner_locale：

- encoder：

- fields：

- force_source：

- fragmenter：

- fragment_offset：

- fragment_size：

- highlight_query：高亮显示除搜索查询外的其他查询匹配到的内容。这特别有用，如果你使用了[rescore query](#Rescore filtered search results)，因为默认情况下这些不会被高亮显示。

  > IMPORTANT：Elasticsearch不验证`highlight_query`中是否包含搜索查询，因此有可能定义它使合法的查询结果不被高亮显示。通常，你应该将搜索查询作为`highlight_query`的一部分。

- matched_fields：

- no_match_size：

- number_of_fragments：

- order：

- phrase_limit：

- post_tags：

- require_field_match：默认满足query的字段才会被高亮，将`require_field_match`设置为`false`则高亮所有字段，默认值为`false`

- max_analyzed_offset：

- tags_schema：

- type：

#### Highlighting examples

- [Override global settings](#Override global settings)
- [Specify a highlight query](#Specify a highlight query)
- [Set highlighter type](#Set highlighter type)
- [Configure highlighting tags](#Configure highlighting tags)
- [Highlight source](#Highlight on source)
- [Highlight all fields](#Highlight in all fields)
- [Combine matches on multiple fields](#Combine matches on multiple fields)
- [Explicitly order highlighted fields](#Explicitly order highlighted fields)
- [Control highlighted fragments](#Control highlighted fragments)
- [Highlight using the postings list](#Highlight using the postings list)
- [Specify a fragmenter for the plain highlighter](#Specify a fragmenter for the plain highlighter)

##### Override global settings

&emsp;&emsp;你可以指定全局设置并且可以为不同的字段选择性的覆盖这些设置、

```text
GET /_search
{
  "query" : {
    "match": { "user.id": "kimchy" }
  },
  "highlight" : {
    "number_of_fragments" : 3,
    "fragment_size" : 150,
    "fields" : {
      "body" : { "pre_tags" : ["<em>"], "post_tags" : ["</em>"] },
      "blog.title" : { "number_of_fragments" : 0 },
      "blog.author" : { "number_of_fragments" : 0 },
      "blog.comment" : { "number_of_fragments" : 5, "order" : "score" }
    }
  }
}
```

##### Specify a highlight query

&emsp;&emsp;你可以指定一个`highlight_query`，在高亮时可以添加额外的高亮。例如下面的例子中在`highlight_query`中同时包含了`search query`以及`rescore query`。如果没有`highlight_query`，则只会考虑`search query`中的条件。

```text
GET /_search
{
  "query": {
    "match": {
      "comment": {
        "query": "foo bar"
      }
    }
  },
  "rescore": {
    "window_size": 50,
    "query": {
      "rescore_query": {
        "match_phrase": {
          "comment": {
            "query": "foo bar",
            "slop": 1
          }
        }
      },
      "rescore_query_weight": 10
    }
  },
  "_source": false,
  "highlight": {
    "order": "score",
    "fields": {
      "comment": {
        "fragment_size": 150,
        "number_of_fragments": 3,
        "highlight_query": {
          "bool": {
            "must": {
              "match": {
                "comment": {
                  "query": "foo bar"
                }
              }
            },
            "should": {
              "match_phrase": {
                "comment": {
                  "query": "foo bar",
                  "slop": 1,
                  "boost": 10.0
                }
              }
            },
            "minimum_should_match": 0
          }
        }
      }
    }
  }
}
```

##### Set highlighter type

&emsp;&emsp;`type`字段可以强制指定一个高亮器的类型。可选值有`unified`、`plain`、`fvh`。下面的例子使用了plain高亮器：

```text
GET /_search
{
  "query": {
    "match": { "user.id": "kimchy" }
  },
  "highlight": {
    "fields": {
      "comment": { "type": "plain" }
    }
  }
}
```

##### Configure highlighting tags

&emsp;&emsp;Elasticsearch默认使用`<em>`跟`</em>`指定高亮范围。可以通过`pre_tags`和`post_tags`设置。

```text
GET /_search
{
  "query" : {
    "match": { "user.id": "kimchy" }
  },
  "highlight" : {
    "pre_tags" : ["<tag1>"],
    "post_tags" : ["</tag1>"],
    "fields" : {
      "body" : {}
    }
  }
}
```

&emsp;&emsp;当使用fast vector 高亮器时，你可以指定额外的标签以及"importance" is ordered。

```text
GET /_search
{
  "query" : {
    "match": { "user.id": "kimchy" }
  },
  "highlight" : {
    "pre_tags" : ["<tag1>", "<tag2>"],
    "post_tags" : ["</tag1>", "</tag2>"],
    "fields" : {
      "body" : {}
    }
  }
}
```

&emsp;&emsp;你可以使用内置的`styled`标签策略（tag scheme）：

```text
GET /_search
{
  "query" : {
    "match": { "user.id": "kimchy" }
  },
  "highlight" : {
    "tags_schema" : "styled",
    "fields" : {
      "comment" : {}
    }
  }
}
```

##### Highlight on source

&emsp;&emsp;强制高亮器基于`_soure`对字段进行高亮，即使字段没有单独存储。默认为`false`：

```text
GET /_search
{
  "query" : {
    "match": { "user.id": "kimchy" }
  },
  "highlight" : {
    "fields" : {
      "comment" : {"force_source" : true}
    }
  }
}
```

##### Highlight in all fields

&emsp;&emsp;默认只会请求中匹配到的字段进行高亮。将`require_field_match`设置为`false`则对所有字段高亮。

```text
GET /_search
{
  "query" : {
    "match": { "user.id": "kimchy" }
  },
  "highlight" : {
    "require_field_match": false,
    "fields": {
      "body" : { "pre_tags" : ["<em>"], "post_tags" : ["</em>"] }
    }
  }
}
```

##### Combine matches on multiple fields

> WARNING：只能在`fvh`高亮器中支持

&emsp;&emsp;

##### Explicitly order highlighted fields
&emsp;&emsp;

##### Control highlighted fragments
&emsp;&emsp;

##### Highlight using the postings list
&emsp;&emsp;

##### Specify a fragmenter for the plain highlighter
&emsp;&emsp;

##### How highlighters work internally
&emsp;&emsp;

##### An example of the work of the unified highlighter
&emsp;&emsp;

### Long-running searches
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/async-search-intro.html)

&emsp;&emsp;Elasticsearch通常允许你搜索大量的数据。有这样的场景，当一个查询在很多分片上查询，可能查询了非常大的数据集或者多个[remote cluster](#Remote clusters)，对于这种查询可能无法在毫秒级别返回。当你需要执行long-running查询时，同步等待结果返回不是一种好的方式。异步查询（async search）可以让你提交一个查询请求，然后异步执行。可以监控请求的过程，随后去获取结果。你也可以在这个查询全部完成前让部分结果可见并且返回。

&emsp;&emsp;你可以使用 [submit async search](#Submit async search API) API提交一个异步查询请求。[get async search](#Get async search) API允许你监控一个异步查询请求的过程并且获取结果。可以通过[delete async search API](#Delete async search)删除一个正在进行中的异步查询。

### Near real-time search
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/near-real-time.html)

&emsp;&emsp;[documents and indices](#Data in documents and indices)中说到当在Elasticsearch中存储一篇文档时，文档在被索引后可以接近实时（near real-time search，简称NRT）（1秒内）的进行搜索。如何定义NRT？

&emsp;&emsp;Elasticsearch依赖的Java库-Lucene中介绍了按段搜索（per-segment search）的概念。一个段类似一个倒排索引（inverted index），但是`index`在Lucene中的概念是"段的集合+ commit point"。 在一次提交（commit）后，一个新的段添加到commit point并且清空缓存（buffer is cleared）。

&emsp;&emsp;位于Elasticsearch和磁盘中间的是文件系统缓存（filesystem cache）。内存中的索引缓存（FIgure 1）会写入到一个新的段中（Figure 2）。新的段首先写入到文件系统缓存中（开销小）然后随后写入到磁盘中（开销大）。然而当文档还在缓存中时，它就可以被打开并且像其他文件读取。

> Figure 1. Lucene中新的文档内存中的索引缓存

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/lucene-in-memory-buffer.png">

&emsp;&emsp;Lucene允许新的段用于写入或者打开，使得这些段包含的段对搜索可见，并且不需要完整的提交（full commit）。相较于提交到磁盘这是一种轻量的操作，可以在不降低性能的情况下频繁的执行这些操作。

> Figure 2. 缓存的内容写入到了一个段中，并且这个段对搜索可见，但是这个段还没有提交

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/lucene-written-not-committed.png">

&emsp;&emsp;在Elasticsearch中，写入并打开一个新段成为`refresh`。一次refresh操作使得自上一次refresh后在索引上的所有操作对查询可见。你可以通过下面的方式来控制refresh：

- 等待刷新间隔（默认一秒）
- 设置[?refresh](#?refresh(api))选项
- 使用[Refresh API](#Refresh API)显示的完成一次refresh（POST \_refresh）

&emsp;&emsp;默认情况下，Elasticsearch每一秒周期性的执行refresh，但是只在最新30s内收到一次或者多次查询的索引上才会执行。这就是为什么说Elasticsearch是近实时搜索：文档的变化不会马上对搜索可见，但是在这个时间（timeframe）内变成可见。

### Paginate search results
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/paginate-search-results.html)

&emsp;&emsp;默认情况下，查询只会返回10条匹配的结果。你可以通过[search API](#Search API)中的`from + size`参数来通过翻页方式查询一个较大的数据集。`from`参数描述了结果集的一个起始位置，`size`参数描述了从`from`位置开始的size条结果。这两个参数的组合定义了一个页面的结果集。

```text
GET /_search
{
  "from": 5,
  "size": 20,
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  }
}
```

&emsp;&emsp;避免使用`from + size`来分页太深或一次请求太多结果。查询通常会扫描多个分片，每一个分片会将查询对应的命中的结果以及`from`之前的结果都读取到内存中。所以深度分页或者获取一个很大的结果集这些操作会极大地提高内存跟CPU的使用量，导致节点性能降低或者失败。

&emsp;&emsp;默认情况下，你不能通过`from + size`的方式来获得超过10000条的结果集。通过[index.max_result_window](#index.max_result_window)这个索引配置进行安全的限制。

&emsp;&emsp;如果你需要通过分页查询来获得超过10000的结果集，那么可以换成[search_after](#Search after)参数来实现。

>WARNING：Elasticsearch使用Lucene的内部文档id作为tie-breakers。相同数据的分片之间，这些内部文档id可能完全不同。在进行分页搜索时，您可能偶尔会看到具有相同排序值的文档排序不一致

#### Search after

&emsp;&emsp;你可以使用`search_after`参数和上一页的[sort values](#Sort search results)来获取下一页的结果。

&emsp;&emsp;使用`search_after`要求每次的查询都要有相同的`query`以及`sort value`。如果在多次的上述请求时发生了[refresh](#Near real-time search)，结果的顺序可能会改变，导致页面之间的结果不一致，为了避免这种情况，可以创建一个[point in time (PIT) ](#Point in time API)来保存搜索的当前索引状态。

```text
POST /my-index-000001/_pit?keep_alive=1m
```

&emsp;&emsp;这个接口返回一个PIT ID。

```text
{
  "id": "46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA=="
}
```

&emsp;&emsp;提交一次查询并带上`sort`参数来获得第一页的结果，如果使用PIT，在`pit.id`参数中指定PIT ID并且请求参数中移除目标data stream或者index。


>IMPORTANT：所有的PIT请求会带一个名为`_shard_doc`用于排序的内置的tiebreaker字段，tiebreaker字段也可以显示指定。如果你不能使用PIT，建议在`sort`中增加一个tiebreaker字段。每篇文档中应该有一个tiebreaker字段并且字段值是唯一的。如果文档中没有tiebreaker字段，分页结果中可能有丢失或者重复的数据。


>NOTE：当排序顺序是`_shard_doc`并且不需要命中总数时，Search after请求会得到优化并且查询更快。如果你想遍历所有的结果并且不关心排序，那么这是一个最高效的选项。

>IMPORTANT：如果`sort`字段在一些目标data stream或者index中是[date](#Date field type)类型但是在其他目标data stream或者index是[date_nanos](#Date nanoseconds field type)类型，那么使用`numeric_type`参数和`format`参数为`sort`字段指定一个[date format](#format(mapping parameter))，将`sort`字段的值转化为相同格式的值（single resolution）。否则Elasticsearch 不会在每个请求中正确解释（interpret） search after 参数。

```text
GET /_search
{
  "size": 10000,
  "query": {
    "match" : {
      "user.id" : "elkbee"
    }
  },
  "pit": {
    "id":  "46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==", 
    "keep_alive": "1m"
  },
  "sort": [ 
    {"@timestamp": {"order": "asc", "format": "strict_date_optional_time_nanos", "numeric_type" : "date_nanos" }}
  ]
}
```

&emsp;&emsp;第10行，这次查询对应的PIT ID。
&emsp;&emsp;第13行，`_shard_doc`上显示指定一个升序的tiebreak字段，为这次查询返回有序的结果。

&emsp;&emsp;查询响应中包含了一个数组，数组中的值是用于排序这条结果的`sort`的字段值（排序值）。如果你使用了PIT，数组中最后一个排序值是一个tiebreaker。在使用PIT时，名为`_shard_doc`的 tiebreaker值会在每一次查询请求中自动的添加。`_shard_doc`的值是 PIT 中的分片索引和 Lucene 的内部 doc ID的组合值。你也可以在查询请求中显示指定tiebreaker实现自定义排序：

```text
GET /_search
{
  "size": 10000,
  "query": {
    "match" : {
      "user.id" : "elkbee"
    }
  },
  "pit": {
    "id":  "46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==", 
    "keep_alive": "1m"
  },
  "sort": [ 
    {"@timestamp": {"order": "asc", "format": "strict_date_optional_time_nanos"}},
    {"_shard_doc": "desc"}
  ]
}
```

&emsp;&emsp;第10行，这次查询对应的PIT ID。
&emsp;&emsp;第13行，`_shard_doc`上显示指定一个降序的tiebreak字段，为这次查询返回有序的结果。

```text
{
  "pit_id" : "46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==", 
  "took" : 17,
  "timed_out" : false,
  "_shards" : ...,
  "hits" : {
    "total" : ...,
    "max_score" : null,
    "hits" : [
      ...
      {
        "_index" : "my-index-000001",
        "_id" : "FaslK3QBySSL_rrj9zM5",
        "_score" : null,
        "_source" : ...,
        "sort" : [                                
          "2021-05-20T05:30:04.832Z",
          4294967298                              
        ]
      }
    ]
  }
}
```

&emsp;&emsp;第2行，Updated `id` for the point in time
&emsp;&emsp;第16行，最后一个结果对应的两个排序值
&emsp;&emsp;第18行，`pid_id`中每一篇文档的唯一的tiebreaker值

&emsp;&emsp;为了获得下一页的结果，使用上一次查询中最后一个结果对应的排序值（包含tiebreaker）作为`search_after`的参数。如果使用了一个PIT，那么在`pit.id`参数中使用最新的PIT ID。这次查询的`query`和`sort`参数必须不能被更改，`from`参数必须是0（默认值）或者`-1`。

```text
GET /_search
{
  "size": 10000,
  "query": {
    "match" : {
      "user.id" : "elkbee"
    }
  },
  "pit": {
    "id":  "46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==", 
    "keep_alive": "1m"
  },
  "sort": [
    {"@timestamp": {"order": "asc", "format": "strict_date_optional_time_nanos"}}
  ],
  "search_after": [                                
    "2021-05-20T05:30:04.832Z",
    4294967298
  ],
  "track_total_hits": false                        
}
```

&emsp;&emsp;第10行，上一次查询中返回的PIT ID
&emsp;&emsp;第16行，上一次查询结果中最后一个结果的排序值
&emsp;&emsp;第20行，关闭追踪命中的总数来加速分页查询

&emsp;&emsp;你可以重复上述操作来获得额外的分页结果。如果使用了PIT，你可以在每一次查询请求中使用`keep_alive`参数来增加PIT的保留周期（retention period）。

&emsp;&emsp;当你结束查询时，你应该删除PIT。

```text
DELETE /_pit
{
    "id" : "46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA=="
}
```

#### Scroll search results

>IMPORTANT：我们不再建议使用scroll API进行深度分页查询。如果你需要保留索引状态（index state）同时分页查询10000+的结果，使用PIT的[search_after](#Search after)。

&emsp;&emsp;尽管一次查询请求返回的是单页（single page）的结果，`scroll` API 可以用于在单次查询请求中检索大量的结果（甚至是所有的结果），就像在传统数据库上使用游标一样。

&emsp;&emsp;Scrolling不是用来（intend for）实时用户请求（real time user request），而是用于处理大量的数据。 比如说为了将一个data stream或index的内容重新索引到具有不同配置的新的data stream或索引中。

> **Client support for scrolling and reindexing**
> Some of the officially supported clients provide helpers to assist with scrolled searches and reindexing:
>Perl
>    &emsp;&emsp;See [Search::Elasticsearch::Client::5_0::Bulk](https://metacpan.org/pod/Search::Elasticsearch::Client::5_0::Bulk) and [Search::Elasticsearch::Client::5_0::Scroll](https://metacpan.org/pod/Search::Elasticsearch::Client::5_0::Scroll)
>Python
>    &emsp;&emsp;See [elasticsearch.helpers.\*](https://elasticsearch-py.readthedocs.org/en/master/helpers.html)
>JavaScript
>    &emsp;&emsp;See [client.helpers.\*](https://www.elastic.co/guide/en/elasticsearch/client/javascript-api/current/client-helpers.html)

>NOTE：从scroll查询返回的数据反应了data stream或者index在最初的执行查询时的状态，就像是快照一样。接下来文档发生的变化（增加，更新或者删除）只有在下次新的scroll查询中生效。

&emsp;&emsp;为了能使用scrolling，最初的查询请求应该在query string中指定`scroll`参数，这个参数用来告诉Elasticsearch保留"search context"的时间（见[Keeping the search context alive](#Keeping the search context alive)）， 比如`?scroll=1m`。

```text
POST /my-index-000001/_search?scroll=1m
{
  "size": 100,
  "query": {
    "match": {
      "message": "foo"
    }
  }
}
```

&emsp;&emsp;上述的查询结果中会包含一个`_scroll_id`，这个值需要传给`scroll` API用于检索下一批数据。

```text
POST /_search/scroll                                                               
{
  "scroll" : "1m",                                                                 
  "scroll_id" : "DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAD4WYm9laVYtZndUQlNsdDcwakFMNjU1QQ==" 
}
```

&emsp;&emsp;第1行，可以使用`GET`或`POST`并且URL中不应该包含`index`的名称----索引的名称在最开始的查询中指定
&emsp;&emsp;第3行，`scroll`参数告诉Elasticsearch接着保留1分钟的search context
&emsp;&emsp;第4行，`scroll_id`参数

&emsp;&emsp;`size`参数允许你配置每次返回的结果数量上限。每次调用`scroll` API会返回下一批数据直到没有数据返回，比如`hits`数组为空。

>IMPORTANT：最初的查询请求以及每一个接下来的scroll请求都会返回`_scroll_id`。`_scroll_id`可能会发生变化，但是不总是每次都会变化。无论如何，应该使用最新收到的`_scroll_id`。

>NOTE：如果请求中指定了聚合（aggregation），只有最开始的查询响应中会包含聚合结果。

>NOTE：当排序字段是`_doc`时使用Scroll查询会得到优化。如果你想要遍历所有的结果而不关心结果顺序，这是最高效的选项：

```text
GET /_search?scroll=1m
{
  "sort": [
    "_doc"
  ]
}
```

##### Keeping the search context alive

&emsp;&emsp;scroll查询将返回最开始的查询请求匹配的所有文档。它会忽略随后对这些文档的更改。`scroll_id`定义了一个`search context`来保留一切（keep track of everything）使得Elasticsearch能正确的返回文档。这个`search context`在最开始的请求中创建并且通过随后的请求来保留这个`search context`。

&emsp;&emsp;`scroll`参数（传给`search`请求以及每一个`scroll`请求）告诉Elasticsearch保留search context的时间。该值（比如`1m`，见 [Time units](#API conventions)）不需要设置的太大来处理所有的数据--而是只需要足够长的时间来处理上一次的结果集。每一次`scroll`查询（使用`scroll`参数）设置一个新的过期时间。如果某次`scroll`请求没有传递`scroll`参数，那么释放`search context`回作为这次`scroll`请求的一部分。

&emsp;&emsp;通常来说，后台的合并程序会把较小的段合并为一个新的，更大的段来优化索引。一旦这些较小的段不再被使用就会被删除。在执行`scroll`查询时，合并既然会继续，但是`scroll`查询会阻止旧的段被删除，因为这些段正在被`scroll`查询使用。

> TIP：保留旧的段意味着需要更多的磁盘空间以及文件句柄（file handle）。保证你已经将你的节点配置了足够的空闲的文件句柄。见[File Descriptors](#File Descriptors)。

&emsp;&emsp;另外， 如果一个段中包含了被删除或者更新的文档，那么`search context`会在最开始的查询请求的那个时间点保持追踪（keep track）段中的每一篇文档是否都live 。如果你在一个索引上有很多open scroll并且这个索引还不断的删除或者更新文档，那么你要保证你的节点有足够的堆空间。

> NOTE：为了防止出现打开太多的scroll而产生问题，在超过一定的限制后。用户不允许再open scroll。默认情况下，open scroll的最大值是500.可以通过`search.max_open_scroll_context`来修改这个限制。

&emsp;&emsp; 你可以通过[note stats API](#Nodes stats API)来检查打开了多少个`search context`：

```text
GET /_nodes/stats/indices/search
```

##### Clear scroll

&emsp;&emsp;`search context`会在`scroll`超时后自动的移除。然而保留scroll打开需要一定的开销，如[previous section](#Keeping the search context alive)中讨论的。所以一旦不需要scroll查询后 就可以通过`clear-scroll`API来显示的关闭：

```text
DELETE /_search/scroll
{
  "scroll_id" : "DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAD4WYm9laVYtZndUQlNsdDcwakFMNjU1QQ=="
}
```

&emsp;&emsp;可以通过数组方式删除多个scroll IDs：

```text
DELETE /_search/scroll
{
  "scroll_id" : [
    "DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAD4WYm9laVYtZndUQlNsdDcwakFMNjU1QQ==",
    "DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAABFmtSWWRRWUJrU2o2ZExpSGJCVmQxYUEAAAAAAAAAAxZrUllkUVlCa1NqNmRMaUhiQlZkMWFBAAAAAAAAAAIWa1JZZFFZQmtTajZkTGlIYkJWZDFhQQAAAAAAAAAFFmtSWWRRWUJrU2o2ZExpSGJCVmQxYUEAAAAAAAAABBZrUllkUVlCa1NqNmRMaUhiQlZkMWFB"
  ]
}
```

&emsp;&emsp;所有的`search context`都可以通过`_all`参数清除：

```text
DELETE /_search/scroll/_all
```

&emsp;&emsp;`scroll_id`可以作为一个query string参数或者在请求body中。多个scroll IDs使用逗号分隔：

```text
DELETE /_search/scroll/DXF1ZXJ5QW5kRmV0Y2gBAAAAAAAAAD4WYm9laVYtZndUQlNsdDcwakFMNjU1QQ==,DnF1ZXJ5VGhlbkZldGNoBQAAAAAAAAABFmtSWWRRWUJrU2o2ZExpSGJCVmQxYUEAAAAAAAAAAxZrUllkUVlCa1NqNmRMaUhiQlZkMWFBAAAAAAAAAAIWa1JZZFFZQmtTajZkTGlIYkJWZDFhQQAAAAAAAAAFFmtSWWRRWUJrU2o2ZExpSGJCVmQxYUEAAAAAAAAABBZrUllkUVlCa1NqNmRMaUhiQlZkMWFB
```

#### Sliced scroll

&emsp;&emsp;

### Retrieve selected fields from a search
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-fields.html)

&emsp;&emsp;默认情况下，查询响应中的每一个命中的结果中都会包含文档的[\_source](#_source field)，它是一个整个JSON object，即被索引的文档。下面有两个推荐的方法在一次查询请求中获取指定的（selected）的字段：

- 在index mapping中使用[fields option](#The fields option)展示要提取的字段
- 在索引期间使用[\_source option](#The _source option)指定你需要访问的原始数据

&emsp;&emsp;你可以同时使用这些方法，`fields`这个方法更好些因为它能同时参考（consult）文档数据和index mappings。在某些情况下，你可能想要[other methods](#Other methods of retrieving data)来获取数据。

#### The fields option

&emsp;&emsp;若要在查询响应中获取指定的字段，则使用`fields`参数。相较于引用`_source`，`fields`参数提供了几个有点，特别是，使用`fields`参数可以：

- 以一种标准化的方式返回每一个匹配mapping类型的值
- accepts [multi-fields ](#fields)和[field aliases](#Alias field type)
- 格式化日期和空间数据类型
- 获取出[runtime field values](#Retrieve a runtime field)
- 返回在索引期间通过脚本计算出的字段
- 使用[lookup runtime fields](#Retrieve fields from related indices)返回关联索引中的字段

&emsp;&emsp;Other mapping options are also respected。包括[ignore_above](#ignore_above)，[ignore_malformed](#ignore_malformed)，[null_value](#null_value)。

&emsp;&emsp;`fields`选项返回的值的方式跟Elasticsearch索引它们的方式是匹配的。对于standard fields，`fields`选项在`_source`中找到值，然后解析并使用mappings进行format。

##### Retrieve specific fields

&emsp;&emsp;下面的查询请求使用`fields`参数获取`user.id`、`http.response`下所有的字段、`@timestamp`这些字段的值。

&emsp;&emsp;使用object notation，你可以传递一个[format](#format(mapping parameter))自定义时间或者地理空间的值的格式。

```text
POST my-index-000001/_search
{
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  },
  "fields": [
    "user.id",
    "http.response.*",         
    {
      "field": "@timestamp",
      "format": "epoch_millis" 
    }
  ],
  "_source": false
}
```

&emsp;&emsp;第10行，可以是完整的字段名或者是通配符pattern。
&emsp;&emsp;第13行，使用`format`参数对字段值进行format

> NOTE：默认情况下，当请求的`fields`选项使用了像`*`的通配符pattern，例如`_id`或者`_index`这些文档元数据是不会返回的。然而，当显示的使用这些字段名时，`_id`、`_routing`、`_ignored`、`_index`、`_version`这些元数据字段才返回。

##### Response always returns an array

&emsp;&emsp;`fields`响应中返回每一个字段的数组值，即使有些字段在`_source`是单值。这是因为Elasticsearch没有专门的数组类型，并且任何字段都有可能存在多值。`fields`参数同样不能保证数组中的值有一定的顺序。见[arrays](#Arrays)的mapping文档了解更多背景。

&emsp;&emsp;响应中的`fields`块中包含一个平铺的list。因为`fields`参数不会获取整个object，只会返回leaf fields。

```text
{
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_id" : "0",
        "_score" : 1.0,
        "fields" : {
          "user.id" : [
            "kimchy"
          ],
          "@timestamp" : [
            "4098435132000"
          ],
          "http.response.bytes": [
            1070000
          ],
          "http.response.status_code": [
            200
          ]
        }
      }
    ]
  }
}
```

##### Retrieve nested fields

&emsp;&emsp;使用`fields`返回[nested fields](#Nested field type)的响应中其他普通的object fields都少许不同。在普通的`object`fields 中的leaf values的返回内容是平铺的列表（flat list），而`nested field`返回的内容也是一个平铺的列表，列表中是分组为一个个object，每一个object是nested fields的一个字段并且字段值也是一个数组。多层的nested fields按照这个规则嵌套下去。

&emsp;&emsp;下面的mapping中，`user`是一个nested field，索引下面的文档并获取`user`field中所有的字段：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "group" : { "type" : "keyword" },
      "user": {
        "type": "nested",
        "properties": {
          "first" : { "type" : "keyword" },
          "last" : { "type" : "keyword" }
        }
      }
    }
  }
}

PUT my-index-000001/_doc/1?refresh=true
{
  "group" : "fans",
  "user" : [
    {
      "first" : "John",
      "last" :  "Smith"
    },
    {
      "first" : "Alice",
      "last" :  "White"
    }
  ]
}

POST my-index-000001/_search
{
  "fields": ["*"],
  "_source": false
}
```

&emsp;&emsp;下面的响应中，对`first`和`last`名称进行了分组而不是平铺出来。

```text
{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 1,
      "relation": "eq"
    },
    "max_score": 1.0,
    "hits": [{
      "_index": "my-index-000001",
      "_id": "1",
      "_score": 1.0,
      "fields": {
        "group" : ["fans"],
        "user": [{
            "first": ["John"],
            "last": ["Smith"]
          },
          {
            "first": ["Alice"],
            "last": ["White"]
          }
        ]
      }
    }]
  }
}
```

&emsp;&emsp;nested field会根据它们的nested path进行分组，不受获取时使用的pattern的影响。例如，如果你在上面的例子中只查询`user.first`：

```text
POST my-index-000001/_search
{
  "fields": ["user.first"],
  "_source": false
}
```

&emsp;&emsp;响应中只返回user的first name，但是仍然是上面的例子的结构：

```text
{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 1,
      "relation": "eq"
    },
    "max_score": 1.0,
    "hits": [{
      "_index": "my-index-000001",
      "_id": "1",
      "_score": 1.0,
      "fields": {
        "user": [{
            "first": ["John"]
          },
          {
            "first": ["Alice"]
          }
        ]
      }
    }]
  }
}
```

&emsp;&emsp;然而，如果`fields`中的目标只填写`user`字段，那不会返回任何值因为没法匹配任何一个leaf fields。

##### Retrieve unmapped fields

&emsp;&emsp;默认情况下，`fields`参数只返回mapped fields的值，然而，Elasticsearch允许存储`_source`中的字段，并且这些字段是unmapped。 例如设置[dynamic  field mapping](#Dynamic field mapping)的值为`false`或者使用一个object字段并且设置`enabled`为`false`。这些选项使得关闭解析并且不索引object的内容。

&emsp;&emsp;若要在object中获取`_source`中的unmapped字段，那么在`fields`中 使用`include_unmapped`选项：

```text
PUT my-index-000001
{
  "mappings": {
    "enabled": false 
  }
}

PUT my-index-000001/_doc/1?refresh=true
{
  "user_id": "kimchy",
  "session_data": {
     "object": {
       "some_field": "some_value"
     }
   }
}

POST my-index-000001/_search
{
  "fields": [
    "user_id",
    {
      "field": "session_data.object.*",
      "include_unmapped" : true 
    }
  ],
  "_source": false
}
```

&emsp;&emsp;第4行，关闭所有的mapping
&emsp;&emsp;第24行，包含匹配这个field pattern的unmapped fields

&emsp;&emsp;这个响应会包含`session_data.object.*`路径中的字段值，即使这些字段是unmapped。`user.id`同样是unmapped，但是没有出现在响应中因为没有为那个field pattern设置`include_unmapped`。

```text
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_id" : "1",
        "_score" : 1.0,
        "fields" : {
          "session_data.object.some_field": [
            "some_value"
          ]
        }
      }
    ]
  }
}
```


##### Ignored field values

&emsp;&emsp;响应中的`fields`块中只返回合法索引的值。如果你的查询请求要求的字段值忽略某些值，这些值是malformed或者太大，这些字段值会在另一个`ignored_field_values`块中返回。

&emsp;&emsp;在这个例子中，我们索引了一篇文档，文档中的一个值被限制了长度导致没有添加到索引中，所以在查询请求中分开的显示：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "my-small" : { "type" : "keyword", "ignore_above": 2 }, 
      "my-large" : { "type" : "keyword" }
    }
  }
}

PUT my-index-000001/_doc/1?refresh=true
{
  "my-small": ["ok", "bad"], 
  "my-large": "ok content"
}

POST my-index-000001/_search
{
  "fields": ["my-*"],
  "_source": false
}
```

&emsp;&emsp;第5行，这个字段有长度限制
&emsp;&emsp;第13行，这个文档有一个字段值超过了长度限制所以没有被索引

&emsp;&emsp;在这个响应中，在`ignored_field_values`路径中包含了被限制了长度的字段值。这个字段值是从文档的原始JSON source中获取出来的并且就是原始的数据，不会被formatted或者任何方式处理。不同于成功写入到索引的字段，它们的字段值在`fields`块中返回。

```text
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_id" : "1",
        "_score" : 1.0,
        "_ignored" : [ "my-small"],
        "fields" : {
          "my-large": [
            "ok content"
          ],
          "my-small": [
            "ok"
          ]
        },
        "ignored_field_values" : {
          "my-small": [
            "bad"
          ]
        }
      }
    ]
  }
}
```

#### The \_source option

&emsp;&emsp;你可以使用`_source`参数选择返回source中的哪些字段。这种称为`source filtering`。

&emsp;&emsp;下面的search API请求设置了请求body参数`_source`为`false`。故响应中没有包括document source。

```text
GET /_search
{
  "_source": false,
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  }
}
```

&emsp;&emsp;若要返回一部分source fields，可以在`_source`参数中指定 一个通配符（`*`）pattern。下面的search API请求只返回`obj`字段和它的properties。

```text
GET /_search
{
  "_source": "obj.*",
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  }
}
```

&emsp;&emsp;你也可以在`_source`中指定一个通配符pattern数组。下面的search API请求只返回`obj1`和`obj2`字段和它们的properties。

```text
GET /_search
{
  "_source": [ "obj1.*", "obj2.*" ],
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  }
}
```

&emsp;&emsp;为了更好的控制，你可以在`_source`中指定一个object，里面包含`includes`和`excludes` patterns。

&emsp;&emsp;如果使用了`includes`属性，只返回匹配它里面的pattern的字段。你可以使`excludes`从这个子集中进一步排除。

&emsp;&emsp;如果没有指定`includes`属性，所有的document source都会返回，除了匹配`excludes`中的pattern的字段。

&emsp;&emsp;下面的search API请求返回`obj1`和`obj2`的字段，但是排除所有child field为`description`的字段。

```text
GET /_search
{
  "_source": {
    "includes": [ "obj1.*", "obj2.*" ],
    "excludes": [ "*.description" ]
  },
  "query": {
    "term": {
      "user.id": "kimchy"
    }
  }
}
```

#### Other methods of retrieving data

&emsp;&emsp;通常来说使用`field`是比较好的，除非你必须一定要载入stored或者`docvalue_fields`。

&emsp;&emsp;文档的`_source`在Lucene中作为单独一个字段存储的。这种结构意味着需要加载整个`_source`对象，即使你只需要里面的部分内容。为了避免这种情况，你可以尝试其他办法加载字段：

- 使用[docvalue_fields](#Doc value fields)参数获取选择的字段的字段值。当需要返回较小数量的字段时是一个好的选择，支持doc values例如keyword和date。
- 使用[stored_fields](#Stored fields-1)获取stored field的字段值（使用了[store](#store) mapping parameter的字段）

&emsp;&emsp;Elasticsearch总是尝试从`_source`中加载字段值。这种方式跟source filtering是一样的实现方式，即Elasticsearch需要加载并解析整个`_source`进行检索，即使只需要获取某一个字段。

##### Doc value fields

&emsp;&emsp;你可以使用[docvalue_fields](#Doc value fields)参数在查询响应中返回一个或多个[dov values](#doc_values)的字段。

&emsp;&emsp;Doc Values跟`_source`一样都会存储，但以列式存储的数据结构存放，用于排序和聚合。由于每一个字段都是各自独立存储，Elasticsearch只需要读取请求中指定的字段，并能避免加载整个文档`_source`。

&emsp;&emsp;Doc values are stored for supported fields by default。然而，doc value不支持[text](#Text type family)或者[text_annotated](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/mapper-annotated-text-usage.html)字段。

&emsp;&emsp;下面的查询请求使用`docvalue_fields`参数获取user.id，所有以`http.response`开头的以及`@timestamp`的doc value。

```text
GET my-index-000001/_search
{
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  },
  "docvalue_fields": [
    "user.id",
    "http.response.*", 
    {
      "field": "date",
      "format": "epoch_millis" 
    }
  ]
}
```

&emsp;&emsp;第10行，同时支持完整的字段名和通配符pattern
&emsp;&emsp;第13行，使用object notation，你可以传递一个`format`参数指定自定义的format应用到doc value上。[Date fields](#Date field type)支持[date format](#format(mapping parameter))，[Numeric fields](#Numeric field types)支持[DecimalFormat pattern](https://docs.oracle.com/javase/8/docs/api/java/text/DecimalFormat.html)。其他的字段不支持`format`参数。

> TIP：你不可以为nested object使用`docvalue_fields`参数 来获取doc value。如果你指定了一个nested object，则返回一个空的数组。若要访问nested fields，使用[inner_hits](#Retrieve inner hits)参数中的`docvalue_fields`属性

##### Stored fields

&emsp;&emsp;可以使用mapping 参数[store](#store(mapping parameter))来存储字段值。你可以使用`stored_fields`参数在响应中包含存储字段的字段值。

> WARNING：`stored_fields`用于那些在mapping中显示使用`stroed`的字段，默认是不启动并且通常不建议启用。建议使用[source fitlering](#The _source option)从原始数据（也就是`_source`）中获取选择的字段。

&emsp;&emsp;运行在查询中选择指定文档中的存储字段。

```text
GET /_search
{
  "stored_fields" : ["user", "postDate"],
  "query" : {
    "term" : { "user" : "kimchy" }
  }
}
```

&emsp;&emsp;`*`可以用来获取文档中所有的存储字段。

&emsp;&emsp;空的数组只能返回`_id`和`_type`这两个存储字段，例如：

```text
GET /_search
{
  "stored_fields" : [],
  "query" : {
    "term" : { "user" : "kimchy" }
  }
}
```

&emsp;&emsp;想要查询的字段如果没有存储（mapping参数`store`设置为`false`），这些字段则会忽略。

&emsp;&emsp;从文档中获取的存储字段总是以数组形式返回。相反，像`_routing`这样的元数据字段从不以数组形式返回。

&emsp;&emsp;此外，只有leaf field可以通过`stored_fields`选项返回。如果指定了一个对象字段，它将被忽略。

> NOTE：单独使用`stored_fields`无法加载嵌套对象中的字段——如果一个字段在其路径中包含一个嵌套对象，那么该存储字段将不会返回任何数据。要访问嵌套字段，必须在[inner_hits](#Retrieve inner hits)块内使用stored_fields。

##### Disable stored fields

&emsp;&emsp;若要完全禁用存储字段（以及元数据字段）可以使用：`_none_`：

```text
GET /_search
{
  "stored_fields": "_none_",
  "query" : {
    "term" : { "user" : "kimchy" }
  }
}
```

> NOTE：如果使用了`_none，`[\_srouce](#The _source option)跟[version](#Search API)也不能使用

##### Script fields

&emsp;&emsp;你可以使用`script_fields`参数获取一个[script evaluation](#Scripting)（基于不同的字段），例如：

```text
GET /_search
{
  "query": {
    "match_all": {}
  },
  "script_fields": {
    "test1": {
      "script": {
        "lang": "painless",
        "source": "doc['price'].value * 2"
      }
    },
    "test2": {
      "script": {
        "lang": "painless",
        "source": "doc['price'].value * params.factor",
        "params": {
          "factor": 2.0
        }
      }
    }
  }
}
```

&emsp;&emsp;Script fields可以应用在非存储字段上（比如上面的`price`）并且允许返回自定义的值（脚本中计算出的值）

&emsp;&emsp;Script fields同样可以访问实际的`_source`文档并且通过`params['_source']`提取指定的内容。例子：

```text
GET /_search
{
  "query": {
    "match_all": {}
  },
  "script_fields": {
    "test1": {
      "script": "params['_source']['message']"
    }
  }
}
```

&emsp;&emsp;注意的是，请求中的关键字`_source`用来导出类似json的模型（json-like model）

&emsp;&emsp;理解`doc['my_field'].value`和`params['_source']['my_field']`之间的区别很重要。首先，使用doc关键字，将导致该字段的术语被加载到内存中（缓存），这将导致执行速度更快，但消耗更多内存。此外，doc[...]表示法仅允许简单值字段（你不能从中返回一个json对象）并且只对`non-analyzed`或single term的字段有意义。然而，如果可能的话，使用doc仍然是访问文档内容的推荐方式，因为每次使用时都必须加载和解析\_source。使用\_source是非常慢的。

### Retrieve inner hits
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/inner-hits.html)

### Search across clusters
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-cross-cluster-search.html)

&emsp;&emsp;**Cross-cluster search**能让你对一个或者多个集群运行单个查询请求。例如你可以使用CCS（Cross-cluster Search）对存储在不同的数据中心的集群上的日志数据进行过滤和分析。

#### Supported APIs

&emsp;&emsp;下面的APIs支持CCS：

- [Search](#Search API)
- [Async search](#Async search)
- [Multi search](#Multi search API)
- [Search template](#Search templates)
- [Multi search template](#Multi search template API)
- [Field capabilities](#Field capabilities API)
- preview[EQL search](#EQL search API)
- preview[SQL search](#SQL search API)
- preview[Vector tile search](#Vector tile search API)

#### Prerequisites

- CCS要求有remote cluster。若要在Elasticsearch服务上设置remote cluster，见[configure remote clusters on Elasticsearch Service](https://www.elastic.co/guide/en/cloud/current/ec-enable-ccs.html)，如果你在自己的硬件上运行Elasticsearch，见[Remote clusters](#Remote clusters)。

- 确保你的remote cluster配置支持CCS，见[Supported cross-cluster search configurations](#Supported cross-cluster search configurations)

- 本地的coordinating node必须要有[remote_cluster_client](#Remote-eligible node)角色

- 如果你使用[sniff mode](#Sniff mode)，本地的coordinating node必须能够连接到remote cluster上的seed和gateway节点
  - 我们建议使用gateway node，它具有coordinating node的服务能力。seed node可以是这些gateway node的一个子集

- 如果你使用[proxy mode](#Proxy mode)，本地的coordinating node必须能够连接配置的`proxy_address`。这个地址必须能够路由到remote cluster上的gateway和coordinating节点

- CCS在本地集群和remote cluster上要求有不同的security privilege。见[Configure privileges for cross-cluster search](#Configure privileges for cross-cluster search)和[Configure privileges for cross-cluster search and Kibana](#Configure privileges for cross-cluster search and Kibana)

#### Cross-cluster search examples

##### Remote cluster setup

&emsp;&emsp;下面的[cluster update settings](#Cluster update settings API) API请求添加了三个remote集群：`cluster_one`，`cluster_two`，`cluster_three`。

```text
PUT _cluster/settings
{
  "persistent": {
    "cluster": {
      "remote": {
        "cluster_one": {
          "seeds": [
            "127.0.0.1:9300"
          ]
        },
        "cluster_two": {
          "seeds": [
            "127.0.0.1:9301"
          ]
        },
        "cluster_three": {
          "seeds": [
            "127.0.0.1:9302"
          ]
        }
      }
    }
  }
}
```

##### Search a single remote cluster

&emsp;&emsp;在查询请求中，指定remote cluster上的data stream以及索引：`<remote_cluster_name>:<target>`。

&emsp;&emsp;下面的[search](#Search API) API请求在名为`cluster_one`的单个remote cluster上查询索引`my-index-000001`。

```text
GET /cluster_one:my-index-000001/_search
{
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  },
  "_source": ["user.id", "message", "http.response.status_code"]
}
```

&emsp;&emsp;API返回下面的响应：

```text
{
  "took": 150,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "failed": 0,
    "skipped": 0
  },
  "_clusters": {
    "total": 1,
    "successful": 1,
    "skipped": 0
  },
  "hits": {
    "total" : {
        "value": 1,
        "relation": "eq"
    },
    "max_score": 1,
    "hits": [
      {
        "_index": "cluster_one:my-index-000001", 
        "_id": "0",
        "_score": 1,
        "_source": {
          "user": {
            "id": "kimchy"
          },
          "message": "GET /search HTTP/1.1 200 1070000",
          "http": {
            "response":
              {
                "status_code": 200
              }
          }
        }
      }
    ]
  }
}
```

&emsp;&emsp;第23行，查询响应中的`_index`参数包含了remote cluster的名称

##### Search multiple remote clusters

&emsp;&emsp;下面的[search](#Search APIs) API请求在三个集群上查询索引`my-index-000001`：

- 你的本地集群（local cluster）
- 两个远程集群（remote cluster），`cluster_one`和`cluster_two`

```text
GET /my-index-000001,cluster_one:my-index-000001,cluster_two:my-index-000001/_search
{
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  },
  "_source": ["user.id", "message", "http.response.status_code"]
}
```

&emsp;&emsp;API返回下面的响应：

```text
{
  "took": 150,
  "timed_out": false,
  "num_reduce_phases": 4,
  "_shards": {
    "total": 3,
    "successful": 3,
    "failed": 0,
    "skipped": 0
  },
  "_clusters": {
    "total": 3,
    "successful": 3,
    "skipped": 0
  },
  "hits": {
    "total" : {
        "value": 3,
        "relation": "eq"
    },
    "max_score": 1,
    "hits": [
      {
        "_index": "my-index-000001", 
        "_id": "0",
        "_score": 2,
        "_source": {
          "user": {
            "id": "kimchy"
          },
          "message": "GET /search HTTP/1.1 200 1070000",
          "http": {
            "response":
              {
                "status_code": 200
              }
          }
        }
      },
      {
        "_index": "cluster_one:my-index-000001", 
        "_id": "0",
        "_score": 1,
        "_source": {
          "user": {
            "id": "kimchy"
          },
          "message": "GET /search HTTP/1.1 200 1070000",
          "http": {
            "response":
              {
                "status_code": 200
              }
          }
        }
      },
      {
        "_index": "cluster_two:my-index-000001", 
        "_id": "0",
        "_score": 1,
        "_source": {
          "user": {
            "id": "kimchy"
          },
          "message": "GET /search HTTP/1.1 200 1070000",
          "http": {
            "response":
              {
                "status_code": 200
              }
          }
        }
      }
    ]
  }
}
```

&emsp;&emsp;第24行，这篇文档的`_index`参数没有集群的名称，说明这个文档来自本地集群
&emsp;&emsp;第41行，这篇文档来自`cluster_one`
&emsp;&emsp;第58行，这篇文档来自`cluster_two`

#### Optional remote clusters

&emsp;&emsp;默认情况下，如果请求中的某个remote cluster返回了一个错误或者不可用（unavailable），那么CCS就会失败。使用cluster setting `skip_unavailable`标记特定的remote cluster在CCS时是可选的。

&emsp;&emsp;如果`skip_unavailable`为`true`。CCS将：

- 在搜索过程中如果节点不可用则跳过这个remote cluster。响应中的`_cluster.skipped`的值包含跳过的集群数量
- 忽略remote cluster返回的错误，比如不可用的分片或者索引的相关错误。这可能包含跟查询参数例如[allow_no_indices](#allow_no_indices)以及[ignore_unavailable](#ignore_unavailable)相关的错误
- 在查询remote cluster时，忽略[allow_partial_search_results](#allow_partial_search_results)参数和相关集群设置`search.default_allow_partial_results`。这意味着remote cluster可能返回部分结果

&emsp;&emsp;下面的[cluster update settings ](#Cluster update settings API) API请求将`cluster_two`的`skip_unavailable`设置为`true`。

```text
PUT _cluster/settings
{
  "persistent": {
    "cluster.remote.cluster_two.skip_unavailable": true
  }
}
```

&emsp;&emsp;如果`cluster_two`在CCS期间失去连接或者不可用。Elasticsearch不会在最终的结果中包含在那个集群中匹配到的文档。

#### How cross-cluster search handles network delays

&emsp;&emsp;由于CCS涉及到发送请求到remote cluster上，任何的网络延迟都会影响查询速度。为了避免慢查询（slow search），CCS提供了两个的方法来处理网络延迟：

- [Minimize network roundtrips](#Minimize network roundtrips)
  - 默认情况下，Elasticsearch会降低remote  cluster之间的网络往返（network roundtrip）的次数，能降低网络延迟对查询速度的影响。然而，Elasticsearch不能为large search request降低网络往返，比如[scroll](#Scroll search results)或[inner hits](#Retrieve inner hits)
  - 见[Minimize network roundtrips](#Minimize network roundtrips)了解其工作方式
- [Don’t minimize network roundtrips](#Don’t minimize network roundtrips)
  - 对于scroll或者inner hits这种查询，Elasticsearch会像每一个remote cluster发送多个传出（outgoing）传入（ingoing）请求。你可以将参数[ccs_minimize_roundtrips](#ccs_minimize_roundtrips)设置为`false`来选择当前的方法。While typically slower, this approach may work well for networks with low latency
  - 见[Don’t minimize network roundtrips](#Don’t minimize network roundtrips)了解其工作方式

##### Minimize network roundtrips

&emsp;&emsp;下面是在你minimize network roundtrips后，CCS的工作方式。

1. 你发送一个CCS请求到你的本地集群。集群中的coordinating node接收并解析这个请求。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ccs-min-roundtrip-client-request.svg">

2. coordinating node向每一个集群发送一个查询请求，包括本地的集群。每一个集群各自独立的执行这个查询请求，将它们自己的cluster-level的设置应用到这个请求上。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ccs-min-roundtrip-cluster-search.svg">

3. 每一个remote cluster将其查询结果返回到coordinating node。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ccs-min-roundtrip-cluster-results.svg">

4. 从每一个集群收集完结果后，coordinating node将最终的结果到CCS的响应中。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ccs-min-roundtrip-client-response.svg">

##### Don’t minimize network roundtrips

&emsp;&emsp;下面是没有minimize network roundtrips后，CCS的工作方式。

1. 你发送一个CCS请求到你的本地集群。集群中的coordinating node接收并解析这个请求。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ccs-min-roundtrip-client-request.svg">

2. coordinating node向每一个remote cluster发送一个[search shards](#Search shards API) API请求。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ccs-min-roundtrip-cluster-search.svg">

3. 每一个remote cluster将其响应发送回coordinating node。这个响应中包含了CCS将会执行索引和分片信息。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ccs-min-roundtrip-cluster-results.svg">

4. coordinating node发送一个查询请求到每一个分片，包括它所在的集群。每一个分片各自独立的执行查询请求。

> WARNING：当没有最小化网络往返时，这个查询的执行过程就像是所有的数据都在coordinating node的集群中。我们建议更新cluster-level的设置来限制查询，例如`action.search.shard_count.limit`，`pre_filter_shard_size`和`max_concurrent_shard_requests`。如果限制值太低，查询可能被reject。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ccs-dont-min-roundtrip-shard-search.svg">

5. 每一个分片将其查询结果返回到coordinating node。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ccs-dont-min-roundtrip-shard-results.svg">

6.  从每一个集群收集完结果后，coordinating node将最终的结果到CCS的响应中。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ccs-min-roundtrip-client-response.svg">

#### Supported cross-cluster search configurations

&emsp;&emsp;在8.0+中，Elastic支持本地从本地集群到remote cluster的查询的版本：

- 之前的minor版本
- 相同的版本
- 相同major版本下更新的minor版本

&emsp;&emsp;Elastic同样支持从本地集群（某个major版本中的最新的minor版本）到remote cluster（下一个major版本中的所有minor版本）的查询。例如，7.17版本的本地集群可以从8.x的任意版本的集群上查询。

|                           | **Remote cluster version** |          |      |      |      |      |
| :-----------------------: | :------------------------: | :------: | :--: | :--: | ---- | ---- |
| **Local cluster version** |            6.8             | 7.1–7.16 | 7.17 | 8.0  | 8.1  | 8.2  |
|            6.8            |             √              |    √     |  √   |  ×   | ×    | ×    |
|         7.1–7.16          |             √              |    √     |  √   |  ×   | ×    | ×    |
|           7.17            |             √              |    √     |  √   |  √   | √    | √    |
|            8.0            |             ×              |    ×     |  √   |  √   | √    | √    |
|            8.1            |             ×              |    ×     |  ×   |  √   | √    | √    |
|            8.2            |             ×              |    ×     |  ×   |  ×   | √    | √    |


> IMPORTANT：对于[EQL search API](#EQL search API)，本地和远程集群必须使用相同的Elasticsearch版本。

&emsp;&emsp;例如，8.0的本地集群可以查询7.17和所有8.x的集群。然而，8.0的本地集群不能查询7.16或者6.8的集群。

&emsp;&emsp;所有集群都有的功能才能被支持。在某个remote cluster使用某个不具备的功能会导致undefined behavior。

&emsp;&emsp;CCS中使用了不支持的配置可能可以正常工作。然而，Elastic没有测试过这种查询，不能保证其行为能正确工作。

##### Ensure cross-cluster search support

&emsp;&emsp;保证支持CCS最简单的方式就是每一个集群保持相同的版本。如果需要维护不同版本的集群，你可以：

- 维护一个专用的集群用于CCS。Keep this cluster on the earliest version needed to search the other clusters。例如，如果你有7.17和8.x集群，你可以维护一个专用的7.17集群作为本地集群用于CCS。
- 让每一个集群之间保持不超过一个minor版本的差异。这样能让你使用任意一个集群作为本地集群用于CCS。

##### Cross-cluster search during an upgrade

&emsp;&emsp;在本地集群上执行rolling upgrade时仍然可以查询remote cluster。然而，本地集群中的coordinating node的"upgrade from"和"upgrade to"的版本必须和remote cluster上的gateway node兼容。

> WARNING：Running multiple versions of Elasticsearch in the same cluster beyond the duration of an upgrade is not supported。

&emsp;&emsp;见[Upgrading Elasticsearch](https://www.elastic.co/guide/en/elastic-stack/8.2/upgrading-elasticsearch.html)了解更多升级信息。

### Search multiple data streams and indices
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-multiple-indices.html)

&emsp;&emsp;若要搜索多个data stream和索引，则使用逗号隔开添加到[search API](#Search API)请求路径上。

&emsp;&emsp;下面的请求查询索引`my-index-000001`和`my-index-000002`。

```text
GET /my-index-000001,my-index-000002/_search
{
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  }
}
```

&emsp;&emsp;你也可以使用一个index pattern查询多个data stream和索引。

&emsp;&emsp;下面的请求目标是`my-index-*`的index pattern。这个请求会查询集群中名称以`my-index-`开头的data stream或者索引。

```text
GET /my-index-*/_search
{
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  }
}
```

&emsp;&emsp;若要查询集群中所有的data stream和索引，将请求参数的目标改为`_all`或者`*`。

&emsp;&emsp;下面的请求都是相同的，会查询集群中所有的data stream和index。

```text
GET /_search
{
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  }
}

GET /_all/_search
{
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  }
}

GET /*/_search
{
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  }
}
```

#### Index boost

&emsp;&emsp;在查询多个索引索引时，你可以使用`indices_boost`参数boost一个或多个索引的结果。适用于从某些索引返回的结果比其他索引中的结果重要的场景。

> NOTE：你不能对data stream使用`indices_boost`。

```text
GET /_search
{
  "indices_boost": [
    { "my-index-000001": 1.4 },
    { "my-index-000002": 1.3 }
  ]
}
```

&emsp;&emsp;也可以用于aliases和index pattern：

```text
GET /_search
{
  "indices_boost": [
    { "my-alias":  1.4 },
    { "my-index*": 1.3 }
  ]
}
```

&emsp;&emsp;如果匹配到了多个，则使用先匹配到的对应的boost。例如，如果某个索引属于`alias1`同时匹配了`my-index*` pattern，那么 boost value为`1.4`。

### Search shard routing
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-shard-routing.html#search-adaptive-replica)

&emsp;&emsp;为了应对硬件故障以及提高搜索能力（search capacity），Elasticsearch会在多个节点上存储索引数据对应的多个副本分片。当允许一个查询请求时，Elasticsearch会选择一个包含索引数据副本的节点并且将这个查询请求转发到那个节点上的分片上。这个处理过程就是众所周知的`search shard routing`或者`routing`。

#### Adaptive replica selection

&emsp;&emsp;默认情况下，Elasticsearch使用`adaptive replica selection`来路由查询请求。这个方法使用[shard allocation awareness](#Cluster-level shard allocation and routing settings)和下面的条件来选择有资格的节点（eligible node）：

- coordinating node和eligible node之间上一次的响应时间
- eligible node上一次（previous）查询花费的时间
- eligible node的`search` [threadpool](#Thread pools) 的队列大小

&emsp;&emsp;Adaptive replica selection被设计为用于降低查询延迟。你可以使用[cluster settings API](#Cluster update settings API)设置`cluster.routing.use_adaptive_replica_selection`为`false`来关闭它。如果关闭了adaptive replica selection，Elasticsearch使用轮询（round-robin）方式来路由查询请求，这可能会导致查询缓慢。

#### Set a preference

&emsp;&emsp;默认情况下，adaptive replica selection从所有的eligible node和分片中进行选择。然而你可能只想从一个本地节点（local node）获取数据或者基于硬件因素将查询路由到指定的节点。或者你想要将重复的查询（repeated searcher）发送到同一个分片，使得可以利用cache。

&emsp;&emsp;若要为查询请求限制eligible node和分片集合，你可以使用查询的API's的参数[preference](#preference)。

&emsp;&emsp;例如，下面的查询请求中使用了`preference`为`_local`的参数来查询索引`my-index-000001`。它将查询限制到本地节点（local node）。如果local node没有这个索引数据的分片，这个请求会使用adaptive replica selection去其他eligible node，这个节点作为一个fallback。

```text
GET /my-index-000001/_search?preference=_local
{
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  }
}
```

&emsp;&emsp;你也可以使用`preference`参数并基于提供的值将查询路由到指定的分片。如果集群状态（cluster state）和选择的分片没有发生变更，查询会使用相同的`preference`的值以相同的顺序被路由到相同的分片。

&emsp;&emsp;我们建议使用一个独一无二的`preference`值，例如用户的名称或者web session ID。这个值不能以`_`开头。

> TIP：你可以使用这个方法为频繁的以及资源密集（resource-intensive）的查询使用缓存结果。如果分片没有发生变更，使用相同`preference`值的重复的查询会从相同的[shard request cache](#Shard request cache settings)中检索到结果。对于时序用例，比如说日志，旧的索引中的数据几乎不会更新，就可以从这个cache中直接返回结果

&emsp;&emsp;下面的查询请求使用了值为`my-custom-shard-string`的`preference`去索引`my-index-000001`上进行查询。

```text
GET /my-index-000001/_search?preference=my-custom-shard-string
{
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  }
}
```

> NOTE：如果集群状态和选择的分片发生了变更，相同的`preference`的查询可能不会以相同的顺序路由到相同的分片。导致这个的原因很多，包括分片重新分配（shard relocation）以及分片错误（shard failure）。节点也会reject一个查询请求，Elasticsearch会将它路由到其他的节点。

#### Use a routing value

&emsp;&emsp;当你索引一篇文档时，你可以指定一个可选的[routing value](#\_routing field
)，这样会将这篇文档索引到一个指定的分片上。

&emsp;&emsp;例如，下面的请求使用`my-routing-value`来路由一篇文档。

```text
POST /my-index-000001/_doc?routing=my-routing-value
{
  "@timestamp": "2099-11-15T13:12:00",
  "message": "GET /search HTTP/1.1 200 1070000",
  "user": {
    "id": "kimchy"
  }
}
```

&emsp;&emsp;你可以在查询的API's参数`routing`中使用相同的路由值。这能保证在相同的分片上查询。

```text
GET /my-index-000001/_search?routing=my-routing-value
{
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  }
}
```

&emsp;&emsp;你可以指定多个用逗号分隔的路由值：

```text
GET /my-index-000001/_search?routing=my-routing-value,my-routing-value-2
{
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  }
}
```

#### Search concurrency and parallelism

&emsp;&emsp;默认情况下，Elasticsearch不会基于请求命中的分片数量来reject这个查询请求。然而命中大量的分片会带来大量的CPU和内存的使用。

> TIP：为了防止索引有大量的分片，见[Avoid oversharding](#Avoid oversharding)。

&emsp;&emsp;你可以使用名为`query parameter`的查询参数（query parameter）来控制一个查询请求在一个节点上并发查询的分片数量。 这可以防止某个请求过度消耗（overloading）某个集群。这个查询参数的默认最大值是`5`。

```text
GET /my-index-000001/_search?max_concurrent_shard_requests=3
{
  "query": {
    "match": {
      "user.id": "kimchy"
    }
  }
}
```

&emsp;&emsp;你也可以使用集群设置`action.search.shard_count.limit`来限制查询命中的分片数量，在命中太多分片后就reject这个请求。你可以使用[cluster settings API](#Cluster update settings API)来配置`action.search.shard_count.limit`。

### Search templates
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-template.html)

&emsp;&emsp;search template一种查询模版并且保存在Elasticsearch中-，你可以使用不同的变量覆盖search template中的变量。

&emsp;&emsp;如果你使用Elasticsearch作为查询后端（search backend），你可以把用户在搜索框的输入作为search template的变量。这使得你不需要将Elasticsearch的查询语法暴露给用户而执行查询。

&emsp;&emsp;如果你使用Elasticsearch用于自定义的应用，search template可以让你在不更改你的应用代码的情况下修改你的查询（DSL）。

#### Create a search template

&emsp;&emsp;可以使用[create stored script API](#Create or update stored script API)创建或者更新一个search template。

&emsp;&emsp;请求中的`source`支持[search API](#Search API)的请求body中相同的参数。`source`同样支持[Mustache](https://mustache.github.io/)变量，通常用双括号包裹：`{{my-var}}`。当你允许某个template search，Elasticsearch会使用`params`中的值替换这些变量。

&emsp;&emsp;search template必须使用`mustache`语法。

&emsp;&emsp;下面的请求创建了一个`id`为`my-search-template`的search template。

```text
PUT _scripts/my-search-template
{
  "script": {
    "lang": "mustache",
    "source": {
      "query": {
        "match": {
          "message": "{{query_string}}"
        }
      },
      "from": "{{from}}",
      "size": "{{size}}"
    },
    "params": {
      "query_string": "My query string"
    }
  }
}
```

&emsp;&emsp;Elasticsearch存储search template作为集群状态中的Mustache [scripts](#Scripting)。Elasticsearch在`template` script context中编译search template。设置中限制或者关闭script同样会影响search template。

#### Validate a search template

&emsp;&emsp;使用[render search template API](#Render search template API)以及不同的`params`测试一个模板。

```text
POST _render/template
{
  "id": "my-search-template",
  "params": {
    "query_string": "hello world",
    "from": 20,
    "size": 10
  }
}
```

&emsp;&emsp;替换变量后（rendered），template输出一个[search request body](#Search API)。

```text
{
  "template_output": {
    "query": {
      "match": {
        "message": "hello world"
      }
    },
    "from": "20",
    "size": "10"
  }
}
```

&emsp;&emsp;你也可以使用API来测试inline template。

```text
POST _render/template
{
    "source": {
      "query": {
        "match": {
          "message": "{{query_string}}"
        }
      },
      "from": "{{from}}",
      "size": "{{size}}"
    },
  "params": {
    "query_string": "hello world",
    "from": 20,
    "size": 10
  }
}
```

#### Run a templated search

&emsp;&emsp;可以使用[search template API](#Search template API)运行一个search template。你可以在每一次的请求中指定不同的`params`。

```text
GET my-index/_search/template
{
  "id": "my-search-template",
  "params": {
    "query_string": "hello world",
    "from": 0,
    "size": 10
  }
}
```

&emsp;&emsp;响应中返回的属性跟[search API](#Search API)的响应是一样的。

```text
{
  "took": 36,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 1,
      "relation": "eq"
    },
    "max_score": 0.5753642,
    "hits": [
      {
        "_index": "my-index",
        "_id": "1",
        "_score": 0.5753642,
        "_source": {
          "message": "hello world"
        }
      }
    ]
  }
}
```

#### Run multiple templated searches

&emsp;&emsp;可以使用[multi search template API](#Multi search template API)在单个请求中运行多个template search。相较于多个独立的请求，这种方式的请求的开销更小并且速度更快。

```text
GET my-index/_msearch/template
{ }
{ "id": "my-search-template", "params": { "query_string": "hello world", "from": 0, "size": 10 }}
{ }
{ "id": "my-other-search-template", "params": { "query_type": "match_all" }}
```

#### Get search templates

&emsp;&emsp;可以使用[get stored script API](#Get stored script API)获取search template。

```text
GET _scripts/my-search-template
```

&emsp;&emsp;可以使用[cluster state API](#Cluster state API)获取所有的search template列表以及其他存储的脚本。

```text
GET _cluster/state/metadata?pretty&filter_path=metadata.stored_scripts
```

#### Delete a search template

&emsp;&emsp;可以使用[delete stored script API](#Delete stored script API)删除一个search template。

```text
DELETE _scripts/my-search-template
```

#### Set default values

&emsp;&emsp;使用下面的语法为变量设置一个默认值：

```text
{{my-var}}{{^my-var}}default value{{/my-var}}
```

&emsp;&emsp;如果某个template search中没有在`params`中指定一个值，那么查询会使用默认值。例如，下面的template中为`from`和`size`设置了默认值。

```text
POST _render/template
{
  "source": {
    "query": {
      "match": {
        "message": "{{query_string}}"
      }
    },
    "from": "{{from}}{{^from}}0{{/from}}",
    "size": "{{size}}{{^size}}10{{/size}}"
  },
  "params": {
    "query_string": "hello world"
  }
}
```

#### URL encode strings

&emsp;&emsp;使用`{{#url}}`功能进行URL编码。

```text
POST _render/template
{
  "source": {
    "query": {
      "term": {
        "url.full": "{{#url}}{{host}}/{{page}}{{/url}}"
      }
    }
  },
  "params": {
    "host": "http://example.com",
    "page": "hello-world"
  }
}
```

&emsp;&emsp;填充后的值如下：

```text
{
  "template_output": {
    "query": {
      "term": {
        "url.full": "http%3A%2F%2Fexample.com%2Fhello-world"
      }
    }
  }
}
```

#### Concatenate values

&emsp;&emsp;使用`{{#join}}`功能对数组里面的值用逗号拼接。例如下面的例子中拼接了两个email地址。

```text
POST _render/template
{
  "source": {
    "query": {
      "match": {
        "user.group.emails": "{{#join}}emails{{/join}}"
      }
    }
  },
  "params": {
    "emails": [ "user1@example.com", "user_one@example.com" ]
  }
}
```

&emsp;&emsp;填充后的值如下：

```text
{
  "template_output": {
    "query": {
      "match": {
        "user.group.emails": "user1@example.com,user_one@example.com"
      }
    }
  }
}
```

&emsp;&emsp;你也可以自定义指定一个分隔符。

```text
POST _render/template
{
  "source": {
    "query": {
      "range": {
        "user.effective.date": {
          "gte": "{{date.min}}",
          "lte": "{{date.max}}",
          "format": "{{#join delimiter='||'}}date.formats{{/join delimiter='||'}}"
	      }
      }
    }
  },
  "params": {
    "date": {
      "min": "2098",
      "max": "06/05/2099",
      "formats": ["dd/MM/yyyy", "yyyy"]
    }
  }
}
```

&emsp;&emsp;填充后的值如下：

```text
{
  "template_output": {
    "query": {
      "range": {
        "user.effective.date": {
          "gte": "2098",
          "lte": "06/05/2099",
          "format": "dd/MM/yyyy||yyyy"
        }
      }
    }
  }
}
```

#### Convert to JSON

&emsp;&emsp;使用`{{#toJson}}`功能将变量值用JSON表示。

&emsp;&emsp;例如，下面的template中使用`{{#toJson}}`传递一个数组。为了保证请求体式一个合法的JSON，`source`的值需要为一个string format。

```text
POST _render/template
{
  "source": "{ \"query\": { \"terms\": { \"tags\": {{#toJson}}tags{{/toJson}} }}}",
  "params": {
    "tags": [
      "prod",
      "es01"
    ]
  }
}
```

&emsp;&emsp;填充后的值如下：

```text
{
  "template_output": {
    "query": {
      "terms": {
        "tags": [
          "prod",
          "es01"
        ]
      }
    }
  }
}
```

&emsp;&emsp;你也可以`{{#toJson}}`传递object。

```text
POST _render/template
{
  "source": "{ \"query\": {{#toJson}}my_query{{/toJson}} }",
  "params": {
    "my_query": {
      "match_all": { }
    }
  }
}
```

&emsp;&emsp;填充后的值如下：

```text
{
  "template_output" : {
    "query" : {
      "match_all" : { }
    }
  }
}
```

&emsp;&emsp;你也可以传递一个object数组。

```text
POST _render/template
{
  "source": "{ \"query\": { \"bool\": { \"must\": {{#toJson}}clauses{{/toJson}} }}}",
  "params": {
    "clauses": [
      {
        "term": {
          "user.id": "kimchy"
        }
      },
      {
        "term": {
          "url.domain": "example.com"
        }
      }
    ]
  }
}
```

&emsp;&emsp;填充后的值如下：

```text
{
  "template_output": {
    "query": {
      "bool": {
        "must": [
          {
            "term": {
              "user.id": "kimchy"
            }
          },
          {
            "term": {
              "url.domain": "example.com"
            }
          }
        ]
      }
    }
  }
}
```

#### Use conditions

&emsp;&emsp;使用下面的语法来根据条件进行创建：

```text
{{#condition}}content{{/condition}}
```

&emsp;&emsp;如果条件变量为`true`。Elasticsearch会展示内容。例如，如果`year_scope`的值为`true`，下面的template search中会选择过去一年的数据。

```text
POST _render/template
{
  "source": "{ \"query\": { \"bool\": { \"filter\": [ {{#year_scope}} { \"range\": { \"@timestamp\": { \"gte\": \"now-1y/d\", \"lt\": \"now/d\" } } }, {{/year_scope}} { \"term\": { \"user.id\": \"{{user_id}}\" }}]}}}",
  "params": {
    "year_scope": true,
    "user_id": "kimchy"
  }
}
```

&emsp;&emsp;填充后的值如下：

```text
{
  "template_output" : {
    "query" : {
      "bool" : {
        "filter" : [
          {
            "range" : {
              "@timestamp" : {
                "gte" : "now-1y/d",
                "lt" : "now/d"
              }
            }
          },
          {
            "term" : {
              "user.id" : "kimchy"
            }
          }
        ]
      }
    }
  }
}
```

&emsp;&emsp;如果`year_scope`为`false`。这个template search会查询任何时间段的数据。

```text
POST _render/template
{
  "source": "{ \"query\": { \"bool\": { \"filter\": [ {{#year_scope}} { \"range\": { \"@timestamp\": { \"gte\": \"now-1y/d\", \"lt\": \"now/d\" } } }, {{/year_scope}} { \"term\": { \"user.id\": \"{{user_id}}\" }}]}}}",
  "params": {
    "year_scope": false,
    "user_id": "kimchy"
  }
}
```

&emsp;&emsp;填充后的值如下：

```text
{
  "template_output" : {
    "query" : {
      "bool" : {
        "filter" : [
          {
            "term" : {
              "user.id" : "kimchy"
            }
          }
        ]
      }
    }
  }
}
```

&emsp;&emsp;可以使用下面的语法创建if-else条件：

```text
{{#condition}}if content{{/condition}} {{^condition}}else content{{/condition}}
```

&emsp;&emsp;例如，如果`year_scope`为`true`，则获取过去一年的数据，否则获取过去一天的数据。

```text
POST _render/template
{
  "source": "{ \"query\": { \"bool\": { \"filter\": [ { \"range\": { \"@timestamp\": { \"gte\": {{#year_scope}} \"now-1y/d\" {{/year_scope}} {{^year_scope}} \"now-1d/d\" {{/year_scope}} , \"lt\": \"now/d\" }}}, { \"term\": { \"user.id\": \"{{user_id}}\" }}]}}}",
  "params": {
    "year_scope": true,
    "user_id": "kimchy"
  }
}
```


### Sort search results
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/sort-search-results.html)

&emsp;&emsp;允许你添加一个或者多个字段用于排序。每一个排序即可以正序也可以是倒序。排序规则按字段定义，也可以指定一些特殊的字段名例如`_score`意味着根据打分排序，`_doc`意味着根据索引顺序排序。

&emsp;&emsp;假设有以下的index mapping：

```text
PUT /my-index-000001
{
  "mappings": {
    "properties": {
      "post_date": { "type": "date" },
      "user": {
        "type": "keyword"
      },
      "name": {
        "type": "keyword"
      },
      "age": { "type": "integer" }
    }
  }
}
```

```text
GET /my-index-000001/_search
{
  "sort" : [
    { "post_date" : {"order" : "asc", "format": "strict_date_optional_time_nanos"}},
    "user",
    { "name" : "desc" },
    { "age" : "desc" },
    "_score"
  ],
  "query" : {
    "term" : { "user" : "kimchy" }
  }
}
```

> NOTE：`_doc`不是很实用，但用于排序时，却是效率最高的。所以如果你不关心返回的文档顺序，那你应该使用`_doc`排序。特别有助于[scrolling](#Scroll search results)。


#### Sort Values

&emsp;&emsp;查询响应中包含每一个文档的排序值（`sort` value）。可以使用`format`参数为[date](#Date field type)和[date_nanos](#Date nanoseconds field type)字段的sort value指定[date format](#format(mapping parameter))。下面的查询返回中，`post_date`字段的字段值作为排序值，并且使用format格式为：`strict_date_optional_time_nanos`。

```text
GET /my-index-000001/_search
{
  "sort" : [
    { "post_date" : {"format": "strict_date_optional_time_nanos"}}
  ],
  "query" : {
    "term" : { "user" : "kimchy" }
  }
}
```

#### Sort Order

&emsp;&emsp;`order`选项有两个值可选：

- asc
  - Sort in ascending order
- desc
  - Sort in descending order

&emsp;&emsp;根据`_score`排序时，默认的`order`选项为`desc`，而根据其他字段排序时，`order`选项默认值为`asc`。

#### Sort mode option

&emsp;&emsp;Elasticsearch支持根据数组或者多值字段（multi-valued field）排序。`mode`选项控制了选择数组中哪一个值用于作为文档的排序值。`mode`选项有以下的值可选：

- min
  - 选择最小的值
- max
  - 选择最大的值
- sum
  - 所有值的和作为排序值。只适用于数值类型的数组
- avg
  - 所有值的平均值作为排序值。只适用于数值类型的数组
- median
  - 所有值的中位数作为排序值。只适用于数值类型的数组

&emsp;&emsp;sort order为`asc`的默认sort mode为`min`，即选择最小值。sort order为`desc`的默认sort mode为`max`，即选择最大值。

##### Sort mode example usage

&emsp;&emsp;下面的例子中，price字段是一个多值字段。在这个例子中命中的结果会按照`asc`以及price中所有值的平均值排序。

```text
PUT /my-index-000001/_doc/1?refresh
{
   "product": "chocolate",
   "price": [20, 4]
}

POST /_search
{
   "query" : {
      "term" : { "product" : "chocolate" }
   },
   "sort" : [
      {"price" : {"order" : "asc", "mode" : "avg"}}
   ]
}
```

#### Sorting numeric fields

&emsp;&emsp;对于数值类型的字段也是可以通过`numeric_typ`选项进行类型转换。该选项可选的值为：`["double", "long", "date", "date_nanos"]`，使得可以跨多个data stream或者索引对不同mapping类型的相同字段名进行排序。

&emsp;&emsp;如果有以下两个索引：

```text
PUT /index_double
{
  "mappings": {
    "properties": {
      "field": { "type": "double" }
    }
  }
}
```

```text
PUT /index_long
{
  "mappings": {
    "properties": {
      "field": { "type": "long" }
    }
  }
}
```

&emsp;&emsp;由于`field`这个字段在两个索引中分别是`double`和`long`类型，所以默认情况下不能在同时查询这两个索引时根据这个字段进行排序。然而你可以使用`numeric_type`选项强制转化为同一个类型使得可以用于排序：

```text
POST /index_long,index_double/_search
{
   "sort" : [
      {
        "field" : {
            "numeric_type" : "double"
        }
      }
   ]
}
```

&emsp;&emsp;在上面的例子中，索引`index_long`中的值转化为一个double使得兼容索引`index_double`中的值。当然也可以将一个floating值转化为long，but note that in this case floating points are replaced by the largest value that is less than or equal (greater than or equal if the value is negative) to the argument and is equal to a mathematical integer。

&emsp;&emsp;同样的也可以用于`date`和`date_nanos`字段。如果有以下两个索引：

```text
PUT /index_double
{
  "mappings": {
    "properties": {
      "field": { "type": "date" }
    }
  }
}
```

```text
PUT /index_long
{
  "mappings": {
    "properties": {
      "field": { "type": "date_nanos" }
    }
  }
}
```

&emsp;&emsp;两个索引中`field`的值使用不同的格式（different resolution）存储使得`date`类型的值总是排在`date_nanos`之后（asc）。在使用了`numeric_typ`选项后，就可以将它们设置为单个格式（single resolution）。如果设置为`date`，那么`date_nanos`的值会被转化为millisecond，如果设置为`date_nanos`，那么`date`会被转化为nanoseconds。

```text
POST /index_long,index_double/_search
{
   "sort" : [
      {
        "field" : {
            "numeric_type" : "date_nanos"
        }
      }
   ]
}
```

> WARNING：为了防止出现溢出，对于1970年之前和2262年之后的`date_nanos`的转化不能表示为long。

#### Sorting within nested objects.

&emsp;&emsp;Elasticsearch同样支持根据object内部的字段或者更深层的字段进行排序。根据nested filed排序时使用的`nested`排序选项可以有以下的属性：

- path
  - 定义了对哪一个object排序（Defines on which nested object to sort）。真正的排序字段必须是这个object中的字段。 When sorting by nested field, this field is mandatory
- filter
  - 每一层nested object以及inner nested object都可以指定filter，判断排序字段能否用于排序，如果条件不满足，则视为missing value（见下面的例子）。Common case is to repeat the query / filter inside the nested filter or query. By default no filter is active.
- max_children
  - 选择排序值（Sort value）时，object的最大深度（maximum number of children）。默认值为`unlimited`
- nested
  - 跟顶层（top-level）的`nested`是一样的，只是在当前的nested object中需要使用其他的nested path（见下面的例子）

> NOTE：如果排序中定义了一个nested field但是没有`nested` 的上下文（见下面的例子），Elasticsearch会抛出错误异常

##### Nested sorting examples

&emsp;&emsp;在下面的例子中，`offer`是一个`nested`类型，需要指定`path`，否则Elasticsearch不知道选择哪一层的nested中的排序值进行排序

```text
POST /_search
{
   "query" : {
      "term" : { "product" : "chocolate" }
   },
   "sort" : [
       {
          "offer.price" : {
             "mode" :  "avg",
             "order" : "asc",
             "nested": {
                "path": "offer",
                "filter": {
                   "term" : { "offer.color" : "blue" }
                }
             }
          }
       }
    ]
}
```

&emsp;&emsp;在下面的例子中，`parent`和`child`是`nested`字段。需要在每一层指定`nested.path`，否则Elasticsearch不知道选择哪一层的nested中的排序值进行排序。

```text
POST /_search
{
   "query": {
      "nested": {
         "path": "parent",
         "query": {
            "bool": {
                "must": {"range": {"parent.age": {"gte": 21}}},
                "filter": {
                    "nested": {
                        "path": "parent.child",
                        "query": {"match": {"parent.child.name": "matt"}}
                    }
                }
            }
         }
      }
   },
   "sort" : [
      {
         "parent.child.age" : {
            "mode" :  "min",
            "order" : "asc",
            "nested": {
               "path": "parent",
               "filter": {
                  "range": {"parent.age": {"gte": 21}}
               },
               "nested": {
                  "path": "parent.child",
                  "filter": {
                     "match": {"parent.child.name": "matt"}
                  }
               }
            }
         }
      }
   ]
}
```

&emsp;&emsp;nested sorting 同样支持根据脚本或者地理位置排序。

#### Missing Values

&emsp;&emsp;`missing`参数指的是如何对哪些缺少排序字段的文档进行排序：`missing`的值可以设置为`_last`、`_first`或者自定义的值（该值会被用于排序）。默认值为`_last`。

&emsp;&emsp;例如：

```text
GET /_search
{
  "sort" : [
    { "price" : {"missing" : "_last"} }
  ],
  "query" : {
    "term" : { "product" : "chocolate" }
  }
}
```

> NOTE：如果nested inner object没有匹配到`nested.filter`，则视为缺少排序字段，即使用missing value。

#### Ignoring Unmapped Fields

&emsp;&emsp;默认情况下，如果某个字段没有对应的mapping，那么根据这个字段排序的请求会失败。`unmapped_type`选项允许你忽略没有对应mapping的字段，不根据它们进行排序。这个参数的值可以用于determine what sort values to emit。下面是使用这个参数的例子：

```text
GET /_search
{
  "sort" : [
    { "price" : {"unmapped_type" : "long"} }
  ],
  "query" : {
    "term" : { "product" : "chocolate" }
  }
}
```

&emsp;&emsp;如果所有的索引都没有`price`对应的mapping，Elasticsearch将认为存在`long`类型的mapping（防止报错），并且所有的文档都没有这个字段的字段值。

#### Geo Distance Sorting

&emsp;&emsp;允许根据`_geo_distance`排序。下面的例子中，假设`pin.location`是`geo_point`类型的字段：

```text
GET /_search
{
  "sort" : [
    {
      "_geo_distance" : {
          "pin.location" : [-70, 40],
          "order" : "asc",
          "unit" : "km",
          "mode" : "min",
          "distance_type" : "arc",
          "ignore_unmapped": true
      }
    }
  ],
  "query" : {
    "term" : { "user" : "kimchy" }
  }
}
```

- distance_type

&emsp;&emsp;如果计算距离。可以是`arc`（默认值）或者`plane`（计算更快，但是长距离以及靠近两极时的计算的结果不精确）

- mode

&emsp;&emsp;如果有多个字段值时选择哪一个用于排序。默认情况下，当按照ascending order时会采用最短距离，按照descending order时采用最长距离。支持的可选值有`min`、`max`、`medina`以及`avg`。

- unit

&emsp;&emsp;计算排序时使用的单位。默认值为`m`(米)。

- ignore_unmapped

&emsp;&emsp;unmapped filed是否视为missing value。设置为`true`后跟上文中的Ignoring Unmapped Fields是一样的处理方式，设置为`false`后，unmapped filed会导致查询失败。

> NOTE：geo distance sorting不支持配置missing value：当文档中没有字段值用于计算距离时总是被认为距离为`Infinity`

&emsp;&emsp;下面提供的坐标格式都是支持的：

##### Lat Lon as Properties

```text
GET /_search
{
  "sort" : [
    {
      "_geo_distance" : {
        "pin.location" : {
          "lat" : 40,
          "lon" : -70
        },
        "order" : "asc",
        "unit" : "km"
      }
    }
  ],
  "query" : {
    "term" : { "user" : "kimchy" }
  }
}
```

##### Lat Lon as String

&emsp;&emsp;`lat`、`lon`的格式。

```text
GET /_search
{
  "sort": [
    {
      "_geo_distance": {
        "pin.location": "40,-70",
        "order": "asc",
        "unit": "km"
      }
    }
  ],
  "query": {
    "term": { "user": "kimchy" }
  }
}
```

##### Geohash

```text
GET /_search
{
  "sort": [
    {
      "_geo_distance": {
        "pin.location": "drm3btev3e86",
        "order": "asc",
        "unit": "km"
      }
    }
  ],
  "query": {
    "term": { "user": "kimchy" }
  }
}
```

##### Lat Lon as Array

&emsp;&emsp;[`lon`, `lat`]的格式，注意的时这里lon/lat的前后顺序是为了符合[GeoJSON](https://geojson.org)。

```text
GET /_search
{
  "sort": [
    {
      "_geo_distance": {
        "pin.location": [ -70, 40 ],
        "order": "asc",
        "unit": "km"
      }
    }
  ],
  "query": {
    "term": { "user": "kimchy" }
  }
}
```

##### Multiple reference points

&emsp;&emsp;可以使用数组提供多个geo points，数组元素的格式可以是上文中提到的所有的格式，例如

```text
GET /_search
{
  "sort": [
    {
      "_geo_distance": {
        "pin.location": [ [ -70, 40 ], [ -71, 42 ] ],
        "order": "asc",
        "unit": "km"
      }
    }
  ],
  "query": {
    "term": { "user": "kimchy" }
  }
}
```

&emsp;&emsp;最终一篇文档的距离是文档中所有点的 min/max/avg(取决于`mode`的值)到排序请求中给出的点的距离。

#### Script Based Sorting

&emsp;&emsp;允许基于自定义的脚本进行排序，见下面的例子：

```text
GET /_search
{
  "query": {
    "term": { "user": "kimchy" }
  },
  "sort": {
    "_script": {
      "type": "number",
      "script": {
        "lang": "painless",
        "source": "doc['field_name'].value * params.factor",
        "params": {
          "factor": 1.1
        }
      },
      "order": "asc"
    }
  }
}
```

#### Track Scores

&emsp;&emsp;当根据某个字段排序后，就不会对文档打分。设置`track_scores`为true后才会进行打分计算。

```text
GET /_search
{
  "track_scores": true,
  "sort" : [
    { "post_date" : {"order" : "desc"} },
    { "name" : "desc" },
    { "age" : "desc" }
  ],
  "query" : {
    "term" : { "user" : "kimchy" }
  }
}
```

#### Memory Considerations

&emsp;&emsp;排序时，用于排序的字段会加载到内存中。这意味着每一个分片都应该有足够的内存来处理它们。对于用于排序的string类型，它不应该设置为analyzed / tokenized。对于数值类型，如果可以的话，建议显示的（explicit）设置为narrower types（例如`short`、`ingeger`以及`float`）。

## Query DSL
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl.html)

#### Allow expensive queries（Query DSL）

### Query and filter context
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-filter-context.html)

#### Relevance scores

&emsp;&emsp;默认情况下，Elasticsearch根据相关性分数（relevance score）对匹配到的结果进行排序。相关性分数衡量每个文档与查询的匹配程度。

&emsp;&emsp;relevance score是一个正浮点数，在[search API](#Request body search-1) 的` _score`元数据字段中返回。`_score`越高，跟文档越相关。每一个查询类型计算相关性分数是不同的，分数的计算也取决于query clause是在**query**还是**filter**的上下文（context）中运行。

#### Query context

&emsp;&emsp;在query context中，一个query clause会回答这个问题：文档跟这个query clause的匹配程度是多少？除了决定是否匹配文档，另外query clause还要计算一个相关性分数，这个分数在`_source`元数据字段中返回。

&emsp;&emsp;query context通过参数`query`生效，见[search](#query) API中的query参数。

#### Filter context

&emsp;&emsp;在filter  context中，一个query clause会回答这个问题：文档跟这个query clause匹配吗？答案是简单的`匹配`或者`不匹配`。不会计算分数。filter context最常用于过滤结构化的数据：

- `timestamp`字段的字段值在2015到2016的范围内吗
- `status`字段的字段值是`published`吗

&emsp;&emsp;常用的filters会被Elasticsearch自动缓存来提高性能。

&emsp;&emsp;filter context通过参数`filter`生效，比如[bool  query](#Boolean query)中的`filter`或者`must_not`参数，以及 [constan_score](#Constant score query) query和 [filter](#Filter aggregation)聚合中的参数`filter`。

#### Example of query and filter contexts

&emsp;&emsp;下面的例子中是一个同时使用了query context和filter context的`search` API。这个查询将会匹配满足下面所有条件的文档：

- `title`字段的字段值中包含`search`
- `content`字段的字段值中包含`Elasticsearch`
- `exact`字段的字段值为`published`
- `publish_date`字段的字段值大于等于`2015-01-01`

```text
GET /_search
{
  "query": { 
    "bool": { 
      "must": [
        { "match": { "title":   "Search"        }},
        { "match": { "content": "Elasticsearch" }}
      ],
      "filter": [ 
        { "term":  { "status": "published" }},
        { "range": { "publish_date": { "gte": "2015-01-01" }}}
      ]
    }
  }
}
```

&emsp;&emsp;第3行，参数`query`说明使用了query context
&emsp;&emsp;第4行，`bool`和`must`在query context中用来描述文档的匹配程度
&emsp;&emsp;第8行，参数`filter`说明使用了filter context，`term`和`query`在filter context中用来过滤掉不匹配的文档，并且不会影响匹配的文档的分数。

>WARNING：query context中计算出的分数是一个精确的浮点型数值。只有24位的精度。超过有效数字精度的分数将被转换为浮点数，但会丢失精度。

>TIP：用于影响匹配的文档的分数（文档的匹配程度）的query clause放在query context中，其他的query clause放在filter context中。


### Compound queries
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/compound-queries.html)

&emsp;&emsp;复合查询（compound query）中封装了符合的或者叶子查询（compound or leaf query）。

&emsp;&emsp;这组复合查询中有这些query：

- [bool query](#Boolean query)：通过定义`must`, `should`,`must_not`, `filter`  clause来组合多个leaf或者compound query的默认query。`must`和`should`的query clause都会进行打分并且组合打分值。the more matching clauses, the better。`must_not`和`filter`的query clause则在[filter context](#Filter context)中执行。
- [boosting query](#Boosting query)：返回满足`positive` query的文档，如果该文档同时满足`negative` query则降低这篇文档的分数。
- [constant_score query](#Constant score query)：这是一个封装其他query的query，并且这个constant_score_query在filter context中运行。所有匹配的文档在`_score`中返回一个常量分数
- [dis_max query](#Disjunction max query)：这是一个封装了多个query（我们称之为子query）的query。满足匹配的文档至少满足了一个子query的查询条件。bool query中对于一篇文档的打分值会组合每一个子query对文档的打分，而dis_max_query则是使用最佳的子query对一篇文档的打分值
- [function_score query](#Function score query)：使用function修改main query返回的分数，以考虑流行度（popularity）、新近度（recency）、距离或使用脚本实现的自定义算法（custom algorithms）等因素

#### Boolean query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-bool-query.html)

&emsp;&emsp;Boolean query是一种组合了其他query用于匹配文档的query。bool query对应（map）Lucene中的`BooleanQuery`。通过一个或者多个boolean clause来构建Boolean query，这些clause各自有一个`occur`的类型。类型的种类包括：

|  Occur   |                         Description                          |
| :------: | :----------------------------------------------------------: |
|   must   | 匹配到的文档必须满足这个clause（query），同时这个query会对文档进行打分 |
|  filter  | 匹配到的文档必须满足这个clause（query），但是跟`must`不同的是，不会计算这个query对文档的打分值。Filter clause在[filter context](#Filter context)下执行，意味着忽略打分值并且这个query可以被缓存（Lucene中会缓存满足这个query的文档号） |
|  should  | 在匹配到的文档中，存在满足这个clause（query）的查询条件的一个或者多个文档 |
| must_not | 匹配到的所有文档都不能满足这个clause（query） 的查询条件。这个clause在[filter context](#Filter context)下执行，意味着忽略打分值并且这个query可以被缓存。因为忽略了打分值，所有的文档分数都是`0` |

&emsp;&emsp;`bool query`采取的是`more-matches-is-better`的方法。所以如果某篇文档同时满足`must`和`should`的查询条件，那么这篇文档的打分值会累加，最后在`_sorce`中返回。

```text
POST _search
{
  "query": {
    "bool" : {
      "must" : {
        "term" : { "user.id" : "kimchy" }
      },
      "filter": {
        "term" : { "tags" : "production" }
      },
      "must_not" : {
        "range" : {
          "age" : { "gte" : 10, "lte" : 20 }
        }
      },
      "should" : [
        { "term" : { "tags" : "env1" } },
        { "term" : { "tags" : "deployed" } }
      ],
      "minimum_should_match" : 1,
      "boost" : 1.0
    }
  }
}
```

##### Using minimum_should_match

&emsp;&emsp;你可以使用参数`minimum_should_match`来指定匹配到的文档必须满足至少`minimum_should_match`个（或者某个比例）`occur`为`should`的clause（query）。

&emsp;&emsp;如果 `bool query`中至少包含一个`should` clause并且没有`must`或`filter`的clause，这个参数的默认值为`1`。

&emsp;&emsp;见[minimum_should_match](#minimum_should_match parameter)了解这个参数更多的可选值。

##### Scoring with bool.filter

&emsp;&emsp;在元素（element）`filter`下指定的query不会影响文档的打分—返回的打分值（score）都是`0`。只有在元素`query`中指定的query才会影响文档的打分值（下面例子中的bool query，不过这个query中的子query仍然是不用打分的query）。接下来的三个例子中都是返回那些满足`status`字段的字段值为`active`的文档。

&emsp;&emsp;下面第一个query返回的所有文档的打分值都是`0`，因为没有指定用于打分的query。

```text
GET _search
{
  "query": {
    "bool": {
      "filter": {
        "term": {
          "status": "active"
        }
      }
    }
  }
}
```

&emsp;&emsp;`bool query`中有一个`match_all`的query，这个query将另所有的文档的打分值为`1.0`。

```text
GET _search
{
  "query": {
    "bool": {
      "must": {
        "match_all": {}
      },
      "filter": {
        "term": {
          "status": "active"
        }
      }
    }
  }
}
```

&emsp;&emsp;`constant_score`这个query的行为跟上面第二个例子是一模一样的。这个query将另所有的文档的打分值为`1.0`。

```text
GET _search
{
  "query": {
    "constant_score": {
      "filter": {
        "term": {
          "status": "active"
        }
      }
    }
  }
}
```

##### Named queries

&emsp;&emsp;每一个query都可以在顶层定义中设置一个`_name`。你可以使用被命名的query（named query）来追踪返回的文档被哪个query命中了。如果使用了named query，响应中的每一个结果中都会包含一个`mathced_queries`的属性。

```text
GET /_search
{
  "query": {
    "bool": {
      "should": [
        { "match": { "name.first": { "query": "shay", "_name": "first" } } },
        { "match": { "name.last": { "query": "banon", "_name": "last" } } }
      ],
      "filter": {
        "terms": {
          "name.last": [ "banon", "kimchy" ],
          "_name": "test"
        }
      }
    }
  }
}
```


#### Boosting query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-boosting-query.html)

&emsp;&emsp;返回匹配`positive` query的文档并且降低匹配`negative` query的文档的相关性分数（[relevance score](#Relevance scores)）。

&emsp;&emsp;你可以通过`boosting` query来对相关文档进行降级（demote）而不是从查询结果中排除掉这些文档。

##### Example request

```text
GET /_search
{
  "query": {
    "boosting": {
      "positive": {
        "term": {
          "text": "apple"
        }
      },
      "negative": {
        "term": {
          "text": "pie tart fruit crumble tree"
        }
      },
      "negative_boost": 0.5
    }
  }
}
```

##### Top-level parameters for boosting

- positive：（Required,query object）你想要执行的query。所有的文档都必须匹配这个query。
- negative：（Required,query object）用来降低匹配到的文档的[relevance score](#Relevance scores)。
  - 如果返回的文档匹配了`positive`并且`negative` query，`boosting` query会按照下面的方式来计算[relevance score](#Relevance scores)：
  1. 从`positive` query中获取原始的相关性分数
  2. 与`negative_boost`的值做乘法
  
- negative_boost：（Required,float）该值0~1.0范围内的浮点数。用来降低匹配了`negative` query 的文档的[relevance score](#Relevance scores)。

#### Constant score query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-constant-score-query.html)

&emsp;&emsp;封装了一个[ filter query ](#Boolean query)并且每一个匹配到的文档的相关性分数等于参数`boos`的值。

```text
GET /_search
{
  "query": {
    "constant_score": {
      "filter": {
        "term": { "user.id": "kimchy" }
      },
      "boost": 1.2
    }
  }
}
```

##### Top-level parameters for constant_score

- filter：（Required,query object）[Filter query](#Boolean query)是你想要运行的query，任何返回的文档都必须匹配这个query。
  - Filter query 不会计算 [relevance score](#Relevance scores)，为了提高性能，Elasticsearch会自动的缓存使用频繁的filter query。
- boost：（Optional,float）一个浮点型的数字用于作为满足`filter` query的所有文档固定的[relevance score](#Relevance scores)。默认值是`1.0`。

#### Disjunction max query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-dis-max-query.html)

&emsp;&emsp;返回的文档匹配一个或者多个封装的query，成为query clause或者clause。

&emsp;&emsp;如果一篇返回的文档匹配了多个query clause，`dis_max` query会将query clause中打分最高的值作为相关性分数，基于参数`tie_breaker`再加上其他匹配的子query（subquery）对应的打分值。

##### Example

```text
GET /_search
{
  "query": {
    "dis_max": {
      "queries": [
        { "term": { "title": "Quick pets" } },
        { "term": { "body": "Quick pets" } }
      ],
      "tie_breaker": 0.7
    }
  }
}
```

##### Top-level parameters for dis_max

- queries：（Required, array of query objects） 包含一个或者多个query clause。返回的文档必须匹配一个或者多个这些query。如果一篇文档匹配了多个query，Elasticsearch会采用最大的相关性分数（每个query都会对这篇文档打分）。

- tie_breaker：（Optional, float） 该值是0~1.0范围内的浮点数，用来增加匹配到了多个query clause的文档的 [relevance score](#Relevance scores)。默认值为`0.0`。

  - You can use the `tie_breaker` value to assign higher relevance scores to documents that contain the same term in multiple fields than documents that contain this term in only the best of those multiple fields, without confusing this with the better case of two different terms in the multiple fields。
  - 如果一篇文档匹配了多个clause，`dis_max` query 按照下面的方式为这篇文档计算relevance score：

  1. 从匹配到的clause中选出最高的relevance score
  2. `tie_breaker`参数的值与其他任何匹配的clause的分数做乘法
  3. 上述两个结果相加
     - Lucene中的计算公式：`scoreMax + otherScoreSum * tieBreakerMultiplier`
     - scoreMax为clause打分最高的，otherScoreSum为其他clause的打分和值，tieBreakerMultiplier即tie_breaker的值

  如果`tie_breaker`的值大于`0.0`，所有匹配到的clause都有用，但是分数最高的那个clause最重要。（If the tie_breaker value is greater than 0.0, all matching clauses count, but the clause with the highest score counts most）

#### Function score query
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-function-score-query.html)

### Full text queries
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/full-text-queries.html)

&emsp;&emsp;full text query可以查询[analyzed text fields](#Text analysis)，例如邮件的内容。查询的内容会跟索引期间的内容一样使用相同的分词器处理。

&emsp;&emsp;属于full text query的query包括：

- [intervals query](#Intervals query)：
- [match query](#Match query)：
- [match_bool_prefix query](#Match boolean prefix query)：
- [match_phrase query](#Match phrase query)：
- [match_phrase_prefix query](#Match phrase prefix query)：
- [multi_match query](#Multi-match query)：
- [combined_fields query](#Combined fields)：
- [query_string query](#Query string query)：
- [simple_query_string query](#Simple query string query)：

#### Intervals query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-intervals-query.html)

&emsp;&emsp;基于匹配到的terms的顺序以及接近度（proximity。term之间在文档中的物理距离或位置接近程度）返回文档。

&emsp;&emsp;`intervals`使用由一小组**matching rules**定义组成。这些规则会应用到指定的一个字段的字段值中。

&emsp;&emsp;定义的规则会生成文本内容中term之间的最小间隔序列（也就是term的顺序跟接近度），并且还可以进一步跟父级中的定义进行组合

##### Example request

&emsp;&emsp;下面的`intervals`查询返回的文档中，字段名为`my_text`的字段值中包含`my favorite food`，这三个term（my、favorite、food）在文档中是先后有序紧挨着的，并且紧跟着`hot water`或者`cold porridge`。

&emsp;&emsp;这个查询会匹配字段名为`my_text`中包含`my favorite food is cold porridge`的文档，但是不会匹配`when it's cold my favorite food is porridge`（因为在cold porridge跟my favorite food没有顺序出现）。

```text
POST _search
{
  "query": {
    "intervals" : {
      "my_text" : {
        "all_of" : {
          "ordered" : true,
          "intervals" : [
            {
              "match" : {
                "query" : "my favorite food",
                "max_gaps" : 0,
                "ordered" : true
              }
            },
            {
              "any_of" : {
                "intervals" : [
                  { "match" : { "query" : "hot water" } },
                  { "match" : { "query" : "cold porridge" } }
                ]
              }
            }
          ]
        }
      }
    }
  }
}
```

##### Top-level parameters for intervals

- `<field>`：（Required,rule object）你想要查询的字段名。
  - 这个参数的值是一个规则对象（rule object），基于待匹配的terms、顺序以及接近度匹配文档。
  - 可用的规则包括：
    - [match](#match rule parameters)
    - [prefix](#prefix rule parameters)
    - [wildcard](#wildcard rule parameters)
    - [fuzzy](#fuzzy rule parameters)
    - [all_of](#all_of rule parameters)
    - [any_of](#any_of rule parameters)

##### match rule parameters

&emsp;&emsp;`match`规则匹配被分词后的文本（analyzed text）

- query：（Required, string）你想要从`<field>`中查找的内容
- max_gaps：（Optional,integer）匹配到的term之间的最大距离。超过这个距离被认为是不满足匹配的。默认值为`-1`。
  - 如果没有指定这个参数或者指定为`-1`，那么就没有距离的限制。如果设置为`0`，这些term必须相互连续出现
- ordered：（Optional, Boolean）如果为`true`，匹配到的term必须按照定义的规则有序。默认是`false`
- analyzer：（Optional, string）[analyzer](#Text analysis)用来对`query`中的文本进行分词。默认是`<field>`中的分词器
- filter：（Optional,[interval filter](#Intervals query) 规则对象）可选的interval filter
- use_field：（Optional, string）如果指定，将从该字段而不是顶层字段`<field>`中匹配间隔。使用search analyzer对该字段的字段值进行分词。可以让你跨多个字段搜索，就好像它们是同一个字段一样；例如，你可以将相同的文本索引到stemmed或者unstemmed的字段中，并搜索stemmed tokens 附近的unstemmed ones

##### prefix rule parameters

&emsp;&emsp;`prefix`规则匹配以指定字符开头的term。最多可以扩展到128个term。如果匹配出超过128个term，Elasticsearch会返回一个错误。你可以使用字段的mapping中的[index-prefixes](#index_prefixes)选项避免这种限制。

- prefix：（Required, string）你想要从顶层的`<field>`中以这个参数为前缀的term
- analyzer：（Optional, string）[analyzer](#Text analysis)用来标准化`prefix`。默认是`<field>`中的分词器
- use_field：（Optional, string）如果指定，将从该字段名而不是顶层字段`<field>`中匹配间隔。
  - The `prefix` is normalized using the search analyzer from this field, unless a separate analyzer is specified.

##### wildcard rule parameters

&emsp;&emsp;`wildcard`规则使用通配符匹配term。最多可以扩展到128个term。如果匹配出超过128个，Elasticsearch会返回一个错误。

- pattern：（Required, string）用来找到匹配的term的通配符
  - 这个参数支持两个通配符字符：
    - `?`：匹配任意的单个字符
    - `*`：匹配一个或多个字符，包括空的情况

> WARNING：最高避免以`*`或者`?`为开头的通配符。为了能找到匹配 的term会会增加迭代次数，使得降低查询性能

- analyzer：（Optional, string）[analyzer](#Text analysis)用来标准化`prefix`。默认是`<field>`中的分词器
- use_field：（Optional, string）如果指定，将从该字段名而不是顶层字段`<field>`中匹配间隔。
  - The `prefix` is normalized using the search analyzer from this field, unless a separate analyzer is specified.

##### fuzzy rule parameters

&emsp;&emsp;`fuzzy`规则用来匹配跟提供的term相似的term，基于[Fuzziness](#Fuzziness（Common options）)中的编辑距离实现。如果模糊表达式匹配超过128个term，Elasticsearch会返回一个错误。

- term：（Required, string）待匹配的term
- prefix_length：（Optional,integer）模糊匹配出的term跟`term`中前`prefix_length`个字符相同。默认值为`0`
- transpositions：（Optional, Boolean）
- fuzziness：（Optional, Boolean）编辑距离中是否允许相邻的两个字符进行交换（比如ab->ba）
- analyzer：（Optional, string）[analyzer](#Text analysis)用来标准化`prefix`。默认是`<field>`中的分词器
- use_field：（Optional, string）如果指定，将从该字段名而不是顶层字段`<field>`中匹配间隔。
  - The `prefix` is normalized using the search analyzer from this field, unless a separate analyzer is specified.

##### all_of rule parameters

&emsp;&emsp;`all_of`规则中由多个其他规则组成，这些规则都满足则返回对应文档

- intervals：（Required, array of rule objects）：由规则数组组成。对于某篇文档，所有的规则必须匹配
- max_gaps：（Optional,integer）匹配到的term之间最大的位置距离。超过这个距离不会被匹配。默认值为`-1`
  - 如果没有指定这个参数或者指定为`-1`，那么就没有距离的限制。如果设置为`0`，这些term必须相互连续出现
- ordered：如果为`true`，每一个规则要按照在定义中的先后顺序依次匹配。默认值为`false`
- filter：（Optional,[interval filter](#Intervals query) 规则对象）用来过滤返回的`intervals`的规则

##### any_of rule parameters

&emsp;&emsp;`any_of`规则返回它包含的规则中的任意一个。

- intervals：（Required, array of rule objects）：用于匹配的规则数组
- filter：（Optional,[interval filter](#Intervals query) 规则对象）用来过滤返回的`intervals`的规则

##### filter rule parameters

&emsp;&emsp;`filter`规则基于一个query来过滤`intervals`。见[Filter example](#Filter example)中的例子。

- after：（Optional, query object）
- before：（Optional, query object）
- contained_by：（Optional, query object）
- containing：（Optional, query object）
- not_contained_by：（Optional, query object）
- not_containing：（Optional, query object）
- not_overlapping：（Optional, query object）
- overlapping：（Optional, query object）
- script：（Optional, [script object](#How to write scripts)）

##### Note

###### Filter example

&emsp;&emsp;下面的查询中定义了一个`filter`规则。返回的文档中有`hot`以及`porridge`，这两个term之间的位置距离不超过10，并且这两个term之间没有`salty`。

```text
POST _search
{
  "query": {
    "intervals" : {
      "my_text" : {
        "match" : {
          "query" : "hot porridge",
          "max_gaps" : 10,
          "filter" : {
            "not_containing" : {
              "match" : {
                "query" : "salty"
              }
            }
          }
        }
      }
    }
  }
}
```

**Script filters**

&emsp;&emsp;你可以使用一个脚本，基于开始、结束以及间隔来过滤`intervals`。下面的`filter`脚本中使用了`interval`变量以及`start`、`end`、`gaps`方法：

```text
POST _search
{
  "query": {
    "intervals" : {
      "my_text" : {
        "match" : {
          "query" : "hot porridge",
          "filter" : {
            "script" : {
              "source" : "interval.start > 10 && interval.end < 20 && interval.gaps == 0"
            }
          }
        }
      }
    }
  }
}
```

**Minimization**

&emsp;&emsp;`intervals` query总是最小化间隔，以确保查询可以在线性时间内运行。有时这可能导致意想不到的结果，尤其是在使用max_gaps限制或过滤器时。例如，考虑以下查询，搜索包含在短语`hot porridge`中的`salty`：

```text
POST _search
{
  "query": {
    "intervals" : {
      "my_text" : {
        "match" : {
          "query" : "salty",
          "filter" : {
            "contained_by" : {
              "match" : {
                "query" : "hot porridge"
              }
            }
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;这个查询不会匹配包含这个短语的文档：`hot porridge is salty porridge`，因为只匹配到文档中最开始的`hot porridge`这两个term后就返回了，并且这两个term之间没有包含`salty`。

&emsp;&emsp;在`any_of`规则中需要注意到另一个限制，那就是子规则出现重叠的情况。特别是其中一个规则是另一个规则的前缀，那么较长的规则不会被匹配，这使得在使用`max_gaps`时会带来一些意想不到的结果。下面的query中，查询`the`以及紧接着`big`或`big bad`，紧接着`wolf`：

```text
POST _search
{
  "query": {
    "intervals" : {
      "my_text" : {
        "all_of" : {
          "intervals" : [
            { "match" : { "query" : "the" } },
            { "any_of" : {
                "intervals" : [
                    { "match" : { "query" : "big" } },
                    { "match" : { "query" : "big bad" } }
                ] } },
            { "match" : { "query" : "wolf" } }
          ],
          "max_gaps" : 0,
          "ordered" : true
        }
      }
    }
  }
}
```

&emsp;&emsp;这个query不会匹配包含`the big bad wolf`的文档，因为`any_of`规则在`big`和`big bad`中仅为`big`生成间隔。由于`big bad`的间隔比`big`的长但起始位置相同，因此它会被最小化处理而不被考虑。在这种情况下，建议重新编写查询，将所有选项在顶层明确列出：

```text
POST _search
{
  "query": {
    "intervals" : {
      "my_text" : {
        "any_of" : {
          "intervals" : [
            { "match" : {
                "query" : "the big bad wolf",
                "ordered" : true,
                "max_gaps" : 0 } },
            { "match" : {
                "query" : "the big wolf",
                "ordered" : true,
                "max_gaps" : 0 } }
           ]
        }
      }
    }
  }
}
```

#### Match query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-match-query.html)

&emsp;&emsp;返回匹配了查询条件中的文本、数值、日期或者布尔值的文档。如果查询条件中提供的是文本，那么在匹配前会对该文本进行分词。

&emsp;&emsp;`match` query是一个用于全文检索的标准query，包含了可以用于模糊查询（fuzzy）的选项。

##### Example request

```text
GET /_search
{
  "query": {
    "match": {
      "message": {
        "query": "this is a test"
      }
    }
  }
}
```

##### Top-level parameters for match

- `<filed>`：（Required, object） 待查询的字段。

##### Parameters for \<field>

- query：（Required）你想要从`<field>`中查询的文本、数值、布尔值或者日期。
  - `match` query在执行查询前会对`query`中的文本进行[analyze](#Text analysis)，意味着`match` query可以对分词后的token进行[text](#Text type family)字段的查询而不是进行精确匹配。
- analyzer：（Optional, string）使用[Analyzer](#Text analysis)将`query`中的文本转化为tokens。默认使用在[index-time analyzer](#How Elasticsearch determines the index analyzer)时`<field>`使用的分词器。如果没有指定分词器，则使用索引默认的分词器。
- auto_generate_synonyms_phrase_query：（Optional, Boolean）如果为`true`，会为multi-term synonyms自动创建[match phrase](#Match phrase query) 。默认值为`true`。
  - 见[Use synonyms with match query](#Synonyms(match))给出的例子。
- fuzziness：（Optional, string）允许用于匹配的最大编辑距离。见[Fuzziness](#Fuzziness（Common options）)了解更多信息。见[Fuzziness in the match query](#Fuzziness in the match query)给出的例子。
- max_expansions：（Optional, integer）query扩展（expand）出的term数量最大值。默认值为`50`。
- prefix_length：（Optional, integer）模糊匹配时，不变的（unchange）起始字符的数量。默认值为`0`。
- fuzzy_transpositions：（Optional, Boolean）如果为`true`，模糊匹配中的编辑距离包含两个字符交换（ab->ba）。默认值为`true`。
- fuzzy_rewrite：（Optional, string）用于重写query的方法。见[rewrite parameter](#rewrite parameter)了解更多信息。
  - 如果`fuzziness`参数为0，`match` query默认使用`top_terms_blended_freqs_${max_expansions}`的`fuzzy_rewrite`方法。
- lenient：（Optional, Boolean）如果为`true`，例如当在一个[numeric](#Numeric field types)字段中，`query`的内容为文本时会忽略format-based的错误，默认值为`false`。
- operator：（Optional, string）用于`query`中值之间的布尔关系。该参数的可选值为：
  - OR（Default）
    - 例如，`query`的值是`capital of Hungary`会解析成（interpret）`capital OR of OR Hungary`
  - AND
  - 例如，`query`的值是`capital of Hungary`会解析成（interpret）`capital AND of AND Hungary`
- minimum_should_match：（Optional, string）返回的文档必须至少匹配到term的数量（`query`分词后的token）。见[minimum_should_match parameter](#minimum_should_match parameter)了解更多信息。
- zero_terms_query：（Optional, string）如果分词（`analyzer`）之后移除了所有的token（比如使用了`stop`过滤器），是否要返回文档。可选值：
  - none（default）
    - 如果在分词后移除了所有的token不返回任何文档
  - all
    - 返回所有的文档，类似[match_all](#Match all query)
  - 见[Zero terms query](#Zero terms query)中的例子。

##### Notes

###### Short request example

&emsp;&emsp;最简单的match query由`<field>`和`query`参数组成。：

```text
GET /_search
{
  "query": {
    "match": {
      "message": "this is a test"
    }
  }
}
```

###### How the match query works

&emsp;&emsp;`match` query实际是`boolean`类型，意味着`query`中提供的文本会被分词并且构建一个Boolean query。`operator`参数可以设置为`or`或者`and`来控制boolean clause（默认值为`or`）之间的关系。可以设置[minimum_should_match](#minimum_should_match parameter)参数指定至少要匹配多少个`should`关系中的clause。

&emsp;&emsp;下面是`operator`参数：

```text
GET /_search
{
  "query": {
    "match": {
      "message": {
        "query": "this is a test",
        "operator": "and"
      }
    }
  }
}
```

&emsp;&emsp;设置`analyzer`可以用来控制如何对文本进行分析处理。默认是mapping定义中这个字段对应的设置，或者是默认的search analyzer。

&emsp;&emsp;设置`lenient`为`true`可以用来忽略数据类型不一致导致的异常，比如一个数值类型的字段，在`query`中提供了一个string。默认为`false`。

###### Fuzziness in the match query

&emsp;&emsp;`fuzziness`根据待查询的字段的类型允许执行 `fuzzy matching`。见[Fuzziness](#Common options)了解相关的设置。

&emsp;&emsp;设置`prefix_length`和`max_expansions`用来控制模糊匹配的处理过程。如果设置了fuzzy选项，那么query会使用`top_terms_blended_freqs_${max_expansions}`作为[rewrite method](#rewrite parameter)，`fuzzy_rewrite`参数允许控制如何对query进行重写（rewrite）。

&emsp;&emsp;默认允许Fuzzy transpositions(`ab` -> `ba`)，可以通过`fuzzy_transpositions`设置为`false`的方式关闭。

> NOTE：Fuzzy matching is not applied to terms with synonyms or in cases where the analysis process produces multiple tokens at the same position. Under the hood these terms are expanded to a special synonym query that blends term frequencies, which does not support fuzzy expansion.

```text
GET /_search
{
  "query": {
    "match": {
      "message": {
        "query": "this is a testt",
        "fuzziness": "AUTO"
      }
    }
  }
}
```

###### Zero terms query

&emsp;&emsp;如果使用了`stop` filter后，分词器移除了所有的token，默认的行为是不匹配任何的文档。使用`zero_terms_query`设置为`none`（默认值）或者设置为`all`后，query就相当于`match_all` 。

```text
GET /_search
{
  "query": {
    "match": {
      "message": {
        "query": "to be or not to be",
        "operator": "and",
        "zero_terms_query": "all"
      }
    }
  }
}
```

###### Synonyms(match)

&emsp;&emsp;`match` query支持使用[synonym_graph  token filter](#Synonym graph token filter)实现multi-terms 同义词扩展（synonym ）。使用这个filter后，解析器会对每一个multi-terms synonyms创建一个短语查询。例如，下面的同义词：`"ny, new york"`会产生：`(ny OR ("new york"))`

&emsp;&emsp;同样的也可以匹配多个同义词，这些同义词之间的关系为`AND`，而不是这几个同义词作为短语查询。

```text
GET /_search
{
   "query": {
       "match" : {
           "message": {
               "query" : "ny city",
               "auto_generate_synonyms_phrase_query" : false
           }
       }
   }
}
```

&emsp;&emsp;上面的例子创建出下面的boolean query：

&emsp;&emsp;`(ny OR (new AND york)) city`

&emsp;&emsp;满足条件的文档中要么匹配到`ny`，或者匹配到`new AND york`，或者都匹配到。`auto_generate_synonyms_phrase_query`默认值为`true`。

#### Match boolean prefix query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-match-bool-prefix-query.html)

&emsp;&emsp;`match_bool_prefix` query对输入进行解析后，会基于生成的term构造一个[bool query](#Boolean query)。除了最后一个term，其他的每一个term构造为一个`term` query，最后一个term构造为`prefix` query。例如：

```text
GET /_search
{
  "query": {
    "match_bool_prefix" : {
      "message" : "quick brown f"
    }
  }
}
```

&emsp;&emsp;对上面的输入进行解析后会生成`quick`、`brown`和`f`这三个term，相当于下面的`bool` query：

```text
GET /_search
{
  "query": {
    "bool" : {
      "should": [
        { "term": { "message": "quick" }},
        { "term": { "message": "brown" }},
        { "prefix": { "message": "f"}}
      ]
    }
  }
}
```

&emsp;&emsp;[match_phrase_prefix](#Match phrase prefix query) query跟`match_bool_prefix` query一个很重要的不同点是，`match_phrase_prefix` query 将这些term作为一个短语，而`match_bool_prefix` query 则是在文档中的任意位置去分别匹配这些term。上面的例子中，`match_bool_prefix` query 可能会匹配`quick brown fox`，同时也可以匹配`brown fox quick`，甚至还会分别去匹配包含`quick`、`brown`、以`f`开头的term，并且这些term可以在文档的任意位置。

##### Parameters

&emsp;&emsp;默认情况下，`match_bool_prefix` query会对输入使用字段在mapping中的分词器（analyzer）。不过可以通过`analyzer` 参数在查询时配置一个分词器。

```text
GET /_search
{
  "query": {
    "match_bool_prefix": {
      "message": {
        "query": "quick brown f",
        "analyzer": "keyword"
      }
    }
  }
}
```

&emsp;&emsp;`match_bool_prefix` query支持[match query](#How the match query works)中提到的[minimum_should_match](#minimum_should_match parameter)以及`operator`参数。构造出来的`bool` query中的clause的数量大多数情况下和分词器对输入解析后生成的term的数量一样。

&emsp;&emsp;[fuzziness](#Fuzziness in the match query), `prefix_length`, `max_expansions`, `fuzzy_transpositions`, and `fuzzy_rewrite`这些参数都可以应用到所有term对应的 `term` query中，除了最后一个term。这些参数不会对最后一个term对应的prefix query有任何的应用。

#### Match phrase query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-match-query-phrase.html)

&emsp;&emsp;`match_phrase` query解析（analyze）文本后会基于分析后的文本创建一个短语查询（`phrase` query）。

```text
GET /_search
{
  "query": {
    "match_phrase": {
      "message": "this is a test"
    }
  }
}
```

&emsp;&emsp;A phrase query matches terms up to a configurable slop (which defaults to 0) in any order. Transposed terms have a slop of 2。

&emsp;&emsp;`analyzer` 可以用于控制如果对文本进行解析。默认使用该字段在mapping中定义的分词器，或者查询中默认使用的分词器：

```text
GET /_search
{
  "query": {
    "match_phrase": {
      "message": {
        "query": "this is a test",
        "analyzer": "my_analyzer"
      }
    }
  }
}
```

&emsp;&emsp;这个查询同样接收`zero_terms_query`参数，见[match query](#Match query)中的介绍。

#### Match phrase prefix query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-match-query-phrase-prefix.html)

&emsp;&emsp;返回的文档中包含查询中提供的文本中的单词，并且要求有相同的单词间的先后顺序。提供的文本中的最后一个term会认为是一个[prefix](#Prefix query)，匹配以这个term开头的其他term。

##### Example request

&emsp;&emsp;下面的查询返回的文档中包含了`message`字段中以`quick brown f`开头的短语。

&emsp;&emsp;这个查询将会匹配`message`字段的`quick brown fox or two quick brown ferrets`，但是不会匹配`the fox is quick and brown`。

```text
GET /_search
{
  "query": {
    "match_phrase_prefix": {
      "message": {
        "query": "quick brown f"
      }
    }
  }
}
```

##### Top-level parameters for match_phrase_prefix

-  `<field>`：（Required, object）待查询的字段。

##### Parameters for \<field>

- query：（Required, string）你想要在`<field>`中查找的文本。
  - `match_phrase_prefix` query在执行查询前会[analyze](#Text analysis) `query`中的文本。最后一个term会作为一个[prefix](#Prefix query)，匹配以这个term开头的其他term。
- analyzer：（Optional, string）用于将`query`中的文本转化为token的[Analyzer](#Text analysis)。默认是`<field>`这个字段的[index-time analyzer](#How Elasticsearch determines the index analyzer)。如果在mapping中没有指定analyzer，则使用索引默认的分词器。
- max_expansions：（Optional, integer）最后一个term扩展出其他term的数量最大值。默认值为`50`。
- slop：匹配到的token之间的最大位置距离。默认为`0`。Transposed terms的slop为`2`。
- zero_terms_query：（Optional, string）当`analyzer`解析`query`后移除了所有的token，比如使用了`stop` filter，是否不返回任何文档。可选值为：
  - none（Default）
    - 如果`analyzer`移除了所有的token，不返回任何文档
  - all
    - 返回所有的文档，相当于[match_all](#Match all query) query

##### Notes

###### Using the match phrase prefix query for search autocompletion

&emsp;&emsp;尽管使用起来很简单，但是将`match_phrase_prefix` query用于查询时的自动提示（search autocompletion）可能会返回一些让人困惑的结果。

&emsp;&emsp;比如说，如果查询关键字是`quick brown f`。这个请求会创建一个`quick`和`brown`的短语查询（文档中`quick`必须存在紧接着它的下一个term必须是`brown`）。然后从[sorted term dictionary](https://www.amazingkoala.com.cn/Lucene/suoyinwenjian/2019/0401/索引文件之tim&&tip)中找到50个以`f`开头的term，最后将这些term添加到短语查询中。

&emsp;&emsp;问题在于这前50个term可能没有`fox`这个term，使得没法找到`quick brown fox`这个短语。但这通常又不是一个问题，因为用户会继续输入更多的字母，直到出现它们想要查找的词。

&emsp;&emsp;见[completion suggester](#Completion Suggester)和[search_as_you_type field type](#Search-as-you-type field type)了解更多`search-as-you-type`更好的解决方案。

#### Combined fields
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-combined-fields-query.html)

&emsp;&emsp;`Combined fields` query支持查询多个`text`字段，就像是这些字段的值索引到一个组合字段（combined fields）中。该query是一种"以term为中心"的（term-centric）的视角（见[multi_match](#Multi-match query)中以field为中心的内容）：首先将待查询的字符串分词为一个个独立的term，然后在所有字段中去查找每一个term。这个query特别适用在多个text字段中进行匹配，例如一篇文章中的`title`、`abstract`和`body`中。

```text
GET /_search
{
  "query": {
    "combined_fields" : {
      "query":      "database systems",
      "fields":     [ "title", "abstract", "body"],
      "operator":   "and"
    }
  }
}
```

&emsp;&emsp;`combined_fields` query基于simple BM25F（见[The Probabilistic Relevance Framework: BM25 and Beyond](http://www.staff.city.ac.uk/~sbrp622/papers/foundations_bm25_review.pdf)）公式这个原则方法进行打分。在对匹配到的文档进行打分时，该query会跨多个字段结合term和收集到的统计值，就像是指定的这些字段索引到了单个的、组合字段中。This scoring is a best attempt。`combined_fields` makes some approximations and scores will not obey the BM25F model perfectly。

> WARNING：Filed number limit
> 默认情况下，某个query中包含的clause的数量是由限制的。定义在[indices.query.bool.max_clause_count](#indices.query.bool.max_clause_count)中。默认值为`4096`。对于combined fields query。clause的数量的计算方式为：字段的数量和term的数量的乘积。
> 

##### Per-field boosting

&emsp;&emsp;每个字段的boost通过combined field model描述。例如，如果`title`的boost为`2`，就像是title中的每一个term在synthetic combined field中出现两次。

```text
GET /_search
{
  "query": {
    "combined_fields" : {
      "query" : "distributed consensus",
      "fields" : [ "title^2", "body" ] 
    }
  }
}
```

&emsp;&emsp;第6行，每一个字段都可以通过`^`符号实现boost。

> NOTE：`combined_fields` query中字段的boost是一个大于等于1的值，可以是小数。

##### Top-level parameters for combined_fields

- fields：（Required, array of strings）待查询的字段名列表。字段名可以是wildcard patterns。只支持[text](#Text type family)，并且只能是相同的[analyzer](#analyzer(mapping parameter))。
- query：（Required, strings）待查询的内容。`combined_fields` query在执行查询前会[analyzer](#analyzer(mapping parameter)) 待查询的内容。
- auto_generate_synonyms_phrase_query：（Optional, Boolean）如果为`true`，会为多个term同义词创建 [match phrase](#Match phrase query) query。默认为`true`。见[Use synonyms with match query](#Match query)。
- operator：（Optional, string）用来描述`query`中的值之间的布尔关系。可选值为：
  - or(默认值)：例如，`query`的值如果是`database systems`会解释（interpret）为`database OR systems`。
  - and：例如，`query`的值如果是`database systems`会解释（interpret）为`database AND systems`。
- minimum_should_match：（Optional, string）返回的文档必须匹配到的clause的数量最小值。见[minimum_should_match parameter](#minimum_should_match parameter)。
- zero_terms_query：（Optional, string）如果分词（`analyzer`）之后移除了所有的token（比如使用了`stop`过滤器），是否要返回文档。可选值：
  - none（default）：如果在分词后移除了所有的token不返回任何文档
  - all：返回所有的文档，类似[match_all](#Match all query)。见[Zero terms query](#Zero terms query)中的例子。

##### Comparison to multi_match query

&emsp;&emsp;`combined_fields` query提供了一种在多个[text](#Text type family)字段中匹配和评分的有原则的方式。为了支持这一点，它要求所有字段具有相同的查询分词器[analyzer](#analyzer(mapping parameter))。

&emsp;&emsp;如果你的某个查询要在不同类型的字段上进行，那么[multi_match](#Multi-match query)比较合适。他同时支持text和非text类型的字段，并且接受在不同的text字段上可以有不同的分词器。

&emsp;&emsp;`multi_match`主要的两种模式`best_fields`和`most_fields`是种"以field为中心"视角（field-centric view）的查询。与之相反的`combined_fields`则是"以term为中心"视角。`operator`和`minimum_should_match`应用在每一个term上，而不是每个字段上。比如有这样的query：

```text
GET /_search
{
  "query": {
    "combined_fields" : {
      "query":      "database systems",
      "fields":     [ "title", "abstract"],
      "operator":   "and"
    }
  }
}
```

&emsp;&emsp;执行后如下：

```text
+(combined("database", fields:["title" "abstract"]))
+(combined("systems", fields:["title", "abstract"]))
```

&emsp;&emsp;也就是说，一篇文档中至少有一个字段有这个term才能被匹配（换句话说，一篇文档中只要有database，system就满足匹配，这两个term可以分布在不同的字段上，最好的情况是这两个term在同一个字段上）。

&emsp;&emsp;`cross_fields` `multi_match`模式同样是"以term为中心"的方法并且`operator`和`minimum_should_match`应用在每一个term上。与`cross_fields`相比，`combined_fields`的主要优势在于其基于BM25F算法的稳健且易于解释的评分方法。

> NOTE：Custom similarities
> `combined_fields`只支持BM25，他是默认的Similarity，可以通过[custom similarity](#Similarity module)配置。同样也不允许[Per-field similarities](#similarity)。否则使用`combined_fields`时报错。


#### Multi-match query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-multi-match-query.html)

&emsp;&emsp;`multi_match`基于[match query](#Match query)，允许在多个字段上查询：

```text
GET /_search
{
  "query": {
    "multi_match" : {
      "query":    "this is a test", 
      "fields": [ "subject", "message" ] 
    }
  }
}
```

&emsp;&emsp;第5行是查询语句（query string）
&emsp;&emsp;第6行，查询的目标字段

##### fields and per-field boosting

&emsp;&emsp;可以使用通配符指定字段：

```text
GET /_search
{
  "query": {
    "multi_match" : {
      "query":    "Will Smith",
      "fields": [ "title", "*_name" ] 
    }
  }
}
```

&emsp;&emsp;第6行，在`title`、`first_name`以及`last_name`字段上查询

&emsp;&emsp;不同的字段可以通过插入`^`符号来提高权重：

```text
GET /_search
{
  "query": {
    "multi_match" : {
      "query" : "this is a test",
      "fields" : [ "subject^3", "message" ] 
    }
  }
}
```

&emsp;&emsp;第6行，在`subject`字段上得出的分数会乘以3，而在`message`字段上的打分则不变

&emsp;&emsp;如果没有提供`fields`，`multi_match`默认在`index.query.default_field`中指定的字段上查询，默认是`*`。`*`会从mapping中提取出所有的合适的字段并且过滤掉metadata fields。所有提取出的字段随后组合构建成一个query。

> WARNING：Field number limit
> 默认情况下，某个query中包含的子query的数量（number of clauses）是有上限的。该上限在[indices.query.bool.max_clause_count](#indices.query.bool.max_clause_count)中定义。默认是`4096`。对于`multi-match`，计算子query数量的方式是：字段的数量\*term的数量

##### Types of multi_match query:

&emsp;&emsp;`multi_match`在内部执行方式取决于`type`参数，可以设置为以下的值：

- best_fields：（default）找出匹配到任意字段的文档，但是`_score`使用最佳字段（best field）的打分值，见[best_fields](#best_fields)
- most_fields：找出匹配到任意字段的文档，但是`_score`的值结合了（combine）每一个字段的打分值，见[most_fields](#most_fields)
- cross_fields：所有的字段使用相同的分词器，并且是视为同一类型的字段（they were one big field，例如，名和姓分别在两个字段中，但用于搜索同一个人），在每一个字段中搜索关键字。见[cross_fields](#cross_fields)
- phrase：在每一个字段上执行`match_phrase`，`_score`使用最佳字段（best field）的打分值。见[phrase and phrase_prefix](#phrase and phrase_prefix)
- phrase_prefix：在每一个字段上执行`match_phrase_prefix`，`_score`使用最佳字段（best field）的打分值。见[phrase and phrase_prefix](#phrase and phrase_prefix)
- bool_prefix：创建一个`match_bool_prefix`，对每一个字段进行查询。`_score`的值结合了（combine）每一个字段的打分值，见[bool_prefix](#bool_prefix)

##### best_fields

&emsp;&emsp;当你搜索的多个关键字（word）在同一个字段中时，`best_fields`是非常有用的。比如说`brown fox`在单个字段中肯定比`brown`在一个字段而`fox`在另一个字段更有意义。

&emsp;&emsp;`best_fields`会为每一个字段生成一个[match query](#Match query)并且封装到一个[dis_max](#Disjunction max query) query，来找出单个最佳匹配的字段。例如：

```text
GET /_search
{
  "query": {
    "multi_match" : {
      "query":      "brown fox",
      "type":       "best_fields",
      "fields":     [ "subject", "message" ],
      "tie_breaker": 0.3
    }
  }
}
```

&emsp;&emsp;将会按下面的方式执行：

```text
GET /_search
{
  "query": {
    "dis_max": {
      "queries": [
        { "match": { "subject": "brown fox" }},
        { "match": { "message": "brown fox" }}
      ],
      "tie_breaker": 0.3
    }
  }
}
```

&emsp;&emsp;通常`best_fields`类型会采用匹配到的单个最佳的字段对应的打分值，但如果指定了`tie_breaker`，那会考虑下面的情况：

- 打分值从最佳匹配的字段中获取
- 加上其他所有匹配到的字段中的分数，计算方式：`tie_breaker * _score`

&emsp;&emsp;同样支持这些参数：`analyzer`, `boost`, `operator`, `minimum_should_match`, `fuzziness`, `lenient`, `prefix_length`, `max_expansions`, `fuzzy_rewrite`, `zero_terms_query`, `auto_generate_synonyms_phrase_query` 以及 `fuzzy_transpositions`，见[match query](#Match query)中的介绍。

> IMPORTANT：operator and minimum_should_match
> `best_fields`和`most_fields`这两个类型是以字段为中心，每一个字段都生成了一个`match query`。意味着`operator`和`minimum_should_match`参数是应用到字段之间，可能不是你期望的那样。例如下面的query：
> GET /\_search
>{
>  "query": {
>    "multi_match" : {
>      "query":      "Will Smith",
>      "type":       "best_fields",
>      "fields":     [ "first_name", "last_name" ],
>      "operator":   "and" 
>    }
> }
>}
>`operator`为`and`说明每个字段中必须出现所有的term
>这个query的执行相当于：
>`(+first_name:will +first_name:smith)| (+last_name:will +last_name:smith)`
>换句话说，对于匹配到的文档，单个字段中必须出现所有的term
>
>[combined_fields](#Combined fields)提供了"以term为中心"的视角，`operator`和`minimum_should_match`应用到每一个term之间。其他的`multi-match`模式[cross_fields](#cross_fields)同样能解决这个问题

##### most_fields

&emsp;&emsp;`most_fields`类型最适合在多个字段中进行查询，并且每个字段都索引了相同的文本，但是使用了不同的分词器的情况。例如，主字段可能包含synonyms、stemming term和没有变音符号（diacritics）的term。第二个字段可能包含原始的term，第三个字段可能包含词缀（shingles）。通过结合来自所有这三个字段的分数，我们可以匹配尽可能多的与主字段相符的文档，同时利用第二和第三字段将最相似的结果推到列表的顶部：

```text
GET /_search
{
  "query": {
    "multi_match" : {
      "query":      "quick brown fox",
      "type":       "most_fields",
      "fields":     [ "title", "title.original", "title.shingles" ]
    }
  }
}
```

&emsp;&emsp;这个查询将通过以下的方式：

```text
GET /_search
{
  "query": {
    "bool": {
      "should": [
        { "match": { "title":          "quick brown fox" }},
        { "match": { "title.original": "quick brown fox" }},
        { "match": { "title.shingles": "quick brown fox" }}
      ]
    }
  }
}
```

&emsp;&emsp;汇总每一个`match`的打分值，然后除以`match`的数量就是最终的打分值。

&emsp;&emsp;同样接受这些参数：`analyzer`， `boost`， `operator`，`minimum_should_match`， `fuzziness`，`lenient`，`prefix_length`，`max_expansions`，`fuzzy_rewrite`，以及 `zero_terms_query`。

##### phrase and phrase_prefix

&emsp;&emsp;`phrase`和`phrase_prefix`类型跟[best_fields](#best_fields)很像，只是它们使用了`match_phrase`或者`match_phrase_prefix`而不是`match` query。

&emsp;&emsp;这个请求：

```text
GET /_search
{
  "query": {
    "multi_match" : {
      "query":      "quick brown f",
      "type":       "phrase_prefix",
      "fields":     [ "subject", "message" ]
    }
  }
}
```

&emsp;&emsp;这个查询将通过以下的方式：

```text
GET /_search
{
  "query": {
    "dis_max": {
      "queries": [
        { "match_phrase_prefix": { "subject": "quick brown f" }},
        { "match_phrase_prefix": { "message": "quick brown f" }}
      ]
    }
  }
}
```

&emsp;&emsp;同样的接受参数`analyzer`，`boost`，`lenient`以及 `zero_terms_query`，见[Match](#Match query)中的介绍。接受参数`slop`，见[Match phrase](#Match phrase query)中的介绍。另外`phrase_prefix`类型接受`max_expansions`参数。

> IMPORTANT：**phrase, phrase_prefix and fuzziness**
> `phrase`和`phrase_prefix`不能使用`fuzziness`参数。

##### cross_fields

&emsp;&emsp;`cross_fields`类型对于在结构化的文档中期望在多个字段中进行匹配这种场景就特别的有用。例如，当在`first_name`和`last_name`中查找`Will Smith`，最好的匹配结果应该是`Will`在上面两个字段中的一个，并且`Smith`在另一个字段中。

- 这看起来好像使用[most_fields](#most_fields)就可以完成，但是这种方法有两个问题。第一个问题是`operator`跟`minimum_should_match`是应用在每一个字段上，而不是每一个term上（见[explanation above](#best_fields)）
- 第二个问题是相关性问题：`first_name`跟`last_name`字段中相同term的词频不同会产生非期望的结果
- 比如说，我们有两个人：`Will Smith`和`Smith Jones`。`Smith`作为last name是非常常见的（因此重要性较低）但是`Smith`作为first name是不常见的（因此重要性高）
- 如果我们查询`Will Smith`，包含`Smith Jones`的文档应该优于包含`Will Smith`的文档，因为`first_name:Smith`的打分会高于`first_name:will`和`last_name:smith`的打分值和值，这不是我们期望的结果。

&emsp;&emsp;一种解决方式是简单的将`first_name`和`last_name`合成为一个`full_name`。当然，只能在索引期间完成。

&emsp;&emsp;`cross_field`类型能够在查询时间通过一种"以term为中心"的方法解决。它首先将query string分词成单独的term，然后在每一个字段中为每一个term进行查找，就好像它们是一个big field。

```text
GET /_search
{
  "query": {
    "multi_match" : {
      "query":      "Will Smith",
      "type":       "cross_fields",
      "fields":     [ "first_name", "last_name" ],
      "operator":   "and"
    }
  }
}
```

&emsp;&emsp;这个查询将通过以下的方式：

```text
+(first_name:will last_name:will)
+(first_name:smith last_name:smith)
```

&emsp;&emsp;换句话说，所有的term必须存在于至少一个字段中（所有的term分布在不同的字段中，最多一个字段中有所有的term）。而`best_field`跟`most_field`则可能做不到这一点。

- 也就说`cross_field`能保证query string中的每一个term都会出现在匹配的文档中（min_should_match），这些term可能分布在多个字段，也有可能同时出现在同一个字段。而`best_field`跟`most_field`匹配到的文档中可能无法找到query string中的某些term

&emsp;&emsp;这种类型解决了上述两个问题中的一个问题。term在不同字段中词频不同的问题通过`混合词频(blending term frequencies)`来平衡差异性。

&emsp;&emsp;在实践中，`first_name:smith`被视为跟`last_name:smith`有相同的词频并且加1。这使得匹配`first_name`和`last_name`时有可比较的打分值，但`last_name`会有轻微优势，因为它更可能包含词`smith`

&emsp;&emsp;`cross_fields`类型查询通常仅适用于短的字符串字段，且这些字段的boost值为1。如果字段有不同的boost、freq和length normalization，则混合词频统计可能失去意义

&emsp;&emsp;如果你通过[Validate](#Validate API)运行上面的query，它会返回这种解释：

```text
+blended("will",  fields: [first_name, last_name])
+blended("smith", fields: [first_name, last_name])
```

&emsp;&emsp;接受这些参数：`analyzer`, `boost`, `operator`, `minimum_should_match`, `lenient` 以及 `zero_terms_query`。

> WARNING：`cross_fields`类型的查询在混合字段统计时可能不总是产生合理的评分（例如评分可能变为负数）。作为替代，Ni可以考虑使用[combined_fields](#Combined-fields)查询，这种查询同样"以term为中心"，但以更稳健的方式结合字段统计。这是针对跨字段查询的一种改进方法。

###### cross_field and analysis

&emsp;&emsp;查询的字段有相同的分词器才能"以term为中心"的模式工作。有相同分词器的字段会被划分为一个组。如果有多个组，查询将会使用这些组中打分值最高的那个。

&emsp;&emsp;例如，如果`first`和`last`字段有相同的分词器，而`first.edge`和`last.edge`有相同的`edge_ngram`分词器，如下所示：

```text
GET /_search
{
  "query": {
    "multi_match" : {
      "query":      "Jon",
      "type":       "cross_fields",
      "fields":     [
        "first", "first.edge",
        "last",  "last.edge"
      ]
    }
  }
}
```

&emsp;&emsp;这个query会按下面的方式执行：

```text
    blended("jon", fields: [first, last])
| (
    blended("j",   fields: [first.edge, last.edge])
    blended("jo",  fields: [first.edge, last.edge])
    blended("jon", fields: [first.edge, last.edge])
)
```

&emsp;&emsp;换句话说，`first`和`last`会被化为同一组，就像是单个字段一样，而而`first.edge`和`last.edge`会被化为同一组，就像是单个字段一样。

&emsp;&emsp;存在多个组是没有问题的，但当使用了`operator`或`minimum_should_match`就会出现上文中提到使用`most_field`和`best_field`的问题。

&emsp;&emsp;你可以很容易的使用`dis_max`query来组合两个`cross_field`类型的查询，并且只将`minimum_should_match`应用到其中一个就行了 ：

```text
GET /_search
{
  "query": {
    "dis_max": {
      "queries": [
        {
          "multi_match" : {
            "query":      "Will Smith",
            "type":       "cross_fields",
            "fields":     [ "first", "last" ],
            "minimum_should_match": "50%" 
          }
        },
        {
          "multi_match" : {
            "query":      "Will Smith",
            "type":       "cross_fields",
            "fields":     [ "*.edge" ]
          }
        }
      ]
    }
  }
}
```

&emsp;&emsp;第11行，`will`或者`smith`必须出现在`frist`或`last`字段中。

&emsp;&emsp;你可以通过在请求中指定分词器将所有的字段归为一个组：

```text
GET /_search
{
  "query": {
   "multi_match" : {
      "query":      "Jon",
      "type":       "cross_fields",
      "analyzer":   "standard", 
      "fields":     [ "first", "last", "*.edge" ]
    }
  }
}
```

&emsp;&emsp;第7行，所有的字段使用`standard`分词器。

&emsp;&emsp;这个请求按照下面的方式执行：

```text
blended("will",  fields: [first, first.edge, last.edge, last])
blended("smith", fields: [first, first.edge, last.edge, last])
```

###### tie_breaker

&emsp;&emsp;默认情况下，一个组中每个term的`blended`query会返回最高的打分值。当跨多个组整合打分值时，会使用组中打分值最高的。`tie_breaker`参数能对这两个步骤进行更改：

| 0.0           | Take the single best score out of (eg) `first_name:will` and `last_name:will` (default) |
| ------------- | ------------------------------------------------------------ |
| 1.0           | Add together the scores for (eg) `first_name:will` and `last_name:will` |
| 0.0 < n < 1.0 | Take the single best score plus `tie_breaker` multiplied by each of the scores from other matching fields/ groups |

> IMPORTANT：cross_fields and fuzziness
> `cross_fields`类型不能使用`fuzziness`参数。

##### bool_prefix

&emsp;&emsp;`bool_prefix`类型的行为跟[most_fields](#most_fields)很像，但是使用了[match_bool_prefix](#Match boolean prefix query) query而不是`match` query。

```text
GET /_search
{
  "query": {
    "multi_match" : {
      "query":      "quick brown f",
      "type":       "bool_prefix",
      "fields":     [ "subject", "message" ]
    }
  }
}
```

&emsp;&emsp;可以接受`analyzer`, `boost`, `operator`, `minimum_should_match`, `lenient`, `zero_terms_query`以及 `auto_generate_synonyms_phrase_query`参数，见[match query](#Match query)中的介绍。`fuzziness`, `prefix_length`, `max_expansions`, `fuzzy_rewrite`, `fuzzy_transpositions`用于terms来构造term query，但是不会对prefix query构造出的term生效。

&emsp;&emsp;不支持`slop`参数。

#### Query string query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-query-string-query.html)

>TIP：本节内容介绍的是`query_string`这个query类型，更多关于在Elasticsearch中运行一个查询query的内容见[Search your data](#Search your data).

&emsp;&emsp;基于提供的查询字符串（query string），使用一个语法严格的解析器并且返回文档。

&emsp;&emsp;这个query使用一个[syntax](#Query string syntax)进行解析并且基于`AND`或者`NOT`操作符对query string进行切分。然后在返回匹配的文档前，这个query独立的 [analyzer](#Text analysis)每一个切分的文本。

&emsp;&emsp;你可以使用`query_string` query创建一个包含通配符，跨多个字段以及更多信息的复杂查询。虽然具备很多功能（versatile），但这个query的语法很严格，如果query string中包含任何无效语法，则返回错误。

>WARNING：由于任意非法语法都会导致这个query返回错误，我们不建议在搜索框（search boxes）中使用`query_string`中使用这个查询。
>如果你不需要支持query syntax，可以考虑使用[match query](#Match query)。如果你需要query syntax的功能，可以使用[simple_query_string](#Simple query string query)，这种query没有那么严格的语法。

##### Example request

&emsp;&emsp;当你运行下面的查询，`query_string` query会将`(new york city) OR (big apple)`划分两个部分：`new york city` 以及 `big apple`。随后在返回匹配到的文档前，`content`字段的分词器分别将每一部分的值分成token。由于query syntax没有使用whitespace作为一个操作符，`new york city`会原样的传给分词器。

```text
GET /_search
{
  "query": {
    "query_string": {
      "query": "(new york city) OR (big apple)",
      "default_field": "content"
    }
  }
}
```

##### Top-level parameters for query_string

- query：（Required, string）你想要解析并且用于查询的查询字符串（query string）。见[Query string syntax](#Query string syntax)。
- default_field：（Required, string）如果没有在query string指定字段，那么就使用这个默认字段进行查询。支持通配符（\*）。
  - 默认是index setting中的[index.query.default_field](#index.query.default_field)，默认值是`*`。`*`会提取出所有符合（eligible）term queries and filters the metadata fields的字段。如果没有指定`prefix`，所有提取出来的字段会构建成一个query。
  - 查询会跨所有符合的字段但是不包括[nested documents](#Nested field type)，可以使用[nested query](#Nested query)来查询。
  - 对于定义了大量字段的mapping，在所有符合的（eligible）字段上进行搜索的开销是很大的。
  - 单次查询有clause数量（字段的数量 \* term数量）的限制，在[search setting](#Search settings)中定义了`indices.query.bool.max_clause_count`，默认值是4096。
- allow_leading_wildcard：（Optional, Boolean)）如果为`true`，通配符`*`以及`?`允许作为query string的首个字符。默认值是`true`。
- analyze_wildcard：（Optional, Boolean）如果为`true`，会尝试分析（analyze）query string中的通配字符。默认值是`false`。
- analyzer：（Optional, string） [Analyzer](#Text analysis)用于将query string中的文本转化成token。默认值为`default_field`字段在[index-time analyzer](#Specify an analyzer)中的分词器。如果没有设置analyzer，则使用索引默认的分词器。
- auto_generate_synonyms_phrase_query：（Optional, Boolean）如果为`true`，[match phrase](#Match phrase query)会自动的创建同义词的查询。默认值为`true`。见[Synonyms and the query_string query](#Synonyms and the query_string query)。
- boost：（Optional, float）浮点值，用于提高或者降低query的[relevance scores](#Relevance-scores)。默认值为`1.0`。
  - boost的值默认关联的值为`1.0`。`0`到`1.0`之间的值会降低relevance score，大于`1.0`的值会提高relevance score。
- default_operator：（Optional, string）如果没有指定该配置，则默认使用布尔逻辑来解析（interpret）query string中的文本。可选值为：
  - OR（Default）
    - 例如，`query`的值是`capital of Hungary`会解析成（interpret）`capital OR of OR Hungary`
  - AND
    - 例如，`query`的值是`capital of Hungary`会解析成（interpret）`capital AND of AND Hungary`
- enable_position_increments：（Optional, Boolean）如果为`true`，在`query_string`查询中构造的query中，开启position increments。默认值为`true`。
- fields：（Optional, array of strings）待查询的字段的列表。支持通配符（`*`）。
  - 你可以使用这个参数跨多个字段进行查询。见[Search multiple fields](#Search multiple fields（query string）)。
- fuzziness：（Optional, strings）fuzzy匹配时允许的最大编辑距离。见[Fuzziness](#Fuzziness（query string）)了解fuzzy语法。
- fuzzy_max_expansions：（Optional, integer）fuzzy匹配时允许扩展出的term的数量最大值。默认值为`50`。
- fuzzy_prefix_length：（Optional, integer）fuzzy匹配时起始字符保留的数量。默认值为`0`。
- fuzzy_transpositions：（Optional, Boolean）如果为`true`，模糊匹配中的编辑距离包含两个字符交换（ab->ba）。默认值为`true`。
- lenient：（Optional, Boolean）如果为`true`，例如当在一个[numeric](#Numeric field types)字段中，`query`的内容为文本时会忽略format-based的错误，默认值为`false`。
- max_determinized_states：（Optional, integer）查询所需最大自动机状态（[automaton states](https://en.wikipedia.org/wiki/Deterministic_finite_automaton)）数。默认值为`10000`。
  - Elasticsearch使用Apache Lucene解析正则表达式。Lucene将正则表达式转化为一个有一定状态数的[有限自动状态机](https://amazingkoala.com.cn/unsupported/Automaton.html)。
  - 可以使用这个参数防止这个转化过程造成资源的过度使用。当然你可能需要提高这个限制来允许复杂的正则表达式。
- minimum_should_match：（Optional, string） 匹配到的文档必须至少满足clause（一个子query视为一个query clause）的数量。见[minimum_should_match parameter](#minimum_should_match parameter)查看更多信息。见[How minimum_should_match works](#How minimum_should_match works)中的例子。
- quote_analyzer：（Optional, string）[Analyzer](#Text analysis)用来将query string中引用的文本（比如说"\\"The Great Gatsby\\""）转化为token。默认使用`default_field`映射的[search_quote_analyzer](#search_quote_analyzer)。
  - 对于引用的文本，这个参数会覆盖`analyzer`参数中指定的分词器。
- phrase_slop：（Optional, integer）短语中的token之间最大的距离。默认是`0`。如果是`0`，需要匹配精准的短语。位置颠倒的term的`slop`为2。
- quote_field_suffix：（Optional, string）query string中引用的文本对应的后缀名。
&emsp;&emsp;例如说有一个mapping：

```text
"mappings": {
    "properties": {
      "body": {
        "type": "text",
        "analyzer": "english",
        "fields": {
          "exact": {
            "type": "text",
            "analyzer": "english_exact"
          }
        }
      }
    }
  }
```

  - ​	这个例子中，`quote_field_suffix`的值就是`exact`。如果指定了 `quote_field_suffix`。那么相当于在`body.exact`字段上查询query string。
    - 你可以根据后缀来使用一个不同的分词器使得可以进行精准匹配。见[Mixing exact search with stemming](#Mixing exact search with stemming)。

- rewrite：（Optional, string）用来用来重写query的方法。更多信息见[rewrite parameter](#rewrite parameter)。
- time_zone：（Optional, string）使用[Coordinated Universal Time (UTC) offset ](https://en.wikipedia.org/wiki/List_of_UTC_offsets)或者[IANA time zone](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones)将query string中的`date`值转化为UTC。
  - 你也可以用 ISO 8601 UTC 偏移来指定时区（`+01:00 or -08:00`）或者 IANA time zone ID，比如`America/Los_Angeles`

  > NOTE：`time_zone`参数不会影响`now`这种[date math](#Date Math)。`now`始终是UTC的当前系统时间。但是，`time_zone`参数会转换使用now和[date math rounding](#Date Math)计算出的日期。例如，使用`time_zone`参数会转换`now/d`的值。

##### Note

###### Query string syntax

&emsp;&emsp;这个query string：`mini-language`用于[Query string](#Query string query)以及[search API](#Search API)中的`q`这个query string parameter。

&emsp;&emsp;query string被解析为一系列的term以及操作符。term可以是单个Word，比如`quick`或者`brown`。也可以是一个短语，使用引号修饰，比如`quick brown`，这种将查找短语中所有的word，并且是有序的。

&emsp;&emsp;操作符使得你可以自定义查询，可用选项如下所示：

###### Field Names

&emsp;&emsp;你可以在查询语法中指定字段名：

- `status`字段中包含`active`

```text
status:active
```

- `title`字段中包含`qucik`或者`brown`

```text
title:(quick OR brown)
```

- `author`字段中包含精确的短语`john smith`

```text
author:"John Smith"
```

- `first name`字段中包含`Alice`（注意的是对空格使用了反斜杠）

```text
first\ name:Alice
```

- `book.title`，`book.content`或者`book.date`中任意一个字段中包含`qucik`或者`brown`（注意的是我们需要\*使用反斜杠）

```text
book.\*:(quick OR brown)
```

- `title`字段没有空值

```text
_exists_:title
```

###### Wildcards（query string）

&emsp;&emsp;通配符查询可以运行在单个term上，使用`?`替换单个字符，使用`*`替换0个或多个字符：

```text
qu?ck bro*
```

&emsp;&emsp;注意的是通配符查询会使用大量的内存，并且性能很差。你可以想象下匹配`a* b* c*`这个query string会有多少个term？

> WARNING：如果你只使用了`*`，那么会被重写为[exists query](#Exists-query)。因此，`field:*`将会匹配空值（不是null）：
{
  "field": ""
}
不会匹配缺失字段或者显示的设置字段值为null：
{
  "field": null
}

> WARNING：如果将`*`作为起始字符（leading wildcards）是特别开销大的，因为索引中所有的term都需要处理。可以通过设置`allow_leading_wildcard`为`false`关闭。
> 

###### Regular expressions（query string）

&emsp;&emsp;可以在query string中插入正则表达式，使用`/`修饰：

```text
name:/joh?n(ath[oa]n)/
```

&emsp;&emsp;正则表达式的语法支持见[Regular expression syntax](#Regular expression syntax)。

> WARNING：`allow_leading_wildcard`参数不会应用于正则表达式。query string中包含下面的内容将会让Elasticsearch访问索引中的每一个term：`/.*n/`。请小心使用。

###### Fuzziness（query string）

&emsp;&emsp;你可以`~`操作符来允许[fuzzy queries](#Fuzzy query)：

```text
quikc~ brwn~ foks~
```

&emsp;&emsp;对于这些query，query string会被[normalized](#Normalizers)（对query string进行处理，处理方式基于[character filters](#Character filters reference)和[token filters](#Token filter reference)），不过只能使用部分filter，见[Normalizers](#Normalizers)。

&emsp;&emsp;query使用[Damerau-Levenshtein distance](https://en.wikipedia.org/wiki/Damerau%E2%80%93Levenshtein_distance)来找出所有最多有两处变化的term，这个变化可以是插入，删除，或者单个字符的替换或者两个相邻字符的位置互换。

&emsp;&emsp;默认的编辑距离是`2`，但是编辑距离为`1`时能满足80%的人为拼写。通过下面的方式指定：

```text
quikc~1

```

> WARNING：Avoid mixing fuzziness with wildcards
> 不支持混合使用[fuzzy](#Fuzziness（Common options）)和[wildcard](#Wildcards（query string）)操作符。如果出现了混合使用，其中一种操作不会被应用（apply）。例如，你可以查询`app~1（fuzzy）`或者`app*`（wildcard），但对于`app~1`，不会应用fuzzy中的操作符`~`

###### Proximity searches

&emsp;&emsp;短语查询（例如`john smith`）期望所有的term有精准的先后顺序，而`Proximity searches`允许指定的word可以离开它相邻的word距离更大或者先手顺序不同。同样的fuzzy query可以为一个word中的字符指定一个最大编辑距离，而`proximity searches`允许我们指定短语中一个word的最大编辑距离：

```text
"fox quick"~5
```

&emsp;&emsp;相比于query string中原有的先手顺序越近，相关性则越高。如果比较上面的这个查询，短语`quick fox`比`quick brown fox`更有相关性。

###### Ranges（query string）

&emsp;&emsp;可以对时间、数值或者字符串字段指定一个范围。包含上下界的范围使用中括号`[min TO max]`，不包含上下界的范围使用花括号`{min TO max}`。

- All days in 2012:

  ```text
  date:[2012-01-01 TO 2012-12-31]
  ```

- Numbers 1..5

  ```text
  count:[1 TO 5]
  ```

- Tags between `alpha` and `omega`, excluding `alpha` and `omega`:

  ```text
  tag:{alpha TO omega}
  ```

- Numbers from 10 upwards

  ```text
  count:[10 TO *]
  ```

- Dates before 2012

  ```text
  date:{* TO 2012-01-01}
  ```

&emsp;&emsp;也可以组合使用花括号跟中括号：

- Numbers from 1 up to but not including 5

```text
count:[1 TO 5}
```

&emsp;&emsp;一边是无界的语法如下：

```text
age:>10
age:>=10
age:<10
age:<=10
```

> NOTE：要使用简化语法同时结合上界和下界，你需要用AND操作符连接两个子句。这种方法允许你在查询中明确指定一个范围，确保结果符合两个条件的交集。
> age:(>=10 AND <20) age:(+>=10 +<20)

&emsp;&emsp;对query string中的范围语句进行解析可能复杂并且容易出错，最好直接使用[range](#Range query)query。

###### Boosting（query string）

&emsp;&emsp;使用增强（boost）操作符`^`使得某个term比其他term更具相关性。例如，如果我们要找到所有关于fox的文档，但是我们又特别对quick fox感兴趣，那么就可以：

```text
quick^2 fox
```

&emsp;&emsp;`boost`的默认值为 `1`，它可以是任意的正浮点数。0到1之间的boost值会降低相关性。

&emsp;&emsp;boost同样可以应用在短语或者组：

```text
"john smith"^2   (foo bar)^4
```

###### Boolean operators（query string）

&emsp;&emsp;默认情况下，所有的term都是optional。如果查询`foo bar baz`，那么包含一个或者多个其中一个term（`foo`或者`bar`或者`baz`）的文档都是满足匹配的。上文中我们已经讨论过`default_operator`，它使得必须所有的term都要匹配到。但可以在query string中使用`boolean operator`来提供更多的控制。

&emsp;&emsp;首选的操作符是`+`（文档中必须有所有的term）以及`-`（文档中不能出现term）。其他的term则是可有可无的。例如这个query：

```text
quick brown +fox -news
```

&emsp;&emsp;表示：

- `fox`必须存在
- `news`不能存在
- `quick`和`brown`是可选的——它们的存在增加了相关性

&emsp;&emsp;虽然也支持常见的布尔操作符`AND`、`OR`和`NOT`（也可以写作`&&`、`||`和`!`），但要注意它们不遵循通常的优先级规则，所以当一起使用多个操作符时应使用括号。例如，上述查询可以重写为：

```text
((quick AND fox) OR (brown AND fox) OR fox) AND NOT news
```

&emsp;&emsp;这种形式正确解释了原始查询的逻辑，但相关性评分与原始查询相去甚远。相比之下，使用[match](#Match query)重写的同一个查询将会是这样的：

```text
{
    "bool": {
        "must":     { "match": "fox"         },
        "should":   { "match": "quick brown" },
        "must_not": { "match": "news"        }
    }
}
```

###### Grouping（query string）

&emsp;&emsp;多个term或者clause可以通过括号组合在一起形成子query（sub-query）：

```text
(quick OR brown) AND fox
```

&emsp;&emsp;分组还可以用于应用到某个特定的字段上，或者boost这个子query的打分：

```text
status:(active OR pending) title:(full text search)^2
```

###### Reserved characters（query string）

&emsp;&emsp;如果在你的查询中，某些字符（保留字符reversed character）的功能是作为操作符或者不作为操作符，那你可以使用反斜杠进行转移控制。例如，若要查询`(1+1)=2`，那么你的query应该是`\(1\+1\)\=2`。

&emsp;&emsp;请求body中使用JSON时，需要用到两个反斜杠(`\\`)，注意在JSON中反斜杠是保留的转义字符。

```text
GET /my-index-000001/_search
{
  "query" : {
    "query_string" : {
      "query" : "kimchy\\!",
      "fields"  : ["user.id"]
    }
  }
}
```

&emsp;&emsp;需要进行转义的保留字符有：`+ - = && || > < ! ( ) { } [ ] ^ " ~ * ? : \ /`

&emsp;&emsp;没有正确对这些特殊字符进行转义会导致语法错误并且阻止query的运行。

> NOTE：`<`跟`>`通常不能被转义，因为它们用来描述范围查询，唯一防止被解析为范围查询的方式就是从query string中完全的移除

###### Whitespaces and empty queries（query string）

&emsp;&emsp;空格不认为是一个操作符。

&emsp;&emsp;如果query string是空的或者只包含空格，那么这个query被认为是空的结果集。

###### Avoid using the query_string query for nested documents（query string）

&emsp;&emsp;`query_string`查询会不会返回[nested](#Nested field type) 文档。如果搜索nested文档，则应使用[nested query](#Nested query)。

###### Search multiple fields（query string）

&emsp;&emsp;你可以使用`field`参数在多个字段上执行`quert_string` query。

&emsp;&emsp;多个字段上执行`quert_string` query的设计是能query term能实现`OR`的查询：

```text
field1:query_term OR field2:query_term | ...
```

&emsp;&emsp;例如有以下的查询：

```text
GET /_search
{
  "query": {
    "query_string": {
      "fields": [ "content", "name" ],
      "query": "this AND that"
    }
  }
}
```

&emsp;&emsp;相当于：

```text
GET /_search
{
  "query": {
    "query_string": {
      "query": "(content:this OR name:this) AND (content:that OR name:that)"
    }
  }
}
```

&emsp;&emsp;由于多个独立的查询term（search term）生成了多个query，因此自动的使用`dis_max`以及`tie_breaker`实现。例如（`name`字段使用`^5`提高了（boost）权重）：

```text
GET /_search
{
  "query": {
    "query_string" : {
      "fields" : ["content", "name^5"],
      "query" : "this AND that OR thus",
      "tie_breaker" : 0
    }
  }
}
```

&emsp;&emsp;简单的通配符可以用于查询文档内部的元素（inner elements of the document）。例如，如果我们有一个`city`字段，它有多个子字段（或者它是一个object），我们可以自动的在`city`的所有字段上查询：

```text
GET /_search
{
  "query": {
    "query_string" : {
      "fields" : ["city.*"],
      "query" : "this AND that OR thus"
    }
  }
}
```

&emsp;&emsp;还有一种方式就是在query string自身里面提供通配符字段（wildcard fields）（正确的对`*`进行转义）。例如：`city.\*:something`：

```text
GET /_search
{
  "query": {
    "query_string" : {
      "query" : "city.\\*:(this AND that OR thus)"
    }
  }
}
```

> NOTE：由于反斜杠在json字符串中是一个特殊字符，所以它需要被转移术，因此上面的`query_string`中有两个反斜杠。

&emsp;&emsp;`field`参数中可以有基于字段名的pattern，允许自动扩展到相关的字段（会自动的引入包含的字段）。例如：

```text
GET /_search
{
  "query": {
    "query_string" : {
      "fields" : ["content", "name.*^5"],
      "query" : "this AND that OR thus"
    }
  }
}
```

###### Additional parameters for multiple field searches

&emsp;&emsp;在多个字段上运行`query_string` query时，支持以下额外的参数：

- type：（Optional, string）决定这个query如何匹配文档以及打分。合法的值如下所示：
  - best_fields (Default)：从任意字段中找到[\_score](#Relevance scores)最高的那个文档。见[best_fileds](#best_fields)
  - bool_prefix：在每个query上创建一个`match_bool_prefix` query并且结合每一个字段上的`_score`。见[bool_prefix](#bool_prefix)
  - cross_fields：将所有的字段视为有相同的`analyzer`，就像它们是一个big filed，在每一个字段中查找每一个word。见[cross_fields](#cross_fields)
  - most_fields：只要匹配到任何一个字段，这篇文档就认为是匹配的，并且结合每一个字段的`_score`。见[most_fields](#most_fields)
  - phrase：在每一个字段上执行`match_phrase`并且使用匹配度最佳的那个字段上的`_score`。见[phrase and phrase_prefix](#phrase and phrase_prefix)
  - phrase_prefix：在每一个字段上执行`match_phrase_prefix` query并且使用匹配度最佳的那个字段上的`_score`。见[phrase and phrase_prefix](#phrase and phrase_prefix)

&emsp;&emsp;注意的是，基于[type](#Types of multi_match query:)值的不同，`multi_match`查询可能支持额外的顶级参数。

###### Synonyms and the query_string query

&emsp;&emsp;`query_string` query支持使用[synonym_graph token filter](#Synonym graph token filter)对multi-terms synonyms进行扩展。在使用filter之后，解析后会为每一个multi-terms synonyms创建一个短语查询（下面用双引号修饰了`new york`）。例如下面的同义词`ny，new york`会生成如下的查询：

```text
(ny OR ("new york"))
```

&emsp;&emsp;也可以通过conjunction方式（也就是使用`AND`）进行匹配：

```text
GET /_search
{
   "query": {
       "query_string" : {
           "default_field": "title",
           "query" : "ny city",
           "auto_generate_synonyms_phrase_query" : false
       }
   }
}
```

&emsp;&emsp;上面的例子会创建一个boolean query：

```text
(ny OR (new AND york)) city
```

&emsp;&emsp;匹配到的文档中有term `ny`或者`new AND york`。`auto_generate_synonyms_phrase_query`参数默认为`true`。

###### How minimum_should_match works

&emsp;&emsp;`query_string` 会分割成多个query，然后使用操作符创建一个boolean query作为整个输入。你可以使用`minimum_should_match`来控制结果中应该匹配多少个`clause`。

```text
GET /_search
{
  "query": {
    "query_string": {
      "fields": [
        "title"
      ],
      "query": "this that thus",
      "minimum_should_match": 2
    }
  }
}
```

&emsp;&emsp;上面的例子创建了一个boolean query：

```text
(title:this title:that title:thus)~2
```

&emsp;&emsp;满足匹配的文档中至少在单个`title`字段上要匹配`this`、`that`、`thus`中的两个。

###### How minimum_should_match works for multiple fields

```text
GET /_search
{
  "query": {
    "query_string": {
      "fields": [
        "title",
        "content"
      ],
      "query": "this that thus",
      "minimum_should_match": 2
    }
  }
}
```

&emsp;&emsp;上面的例子创建了一个boolean query：

```text
((content:this content:that content:thus) | (title:this title:that title:thus))
```

&emsp;&emsp;在`title`跟`content`上使用disjunction max来匹配文档。这里的`minimum_should_match`并没有生效。

```text
GET /_search
{
  "query": {
    "query_string": {
      "fields": [
        "title",
        "content"
      ],
      "query": "this OR that OR thus",
      "minimum_should_match": 2
    }
  }
}
```

&emsp;&emsp;每一个term之间显示的指定操作后会被认为是独立的clause。

&emsp;&emsp;上面的例子创建了一个boolean query：

```text
((content:this | title:this) (content:that | title:that) (content:thus | title:thus))~2
```

&emsp;&emsp;匹配到的文档至少要包含上面3个`should` clause中的两个。每一个term之间使用了disjunction max。

###### How minimum_should_match works for cross-field searches

&emsp;&emsp;`type`参数使用了`cross_files`值后，意味着所有字段使用使用了相同的分词器。

```text
GET /_search
{
  "query": {
    "query_string": {
      "fields": [
        "title",
        "content"
      ],
      "query": "this OR that OR thus",
      "type": "cross_fields",
      "minimum_should_match": 2
    }
  }
}
```

&emsp;&emsp;上面的例子创建了一个boolean query：

```text
(blended(terms:[field2:this, field1:this]) blended(terms:[field2:that, field1:that]) blended(terms:[field2:thus, field1:thus]))~2
```

&emsp;&emsp;匹配到的文档中至少满足有三个per-tem blended query中的两个。

###### Allow expensive queries（Query string query）

&emsp;&emsp;`query_string` query会被自动的转化为一个[prefix query](#Prefix query)，意味着如果禁用了prefix query（见[这里](#Allow expensive queries（Prefix query）)的说明）。那么这个query就不会被执行并且会抛出一个异常。

#### Simple query string query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-simple-query-string-query.html)

&emsp;&emsp;根据提供的query string，使用有限的（比如不能使用`AND`，而应该使用`+`）但可以容错语法的解析器返回匹配的文档。

&emsp;&emsp;这个query使用[simple syntax](#Simple query string syntax)进行解析并且根据指定的操作符将query string切分为term。随后这个query在返回匹配文档之前分别[analyzer](#Text analysis)每一个term。

&emsp;&emsp;尽管这个query中的语法比[query_string query](#Query string query)更具有限制性，但是`simple_query_string` query不会在遇到非法语法时返回错误。而是忽略query string中非法的部分。

##### Example request

```text
GET /_search
{
  "query": {
    "simple_query_string" : {
        "query": "\"fried eggs\" +(eggplant | potato) -frittata",
        "fields": ["title^5", "body"],
        "default_operator": "and"
    }
  }
}
```
##### Top-level parameters for simple_query_string
- query：（Required, string）你想要解析并且用于查询的查询字符串（query string）。见[Simple query string syntax](#Simple query string syntax)。
- fields：（Optional, array of string）你想要查询的字段名数组。
  - 字段名可以使用通配符表达式。你也可以对特定的字段使用`^`符号来提高（boost）匹配时的相关性。见[Wildcards and per-field boosts in the fields parameter](#Wildcards and per-field boosts in the fields parameter)。
  - 默认使用索引设置中的`index.query.default_field`，该参数默认值为`*`，`*`值会提取出所有满足term query的字段，会过滤掉元数据字段（[metadata field](#Metadata fields)）。提取出的字段组成一个query，如果没有指定`prefix`的话。

  > WARNING：字段的数量在查询时候有一定的限制。该值定义在[search setting](#Search settings)中的`indices.query.bool.max_clause_count`中，默认值为`1024`。

- default_operator：（Optional, string）如果没有指定该配置，则默认使用布尔逻辑来解析（interpret）query string中的文本。可选值为：
  - OR（Default）
    - 例如，`query`的值是`capital of Hungary`会解析成（interpret）`capital OR of OR Hungary`
  - AND
    - 例如，`query`的值是`capital of Hungary`会解析成（interpret）`capital AND of AND Hungary`
- analyze_wildcard：（Optional, Boolean）如果为`true`，会尝试分析（analyze）query string中的通配字符。默认值是`false`。
- analyzer：（Optional, string） [Analyzer](https://amazingkoala.com.cn/Elasticsearch/elasticsearch-8.2.html#text-analysis)用于将query string中的文本转化成token。默认值为`default_field`字段在[index-time analyzer](https://amazingkoala.com.cn/Elasticsearch/elasticsearch-8.2.html#specify-an-analyzer)中的分词器。如果没有设置analyzer，则使用索引默认的分词器。
- auto_generate_synonyms_phrase_query：（Optional, Boolean）如果为`true`，会为每一个[multi-position token](#Multi-position tokens)创建一个[match_phrase](#Match phrase query) query。见[Multi-position tokens](#Multi-position tokens（Simple query string query）)。
-  flags：（Optional, Boolean）[simple query string syntax](#Simple query string syntax)中允许生效的操作符列表。默认是`ALL`（所有的操作符）。见[LImit operator](#Limit operators)中的合法值。
- fuzzy_max_expansions：（Optional, integer）fuzzy匹配时允许扩展出的term的数量最大值。默认值为`50`。
- fuzzy_prefix_length：（Optional, integer）fuzzy匹配时起始字符保留的数量。默认值为`0`。
- fuzzy_transpositions：（Optional, Boolean）如果为`true`，模糊匹配中的编辑距离包含两个字符交换（ab->ba）。默认值为`true`。
- lenient：（Optional, Boolean）如果`为true`，例如当在一个[numeric](#Numeric field types)字段中，query的内容为文本时会忽略format-based的错误，默认值为`false`。
- minimum_should_match：（Optional, string） 匹配到的文档必须至少满足clause（一个子query视为一个query clause）的数量。见[minimum_should_match parameter](#minimum_should_match parameter)查看更多信息。
- quote_field_suffix：（Optional, string）query string中引用的文本对应的后缀名。
  - 例如说有一个mapping：

```text
    "properties": {
      "body": {
        "type": "text",
        "analyzer": "english",
        "fields": {
          "exact": {
            "type": "text",
            "analyzer": "english_exact"
          }
        }
      }
    }
  }
```

&emsp;&emsp;这个例子中，`quote_field_suffix`的值就是`exact`。如果指定了 `quote_field_suffix`。那么相当于在`body.exact`字段上查询query string。
&emsp;&emsp;你可以根据后缀来使用一个不同的分词器使得可以进行精准匹配。见[Mixing exact search with stemming](#mixing-exact-search-with-stemming)。

##### Note

###### Simple query string syntax

&emsp;&emsp;`simple_query_string` query 支持以下的操作符：

- `+` 表示 AND 操作符
- `|` 表示 OR 操作符
- `-` 否定（文档中不能包含）一个Token
- `"` 封装一定数量的token表示要作为短语查询
- `*` 在一个term之后使用表示是一个prefix query
- `(` 和 `)` 用双括号修饰表示优先执行括号里的内容
- `~N` 在一个word之后表示编辑距离（fuzziness）
- `~N` 在一个短语后表示slop大小（短语中token之间的距离差距）

&emsp;&emsp;若要使用这些字符的字面值，需要用反斜杠进行转义。

&emsp;&emsp;基于`default_operator`的值，操作符的行为可能有所不同，例如：

```text
GET /_search
{
  "query": {
    "simple_query_string": {
      "fields": [ "content" ],
      "query": "foo bar -baz"
    }
  }
}
```

&emsp;&emsp;这个查询想要返回的文档中，包含`foo`或者 包含`bar`并且不能包含`baz`。然而由于`default_operator`的值为`OR`，这个查询实际上返回的文中包含`foo`或者`bar`，但是所有的文档不能包含`baz`。若要按照你想要的结果返回，需要将query string中的内容改为`foo bar +-baz`

###### Limit operators

&emsp;&emsp;你可以使用`flags`参数限制simple query string语法中支持指定操作符。

&emsp;&emsp;若要显示的只开启指定操作符，使用`|`操作符。例如，`flags`的值为`OR|AND|PREFIX`时将禁用所有操作符，除了`OR`, `AND`和 `PREFIX`。

```text
GET /_search
{
  "query": {
    "simple_query_string": {
      "query": "foo | bar + baz*",
      "flags": "OR|AND|PREFIX"
    }
  }
}
```

&emsp;&emsp;合法的`flags`值包括：

- ALL (Default)：启动所有的可用的操作
- AND：启用`+` AND操作符
- ESCAPE：`\`作为一个反斜杠字符
- FUZZY：启用`~N`操作符，它位于一个word后时表示匹配word时允许的最大编辑距离是`N`。见[Fuzziness](#Fuzziness（Common options）)
- NEAR：启用`~N`操作符，它位于一个短语后时表示匹配到的token之间的距离最远是`N`。同`SLOP`
- NONE：禁用所有操作符
- NOT：启用`-` NOT操作符
- OR：启用`|` OR操作符
- PHRASE：启用`"`用来短语查询
- PRECEDENCE：启用`( )`控制操作优先级
- PREFIX：启用`*`操作符
- SLOP：启用`~N`操作符，它位于一个短语后时表示匹配到的token之间的距离最远是`N`。同`NEAR`
- WHITESPACE：启用空格用来对字符进行分割

###### Wildcards and per-field boosts in the fields parameter

&emsp;&emsp;字段名可以指定为通配符：

```text
GET /_search
{
  "query": {
    "simple_query_string" : {
      "query":    "Will Smith",
      "fields": [ "title", "*_name" ] 
    }
  }
}
```

&emsp;&emsp;第6行，请求`title`、`first_name`以及`last_name`字段

&emsp;&emsp;每个字段可以分别使用`^`符号进行boost，提高该字段的相关性：

```text
GET /_search
{
  "query": {
    "simple_query_string" : {
      "query" : "this is a test",
      "fields" : [ "subject^3", "message" ] 
    }
  }
}
```

&emsp;&emsp;第6行，`subject`字段的重要性是`message`的3倍。

###### Multi-position tokens（Simple query string query）

&emsp;&emsp;默认情况下`simple_query_string`在解析后会为query string中的每一个[multi-position token](#Multi-position tokens)创建一个[match_phrase](#Match phrase query)。例如，会为这个multi-word synonym：`ny, new york`创建一个`match_phrase`：

```text
(ny OR ("new york"))
```

&emsp;&emsp;同样的也可以匹配多个同义词，这些同义词之间的关系为AND，而不是这几个同义词作为短语查询。

```text
GET /_search
{
  "query": {
    "simple_query_string": {
      "query": "ny city",
      "auto_generate_synonyms_phrase_query": false
    }
  }
}
```

&emsp;&emsp;上面的例子创建出下面的[boolean query](#Boolean query)：

```text
(ny OR (new AND york)) city
```

&emsp;&emsp;满足条件的文档中要么匹配到`ny`，或者匹配到`new AND york`，或者都匹配到。

### Joining queries
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/joining-queries.html)

#### Nested query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-nested-query.html#query-dsl-nested-query)

&emsp;&emsp;封装了其他query，用于查询[nested](#Nested field type)字段的一种query。

&emsp;&emsp;`nested` query查询nested对象时，就像这些对象分别索引在不同的文档中一样（见下面中`must_not clauses and nested queries
`的介绍）。如果某个对象满足匹配查询，`nested` query返回所在的root parent document。

##### Example request

##### Index setup

&emsp;&emsp;若要使用`nested` query，你必须有一个mapping类型为[nested](#Nested field type)的字段。例如：

```text
PUT /my-index-000001
{
  "mappings": {
    "properties": {
      "obj1": {
        "type": "nested"
      }
    }
  }
}
```

##### Example query

```text
GET /my-index-000001/_search
{
  "query": {
    "nested": {
      "path": "obj1",
      "query": {
        "bool": {
          "must": [
            { "match": { "obj1.name": "blue" } },
            { "range": { "obj1.count": { "gt": 5 } } }
          ]
        }
      },
      "score_mode": "avg"
    }
  }
}
```

#### Top-level parameters for nested

- path：（Required, string）你想要查询的nested对象的路径
- query：（Required, query object）你想要对`path`中指定的字段使用的query。如果某个对象满足匹配查询，`nested` query返回所在的root parent document。
  - 你可以使用符号`.`给出完整的路径进行查询，比如`obj1.name`
  - 一个嵌套里面还有嵌套，Elasticsearch可以自动识别并处理这种多级嵌套。这就意味着，如果你的查询是在一个嵌套查询里面的嵌套对象上，Elasticsearch会智能地把查询应用在正确的嵌套层级上，而不是只在最顶层或者根部进行搜索。
  - 见[Multi-level nested queries](#Multi-level nested queries)
- score_mode：（Optional, string）child object匹配到后如何影响root parent document的[relevance score](#Relevance scores)
  - avg（default）：所有满足匹配的child object的平均相关打分
  - max：所有满足匹配的child object中最高的相关打分
  - min：所有满足匹配的child object中最低的相关打分
  - none：不是用满足匹配的child object的相关打分。提供给parent document的分数为`0`
  - sum：所有满足匹配的child object的相关打分累加
- ignore_unmapped：（Optional, Boolean）`path`路径错误时，不返回任何文档而不是返回错误。默认是`false`
  - 如果为`false`，如果`path`路径错误，Elasticsearch则返回一个错误
  - 你可以使用这个参数查询多个索引，并且有些索引可能不包含`path`中指定的路径

#### Notes

##### Context of script queries

&emsp;&emsp;如果在nested query中运行[script query](#Script query)，你只能访问nested文档中的doc value，而不能parent或者root document中获取。

##### Multi-level nested queries

&emsp;&emsp;若要了解multi-level的nested query是如何工作的，首先你有一个含有nested类型的字段的索引。下面的请求在索引`drivers`中定义了含有`make`和`model` nested字段的mapping。

```text
PUT /drivers
{
  "mappings": {
    "properties": {
      "driver": {
        "type": "nested",
        "properties": {
          "last_name": {
            "type": "text"
          },
          "vehicle": {
            "type": "nested",
            "properties": {
              "make": {
                "type": "text"
              },
              "model": {
                "type": "text"
              }
            }
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;下一步，像`drivers`中索引一些文档。

```text
PUT /drivers/_doc/1
{
  "driver" : {
        "last_name" : "McQueen",
        "vehicle" : [
            {
                "make" : "Powell Motors",
                "model" : "Canyonero"
            },
            {
                "make" : "Miller-Meteor",
                "model" : "Ecto-1"
            }
        ]
    }
}

PUT /drivers/_doc/2?refresh
{
  "driver" : {
        "last_name" : "Hudson",
        "vehicle" : [
            {
                "make" : "Mifune",
                "model" : "Mach Five"
            },
            {
                "make" : "Miller-Meteor",
                "model" : "Ecto-1"
            }
        ]
    }
}
```

&emsp;&emsp;你可以基于`make`和`model`使用multi-level的nested query。

```text
GET /drivers/_search
{
  "query": {
    "nested": {
      "path": "driver",
      "query": {
        "nested": {
          "path": "driver.vehicle",
          "query": {
            "bool": {
              "must": [
                { "match": { "driver.vehicle.make": "Powell Motors" } },
                { "match": { "driver.vehicle.model": "Canyonero" } }
              ]
            }
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;查询请求返回下面的响应：

```text
{
  "took" : 5,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 3.7349272,
    "hits" : [
      {
        "_index" : "drivers",
        "_id" : "1",
        "_score" : 3.7349272,
        "_source" : {
          "driver" : {
            "last_name" : "McQueen",
            "vehicle" : [
              {
                "make" : "Powell Motors",
                "model" : "Canyonero"
              },
              {
                "make" : "Miller-Meteor",
                "model" : "Ecto-1"
              }
            ]
          }
        }
      }
    ]
  }
}
```

##### must_not clauses and nested queries

&emsp;&emsp;如果`nested` query匹配了文档中的一个或多个nested 对象（因为把对象数组中的每一个对象看成了独立的对象，就像这些对象各自属于独立的文档），就返回命中的文档。即使对象数组中的其他对象不满足匹配也会返回。当在`nested` query中使用[must_not clause](#Boolean query)时要额外注意。

> TIP：使用[inner_hits](#Retrieve inner hits)参数了解`nested` query匹配了哪些nested对象

&emsp;&emsp;例如，下面的查询使用了一个外层的`nested` query并且使用了`must_not`。

```text
PUT my-index
{
  "mappings": {
    "properties": {
      "comments": {
        "type": "nested"
      }
    }
  }
}

PUT my-index/_doc/1?refresh
{
  "comments": [
    {
      "author": "kimchy"
    }
  ]
}

PUT my-index/_doc/2?refresh
{
  "comments": [
    {
      "author": "kimchy"
    },
    {
      "author": "nik9000"
    }
  ]
}

PUT my-index/_doc/3?refresh
{
  "comments": [
    {
      "author": "nik9000"
    }
  ]
}

POST my-index/_search
{
  "query": {
    "nested": {
      "path": "comments",
      "query": {
        "bool": {
          "must_not": [
            {
              "term": {
                "comments.author": "nik9000"
              }
            }
          ]
        }
      }
    }
  }
}
```

&emsp;&emsp;响应：

```text
{
  ...
  "hits" : {
    ...
    "hits" : [
      {
        "_index" : "my-index",
        "_id" : "1",
        "_score" : 0.0,
        "_source" : {
          "comments" : [
            {
              "author" : "kimchy"
            }
          ]
        }
      },
      {
        "_index" : "my-index",
        "_id" : "2",
        "_score" : 0.0,
        "_source" : {
          "comments" : [
            {
              "author" : "kimchy"              
            },
            {
              "author" : "nik9000"             
            }
          ]
        }
      }
    ]
  }
}
```

&emsp;&emsp;第25行，这个nested对象满足匹配，因此，会将这个对象的parent document返回
&emsp;&emsp;第28行，这个nested对象不满足匹配。由于其他的nested object满足匹配，因此这个查询仍然返回parent document

&emsp;&emsp;若任意一个nested对象不满足匹配就排除这篇文档，可以使用另一个外层的`must_not`。

```text
POST my-index/_search
{
  "query": {
    "bool": {
      "must_not": [
        {
          "nested": {
            "path": "comments",
            "query": {
              "term": {
                "comments.author": "nik9000"
              }
            }
          }
        }
      ]
    }
  }
}
```

&emsp;&emsp;响应：

```text
{
  ...
  "hits" : {
    ...
    "hits" : [
      {
        "_index" : "my-index",
        "_id" : "1",
        "_score" : 0.0,
        "_source" : {
          "comments" : [
            {
              "author" : "kimchy"
            }
          ]
        }
      }
    ]
  }
}
```

### Match all query
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-match-all-query.html)

&emsp;&emsp;最简单的一个Query，它将匹配所有的文档，并且所有文档的打分值`_score`都为1.0。

```text
GET /_search
{
    "query": {
        "match_all": {}
    }
}
```

&emsp;&emsp;可以通过`boost`参数修改`_score`的值。

```text
GET /_search
{
  "query": {
    "match_all": { "boost" : 1.2 }
  }
}
```

#### Match None Query

&emsp;&emsp;这是与`match_all`相反的Query，它不匹配任何文档。

```text
GET /_search
{
  "query": {
    "match_none": {}
  }
}
```

### Specialized queries
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/specialized-queries.html)

#### More like this query
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-mlt-query.html)

#### Rank feature query
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-rank-feature-query.html)

#### Script query
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-script-query.html)

#### Script score query
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-script-score-query.html)


### Term-level queries
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/term-level-queries.html)

&emsp;&emsp;你可以使用**term-level queries**基于结构化数据中精确（precise）的值来查找文档。结构化数据包括日期范围、IP地址、价格或者产品ID。

&emsp;&emsp;跟[full-text queries](#Full text queries)不同的是，term-level queries不会对查询词（search term）分词。而是跟字段值精确匹配。

> NOTE：对于mapping类型为`keyword`并且含有`normalizer`属性的字段，Term-level queries查询该字段时仍然会标准化查询词。更多信息见[normalizer](#normalizer)

##### Types of term-level queries

- [`exists` query](#Exists query)：返回的文档中包含待查询的字段，不关心字段值是什么
- [`fuzzy` query](#Fuzzy query)：返回的文档中，待查询的字段包含跟查询词相似的term。Elasticsearch 使用 [Levenshtein edit distance](https://en.wikipedia.org/wiki/Levenshtein_distance)来衡量相似性或模糊度
- [`ids` query](#IDs)：基于[document IDs](#\_id field)返回文档
- [`prefix` query](#Prefix query)：返回的文档中，待查询的字段的字段值包含指定的前缀
- [`range` query](#Range query)：返回的文档中，待查询的字段的字段值属于指定的范围
- [`regexp` query](#Regexp query)：返回的文档中，待查询的字段的字段值匹配了一个[正则表达式](https://en.wikipedia.org/wiki/Regular_expression)
- [`term` query](#Term query)：返回的文档中，待查询的字段的字段值精确匹配查询词
- [`terms` query](#Terms query)：返回的文档中，待查询的字段的字段值精确匹配一个或多个查询词
- [`terms_set` query](#Terms set query)：返回的文档中，待查询的字段的字段值精确匹配一个或多个查询词，你可以使用一个字段或者脚本定义至少要匹配的查询词数量。
- [`wildcard` query](#Wildcard query)：返回的文档中，待查询的字段的字段值匹配了一个通配符模式（wildcard pattern）

#### Exists query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-exists-query.html)

&emsp;&emsp;返回的文档中包含待查询的字段，不关心字段值是什么。

&emsp;&emsp;索引一个文档后，文档中的字段没有被索引的原因有以下几种：

- 在原始的JSON中，这个字段是`null`或者`[]`
- 这个字段的mapping 参数为`"index" : false`
- 这个字段的字段值超过了mapping中`ignore_above`的长度
- 这个字段的字段值格式不对并且同时在mapping中定义了`ignore_malformed`

##### Example request

```text
GET /_search
{
  "query": {
    "exists": {
      "field": "user"
    }
  }
}
```

##### Top-level parameters for exists

- field：（Required, string）待查询的字段名
  - 如果在JSON中的值是`null`或者`[]`，这个字段就被认为是不存在的。下面这些值会认为这个字段不存在：
    - 空的字符串，比如`""`或者`-`
    - 包含`null`跟其他值的数组，比如`[null, "foo"]`
    - 定义在mapping中，自定义的[null-value](#null_value)

##### Notes

###### Find documents missing indexed values

&emsp;&emsp;若要查找的文档中某个字段是缺失的，可以在`must_not` [boolean query](#Boolean query)中使用`exists` query：

&emsp;&emsp;下面的查询返回的文档中缺失了`user.id`字段。

```text
GET /_search
{
  "query": {
    "bool": {
      "must_not": {
        "exists": {
          "field": "user.id"
        }
      }
    }
  }
}
```

#### Fuzzy query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-fuzzy-query.html)

&emsp;&emsp;返回的文档中，待查询的字段包含跟查询词相似的term。Elasticsearch 使用 [Levenshtein edit distance](https://en.wikipedia.org/wiki/Levenshtein_distance)来衡量相似性或模糊度。

&emsp;&emsp;编辑距离是将一个term转为其他term时字符变化的数量。这些变化可以是：

- 修改一个字符（**b**ox → **f**ox）
- 移除一个字符（**b**lack → lack）
- 插入一个字符（sic → sic**k**）
- 交换两个相邻的字符（**ac**t → **ca**t）

&emsp;&emsp;若要找到相似的term，`fuzzy` query根据指定的编辑距离创建了查询词一系列可能的变体、扩展。这个query最后返回每一个扩展后的term的精确匹配结果。

##### Example requests

###### Simple example

```text
GET /_search
{
  "query": {
    "fuzzy": {
      "user.id": {
        "value": "ki"
      }
    }
  }
}
```

###### Example using advanced parameters

```text
GET /_search
{
  "query": {
    "fuzzy": {
      "user.id": {
        "value": "ki",
        "fuzziness": "AUTO",
        "max_expansions": 50,
        "prefix_length": 0,
        "transpositions": true,
        "rewrite": "constant_score"
      }
    }
  }
}
```

##### Top-level parameters for fuzzy

- `<field>`：（Required, object）待查询的字段

##### Parameters for \<field>

- value：（Required, string）：待查询的term
- fuzziness：（Optional, string）允许用于匹配的最大编辑距离。见[Fuzziness](#Fuzziness（Common options）)了解更多信息。
- max_expansions：（Optional, integer）query扩展（expand）出的term数量最大值。默认值为`50`。

  > WARNING：避免将`max_expansions`的值设置的很打，特别是`prefix_length`的值为`0`时，这个参数越大会导致性能变差，因为会处理更多数量的变体

- prefix_length：（Optional, integer）模糊匹配时，不变的（unchange）起始字符的数量。默认值为`0`。
- transpositions：（Optional, Boolean）模糊匹配中的编辑距离是否包含两个字符交换（ab->ba）。默认值为`true`。
- rewrite：（Optional, string）用于重写query的方法。见[rewrite parameter](#rewrite parameter)了解更多信息。

##### Notes

&emsp;&emsp;如果[search.allow_expensive_queries](##Allow expensive queries（Query DSL）)设置为`false`则不允许执行这个query。

#### IDs
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-ids-query.html#query-dsl-ids-query)

&emsp;&emsp;基于IDs返回文档，这个查询使用了文档的IDs，IDs被存储在[\_id](#_id-field)字段中。

##### 请求示例

```text
GET /_search
{
  "query": {
    "ids" : {
      "values" : ["1", "4", "100"]
    }
  }
}
```

##### Top-level parameters for ids

- values：（Required,array of strings）[document IDs](#_id-field)数组。

#### Prefix query
（8,2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-prefix-query.html)

&emsp;&emsp;返回的文档中，待查询的字段的字段值包含指定的前缀。

##### Example request

&emsp;&emsp;下面的查询中，将返回`user.id`字段的字段值以`ki`为前缀的文档。

```text
GET /_search
{
  "query": {
    "prefix": {
      "user.id": {
        "value": "ki"
      }
    }
  }
}
```

##### Top-level parameters for prefix

- `<field>`：（Required, object）待查询的字段

##### Parameters for \<field>

- value：（Required, string）待查询的前缀值
- rewrite：（Optional, string）用于重写query的方法。见[rewrite parameter](#rewrite parameter)了解更多信息。
- case_insensitive：（Optional, Boolean）设置为`true`后，对大小写不敏感。默认为`false`，那么是否大小写敏感取决于字段的mapping
  - 7.10.0新增的参数

##### Notes

###### Short request example

&emsp;&emsp;你可以通过组合`<field>`和`value`简化`prefix`的语法。

```text
GET /_search
{
  "query": {
    "prefix" : { "user" : "ki" }
  }
}
```

###### Speed up prefix queries

&emsp;&emsp;你可以通过使用mapping 参数[ndex_prefixes](#index_prefixes)提高prefix query的查询。开启这个参数后，Elasticsearch会将前缀索引到一个字段中。这样能让Elasticsearch在大型索引中能更快的执行prefix query。

###### Allow expensive queries（Prefix query）

&emsp;&emsp;如果将[search.allow_expensive_queries](#Allow expensive queries（Query DSL）)设置为`false`则不允许执行prefix query。然而如果开启了[index_prefixes](#index_prefixes)，则认为构建了优化的Query，不认为这种查询会很慢，因此允许执行。

#### Range query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-range-query.html)

&emsp;&emsp;返回的文档中，待查询的字段的字段值属于指定的范围。

##### Example request

&emsp;&emsp;下面的查询中，返回`age`字段的字段值在`10`和`28`之间的文档。

```text
GET /_search
{
  "query": {
    "range": {
      "age": {
        "gte": 10,
        "lte": 20,
        "boost": 2.0
      }
    }
  }
}
```

##### Top-level parameters for range

- `<field>`：（Required, object）待查询的字段

##### Parameters for \<field>

- gt：（Optional）大于
- gte：（Optional）大于等于
- lt：（Optional）小于
- lte：（Optional）小于等于
- format：（Optional, string）日期格式用来解析query中指定的日期，使得Elasticsearch可以理解你提供的日期
  - 默认情况下Elasticsearch使用`<field>`在mapping中指定的[date  format]()。这个参数可以覆盖mapping中的设置
  - 更多语法见[format](#format(mapping parameter))

  > WARNING：> WARNING：如果format或者日期值不完整，date range aggregation使用默认值替换缺失的部分。见[Missing date components](#Missing date components)

- relation：（Optional, string）：为`range`字段指定如何进行匹配。可选值有：
  - INTERSECTS (Default)：范围与文档字段值相交的文档将被匹配
  - CONTAINS：只有当文档字段值完全包含在查询指定的范围内时，文档才会被匹配
  - WITHIN：只有当查询指定的范围完全包含在文档字段值内时，文档才会被匹配
- time_zone：（Optional, string）使用[Coordinated Universal Time (UTC) offset ](https://en.wikipedia.org/wiki/List_of_UTC_offsets)或者[IANA time zone](https://en.wikipedia.org/wiki/List_of_tz_database_time_zones)将query string中的`date`值转化为UTC。
  - 你也可以用 ISO 8601 UTC 偏移来指定时区（`+01:00 or -08:00`）或者 IANA time zone ID，比如`America/Los_Angeles`
  - 见[Time zone in range queries](#Example query using time_zone parameter)查看使用了这个参数的例子

  > NOTE：`time_zone`参数不会影响`now`这种[date math](#Date Math)。`now`始终是UTC的当前系统时间。但是，`time_zone`参数会转换使用now和[date math rounding](#Date Math)计算出的日期。例如，使用`time_zone`参数会转换`now/d`的值。

- boost：（Optional, float）浮点值，用于提高或者降低query的[relevance scores](#Relevance-scores)。默认值为`1.0`。
  - 你可以使用这个参数调整包含了两个或更多子query的查询的relevance scores
  - boost的值默认关联的值为`1.0`。`0`到`1.0`之间的值会降低relevance score，大于`1.0`的值会提高relevance score。

##### Notes

###### Using the range query with text and keyword fields

&emsp;&emsp;如果[search.allow_expensive_queries](#Allow expensive queries（Query DSL）)设置为false，则不会运行在[text](#Text type family)或[keyword](#Keyword type family)类型的字段上的range query。

###### Using the range query with date fields

&emsp;&emsp;如果`<field>`是[date](#Date field type)类型，下面的参数可以使用[date math](#Date Math)：

- gt
- gte
- lt
- lte

&emsp;&emsp;例如，下面的查询中根据`timestamp`字段返回今天跟昨天的文档

```text
GET /_search
{
  "query": {
    "range": {
      "timestamp": {
        "gte": "now-1d/d",
        "lt": "now/d"
      }
    }
  }
}
```

###### Missing date components

&emsp;&emsp;对于range query以及[date range](#Date range aggregation) aggregation，Elasticsearch使用下面的值替换缺失的date components。不能替换缺失的year component：

```text
MONTH_OF_YEAR:    01
DAY_OF_MONTH:     01
HOUR_OF_DAY:      23
MINUTE_OF_HOUR:   59
SECOND_OF_MINUTE: 59
NANO_OF_SECOND:   999_999_999
```

&emsp;&emsp;比如说如果format是`yyyy-MM`，那么Elasticsearch会将`gt`的`2099-12`转化为`099-12-01T23:59:59.999_999_999Z`。这个日期使用了年（2099）跟月（12）。但是使用了默认的日（`01`）时（`hour`）分（`59`）秒（`59`）以及纳秒（`999_999_999`）。

###### Numeric date range value

&emsp;&emsp;如果没有指定date format并且range query作用在一个date类型的字段上，那么在查询中提供了数值类型的值后，它会被用自纪元以来的毫秒数（milliseconds-since-the-epoch）表示。如果你提供的数值希望是用来表示年份，比如说`2020`，那么你需要将其作为字符串，Elasticsearch会根据默认的format或者mapping中定义的format进行转化。

###### Date math and rounding

&emsp;&emsp;Elasticsearch根据以下参数来舌入（round）[date math](#Date Math)值：

- gt：向上舍入至四舍五入值（比如如果是11月份，则四舍五入到12月份）之后的第一个毫秒
  - 例如，`2014-11-18||/M` 舍入至 `2014-12-01T00:00:00.000`，不包括整个11月份
- gte：向下舍入至第一个毫秒
  - 例如，`2014-11-18||/M` 舍入至 `2014-11-01T00:00:00.000`，包括整个月份。
- lt：向下舍入至四舍五入值（比如如果是11月份，则四舍五入到10月份）之前的最后一个毫秒。
  - 例如，`2014-11-18||/M` 舍入至 `2014-10-31T23:59:59.999`，不包括整个11月份。
- lte：在舍入间隔中向上舍入至最后一个毫秒。
  - 例如，`2014-11-18||/M` 舍入至 `2014-11-30T23:59:59.999`，包括整个月份。

###### Example query using time_zone parameter

&emsp;&emsp;你可以使用`time_zone`参数使用UTC偏移值将其转化为UTC。例如：

```text
GET /_search
{
  "query": {
    "range": {
      "timestamp": {
        "time_zone": "+01:00",        
        "gte": "2020-01-01T00:00:00", 
        "lte": "now"                  
      }
    }
  }
}
```

&emsp;&emsp;第8行，指明使用`+01:00`这个UTC偏移
&emsp;&emsp;第9行，根据UTC偏移，Elasticsearch会将这个日期转化为`2019-12-31T23:00:00 UTC`
&emsp;&emsp;第10行，`time_zone`参数不会影响`now`值

#### Regexp query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-regexp-query.html)

&emsp;&emsp;返回的文档中，待查询的字段的字段值匹配了一个[正则表达式](https://en.wikipedia.org/wiki/Regular_expression)。

&emsp;&emsp;正则表达式是一种使用称为操作符的占位符字符来匹配数据中模式的方法。有关 regexp 查询支持的操作符列表，请参见[正则表达式语法](#Regular expression syntax)。

##### Example request

&emsp;&emsp;下面的请求中，返回满足`user.id`的字段值以`k`开头并且以`y`结尾的文档。`.*`操作符匹配任意长度的任意字符，包括没有字符。可以匹配`ky`、`kay`以及`kimchy`。

```text
GET /_search
{
  "query": {
    "regexp": {
      "user.id": {
        "value": "k.*y",
        "flags": "ALL",
        "case_insensitive": true,
        "max_determinized_states": 10000,
        "rewrite": "constant_score"
      }
    }
  }
}
```

##### Top-level parameters for regexp

- `<field>`：（Required, object）待查询的字段

##### Parameters for \<field>

- value：（Required, string）用于匹配`<field>`的字段值的正则表达式。支持的操作符列表见[Regular expression syntax](#Regular expression syntax)
  - 默认情况下，正则表达式被限制最多1000个字符。你可以使用[index.max_regex_length](#index.max_regex_length)更改上限值。

  > WARNING：`regexp` query的性能非常取决于提供的正则表达式。若要提高性能，避免使用通配符模式，比如`.*`或者`.*?+`，没有后缀或前缀。

- flags：（Optional, string）允许使用的正则表达式操作符。对于可用的操作符以及更多信息见[Regular expression syntax](#Regular expression syntax)
- case_insensitive：（Optional, Boolean）设置为`true`后，匹配时大小写不敏感。默认为`false`，那么是否大小写敏感取决于字段的mapping
- max_determinized_states：（Optional, integer）Query要求[automaton states](https://en.wikipedia.org/wiki/Deterministic_finite_automaton)的最大值。默认值为`1000`
  - Elasticsearch在内部使用[Lucene](https://lucene.apache.org/core/)的正则表达式。Lucene将每一个正则表达式转化为一个[有限自动状态](https://amazingkoala.com.cn/Lucene/gongjulei/2019/0417/Automaton/)
- rewrite：（Optional, string）用于重写query的方法。见[rewrite parameter](#rewrite parameter)了解更多信息。

##### NOTES

##### Allow expensive queries（Regexp query）

&emsp;&emsp;如果[search.allow_expensive_queries](#Allow expensive queries（Query DSL）)设置为false，则不允许执行regexp Query。

#### Term query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-term-query.html)

&emsp;&emsp;返回的文档中，待查询的字段的字段值精确匹配查询词

&emsp;&emsp;你可以使用`term` query基于精确的值来超找文档。比如价格、产品ID或者用户名。

> WARNING：避免对[text](#Text type family)类型的字段使用`term` query。
> 默认情况下，默认情况下，`text`类型的字段作为[analysis](#Text analysis)的一部分，Elasticsearch 会更改文本字段的值。这可能会使得为`text`类型精确匹配变得困难。
> 若要查询`text`类型的字段，应该使用[match](#Match query)。

##### Example request

```text
GET /_search
{
  "query": {
    "term": {
      "user.id": {
        "value": "kimchy",
        "boost": 1.0
      }
    }
  }
}
```
##### Top-level parameters for term

- `<field>`：（Required, object）待查询的字段

##### Parameters for \<field>

- value：（Required, string）待查询的`<field>`中的字段值。若要返回满足匹配的文档，必须精确匹配字段值，包括空格和大小写
- boost：（Optional, float）浮点值，用于提高或者降低query的[relevance scores](#Relevance-scores)。默认值为`1.0`。
  - 你可以使用这个参数调整包含了两个或更多子query的查询的relevance scores
  - boost的值默认关联的值为`1.0`。`0`到`1.0`之间的值会降低relevance score，大于`1.0`的值会提高relevance score。
- case_insensitive：（Optional, Boolean）设置为`true`后，对大小写不敏感。默认为`false`，那么是否大小写敏感取决于字段的mapping
  - 7.10.0新增的参数

##### NOTES

###### Avoid using the term query for text fields

&emsp;&emsp;默认情况下，Elasticsearch在分词阶段会修改`text`类型的字段的字段值。比如，默认的[standard analyzer](#Standard analyzer)对`text`类型的字段的字段值作以下的修改：

- 移除大部分标点符号
- 将剩下的内容切分为独立的词，称为[tokens](#Tokenizer reference)
- 将tokens小写化

&emsp;&emsp;若要更好的查询`text`字段，`match`总是在执行查询前先对你提供的查询词（search term）进行分词。意味着`match` query是对分词后的token而不是整个查询词对`text`类型的字段查询。

&emsp;&emsp;`term` query不会对查询词分词。`term` query而是根据你提供的查询词作精确匹配。意味着在`text`字段上执行`term` query可能返回很差甚至没有结果。

&emsp;&emsp;若要查看两者的区别，可是尝试下下面的例子：

1. 创建一个名为`full_text`的`text`类型的字段

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "full_text": { "type": "text" }
    }
  }
}
```

2. 索引一篇文档，`full_text`的字段值为`Quick Brown Foxes!`

```text
PUT my-index-000001/_doc/1
{
  "full_text":   "Quick Brown Foxes!"
}
```
&emsp;&emsp;注意的是，`full_text`是`text`类型，Elasticsearch会在分词阶段将`Quick Brown Foxes!`修改为[`quick`, `brown`, `fox`]。

3. 使用`term` qeury在`full_text`字段上查询`Quick Brown Foxes!`，包含`pretty`参数，使得响应中的内容更加易读。

```text
GET my-index-000001/_search?pretty
{
  "query": {
    "term": {
      "full_text": "Quick Brown Foxes!"
    }
  }
}
```

&emsp;&emsp;因为`full_text`中不包含精确的（exact）`Quick Brown Foxes!`，所以`term ` query不会返回结果

4. 使用`match` query在`full_text`上查询`Quick Brown Foxes!`。

```text
GET my-index-000001/_search?pretty
{
  "query": {
    "match": {
      "full_text": "Quick Brown Foxes!"
    }
  }
}
```

&emsp;&emsp;跟`term` query不同的是，`match` query在执行查询前会先对你提供的查询词`Quick Brown Foxes!`进行分词。`match` query会返回`full_field`的字段值中包含`quick`、`brown`或者`fox`的文档。

&emsp;&emsp;`match` query的响应中包含了上文中索引的文档。

```text
{
  "took" : 1,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 0.8630463,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_id" : "1",
        "_score" : 0.8630463,
        "_source" : {
          "full_text" : "Quick Brown Foxes!"
        }
      }
    ]
  }
}
```

#### Terms query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-terms-query.html)

&emsp;&emsp;返回的文档中，待查询的字段的字段值精确匹配一个或多个查询词。

&emsp;&emsp;`terms` query跟[term query](#Term query)是相同的，差别就是你可以查询多个值。

##### Example request

&emsp;&emsp;下面的请求中，返回的文档中的`user.id`的字段值是`kimchy`或者`elkbee`。

```text
GET /_search
{
  "query": {
    "terms": {
      "user.id": [ "kimchy", "elkbee" ],
      "boost": 1.0
    }
  }
}
```

##### Top-level parameters for terms

- `<field>`：（Required, object）待查询的字段。
  - 这个参数的值是一个term数组，这些term是你希望在`<field>`能匹配到的字段值。若要返回一篇文档，必须精确匹配提供的term，包括空格和大小写。
  - 默认情况下，Elasticsearch限制`terms`中的数量最大值为`65,536`。你可以使用[index.max_terms_count](#index.max_terms_count)修改这个限制
  
  > NOTE：若要使用文档中的字段值作为查询词，可以使用[terms lookup](#Terms lookup)参数。

- boost：（Optional, float）浮点值，用于提高或者降低query的[relevance scores](#Relevance-scores)。默认值为`1.0`。
  - 你可以使用这个参数调整包含了两个或更多子query的查询的relevance scores
  - boost的值默认关联的值为`1.0`。`0`到`1.0`之间的值会降低relevance score，大于`1.0`的值会提高relevance score。

##### Notes

###### Highlighting terms queries

&emsp;&emsp;[Highlighting](#Highlighting) is best-effor only。Elasticsearch可能不会为`term` query返回高亮结果的原因可能有：

- Highlighting的类型
- query中term的数量

###### Terms lookup

&emsp;&emsp;Terms lookup获取现有的文档中的字段值。Elasticsearch随后使用这些值作为查询词。当查询大量的term集合时可能比较有用。

&emsp;&emsp;若要运行一个terms lookup。必须开启字段的[\_source](#\_source field)。你不能再远端索引中使用cross-cluster search运行terms lookup。

> NOTE：默认情况下，Elasticsearch限制`terms` query的term数量为`65536`。包括通过terms lookup获取的term。你可以使用[index.max_terms_count](#index.max_terms_count)修改这个限制

&emsp;&emsp;为了能降低网络压力，terms lookup会尽可能在本地的分片中获取文档的值。如果你的terms data不是很大，可以考虑使用单个主分片并复制到所有适用的数据节点上来最小化网络流量。

&emsp;&emsp;若要执行一个terms lookup，可以实现下面的参数。

###### Terms lookup Parameters

- index：（Required, string）索引的名字用来获取字段值
- id：（Required, string）文档的[ID](#\_id field)用来获取字段值
- path：（Required, string）字段的名字用来获取字段值。Elasticsearch使用这些字段值作为query的查询词
  - 如果字段值包含嵌套的内部对象，你可以点符号访问这些对象
- routing：（Optional, string）自定义的文档[routing value](#\_routing field)来获取term值。如果在索引期间这个提供了这个自定义的路由值，那么这个参数是必须要有的

###### Terms lookup example

&emsp;&emsp;若要查看terms lookup是如何工作的，尝试下面的例子：

1. 创建一个索引，包含名为`color`的`keyword`类型的字段

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "color": { "type": "keyword" }
    }
  }
}
```

2. 索引一篇文档，ID为1，`color`字段的值为`["blue", "green"]`。

```text
PUT my-index-000001/_doc/1
{
  "color":   ["blue", "green"]
}
```

3. 再索引一篇文档，ID为2，`color`的字段值为`blue`。

```text
PUT my-index-000001/_doc/2
{
  "color":   "blue"
}
```

4. 使用`terms` query以及terms lookup参数找出跟第2篇文档中有一个或多个相同字段值的文档。包含`pretty`参数，使得响应中的内容更加易读。

```text
GET my-index-000001/_search?pretty
{
  "query": {
    "terms": {
        "color" : {
            "index" : "my-index-000001",
            "id" : "2",
            "path" : "color"
        }
    }
  }
}
```

&emsp;&emsp;由于文档2跟文档1在`color`中都包含`blue`，因此Elasticsearch同时返回这两篇文档。

```text
{
  "took" : 17,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 2,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index-000001",
        "_id" : "1",
        "_score" : 1.0,
        "_source" : {
          "color" : [
            "blue",
            "green"
          ]
        }
      },
      {
        "_index" : "my-index-000001",
        "_id" : "2",
        "_score" : 1.0,
        "_source" : {
          "color" : "blue"
        }
      }
    ]
  }
}
```

#### Terms set query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-terms-set-query.html)

&emsp;&emsp;返回的文档中，待查询的字段的字段值精确匹配一个或多个查询词，你可以使用一个字段或者脚本定义至少要匹配的查询词数量。

&emsp;&emsp;`terms_set`跟[terms query](#Terms query)是一样的，差别是你可以定义一篇文档至少要匹配term的数量：

- 比如有一个名为`programming_languages`的字段，包含了熟知的编程语言，有C++、Java或者php。你可以用这个query来匹配文档，并且要求必须至少匹配到两种编程语言。
- 比如你有一个`permission`的字段，包含某个应用的可能的用户权限。你可以使用这个query来匹配文档，并且要求必须匹配一个权限子集

##### Example request

###### Index setup

&emsp;&emsp;大多数情况下，你需要包含一个[numeric](#Numeric field types)类型的字段来使用`terms_set` query。这个字段的字段值为满足匹配的文档要求匹配的term数量。

&emsp;&emsp;若要查看如何为`terms_set` query设置一个索引，尝试下面的例子

1. 创建一个名为`job-candidates`的索引，该索引有以下的字段
- 名为`name`的[keyword](#Keyword type family)类型的字段。字段值为求职者的姓名
- 名为`programming_languages`的[keyword](#Keyword type family)类型的字段。字段值为求职者熟知的编程语言
- 名为`required_matches`的[numeric](#Numeric field types)类型的字段。字段值为满足匹配的文档要求匹配的term数量

```text
PUT /job-candidates
{
  "mappings": {
    "properties": {
      "name": {
        "type": "keyword"
      },
      "programming_languages": {
        "type": "keyword"
      },
      "required_matches": {
        "type": "long"
      }
    }
  }
}
```

2. 索引一篇文档，ID为1以及这些内容：

- `name`的字段值为`Jane Smith`
- `programming_languages`的字段值为`["c++", "java"]`
- `required_matches`的字段值为`2`

&emsp;&emsp;添加?refresh参数使得索引后马上对搜索可见。

```text
PUT /job-candidates/_doc/1?refresh
{
  "name": "Jane Smith",
  "programming_languages": [ "c++", "java" ],
  "required_matches": 2
}
```

3. 索引另一篇文档，ID为2以及这些内容：

- `name`的字段值为`Jason Response`
- `programming_languages`的字段值为`["java", "php"]`
- `required_matches`的字段值为`2`

```text
PUT /job-candidates/_doc/2?refresh
{
  "name": "Jason Response",
  "programming_languages": [ "java", "php" ],
  "required_matches": 2
}
```

&emsp;&emsp;现在你可以使用`required_matches`作为在`terms_set` query中，满足匹配的文档需要匹配的term数量。

###### Example query

&emsp;&emsp;下面的查询返回的文档中，`programming_languages`中至少包含下面两个term：

- c++
- java
- php

&emsp;&emsp;`minimum_should_match_field`为`required_matches`。意味着匹配term的数量为`2`，也就是`required_matches`的字段值。

```text
GET /job-candidates/_search
{
  "query": {
    "terms_set": {
      "programming_languages": {
        "terms": [ "c++", "java", "php" ],
        "minimum_should_match_field": "required_matches"
      }
    }
  }
}
```

##### Top-level parameters for terms_set

- `<field>`：（Required, object）待查询的字段。

##### Parameters for \<field\>

- terms：（Required, array of strings）这个参数的值是一个term数组，这些term是你希望在`<field>`能匹配到的字段值。若要返回一篇文档，必须精确匹配提供的term，包括空格和大小写。
- minimum_should_match_field：（Optional, string）[Numeric](#Numeric field types)类型的字段名，字段值为满足匹配的文档必须匹配的term数量
- minimum_should_match_script：（Optional, string）自定义的脚本，描述了满足匹配的文档必须匹配的term数量
  - 支持的参数和选项见[Scripting](#Scripting)
  - 见[How to use the minimum_should_match_script parameter](#How to use the minimum_should_match_script parameter)了解使用`minimum_should_match_script`参数的例子

##### Notes

###### How to use the minimum_should_match_script parameter

&emsp;&emsp;你可以使用`minimum_should_match_script`，使用脚本定义满足匹配的文档必须匹配的term数量。如果你需要动态的设置数量时，使用这个参数就非常有用。

**Example query using minimum_should_match_script**

&emsp;&emsp;下面的查询返回的文档中，`programming_languages`中至少包含下面两个term：

- c++
- java
- php

&emsp;&emsp;参数`source`描述的是：

- 匹配的term数量不能超过`params.num_terms`，也就是`terms`字段中term的数量
- 匹配的term数量为`2`，即`required_matches`的字段值

```text
GET /job-candidates/_search
{
  "query": {
    "terms_set": {
      "programming_languages": {
        "terms": [ "c++", "java", "php" ],
        "minimum_should_match_script": {
          "source": "Math.min(params.num_terms, doc['required_matches'].value)"
        },
        "boost": 1.0
      }
    }
  }
}
```

#### Wildcard query
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-wildcard-query.html)

&emsp;&emsp;返回的文档中，待查询的字段的字段值匹配了一个通配符模式（wildcard pattern）。

&emsp;&emsp;通配符的操作符是一个占位符来匹配一个或多个字符。例如，通配符操作符`*`匹配0个或多个字符。你可以组合通配符的操作符来创建一个通配符模式。

##### Example request

&emsp;&emsp;下面的请求中，返回满足`user.id`的字段值以`k`开头并且以`y`结尾的文档。`.*`操作符匹配任意长度的任意字符，包括没有字符。可以匹配`ky`、`kay`以及`kimchy`。

```text
GET /_search
{
  "query": {
    "wildcard": {
      "user.id": {
        "value": "ki*y",
        "boost": 1.0,
        "rewrite": "constant_score"
      }
    }
  }
}
```

##### Top-level parameters for wildcard

- `<field>`：（Required, object）待查询的字段。

##### Parameters for \<field\>

- boost：（Optional, float）浮点值，用于提高或者降低query的[relevance scores](#Relevance-scores)。默认值为`1.0`。
  - 你可以使用这个参数调整包含了两个或更多子query的查询的relevance scores
  - boost的值默认关联的值为`1.0`。`0`到`1.0`之间的值会降低relevance score，大于`1.0`的值会提高relevance score。
- case_insensitive：（Optional, Boolean）设置为`true`后，对大小写不敏感。默认为`false`，那么是否大小写敏感取决于字段的mapping
  - 7.10.0新增的参数
- rewrite：（Optional, string）用来用来重写query的方法。更多信息见[rewrite parameter](#rewrite parameter)
- value：（Required, string）你想要从`<field>`中匹配的，带有通配符的term
  - 这个操作支持两个通配符操作符：
    - `?`，匹配任意单个字符
    - `*`，匹配0个或多个字符
    
    > WARNING：避免使用通配符时以`*`或者`?`开头。这会增加查找匹配的term的迭代次数使得降低查询性能
  
- wildcard：（Required, string）`value`参数的别名。如果你同时指定`value`跟`wildcard`，query会使用请求体中最后一个。

##### NOTES

##### Allow expensive queries（Wildcard query）

&emsp;&emsp;如果[search.allow_expensive_queries](#Allow expensive queries（Query DSL）)设置为false，则不允许执行Wildcard query。

### minimum_should_match parameter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-minimum-should-match.html)

### rewrite parameter
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/query-dsl-multi-term-rewrite.html)

### Regular expression syntax
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/regexp-syntax.html)

&emsp;&emsp;[regular expression](https://en.wikipedia.org/wiki/Regular_expression)是一种使用占位符（称为操作符）匹配数据的方式。

&emsp;&emsp;Elasticsearch支持在下面的请求中使用regular expression：

- [regexp](#Regexp query)
- [query_string](#Query string query)

&emsp;&emsp;Elasticsearch使用[Apache Lucene](https://lucene.apache.org/core/)的正则表达式引擎来解析这些query。

#### Reserved characters

&emsp;&emsp;Lucene的正则表达式引擎支持搜友的Unicode字段，然而下面的字符会被保留用作操作符：

`. ? + * | { } [ ] ( ) " \`

&emsp;&emsp;根据开启的[optional operators](#Optional operators)，下面的字符可能也会被保留：

`# @ & < >  ~`

&emsp;&emsp;若要字面上使用这些字符，需要用反斜杠进行转移或者用双引号修饰

`\@                  # renders as a literal '@'`
`\\                  # renders as a literal '\'`
`"john@smith.com"    # renders as 'john@smith.com'`

#### Standard operators

&emsp;&emsp;Lucene的正则表达式引擎不使用[Perl Compatible Regular Expressions (PCRE)](https://en.wikipedia.org/wiki/Perl_Compatible_Regular_Expressions)库，但支持下面标准的操作符：

- `.`：匹配任意的字符，比如：
  - `ab.     # matches 'aba', 'abb', 'abz' ,etc.`
- `?`：重复前一个字符零次或一次。通常用于使前一个字符变为可选。例如：
  - `abc?     # matches 'ab' and 'abc'`
- `+`：重复前一个字符0次或者多次。例如：
  - `ab+     # matches 'ab', 'abb', 'abbb', etc.`
- `*`：重复前一个字符0次或者多次。例如：
  - `ab*     # matches 'a', 'ab', 'abb', 'abbb', etc.`
- `{}`：重复前一个字符最小或最大次数。例如：
  - `a{2}    # matches 'aa'`
  - `a{2,4}  # matches 'aa', 'aaa', and 'aaaa'`
  - `a{2,}   # matches 'a` repeated two or more times`
- `|`：OR操作符。如果左侧或右侧的最长模式匹配成功，则匹配将成功。例如：
  - `abc|xyz  # matches 'abc' and 'xyz'`
- `( ... )`：形成一个组。您可以使用组将表达式的一部分视为单个字符。例如
  - `abc(def)?  # matches 'abc' and 'abcdef' but not 'abcd'`
- `[ … ]`：匹配括号中的其中一个字符。例如：
  - `[abc]   # matches 'a', 'b', 'c'`
  - 在括号中，`-`表示一个范围，除了`-`作为第一个字符或被转义。例如：
  - `[a-c]   # matches 'a', 'b', or 'c'`
  - `[-abc]  # '-' is first character. Matches '-', 'a', 'b', or 'c'`
  - `[abc\-] # Escapes '-'. Matches 'a', 'b', 'c', or '-'`
  - 括号中某个字符前有一个`^`表示匹配除开字符或某个范围。
  - `[^abc]      # matches any character except 'a', 'b', or 'c'`
  - `[^a-c]      # matches any character except 'a', 'b', or 'c'`
  - `[^-abc]     # matches any character except '-', 'a', 'b', or 'c'`
  - `[^abc\-]    # matches any character except 'a', 'b', 'c', or '-'`

#### Optional operators

&emsp;&emsp;你可以使用`flags`参数开启更多可选的操作符用于Lucene的正则表达式引擎中。

&emsp;&emsp;若要开启多个操作符，可以使用`|`分隔符。例如，`flag`为`COMPLEMENT|INTERVAL`开启了`COMPLEMENT`以及`INTERVAL`两个操作符。

##### Valid values

- `ALL (Default)`：开启所有可选的操作符
- `"" (empty string)`：`ALL`的别名
- `COMPLEMENT`：开启`~`操作符。你可以使用`~`匹配不是后面一个字符的内容。比如：
  - `a~bc   # matches 'adc' and 'aec' but not 'abc'`
- `EMPTY`：开启`#`操作符。这个操作符不匹配任何的string，甚至是一个空的string
  - 如果你通过编程方式组合数值来创建正则表达式，你可以传递#来指定"没有字符串"。这样可以避免意外匹配空字符串或其他不需要的字符串。例如：
    - `#|abc  # matches 'abc' but nothing else, not even an empty string`
- `INTERVAL`：开启`<>`操作符。你可以使用`<>`匹配一个数值范围。例如：
  - `foo<1-100>      # matches 'foo1', 'foo2' ... 'foo99', 'foo100'`
  - `foo<01-100>     # matches 'foo01', 'foo02' ... 'foo99', 'foo100'`
- `INTERSECTION`：
  - 开启`&`操作符，相当于AND操作符。如果左右两边都匹配则满足。例如：
    - `aaa.+&.+bbb  # matches 'aaabbb'`
- `ANYSTRING`：开启`@`操作符。你可以使用`@`匹配任意整个string
  - 你可以组合`@`、`&`以及`~`创建一个`everything except`的逻辑。例如：
    - `@&~(abc.+)  # matches everything except terms beginning with 'abc'`
- `NONE`：不启用可选的操作符

#### Unsupported operators

&emsp;&emsp;Lucene的正则表达式引擎不支持锚点运算符（anchor operator）。比如`^`（行的开头）或`$`（行的结尾）。要匹配一个term，正则表达式必须匹配整个字符串。


## Aggregations
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations.html)

&emsp;&emsp;聚合（aggregation）能汇总你的数据用于指标、统计以及其他的分析。聚合能帮助你回答类似下面的问题：

- 我的网站的平均加载时间是多少？
- 基于交易量，谁是最有价值的客户？
- 在我的网络上，什么被认为是一个大文件？
- 产品分类中的每一个产品有多少数量？

&emsp;&emsp;Elasticsearch将聚合组织成三类：

- [Metric](#Metrics aggregations) aggregation：根据字段值（field value）计算指标值，例如sum、average
- [Bucket](#Bucket aggregations) aggregation：根据字段值、返回或者其他规则将文档分组到桶内，也可以成为`bins`（箱）
- [Pipeline](#Pipeline aggregations) aggregation：这种聚合的输入来自其他聚合的输出而不是来自文档或字段（filed）

#### Run an aggregation

&emsp;&emsp;你可以在[search API](#Search API)中指定`agg`参数，将其作为[search](#Search your data)的一部分。下面的查询在名为`my-field`的字段上运行一个[term aggregation](#Terms aggregation)：

```text
GET /my-index-000001/_search
{
  "aggs": {
    "my-agg-name": {
      "terms": {
        "field": "my-field"
      }
    }
  }
}
```

&emsp;&emsp;聚合的结果在响应中的`aggregations`对象中：

```text
{
  "took": 78,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped": 0,
    "failed": 0
  },
  "hits": {
    "total": {
      "value": 5,
      "relation": "eq"
    },
    "max_score": 1.0,
    "hits": [...]
  },
  "aggregations": {
    "my-agg-name": {                           
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": []
    }
  }
}
```

&emsp;&emsp;第19行，`my-agg-name`聚合的结果。

#### Change an aggregation’s scope

&emsp;&emsp;使用`query`参数限制聚合的目标文档：

```text
GET /my-index-000001/_search
{
  "query": {
    "range": {
      "@timestamp": {
        "gte": "now-1d/d",
        "lt": "now/d"
      }
    }
  },
  "aggs": {
    "my-agg-name": {
      "terms": {
        "field": "my-field"
      }
    }
  }
}
```

#### Return only aggregation results

&emsp;&emsp;默认包含聚合操作的查询结果同时返回查询命中和聚合结果。若只要返回聚合结果，将`size`设置为`0`即可：

```text
GET /my-index-000001/_search
{
  "size": 0,
  "aggs": {
    "my-agg-name": {
      "terms": {
        "field": "my-field"
      }
    }
  }
}
```

#### Run multiple aggregations

&emsp;&emsp;你可以在同一个请求中指定多个聚合操作：

```text
GET /my-index-000001/_search
{
  "aggs": {
    "my-first-agg-name": {
      "terms": {
        "field": "my-field"
      }
    },
    "my-second-agg-name": {
      "avg": {
        "field": "my-other-field"
      }
    }
  }
}
```

#### Run sub-aggregations

&emsp;&emsp;Bucket aggregation支持bucket or metric sub-aggregation。例如，一个带有[avg]() sub-aggregation的term aggregation会计算每一个分桶中的平均值。对于嵌套的sub-aggregation没有层数或深度的限制。

```text
GET /my-index-000001/_search
{
  "aggs": {
    "my-agg-name": {
      "terms": {
        "field": "my-field"
      },
      "aggs": {
        "my-sub-agg-name": {
          "avg": {
            "field": "my-other-field"
          }
        }
      }
    }
  }
}

```

&emsp;&emsp;响应中嵌套的sub-aggregation结果位于parent aggregation中：

```text
{
  ...
  "aggregations": {
    "my-agg-name": {                           
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": [
        {
          "key": "foo",
          "doc_count": 5,
          "my-sub-agg-name": {                 
            "value": 75.0
          }
        }
      ]
    }
  }
}
```

&emsp;&emsp;第4行，名为`my-agg-name`的parent aggregation的结果。
&emsp;&emsp;第11行，名为`my-sub-agg-name`的聚合结果，它是名为`my-agg-name`的parent aggregation的sub-aggregation。

#### Add custom metadata

&emsp;&emsp;在聚合中使用`meta`对象关联自定义的元数据：

```text
GET /my-index-000001/_search
{
  "aggs": {
    "my-agg-name": {
      "terms": {
        "field": "my-field"
      },
      "meta": {
        "my-metadata-field": "foo"
      }
    }
  }
}
```

&emsp;&emsp;响应中会原样返回`meta`对象：

```text
{
  ...
  "aggregations": {
    "my-agg-name": {
      "meta": {
        "my-metadata-field": "foo"
      },
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": []
    }
  }
}
```

#### Return the aggregation type

&emsp;&emsp;默认情况下，聚合结果中包含了聚合名称但没有聚合类型。若要返回聚合类型，可以使用请求参数`typed_keys`。

```text
GET /my-index-000001/_search?typed_keys
{
  "aggs": {
    "my-agg-name": {
      "histogram": {
        "field": "my-field",
        "interval": 1000
      }
    }
  }
}
```

&emsp;&emsp;响应中，聚合类型会作为聚合名称的前缀。

> IMPORTANT：一些聚合返回跟请求中不同的聚合类型。例如，terms、[significant terms](#Significant terms aggregation)、[percentiles aggregations](#Percentiles aggregation) 会返回不同的聚合类型，这取决于被聚合的字段的数据类型。

```text
{
  ...
  "aggregations": {
    "histogram#my-agg-name": {                 
      "buckets": []
    }
  }
}
```

&emsp;&emsp;第4行，聚合类型就是`histogram`后面紧跟`#`以及聚合名称，即`my-agg-name`。

#### Use scripts in an aggregation

&emsp;&emsp;当某个字段不能完全满足你想要的聚合目的，你应该在[runtime field](#Runtime fields)上聚合：

```text
GET /my-index-000001/_search?size=0
{
  "runtime_mappings": {
    "message.length": {
      "type": "long",
      "script": "emit(doc['message.keyword'].value.length())"
    }
  },
  "aggs": {
    "message_length": {
      "histogram": {
        "interval": 10,
        "field": "message.length"
      }
    }
  }
}
```

&emsp;&emsp;脚本中字段值的计算是动态的，他会额外增加一点点聚合的开销。除了计算的时间开销，一些比如[terms](#Terms aggregation)和[filters](#Filters aggregation)的聚合不能在使用runtime field时使用一些优化。总之，使用runtime field的开销根据不同的聚合类型而不同。

#### Aggregation caches

&emsp;&emsp;为了能快速响应，Elasticsearch在[shard request cache](#Shard request cache settings)中缓存了频繁的聚合操作的结果。若要获取缓存结果，可以在每一次查询中使用相同的[preference string](#Set a preference)。如果不需要查询命中，可以将[size](#Return only aggregation results)设置为`0`来避免缓存查询命中。

&emsp;&emsp;Elasticsearch根据相同的preference string将查询路由到相同的分片上。如果在查询时分片的数据没有发生变化，分片会返回缓存的聚合结果。

#### Limits for long values

&emsp;&emsp;当运行聚合时，Elasticsearch使用[double](#Numeric field types)值来处理并且代表数值类型的数据。使得对超过`2^53`的[long](#Numeric field types)数值的聚合结果是近似值。

### Bucket aggregations
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket.html)

&emsp;&emsp;Bucket aggregation不像metric aggregation那样对字段计算指标，而是对文档进行分桶。使用对应的规则（取决于聚合类型）进行分桶，决定某篇文档是否要"落入"当前的分桶中。换句话说，分桶有效的定义了文档集。除了分桶本身，`bucket` aggregation还计算"落入"到每一个分桶中的文档数量。

&emsp;&emsp;Bucket aggregation跟`metric` aggregation不同的是，它可以包含sub-aggregation。这些sub-aggregation可以对由它们的"parent" bucket aggregation创建的分桶，在桶内进行聚合操作。

&emsp;&emsp;不同的bucket aggregation有不同的"bucketing"策略。有些定义了单个分桶，有些定义了固定数量的多个分桶，还有的能在聚合处理期间动态的创建分桶。

> NOTE：[search.max_buckets](#search.max_buckets)这个集群设置限制了在单个响应中分桶的数量

#### Adjacency matrix aggregation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-adjacency-matrix-aggregation.html)

&emsp;&emsp;一种返回邻接矩阵（[adjacency matrix](https://en.wikipedia.org/wiki/Adjacency_matrix)）形式的桶聚合。请求提供了一系列命名的过滤表达式集合，类似于[filters aggregation](#Filters aggregation)请求。响应中的每个桶代表了交叉过滤器矩阵中的一个非空单元格。

&emsp;&emsp;给定名为`A`、`B`、`C`的过滤名，响应中会返回以下名字的分桶：

|      |  A   |  B   |  C   |
| :--: | :--: | :--: | :--: |
|  A   |  A   | A&B  | A&C  |
|  B   |      |  B   | B&C  |
|  C   |      |      |  C   |

&emsp;&emsp;交叉的桶，例如`A&C`，使用两个过滤器名称的组合以及默认的分隔符`&`进行标记。注意，响应中不会包含`C&A`桶，因为这会是与`A&C`相同的一组文档。这个矩阵被认为是对称的，所以我们只返回其中的一半。为了做到这一点，我们对过滤器名称字符串进行排序，并且总是使用一对中较小的值作为分隔符左侧的值。

##### Example

&emsp;&emsp;下面的名为`interactions`的`adjacency_matrix` aggregation哪些人群交换了电子邮件：

```text
PUT emails/_bulk?refresh
{ "index" : { "_id" : 1 } }
{ "accounts" : ["hillary", "sidney"]}
{ "index" : { "_id" : 2 } }
{ "accounts" : ["hillary", "donald"]}
{ "index" : { "_id" : 3 } }
{ "accounts" : ["vladimir", "donald"]}

GET emails/_search
{
  "size": 0,
  "aggs" : {
    "interactions" : {
      "adjacency_matrix" : {
        "filters" : {
          "grpA" : { "terms" : { "accounts" : ["hillary", "sidney"] }},
          "grpB" : { "terms" : { "accounts" : ["donald", "mitt"] }},
          "grpC" : { "terms" : { "accounts" : ["vladimir", "nigel"] }}
        }
      }
    }
  }
}
```

&emsp;&emsp;响应中包含每一个过滤条件以及其组合的分桶和文档数量。不匹配的文档被排除在响应中。

```text
{
  "took": 9,
  "timed_out": false,
  "_shards": ...,
  "hits": ...,
  "aggregations": {
    "interactions": {
      "buckets": [
        {
          "key":"grpA",
          "doc_count": 2
        },
        {
          "key":"grpA&grpB",
          "doc_count": 1
        },
        {
          "key":"grpB",
          "doc_count": 2
        },
        {
          "key":"grpB&grpC",
          "doc_count": 1
        },
        {
          "key":"grpC",
          "doc_count": 1
        }
      ]
    }
  }
}
```

##### Parameters

- filters：（Required,object）用于创建分桶的过滤条件
  - `<filter>`：（Required,[Query DSL object](#Query DSL)）key就是过滤条件的名字
  至少要指定一个过滤条件。过滤条件数量不能超过[indices.query.bool.max_clause_count ](#indices.query.bool.max_clause_count)，见[Filter limits](#Filter limits（Adjacency matrix aggregation）)。
  
- separator：（Optional, string）用来组合过滤名称的符号。默认是`&`

##### Response body

- key：（string）分桶的名字。如果如果这个分桶对应多个过滤条件，那么名字会使用`separator`进行拼装
- document_count：（integer）满足该分桶过滤条件的文档数量

##### Usage

&emsp;&emsp;仅凭这个聚合就可以提供创建无向加权图所需的所有数据。然而，当与子聚合（如日期直方图）一起使用时，结果可以提供执行动态网络分析（[dynamic network analysis](https://en.wikipedia.org/wiki/Dynamic_network_analysis)）所需的额外数据层级，在这种分析中，检查随时间变化的交互变得非常重要。

##### Filter limits

&emsp;&emsp;对于N个过滤条件会生成`N^2 / 2`个分桶。断路器（[ circuit breaker](#Circuit breaker settings)）会组织结果生成太多的分桶来避免过多的磁盘寻道（disk seek）。请使用`indices.query.bool.max_clause_count`来限制过滤条件的数量。


#### Auto-interval date histogram aggregation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-autodatehistogram-aggregation.html)

&emsp;&emsp;它属于multi-bucket aggregation。 类似[Date histogram](#Date histogram aggregation)，Date histogram aggregation使用了间隔（interval）作为每一个分桶之间的"宽度"。而Auto-interval date histogram aggregation则是指定了分桶的数量，来告知需要返回的分桶数。它能自动的选择分桶之间的"宽度"来满足要求。返回的分桶数量将总是不大于指定的分桶数量。

&emsp;&emsp;参数`buckets`是可选的，如果未指定则使用默认值：`10`。

&emsp;&emsp;要求返回10个分桶：

```text
POST /sales/_search?size=0
{
  "aggs": {
    "sales_over_time": {
      "auto_date_histogram": {
        "field": "date",
        "buckets": 10
      }
    }
  }
}
```

##### Keys

&emsp;&emsp;在Elasticsearch内部实现中，日期用64位的时间戳表示，也就是milliseconds-since-the-epoch（01/01/1970 midnight UTC）。这些时间戳将作为分桶的key。`key_as_string`则是使用相同的时间戳根据`format`转化成格式化后的日期字符串：

> TIP：如果你不指定`format`，则使用字段的mapping中第一个date [format](#format(mapping parameter))。

```text
POST /sales/_search?size=0
{
  "aggs": {
    "sales_over_time": {
      "auto_date_histogram": {
        "field": "date",
        "buckets": 5,
        "format": "yyyy-MM-dd" 
      }
    }
  }
}
```

&emsp;&emsp;第8行，支持expressive date [format pattern](#Date range aggregation)

&emsp;&emsp;响应：

```text
{
  ...
  "aggregations": {
    "sales_over_time": {
      "buckets": [
        {
          "key_as_string": "2015-01-01",
          "key": 1420070400000,
          "doc_count": 3
        },
        {
          "key_as_string": "2015-02-01",
          "key": 1422748800000,
          "doc_count": 2
        },
        {
          "key_as_string": "2015-03-01",
          "key": 1425168000000,
          "doc_count": 2
        }
      ],
      "interval": "1M"
    }
  }
}
```

##### Intervals

&emsp;&emsp;返回的分桶之间的间隔基于被聚合的数据进行选择，因此返回的分桶数量将总是不大于指定的分桶数量。可能的间隔如下所示：



| seconds |     以1、5、10和30的倍数      |
| :-----: | :---------------------------: |
| minutes |     以1、5、10和30的倍数      |
|  hours  |       以1、3和12的倍数        |
|  days   |         以1和7的倍数          |
| months  |         以1和3的倍数          |
|  years  | 以1、5、10、20、50和100的倍数 |


&emsp;&emsp;最坏的情况下，当指定的分桶数量由于是按天分桶导致分桶数量过多的话，返回的分桶数量是指定数量的1/7。

##### Time Zone

&emsp;&emsp;Elasticsearch用Coordinated Universal Time (UTC)存储日期时间。默认使用UTC进行分桶和舌入。使用`time_zone`字段来告知应该使用不同的时区。

&emsp;&emsp;你也可以用 ISO 8601 UTC 偏移来指定时区（`+01:00 or -08:00`）或者 IANA time zone ID，比如`America/Los_Angeles`就是在TZ 数据库中使用的标识符。

&emsp;&emsp;比如有这样的例子：

```text
PUT my-index-000001/_doc/1?refresh
{
  "date": "2015-10-01T00:30:00Z"
}

PUT my-index-000001/_doc/2?refresh
{
  "date": "2015-10-01T01:30:00Z"
}

PUT my-index-000001/_doc/3?refresh
{
  "date": "2015-10-01T02:30:00Z"
}

GET my-index-000001/_search?size=0
{
  "aggs": {
    "by_day": {
      "auto_date_histogram": {
        "field":     "date",
        "buckets" : 3
      }
    }
  }
}
```

&emsp;&emsp;如果未指定时区则使用UTC，返回了3个间隔为1个小时的分桶，第一个分桶从2015年10月1日午夜开始：

```text
{
  ...
  "aggregations": {
    "by_day": {
      "buckets": [
        {
          "key_as_string": "2015-10-01T00:00:00.000Z",
          "key": 1443657600000,
          "doc_count": 1
        },
        {
          "key_as_string": "2015-10-01T01:00:00.000Z",
          "key": 1443661200000,
          "doc_count": 1
        },
        {
          "key_as_string": "2015-10-01T02:00:00.000Z",
          "key": 1443664800000,
          "doc_count": 1
        }
      ],
      "interval": "1h"
    }
  }
}
```

&emsp;&emsp;如果指定`time_zone`为`-01:00`，那么分桶的开始时间就是UTC午夜时间的前一个小时。

```text
GET my-index-000001/_search?size=0
{
  "aggs": {
    "by_day": {
      "auto_date_histogram": {
        "field":     "date",
        "buckets" : 3,
        "time_zone": "-01:00"
      }
    }
  }
}
```

&emsp;&emsp;现在分桶之间的间隔仍然是1个小时，但是第一个分桶的开始时间为2015年9月30号下午11点。这是指定时区的本地时间：

```text
{
  ...
  "aggregations": {
    "by_day": {
      "buckets": [
        {
          "key_as_string": "2015-09-30T23:00:00.000-01:00", 
          "key": 1443657600000,
          "doc_count": 1
        },
        {
          "key_as_string": "2015-10-01T00:00:00.000-01:00",
          "key": 1443661200000,
          "doc_count": 1
        },
        {
          "key_as_string": "2015-10-01T01:00:00.000-01:00",
          "key": 1443664800000,
          "doc_count": 1
        }
      ],
      "interval": "1h"
    }
  }
}
```

&emsp;&emsp;第7行，The `key_as_string` value represents midnight on each day in the specified time zone.（感觉说的不对）

> WARNING：许多时区会因夏令时而调整时钟。在这些变更发生时附近的桶可能比你根据calendar_interval或fixed_interval预期的大小略有不同。例如，考虑CET时区的夏令时开始：2016年3月27日凌晨2点，时钟向前调整1小时至当地时间3点。如果你使用day作为calendar_interval，那么覆盖那天的桶将只包含23小时的数据，而不是其他桶的常规24小时。对于更短的间隔，如fixed_interval的12小时，当夏令时变更发生时，3月27日早上你将只有一个11小时的桶

##### Minimum Interval parameter

&emsp;&emsp;`minimum_interval`参数允许使用者指定应该使用的最小舌入间隔（rounding interval）。这使得收集过程更加效率，因为聚合不会尝试在小于`minimum_interval`的任何间隔上进行舍入。

&emsp;&emsp;可接受的`minimum_interval`单位有：

- year
- month
- day
- hour
- minute
- second

```text
POST /sales/_search?size=0
{
  "aggs": {
    "sale_date": {
      "auto_date_histogram": {
        "field": "date",
        "buckets": 10,
        "minimum_interval": "minute"
      }
    }
  }
}
```

##### Missing value

&emsp;&emsp;当文档缺失聚合字段时，`missing`参数定义了在这篇文档中聚合字段的值。默认情况下，它们会被忽略，但是可以将它们视为具有某个值的文档。

```text
POST /sales/_search?size=0
{
  "aggs": {
    "sale_date": {
      "auto_date_histogram": {
        "field": "date",
        "buckets": 10,
        "missing": "2000/01/01" 
      }
    }
  }
}
```

&emsp;&emsp;第8行，没有`date`字段的文档会落入相同的分桶中，并且把这些文档看成有`date`字段，并且值为`2000/01/01`

#### Categorize text aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-filters-aggregation.html)

>WARNING：这个功能目前处于技术预览阶段，未来的版本中可能会进行更改或移除。Elastic会努力修复任何问题，但是技术预览中的功能不受官方正式发布（GA）功能的支持服务级别协议（SLA）约束。

&emsp;&emsp;它属于multi-bucket aggregation，将半结构化（semi-structured）的文档分组到分桶内。每一个`text`类型的字段使用自定义的analyzer重新分析。产生的token随后被分组，创建出格式相似的文本值桶。这种聚合最适合机器生成的文本，如系统日志。仅使用前100个分词后的token来分组文本。

#### Children aggregation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-filters-aggregation.html)

&emsp;&emsp;它属于特殊的single bucket aggregation，选择有指定了类型的子文档（child document），该类型在[join filed](#Join field type)中定义。

&emsp;&emsp;这个聚合有单个选项：

- type：待选择的子文档的类型

&emsp;&emsp;比如，我们有关系为`question`和`answer`的索引（`question`是`answer`的parent）。answer类型在mapping的定义如下所示：

```text
PUT child_example
{
  "mappings": {
    "properties": {
      "join": {
        "type": "join",
        "relations": {
          "question": "answer"
        }
      }
    }
  }
}
```

&emsp;&emsp;下面的例子中，`parent`文档中包含一个名为`tag` 字段并且`answer`文档中包含一个名为`owner`的 字段。`children aggregation`能在单个请求中将tag bucket映射到owner bucket中，尽管这两个字段在不同类型的文档中。

&emsp;&emsp;`question`的文档：

```text
PUT child_example/_doc/1
{
  "join": {
    "name": "question"
  },
  "body": "<p>I have Windows 2003 server and i bought a new Windows 2008 server...",
  "title": "Whats the best way to file transfer my site from server to a newer one?",
  "tags": [
    "windows-server-2003",
    "windows-server-2008",
    "file-transfer"
  ]
}
```

&emsp;&emsp;`answer`的文档：

```text
PUT child_example/_doc/2?routing=1
{
  "join": {
    "name": "answer",
    "parent": "1"
  },
  "owner": {
    "location": "Norfolk, United Kingdom",
    "display_name": "Sam",
    "id": 48
  },
  "body": "<p>Unfortunately you're pretty much limited to FTP...",
  "creation_date": "2009-05-04T13:45:37.030"
}

PUT child_example/_doc/3?routing=1&refresh
{
  "join": {
    "name": "answer",
    "parent": "1"
  },
  "owner": {
    "location": "Norfolk, United Kingdom",
    "display_name": "Troll",
    "id": 49
  },
  "body": "<p>Use Linux...",
  "creation_date": "2009-05-05T13:45:37.030"
}
```

&emsp;&emsp;下面的请求能将他们连接在一起：

```text
POST child_example/_search?size=0
{
  "aggs": {
    "top-tags": {
      "terms": {
        "field": "tags.keyword",
        "size": 10
      },
      "aggs": {
        "to-answers": {
          "children": {
            "type" : "answer" 
          },
          "aggs": {
            "top-names": {
              "terms": {
                "field": "owner.display_name.keyword",
                "size": 10
              }
            }
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;第12行，`type`指向了名为`answer`的类型。

&emsp;&emsp;上面的例子返回了top question tags以及每一个tag的top answer owners。

```text
{
  "took": 25,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped" : 0,
    "failed": 0
  },
  "hits": {
    "total" : {
      "value": 3,
      "relation": "eq"
    },
    "max_score": null,
    "hits": []
  },
  "aggregations": {
    "top-tags": {
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": [
        {
          "key": "file-transfer",
          "doc_count": 1, 
          "to-answers": {
            "doc_count": 2, 
            "top-names": {
              "doc_count_error_upper_bound": 0,
              "sum_other_doc_count": 0,
              "buckets": [
                {
                  "key": "Sam",
                  "doc_count": 1
                },
                {
                  "key": "Troll",
                  "doc_count": 1
                }
              ]
            }
          }
        },
        {
          "key": "windows-server-2003",
          "doc_count": 1, 
          "to-answers": {
            "doc_count": 2, 
            "top-names": {
              "doc_count_error_upper_bound": 0,
              "sum_other_doc_count": 0,
              "buckets": [
                {
                  "key": "Sam",
                  "doc_count": 1
                },
                {
                  "key": "Troll",
                  "doc_count": 1
                }
              ]
            }
          }
        },
        {
          "key": "windows-server-2008",
          "doc_count": 1, 
          "to-answers": {
            "doc_count": 2, 
            "top-names": {
              "doc_count_error_upper_bound": 0,
              "sum_other_doc_count": 0,
              "buckets": [
                {
                  "key": "Sam",
                  "doc_count": 1
                },
                {
                  "key": "Troll",
                  "doc_count": 1
                }
              ]
            }
          }
        }
      ]
    }
  }
}
```

&emsp;&emsp;第67行， tag为`file-transfer`, `windows-server-2003`等question文档的数量
&emsp;&emsp;第69行， tag为`file-transfer`, `windows-server-2003`等关联的answer文档的数量

#### Composite aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-composite-aggregation.html#_date_histogram)

> WARNING：composite aggregation开销是昂贵的。在你生产中部署composite aggregation前先进行测试。

&emsp;&emsp;它属于multi-bucket aggregation，会根据不同的source创建composite buckets。

&emsp;&emsp;跟其他`multi-bucket` aggregation不同的是，你可以使用`composite` aggregation有效地对来自多层次聚合（multi-level）aggregation的所有桶进行分页。这个聚合提供了一个方法流式处理一个指定的聚合的所有分桶，跟[scroll](#Scroll search results)处理文档类似。

&emsp;&emsp;composite buckets通过对提取/创建出的每一篇文档中的值进行组合（combination），并且每一种组合都认为是一个composite bucket。

&emsp;&emsp;例如，在下面这个文档中：

```text
{
  "keyword": ["foo", "bar"],
  "number": [23, 65, 76]
}
```

&emsp;&emsp;下面的composite bucket中，使用`keyword`和`number`作为聚合结果的source filed。

```text
{ "keyword": "foo", "number": 23 }
{ "keyword": "foo", "number": 65 }
{ "keyword": "foo", "number": 76 }
{ "keyword": "bar", "number": 23 }
{ "keyword": "bar", "number": 65 }
{ "keyword": "bar", "number": 76 }
```

##### Value sources

&emsp;&emsp;`sources`参数定义了构建compost bucket时使用的source filed。在`sources`中的定义先后顺序控制了key的返回顺序。

> NOTE: 当你定义`sources`时，你必须使用一个唯一键

&emsp;&emsp;`sources`参数的类型只能是下列中的一种：

- [Terms](#\_Terms)
- [Histogram](#Histogram（Composite）)
- [Date histogram](#Date histogram（Composite）)
- [GeoTile grid](#GeoTile-grid（Composite）)

###### \_Terms

&emsp;&emsp;`terms`这个value source类似一个简答的`terms` aggregation。这个值从一个字段中提出出来，跟`terms` aggregation一样。

&emsp;&emsp;例子：

```text
GET /_search
{
  "size": 0,
  "aggs": {
    "my_buckets": {
      "composite": {
        "sources": [
          { "product": { "terms": { "field": "product" } } }
        ]
      }
    }
  }
}
```

&emsp;&emsp;跟`terms` aggregation一样，可以为composite bucket通过[runtime field](#Runtime fields)创建一个值：

```text
GET /_search
{
  "runtime_mappings": {
    "day_of_week": {
      "type": "keyword",
      "script": """
        emit(doc['timestamp'].value.dayOfWeekEnum
          .getDisplayName(TextStyle.FULL, Locale.ROOT))
      """
    }
  },
  "size": 0,
  "aggs": {
    "my_buckets": {
      "composite": {
        "sources": [
          {
            "dow": {
              "terms": { "field": "day_of_week" }
            }
          }
        ]
      }
    }
  }
}
```

&emsp;&emsp;尽管很相似，但是`terms`这个 value source 不支持`terms` aggregation中的参数。对于支持的value source参数，见：

- [Order](#Order（Composite）)
- [Missing bucket](#Missing bucket（Composite）)

###### Histogram（Composite）

&emsp;&emsp;`histogram` 这个value source可以应用在数值类型的值上，构建值之间固定大小的间隔。`interval`参数定义数值类型的值如何被转化（transform）。比如说一个`interval`为5时，任何数值都会转到其最接近的间隔，`101`这个值会被转化为`100`。这个key用于描述100到105的值。

&emsp;&emsp;例如：

```text
GET /_search
{
  "size": 0,
  "aggs": {
    "my_buckets": {
      "composite": {
        "sources": [
          { "histo": { "histogram": { "field": "price", "interval": 5 } } }
        ]
      }
    }
  }
}
```

&emsp;&emsp;跟`histogram` aggregation 一样，可以为composite bucket通过[runtime field](#Runtime fields)创建一个值：

```text
GET /_search
{
  "runtime_mappings": {
    "price.discounted": {
      "type": "double",
      "script": """
        double price = doc['price'].value;
        if (doc['product'].value == 'mad max') {
          price *= 0.8;
        }
        emit(price);
      """
    }
  },
  "size": 0,
  "aggs": {
    "my_buckets": {
      "composite": {
        "sources": [
          {
            "price": {
              "histogram": {
                "interval": 5,
                "field": "price.discounted"
              }
            }
          }
        ]
      }
    }
  }
}
```

###### Date histogram（Composite）

&emsp;&emsp;`date_histogram` 类似与`histogram`，只是间隔值用的是date/time。

```text
GET /_search
{
  "size": 0,
  "aggs": {
    "my_buckets": {
      "composite": {
        "sources": [
          { "date": { "date_histogram": { "field": "timestamp", "calendar_interval": "1d" } } }
        ]
      }
    }
  }
}
```

&emsp;&emsp;上面的例子创建了一个按天作为时间间隔，并将所有`timestamp`的值转化成最近的时间间隔。可用的间隔表达式有：`year`, `quarter`, `month`, `week`,`day`, `hour`,`minute`, `second`。

&emsp;&emsp;时间值也可以缩写为目前支持的[time units](#Time units)。注意的是小数点的时间值是不支持的，但是你可以转化为其他单位来解决这个问题（比如说`1.5h`可以替换为`90m`）。

- Format

&emsp;&emsp;在内部实现中，日期会用一个64位的milliseconds-since-the-epoch数字表示。这些时间戳返回后会作为bucket key。可以返回格式化的日期字符串，而不是使用格式参数指定的格式：

```text
GET /_search
{
  "size": 0,
  "aggs": {
    "my_buckets": {
      "composite": {
        "sources": [
          {
            "date": {
              "date_histogram": {
                "field": "timestamp",
                "calendar_interval": "1d",
                "format": "yyyy-MM-dd"         
              }
            }
          }
        ]
      }
    }
  }
}
```

&emsp;&emsp; 第13行，见[format pattern](#Date Format/Pattern)

- Time Zone


&emsp;&emsp;Elasticsearch用Coordinated Universal Time (UTC)存储日期时间。默认使用UTC进行分桶和舌入。使用`time_zone`字段来告知应该使用不同的时区。

&emsp;&emsp;你也可以用 ISO 8601 UTC 偏移来指定时区（`+01:00 or -08:00`）或者 IANA time zone ID，比如`America/Los_Angeles`就是在TZ 数据库中使用的标识符。

- Offset

&emsp;&emsp;使用`offset`参数，指定正负号（`+`或`-`）的时间修改每一个分桶的开始值，比如`1h`就是一小时，`1d`就是一天。见[time units](#Time units)了解更多的选项。

&emsp;&emsp;比如，当使用了`day`的间隔，每一个分桶从每天的午夜开始，下一个午夜结束，将`offset`设置为`+6h`后，每一个分桶从早上6点到下一个早上6点：

```text
PUT my-index-000001/_doc/1?refresh
{
  "date": "2015-10-01T05:30:00Z"
}

PUT my-index-000001/_doc/2?refresh
{
  "date": "2015-10-01T06:30:00Z"
}

GET my-index-000001/_search?size=0
{
  "aggs": {
    "my_buckets": {
      "composite" : {
        "sources" : [
          {
            "date": {
              "date_histogram" : {
                "field": "date",
                "calendar_interval": "day",
                "offset": "+6h",
                "format": "iso8601"
              }
            }
          }
        ]
      }
    }
  }
}
```

&emsp;&emsp;上面的请求会将6am开始的文档分组到分桶内，而不是从凌晨开始。

```text
{
  ...
  "aggregations": {
    "my_buckets": {
      "after_key": { "date": "2015-10-01T06:00:00.000Z" },
      "buckets": [
        {
          "key": { "date": "2015-09-30T06:00:00.000Z" },
          "doc_count": 1
        },
        {
          "key": { "date": "2015-10-01T06:00:00.000Z" },
          "doc_count": 1
        }
      ]
    }
  }
}
```

> NOTE: 每一分桶开始的`offse`已经用`time_zone`进行了调整。

###### GeoTile grid（Composite）

##### Order（Composite）

##### Missing bucket（Composite）

#### Date histogram aggregation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-datehistogram-aggregation.html#calendar_and_fixed_intervals)

&emsp;&emsp;它属于multi-bucket aggregation，类似与普通的[histogram](#Histogram aggregation)。但是它只能用于日期或者日期类型的范围值（range value）。由于日期在Elasticsearch内部用long值表示，因此`histogram`aggregation 也可以用于日期，但准确性不如data histogram。这两个聚合的区别在于这个聚合的间隔（interval）可以指定为date/time 表达式。Time-based的数据要求特殊支持，因为time-based的interval不总是一个固定长度（比如间隔为`1M`，那么不同月份的一个月的天数是不一样的）。

&emsp;&emsp;跟histogram一样的是，字段值会被下舍入到最近的分桶中，例如，如果间隔是一个日历日（calendar day），那么`2020-01-03T07:00:01Z`会下舍入到`2020-01-03T00:00:00Z`。字段值按照以下方式舌入：

```text
bucket_key = Math.floor(value / interval) * interval
```

##### Calendar and fixed intervals

&emsp;&emsp;在配置一个date histogram aggregation时，有两种方式指定间隔：calendar-aware time（日历感知，也就是自然时间，比如一个月，一年这种）以及fixed time（固定时间）。

&emsp;&emsp;calendar-aware time能根据夏令时会改变特定日子的长度，不同的月份有不同数量的天数，而闰秒可以加到特定的年份上。

&emsp;&emsp;相比之下，固定间隔总是国际单位制的倍数，并且不会根据日历情境发生变化。

##### Calendar intervals

&emsp;&emsp;calendar-aware interval通过`calendar_interval`配置。你可以使用单位名称，比如`month`、或者单个单位数量，比如`1M`。例如`day`和`1d`是同等的。多个数量，比如`2d`是不支持的。

&emsp;&emsp;可接受的日历间隔（calendar interval）包括：

- minute,1m
  - 所有分钟都从00秒开始。一分钟是指定时区中第一分钟的00秒到下一分钟的00秒之间的间隔，补偿任何介入的闰秒，以便小时过去的分钟数和秒数在开始和结束时是相同的
    - 如果一个时间间隔从某个小时的开始计算（比如，下午3点整），那么这个时间间隔结束时也应该是在某个小时的开始（比如，下午4点整），即使在这个时间段内实际添加了闰秒，也不会改变这个规律
- hour, 1h
  - 所有小时都从00分钟00秒开始。一小时（1h）是指定时区中第一个小时的00:00分到下一个小时的00:00分之间的间隔，补偿任何介入的闰秒，以便小时过去的分钟数和秒数在开始和结束时是相同的
- day, 1d
  - 所有天都从可能的最早时间开始，通常是00:00:00（午夜）。一天（1d）是指定时区中一天的开始到下一天开始之间的间隔，补偿任何介入的时间变化

- week, 1w
  - 一周是从开始的 day_of_week:hour:minute:second 到指定时区的下一周的同一天和时间之间的间隔。
- month, 1M
  - 一个月是从月份的开始日和一天中的时间到下一个月的同一天和时间之间的间隔，以便月份中的日和一天中的时间在开始和结束时是相同的
- quarter, 1q
  - 一个季度是从月份的开始日和一天中的时间到三个月后的同一天和时间之间的间隔，以便月份中的日和一天中的时间在开始和结束时是相同的
- year, 1y
  - 一年是从月份的开始日和一天中的时间到下一年的同一天和时间之间的间隔，以便日期和时间在开始和结束时是相同的

###### Calendar interval examples

&emsp;&emsp;下面的例子中，按照间隔为一个月的日历时间进行分桶：

```text
POST /sales/_search?size=0
{
  "aggs": {
    "sales_over_time": {
      "date_histogram": {
        "field": "date",
        "calendar_interval": "month"
      }
    }
  }
}
```

&emsp;&emsp;如果你尝试使用多个日历单元（calendar unit），那么聚合将会失败，因为只支持单个单位数量。

```text
POST /sales/_search?size=0
{
  "aggs": {
    "sales_over_time": {
      "date_histogram": {
        "field": "date",
        "calendar_interval": "2d"
      }
    }
  }
}
```

```text
{
  "error" : {
    "root_cause" : [...],
    "type" : "x_content_parse_exception",
    "reason" : "[1:82] [date_histogram] failed to parse field [calendar_interval]",
    "caused_by" : {
      "type" : "illegal_argument_exception",
      "reason" : "The supplied interval [2d] could not be parsed as a calendar interval.",
      "stack_trace" : "java.lang.IllegalArgumentException: The supplied interval [2d] could not be parsed as a calendar interval."
    }
  }
}
```

##### Fixed intervals

&emsp;&emsp;固定间隔使用`fixed_interval`配置。

&emsp;&emsp;与Calendar aware intervals不同，固定间隔是固定数量的国际单位制（SI）单位，不会偏离（也就是不受日历上的变化（如月份长度不同、闰年等）或特殊时间调整（如夏令时）的影响），不管它们落在日历上的哪个位置。一秒钟总是由1000毫秒组成。这允许固定间隔以支持单位的任何倍数来指定。

&emsp;&emsp;然而，这意味着固定间隔不能表示像月份这样的单位，因为一个月的持续时间不是一个固定数量。尝试指定一个像月或季度这样的日历间隔将会抛出异常。

&emsp;&emsp;固定间隔接受的单位有：

- milliseconds (ms)：单个毫秒，这是非常非常小的间隔
- seconds (s)：相当于定义了1000ms
- minutes (m)：相当于定义了60s或者60000ms。所有的分钟从00秒开始
- hours (h)：相当于定义了60m或者3600000ms。所有的小时从00分00秒开始
- days (d)：相当于定义了24h或者86400000ms。所有天都从可能的最早时间开始，通常是00:00:00（午夜）

###### Fixed interval examples

&emsp;&emsp;如果我们尝试重新创建一个之前`calendar_interval`的一个月，我们可以使用固定的30天来近似实现。

```text
POST /sales/_search?size=0
{
  "aggs": {
    "sales_over_time": {
      "date_histogram": {
        "field": "date",
        "fixed_interval": "30d"
      }
    }
  }
}
```

&emsp;&emsp;但如果我们尝试使用日历单位时是不支持的，比如多个周，我们会得到一个异常：

```text
POST /sales/_search?size=0
{
  "aggs": {
    "sales_over_time": {
      "date_histogram": {
        "field": "date",
        "fixed_interval": "2w"
      }
    }
  }
}
```

```text
{
  "error" : {
    "root_cause" : [...],
    "type" : "x_content_parse_exception",
    "reason" : "[1:82] [date_histogram] failed to parse field [fixed_interval]",
    "caused_by" : {
      "type" : "illegal_argument_exception",
      "reason" : "failed to parse setting [date_histogram.fixedInterval] with value [2w] as a time value: unit is missing or unrecognized",
      "stack_trace" : "java.lang.IllegalArgumentException: failed to parse setting [date_histogram.fixedInterval] with value [2w] as a time value: unit is missing or unrecognized"
    }
  }
}
```

##### Date histogram usage notes

&emsp;&emsp;在所有情况下，当指定的结束时间不存在时，实际的结束时间是指定结束后最接近的可用时间。

&emsp;&emsp;广泛分布的应用程序还必须考虑到一些变数，比如一些国家在凌晨12:01开始和停止夏令时（daylight savings time），因此一年中会有`一分钟的星期日后接着是额外的59分钟的星期六`，以及那些决定跨越国际日期变更线的国家。这样的情况可能会让不规则的时区偏移看起来简单。

> NOTE：**一分钟的星期日后接着是额外的59分钟的星期六**
夏令时是一种在夏季月份调整时钟以便更好地利用日照的做法，通常涉及在春季将时钟向前调整一小时，在秋季再将其调回
> - 夏令时结束前：时间进入到凌晨12:01 AM，接着是星期日的第一分钟。
> - 夏令时结束：在凌晨12:01 AM，时钟后退一小时到前一个小时的开始，即回到11:01 PM。
> - 夏令时结束后：时间再次经历11:02 PM到11:59 PM的59分钟，这些时间实际上是“重复”的，因为它们在夏令时期间已经发生过一次。这导致了一个有趣的现象，即夏令时结束的那一天会比标准的24小时多出一个小时，总计25小时。
> 因此，这种情况下，看似在星期日的第一分钟后，实际上是回到了星期六的最后一个小时，从而造成了“一分钟的星期日后接着是额外的59分钟的星期六”的现象。

&emsp;&emsp;一如既往，严格的测试，特别是在时间变更事件周围，将确保您的时间间隔规范是您所期望的。

> WARNING：为了避免意外结果，所有连接的服务器和客户端都必须同步到一个可靠的网络时间服务。

> NOTE：不支持小数时间值，但您可以通过转换到另一个时间单位来解决这个问题（例如，1.5小时可以指定为90分钟）。

> NOTE：您还可以使用时间单位解析支持的缩写来指定时间值。

##### Keys

&emsp;&emsp;在Elasticsearch内部实现中，日期用64位的时间戳表示，也就是milliseconds-since-the-epoch（01/01/1970 midnight UTC）。这些时间戳将作为分桶的key。`key_as_string`则是使用相同的时间戳根据`format`转化成格式化后的日期字符串：

> TIP：如果你不指定`format`，则使用字段的mapping中第一个date [format](#format(mapping parameter))。

```text
POST /sales/_search?size=0
{
  "aggs": {
    "sales_over_time": {
      "date_histogram": {
        "field": "date",
        "calendar_interval": "1M",
        "format": "yyyy-MM-dd" 
      }
    }
  }
}
```

&emsp;&emsp;第7行，支持expressive date [format pattern](#Date range aggregation)

&emsp;&emsp;响应：

```text
{
  ...
  "aggregations": {
    "sales_over_time": {
      "buckets": [
        {
          "key_as_string": "2015-01-01",
          "key": 1420070400000,
          "doc_count": 3
        },
        {
          "key_as_string": "2015-02-01",
          "key": 1422748800000,
          "doc_count": 2
        },
        {
          "key_as_string": "2015-03-01",
          "key": 1425168000000,
          "doc_count": 2
        }
      ]
    }
  }
}
```

##### Time zone

&emsp;&emsp;Elasticsearch用Coordinated Universal Time (UTC)存储日期时间。默认使用UTC进行分桶和舌入。使用`time_zone`字段来告知应该使用不同的时区。

&emsp;&emsp;例如，如果间隔是calendar day并且时区是`America/New_York`（西五区），那么`2020-01-03T01:00:01Z`转化为`2020-01-02T18:00:01`（应该是`2020-01-02T20:00:01`吧？），舌入到`2020-01-02T00:00:00`然后转回到UTC的`2020-01-02T05:00:00:00Z`。最终以`America/New_York`显示分桶名称，所以变成了`2020-01-02T00:00:00`。

&emsp;&emsp;也就是：

```text
bucket_key = localToUtc(Math.floor(utcToLocal(value) / interval) * interval))
```

&emsp;&emsp;你也可以用 ISO 8601 UTC 偏移来指定时区（`+01:00 or -08:00`）或者 IANA time zone ID，比如`America/Los_Angeles`。

&emsp;&emsp;看这个例子：

```text
PUT my-index-000001/_doc/1?refresh
{
  "date": "2015-10-01T00:30:00Z"
}

PUT my-index-000001/_doc/2?refresh
{
  "date": "2015-10-01T01:30:00Z"
}

GET my-index-000001/_search?size=0
{
  "aggs": {
    "by_day": {
      "date_histogram": {
        "field":     "date",
        "calendar_interval":  "day"
      }
    }
  }
}
```

&emsp;&emsp;如果不指定一个时区，则使用UTC。这将导致这两个结果放到同一个分桶中，开始于：`midnight UTC on 1 October 2015`：

```text
{
  ...
  "aggregations": {
    "by_day": {
      "buckets": [
        {
          "key_as_string": "2015-10-01T00:00:00.000Z",
          "key":           1443657600000,
          "doc_count":     2
        }
      ]
    }
  }
}
```

&emsp;&emsp;如果你指定的时区为`-01:00`，那么午夜时间是UTC午夜的前一个小时：

```text
GET my-index-000001/_search?size=0
{
  "aggs": {
    "by_day": {
      "date_histogram": {
        "field":     "date",
        "calendar_interval":  "day",
        "time_zone": "-01:00"
      }
    }
  }
}
```

&emsp;&emsp;注意的是第一个文档会落入到2015年9月30的桶，而另一个落入到2015年10月1号的桶

```text
{
  ...
  "aggregations": {
    "by_day": {
      "buckets": [
        {
          "key_as_string": "2015-09-30T00:00:00.000-01:00", 
          "key": 1443574800000,
          "doc_count": 1
        },
        {
          "key_as_string": "2015-10-01T00:00:00.000-01:00", 
          "key": 1443661200000,
          "doc_count": 1
        }
      ]
    }
  }
}
```

&emsp;&emsp;`key_as_string`描述了指定时区中每天的午夜时间。

> WARNING：许多时区会因夏令时而调整时钟。在这些变更发生时附近的桶可能比你根据calendar_interval或fixed_interval预期的大小略有不同。例如，考虑CET时区的夏令时开始：2016年3月27日凌晨2点，时钟向前调整1小时至当地时间3点。如果你使用day作为calendar_interval，那么覆盖那天的桶将只包含23小时的数据，而不是其他桶的常规24小时。对于更短的间隔，如fixed_interval的12小时，当夏令时变更发生时，3月27日早上你将只有一个11小时的桶

##### Offset

&emsp;&emsp;使用`offset`参数，指定正负号（`+`或`-`）的时间修改每一个分桶的开始值，比如`1h`就是一小时，`1d`就是一天。见[time units](#Time units)了解更多的选项。

&emsp;&emsp;比如，当使用了`day`的间隔，每一个分桶从每天的午夜开始，下一个午夜结束，将`offset`设置为`+6h`后，每一个分桶从早上6点到下一个早上6点：

```text
PUT my-index-000001/_doc/1?refresh
{
  "date": "2015-10-01T05:30:00Z"
}

PUT my-index-000001/_doc/2?refresh
{
  "date": "2015-10-01T06:30:00Z"
}

GET my-index-000001/_search?size=0
{
  "aggs": {
    "by_day": {
      "date_histogram": {
        "field":     "date",
        "calendar_interval":  "day",
        "offset":    "+6h"
      }
    }
  }
}
```

&emsp;&emsp;上面的请求将文档分别落入到两个从早上6点开始的分桶中，而不是落入到同一个从午夜开始的单个分桶中：

```text
{
  ...
  "aggregations": {
    "by_day": {
      "buckets": [
        {
          "key_as_string": "2015-09-30T06:00:00.000Z",
          "key": 1443592800000,
          "doc_count": 1
        },
        {
          "key_as_string": "2015-10-01T06:00:00.000Z",
          "key": 1443679200000,
          "doc_count": 1
        }
      ]
    }
  }
}
```

> NOTE：在调整时区后，在根据`offset`计算

##### Keyed Response

&emsp;&emsp;将`keyed`设置为`true`，分桶的key_as_string的值将作为key，响应中所有的分桶用hash展示而不是用数组。

```text
POST /sales/_search?size=0
{
  "aggs": {
    "sales_over_time": {
      "date_histogram": {
        "field": "date",
        "calendar_interval": "1M",
        "format": "yyyy-MM-dd",
        "keyed": true
      }
    }
  }
}
```

&emsp;&emsp;响应：

```text
{
  ...
  "aggregations": {
    "sales_over_time": {
      "buckets": {
        "2015-01-01": {
          "key_as_string": "2015-01-01",
          "key": 1420070400000,
          "doc_count": 3
        },
        "2015-02-01": {
          "key_as_string": "2015-02-01",
          "key": 1422748800000,
          "doc_count": 2
        },
        "2015-03-01": {
          "key_as_string": "2015-03-01",
          "key": 1425168000000,
          "doc_count": 2
        }
      }
    }
  }
}
```

##### Script

&emsp;&emsp;如果文档中的数据不完全满足你聚合的目的，可以使用[runtime filed](#Runtime fields)。比如，如果促销销售的收入应该在销售日期的后一天确认：

```text
POST /sales/_search?size=0
{
  "runtime_mappings": {
    "date.promoted_is_tomorrow": {
      "type": "date",
      "script": """
        long date = doc['date'].value.toInstant().toEpochMilli();
        if (doc['promoted'].value) {
          date += 86400;
        }
        emit(date);
      """
    }
  },
  "aggs": {
    "sales_over_time": {
      "date_histogram": {
        "field": "date.promoted_is_tomorrow",
        "calendar_interval": "1M"
      }
    }
  }
}
```

##### Parameters

&emsp;&emsp;你可以使用`order`参数控制返回的分桶的顺序，以及根据`min_doc_count`过滤分桶（默认第一个和最后一个之间的所有分桶都会返回，意思是即使是空的分桶也会返回）。histogram同样支持`extended_bounds`。使得histogram的范围可以超过数据自身。`hard_bounds`可以限制histogram的范围。更多信息见[Extended Bounds](#Histogram aggregation)和[Hard Bounds](#Histogram aggregation)。

###### Missing value

&emsp;&emsp;`missing`参数定义了文档中缺失被聚合的字段时，如何进行聚合。默认这些文档会被忽略但也可以把这些文档看成有一个值。

```text
POST /sales/_search?size=0
{
  "aggs": {
    "sale_date": {
      "date_histogram": {
        "field": "date",
        "calendar_interval": "year",
        "missing": "2000/01/01" 
      }
    }
  }
}
```

&emsp;&emsp;第8行，没有`date`字段的文档会落入相同的分桶中，并且把这些文档看成有`date`字段，并且值为`2000/01/01`

###### Order

&emsp;&emsp;默认情况下分桶按照他们的key升序排序，不过可以通过`order`来控制这个行为。它跟[Terms Aggregation](#Terms aggregation)中的`order`有相同的功能。

###### Using a script to aggregate by day of the week

&emsp;&emsp;当你需要按星期几聚合结果时，可以在一个[runtime field](#Runtime fields)上执行`terms`聚合，该字段返回星期几。这种方法允许你根据日期数据的特定部分（即星期几）来组织和分析聚合结果

```text
POST /sales/_search?size=0
{
  "runtime_mappings": {
    "date.day_of_week": {
      "type": "keyword",
      "script": "emit(doc['date'].value.dayOfWeekEnum.getDisplayName(TextStyle.FULL, Locale.ROOT))"
    }
  },
  "aggs": {
    "day_of_week": {
      "terms": { "field": "date.day_of_week" }
    }
  }
}
```

&emsp;&emsp;响应：

```text
{
  ...
  "aggregations": {
    "day_of_week": {
      "doc_count_error_upper_bound": 0,
      "sum_other_doc_count": 0,
      "buckets": [
        {
          "key": "Sunday",
          "doc_count": 4
        },
        {
          "key": "Thursday",
          "doc_count": 3
        }
      ]
    }
  }
}
```

&emsp;&emsp;响应中包含了一周中相关星期几的分桶：1代表星期一，2代表星期二……7代表星期日。

#### Date range aggregation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-daterange-aggregation.html)

&emsp;&emsp;专门为日期值（date value）使用的range aggregation。这个聚合根[range](#Range aggregation) aggregation的主要不同是`from`跟`to`可以用[Date Math](#Date Math)表达式。也可以在返回的`from`跟`to`使用格式化后的日期。注意的是在这个聚合中，每一个范围的`from`是包含并且`to`是不包含的。

&emsp;&emsp;例子：

```text
POST /sales/_search?size=0
{
  "aggs": {
    "range": {
      "date_range": {
        "field": "date",
        "format": "MM-yyyy",
        "ranges": [
          { "to": "now-10M/M" },  
          { "from": "now-10M/M" } 
        ]
      }
    }
  }
}
```

&emsp;&emsp;第8行，< now minus 10 months，下舍入到月份的开始时间
&emsp;&emsp;第9行，>= now minus 10 months，下舍入到月份的开始时间

&emsp;&emsp;在上面的例子中，我们创建了两个范围的分桶，10个月前的文档将划分到第一个捅，10个月前开始到现在的文档划分到第二个分桶。

&emsp;&emsp;响应：

```text
{
  ...
  "aggregations": {
    "range": {
      "buckets": [
        {
          "to": 1.4436576E12,
          "to_as_string": "10-2015",
          "doc_count": 7,
          "key": "*-10-2015"
        },
        {
          "from": 1.4436576E12,
          "from_as_string": "10-2015",
          "doc_count": 0,
          "key": "10-2015-*"
        }
      ]
    }
  }
}
```
> WARNING：如果format或者日期值不完整，date range aggregation使用默认值替换缺失的部分。见[Missing date components](#Missing date components)
> - 你进行日期范围聚合时，如果你指定的日期没有包含所有需要的信息（比如，年、月、日、时、分、秒等），Elasticsearch会自动用一些预设的值来填充这些缺失的部分，以确保日期值是完整的

##### Missing Values

&emsp;&emsp;当文档缺失聚合字段时，`missing`参数定义了在这篇文档中聚合字段的值。默认情况下，它们会被忽略，但是可以将它们视为具有某个值的文档。可以通过添加字段名集合完成：字段值被映射为一个默认值

```text
POST /sales/_search?size=0
{
   "aggs": {
       "range": {
           "date_range": {
               "field": "date",
               "missing": "1976/11/30",
               "ranges": [
                  {
                    "key": "Older",
                    "to": "2016/02/01"
                  }, 
                  {
                    "key": "Newer",
                    "from": "2016/02/01",
                    "to" : "now/d"
                  }
              ]
          }
      }
   }
}
```

&emsp;&emsp;第11行，没有`date`字段的文档会被划分到"Older"的分桶中，因为它们的字段值为`1976/11/30`。


##### Date Format/Pattern

> NOTE：以下的内容拷贝于[DateTimeFormatter](https://docs.oracle.com/javase/8/docs/api/java/time/format/DateTimeFormatter.html)。

&emsp;&emsp;所有ASCII字母都被保留作为format pattern letter，其定义如下：

| Symbol |          Meaning           | Presentation |                  Examples                  |
| :----: | :------------------------: | :----------: | :----------------------------------------: |
|   G    |            era             |     text     |             AD; Anno Domini; A             |
|   u    |            year            |     year     |                  2004; 04                  |
|   y    |        year-of-era         |     year     |                  2004; 04                  |
|   D    |        day-of-year         |    number    |                    189                     |
|  M/L   |       month-of-year        | number/text  |            7; 07; Jul; July; J             |
|   d    |        day-of-month        |    number    |                     10                     |
|  Q/q   |      quarter-of-year       | number/text  |           3; 03; Q3; 3rd quarter           |
|   Y    |      week-based-year       |     year     |                  1996; 96                  |
|   w    |  week-of-week-based-year   |    number    |                     27                     |
|   W    |       week-of-month        |    number    |                     4                      |
|   E    |        day-of-week         |     text     |              Tue; Tuesday; T               |
|  e/c   |   localized day-of-week    | number/text  |           2; 02; Tue; Tuesday; T           |
|   F    |       week-of-month        |    number    |                     3                      |
|   a    |        am-pm-of-day        |     text     |                     PM                     |
|   h    | clock-hour-of-am-pm (1-12) |    number    |                     12                     |
|   K    |    hour-of-am-pm (0-11)    |    number    |                     0                      |
|   k    | clock-hour-of-am-pm (1-24) |    number    |                     0                      |
|   H    |     hour-of-day (0-23)     |    number    |                     0                      |
|   m    |       minute-of-hour       |    number    |                     30                     |
|   s    |      second-of-minute      |    number    |                     55                     |
|   S    |     fraction-of-second     |   fraction   |                    978                     |
|   A    |        milli-of-day        |    number    |                    1234                    |
|   n    |       nano-of-second       |    number    |                 987654321                  |
|   N    |        nano-of-day         |    number    |                 1234000000                 |
|   V    |        time-zone ID        |   zone-id    |       America/Los_Angeles; Z; -08:30       |
|   z    |       time-zone name       |  zone-name   |         Pacific Standard Time; PST         |
|   O    |   localized zone-offset    |   offset-O   |        GMT+8; GMT+08:00; UTC-08:00;        |
|   X    |  zone-offset *Z* for zero  |   offset-X   | Z; -08; -0830; -08:30; -083015; -08:30:15; |
| Symbol |          Meaning           | Presentation |                  Examples                  |
|   G    |            era             |     text     |             AD; Anno Domini; A             |
|   u    |            year            |     year     |                  2004; 04                  |
|   y    |        year-of-era         |     year     |                  2004; 04                  |
|   D    |        day-of-year         |    number    |                    189                     |
|  M/L   |       month-of-year        | number/text  |            7; 07; Jul; July; J             |
|   d    |        day-of-month        |    number    |                     10                     |

&emsp;&emsp;pattern字母的数量决定了format。

- Text
  - 文本样式基于使用的pattern letter数量来确定。少于4个pattern letter将使用短格式。恰好4个pattern letter将使用全格式。恰好5个pattern letter将使用窄格式。pattern letter `L`、`c`和`q`指定文本样式的独立形式。
- Number
  - 如果字母数量为一，则值使用最少的数字输出且不填充。否则，数字的数量用作输出字段的宽度，必要时使用零填充。以下pattern letter对字母的数量有限制。c和F只能指定一个字母。`d`、`H`、`h`、`K`、`k`、`m`和`s`最多可以指定两个字母。D最多可以指定三个字母。
- Number/Text
  - 如果pattern letter的数量为3个或更多，使用上述文本规则。否则使用上述数字规则。
- Fraction
  - 以秒的分数形式输出纳秒字段。纳秒值有九位数字，因此pattern letter的数量从1到9。如果少于9，则纳秒值被截断，只输出最重要的数字。
- Year
  - 字母的数量决定了填充使用的最小字段宽度。如果字母数量为两个，则使用缩减的两位数字形式。对于打印，这将输出最右边的两位数字。对于解析，这将使用2000年的基准值进行解析，结果在2000年到2099年之间。如果字母数量少于四个（但不是两个），则只有在年份为负时才输出符号，按照`SignStyle.NORMAL`。否则，如果超过填充宽度，符号将被输出，按照`SignStyle.EXCEEDS_PAD`。
- ZoneId
  - 输出时区ID，例如`Europe/Paris`。如果字母数量为两个，则输出时区ID。任何其他数量的字母都会抛出`IllegalArgumentException`。
- Zone names
  - 输出时区ID的显示名称。如果字母数量为一、二或三，则输出短名称。如果字母数量为四，则输出全名称。五个或更多字母会抛出`IllegalArgumentException`。
- Offset X and x
  - 根据pattern letter的数量格式化偏移。一个字母仅输出小时数，例如`+01`，除非分钟非零，在这种情况下也会输出分钟，例如`+0130`。两个字母输出小时和分钟，没有冒号，例如`+0130`。三个字母输出小时和分钟，有冒号，例如`+01:30`。四个字母输出小时、分钟和可选的秒数，没有冒号，例如`+013015`。五个字母输出小时、分钟和可选的秒数，有冒号，例如`+01:30:15`。六个或更多字母会抛出`IllegalArgumentException`。pattern letter`X`（大写）在偏移为零时输出Z，而pattern letter`x`（小写）在偏移为零时输出`+00`、`+0000`或`+00:00`。
- Offset O
  - 根据pattern letter的数量格式化本地化偏移。一个字母输出本地化偏移的简短形式，即本地化偏移文本，例如`GMT`，小时数不带前导零，可选的2位数字的分`钟和秒（如果非零），并且有冒号，例如`GMT+8`。四个字母输出完整形式，即本地化偏移文本，如`GMT`，带有2位数字的小时和分钟字段，可选的秒字段（如果非零），并且有冒号，例如`GMT+08:00`。任何其他数量的字母都会抛出`IllegalArgumentException`。
- Offset Z
  - 根据pattern letter的数量格式化偏移。一、两或三个字母输出小时和分钟，没有冒号，例如+0130。偏移为零时输出将是`+0000`。四个字母输出本地化偏移的完整形式，等同于Offset-O的四个字母。如果偏移为零，输出将是相应的本地化偏移文本。五个字母输出小时、分钟，如果非零则带有可选的秒数，有冒号。如果偏移为零，则输出`Z`。六个或更多字母会抛出IllegalArgumentException。

- Optional section
  - 可选部分标记的工作方式就像调用`DateTimeFormatterBuilder.optionalStart()`和`DateTimeFormatterBuilder.optionalEnd()`一样。

- Pad modifier
  - 用空格填充紧随其后的模式。填充宽度由pattern letter的数量决定。这与调用`DateTimeFormatterBuilder.padNext(int)`相同。

##### Time zone in date range aggregations

&emsp;&emsp;可以通过`time_zone`参数将日期从UTC转化到另一个时区。

&emsp;&emsp;你也可以用 ISO 8601 UTC 偏移来指定时区（`+01:00 or -08:00`）或者 IANA time zone ID，比如`America/Los_Angeles`就是在TZ 数据库中使用的标识符。

&emsp;&emsp;`time_zone`参数也用于date math表达式中的舍入操作。举个例子，如果你想要按照中欧时间（CET）时区来舍入到当天的开始，你可以这么做：

```text
POST /sales/_search?size=0
{
   "aggs": {
       "range": {
           "date_range": {
               "field": "date",
               "time_zone": "CET",
               "ranges": [
                  { "to": "2016/02/01" }, 
                  { "from": "2016/02/01", "to" : "now/d" }, 
                  { "from": "now/d" }
              ]
          }
      }
   }
}
```

&emsp;&emsp;第8行，这个日期将被转化为`2016-02-01T00:00:00.000+01:00`
&emsp;&emsp;第9行，`now/d`将被舌入到欧时间（CET）时区当天的开始

##### Keyed Response

&emsp;&emsp;将`keyed`设置为`true`，每个分桶名会关联一个唯一的string key，响应中所有的分桶用hash展示而不是用数组：

```text
POST /sales/_search?size=0
{
  "aggs": {
    "range": {
      "date_range": {
        "field": "date",
        "format": "MM-yyy",
        "ranges": [
          { "to": "now-10M/M" },
          { "from": "now-10M/M" }
        ],
        "keyed": true
      }
    }
  }
}
```

&emsp;&emsp;相应：

```text
{
  ...
  "aggregations": {
    "range": {
      "buckets": {
        "*-10-2015": {
          "to": 1.4436576E12,
          "to_as_string": "10-2015",
          "doc_count": 7
        },
        "10-2015-*": {
          "from": 1.4436576E12,
          "from_as_string": "10-2015",
          "doc_count": 0
        }
      }
    }
  }
}
```

&emsp;&emsp;也可以为每一个范围自定义key：

```text
POST /sales/_search?size=0
{
  "aggs": {
    "range": {
      "date_range": {
        "field": "date",
        "format": "MM-yyy",
        "ranges": [
          { "from": "01-2015", "to": "03-2015", "key": "quarter_01" },
          { "from": "03-2015", "to": "06-2015", "key": "quarter_02" }
        ],
        "keyed": true
      }
    }
  }
}
```

&emsp;&emsp;响应：

```text
{
  ...
  "aggregations": {
    "range": {
      "buckets": {
        "quarter_01": {
          "from": 1.4200704E12,
          "from_as_string": "01-2015",
          "to": 1.425168E12,
          "to_as_string": "03-2015",
          "doc_count": 5
        },
        "quarter_02": {
          "from": 1.425168E12,
          "from_as_string": "03-2015",
          "to": 1.4331168E12,
          "to_as_string": "06-2015",
          "doc_count": 2
        }
      }
    }
  }
}
```

#### Diversified sampler aggregation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-diversified-sampler-aggregation.html)

#### Geo-distance aggregation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/current/search-aggregations-bucket-geodistance-aggregation.html)

&emsp;&emsp;它属于multi-bucket aggregation。应用在[geo_point](#Geopoint field type)类型的字段上，概念上跟[range](#Range aggregation) aggregation十分相似。用户可以定义一个名为`origin`的点以及距离范围集合。该聚合根据`origin`中指定的 `point`计算每一个文档跟这个点的的距离，然后根据范围（如果文档跟`origin`的距离属于定义的某个距离范围，那么就落入到对应的分桶中）决定文档落在哪个分桶中。

```text
PUT /museums
{
  "mappings": {
    "properties": {
      "location": {
        "type": "geo_point"
      }
    }
  }
}

POST /museums/_bulk?refresh
{"index":{"_id":1}}
{"location": "POINT (4.912350 52.374081)", "name": "NEMO Science Museum"}
{"index":{"_id":2}}
{"location": "POINT (4.901618 52.369219)", "name": "Museum Het Rembrandthuis"}
{"index":{"_id":3}}
{"location": "POINT (4.914722 52.371667)", "name": "Nederlands Scheepvaartmuseum"}
{"index":{"_id":4}}
{"location": "POINT (4.405200 51.222900)", "name": "Letterenhuis"}
{"index":{"_id":5}}
{"location": "POINT (2.336389 48.861111)", "name": "Musée du Louvre"}
{"index":{"_id":6}}
{"location": "POINT (2.327000 48.860000)", "name": "Musée d'Orsay"}

POST /museums/_search?size=0
{
  "aggs": {
    "rings_around_amsterdam": {
      "geo_distance": {
        "field": "location",
        "origin": "POINT (4.894 52.3760)",
        "ranges": [
          { "to": 100000 },
          { "from": 100000, "to": 300000 },
          { "from": 300000 }
        ]
      }
    }
  }
}
```

&emsp;&emsp;响应：

```text
{
  ...
  "aggregations": {
    "rings_around_amsterdam": {
      "buckets": [
        {
          "key": "*-100000.0",
          "from": 0.0,
          "to": 100000.0,
          "doc_count": 3
        },
        {
          "key": "100000.0-300000.0",
          "from": 100000.0,
          "to": 300000.0,
          "doc_count": 1
        },
        {
          "key": "300000.0-*",
          "from": 300000.0,
          "doc_count": 2
        }
      ]
    }
  }
}
```

&emsp;&emsp;待聚合的字段必须是`geo_point`类型（只能在mapping中显示的指定该类型）。`geo_point`类型也可以包含一个geo_point字段的数组，在聚合时候所有的点都会考虑进去。`origing`的值可以有跟[geo_point](#Geopoint field type)一样的格式：

- 对象格式：`{ "lat" : 52.3760, "lon" : 4.894 }`。这是最安全的格式，因为显示的指明了`lat`和`lon`的值
- 字符串格式：`"52.3760, 4.894"` 。第一个数值是`lat`，第二个数值是`lon`
- 数组格式：`[4.894, 52.3760]`。基于 GeoJSON标准，第一个数值是`lon`，第二个数值是`lat`

&emsp;&emsp;默认情况下，距离单位是`m`（meters）但也可以是：`mi` (miles), `in` (inches), `yd` (yards), `km` (kilometers), `cm` (centimeters), `mm` (millimeters).

```text
POST /museums/_search?size=0
{
  "aggs": {
    "rings": {
      "geo_distance": {
        "field": "location",
        "origin": "POINT (4.894 52.3760)",
        "unit": "km", 
        "ranges": [
          { "to": 100 },
          { "from": 100, "to": 300 },
          { "from": 300 }
        ]
      }
    }
  }
}
```

&emsp;&emsp;第8行，按照千米进行计算

&emsp;&emsp;有两种计算模式：`arc`（弧度，默认计算方式）和`plane`（平面）。平面计算是最快但最不准确的。当你的搜索上下文是“狭窄的”且跨越较小的地理区域（约5公里）时，考虑使用平面计算。对于跨越非常大的区域（例如，跨洲搜索）的搜索，平面计算将返回更高的误差范围。距离计算类型可以使用`distance_type`参数来设置：

```text
POST /museums/_search?size=0
{
  "aggs": {
    "rings": {
      "geo_distance": {
        "field": "location",
        "origin": "POINT (4.894 52.3760)",
        "unit": "km",
        "distance_type": "plane",
        "ranges": [
          { "to": 100 },
          { "from": 100, "to": 300 },
          { "from": 300 }
        ]
      }
    }
  }
}
```

##### Keyed Response

&emsp;&emsp;将`keyed`设置为`true`，每个分桶名会关联一个唯一的string key，响应中所有的分桶用hash展示而不是用数组：

```text
POST /museums/_search?size=0
{
  "aggs": {
    "rings_around_amsterdam": {
      "geo_distance": {
        "field": "location",
        "origin": "POINT (4.894 52.3760)",
        "ranges": [
          { "to": 100000 },
          { "from": 100000, "to": 300000 },
          { "from": 300000 }
        ],
        "keyed": true
      }
    }
  }
}
```

&emsp;&emsp;响应：

```text
{
  ...
  "aggregations": {
    "rings_around_amsterdam": {
      "buckets": {
        "*-100000.0": {
          "from": 0.0,
          "to": 100000.0,
          "doc_count": 3
        },
        "100000.0-300000.0": {
          "from": 100000.0,
          "to": 300000.0,
          "doc_count": 1
        },
        "300000.0-*": {
          "from": 300000.0,
          "doc_count": 2
        }
      }
    }
  }
}
```

&emsp;&emsp;也可以为每一个范围指定自定义的key：

```text
POST /museums/_search?size=0
{
  "aggs": {
    "rings_around_amsterdam": {
      "geo_distance": {
        "field": "location",
        "origin": "POINT (4.894 52.3760)",
        "ranges": [
          { "to": 100000, "key": "first_ring" },
          { "from": 100000, "to": 300000, "key": "second_ring" },
          { "from": 300000, "key": "third_ring" }
        ],
        "keyed": true
      }
    }
  }
}
```

&emsp;&emsp;响应：

```text
{
  ...
  "aggregations": {
    "rings_around_amsterdam": {
      "buckets": {
        "first_ring": {
          "from": 0.0,
          "to": 100000.0,
          "doc_count": 3
        },
        "second_ring": {
          "from": 100000.0,
          "to": 300000.0,
          "doc_count": 1
        },
        "third_ring": {
          "from": 300000.0,
          "doc_count": 2
        }
      }
    }
  }
}
```


#### Filter aggregation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-filter-aggregation.html)

&emsp;&emsp;它属于single bucket aggregation（一个过滤条件只会生成一个分桶），将满足[query](#Query DSL)的文档落入到单个桶内。

```text
POST /sales/_search?size=0&filter_path=aggregations
{
  "aggs": {
    "avg_price": { "avg": { "field": "price" } },
    "t_shirts": {
      "filter": { "term": { "type": "t-shirt" } },
      "aggs": {
        "avg_price": { "avg": { "field": "price" } }
      }
    }
  }
}
```

&emsp;&emsp;这个例子尖酸了所有衣服的平均价格同时将`t-shirt`类型的衣服划分为一个桶，并且计算了桶内的平均价格。

&emsp;&emsp;响应如下：

```text
{
  "aggregations": {
    "avg_price": { "value": 140.71428571428572 },
    "t_shirts": {
      "doc_count": 3,
      "avg_price": { "value": 128.33333333333334 }
    }
  }
}
```

##### Use a top-level query to limit all aggregations

&emsp;&emsp;若要限制在查询运行时目标文档数量，可以使用一个top-level `query`。这样能加快带有sub-aggregation的`filter` aggregation。

&emsp;&emsp;使用下面的例子：

```text
POST /sales/_search?size=0&filter_path=aggregations
{
  "query": { "term": { "type": "t-shirt" } },
  "aggs": {
    "avg_price": { "avg": { "field": "price" } }
  }
}
```

&emsp;&emsp;而不是使用下面的例子：

```text
POST /sales/_search?size=0&filter_path=aggregations
{
  "aggs": {
    "t_shirts": {
      "filter": { "term": { "type": "t-shirt" } },
      "aggs": {
        "avg_price": { "avg": { "field": "price" } }
      }
    }
  }
}
```

##### Use the filters aggregation for multiple filters

&emsp;&emsp;若要使用多个filter对文档进行分组，可以使用[filters aggregation](#Filters aggregation)，它比使用多个`filter` aggregations更快。

&emsp;&emsp;例如应该这个例子：

```text
POST /sales/_search?size=0&filter_path=aggregations
{
  "aggs": {
    "f": {
      "filters": {
        "filters": {
          "hats": { "term": { "type": "hat" } },
          "t_shirts": { "term": { "type": "t-shirt" } }
        }
      },
      "aggs": {
        "avg_price": { "avg": { "field": "price" } }
      }
    }
  }
}
```

&emsp;&emsp;而不是使用这个例子：

```text
POST /sales/_search?size=0&filter_path=aggregations
{
  "aggs": {
    "hats": {
      "filter": { "term": { "type": "hat" } },
      "aggs": {
        "avg_price": { "avg": { "field": "price" } }
      }
    },
    "t_shirts": {
      "filter": { "term": { "type": "t-shirt" } },
      "aggs": {
        "avg_price": { "avg": { "field": "price" } }
      }
    }
  }
}
```

#### Filters aggregation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-filters-aggregation.html)

&emsp;&emsp;它属于multi-bucket aggregation，每一个分桶中的文档满足定义的[query](#Query DSL)。

&emsp;&emsp;例子：

```text
PUT /logs/_bulk?refresh
{ "index" : { "_id" : 1 } }
{ "body" : "warning: page could not be rendered" }
{ "index" : { "_id" : 2 } }
{ "body" : "authentication error" }
{ "index" : { "_id" : 3 } }
{ "body" : "warning: connection timed out" }

GET logs/_search
{
  "size": 0,
  "aggs" : {
    "messages" : {
      "filters" : {
        "filters" : {
          "errors" :   { "match" : { "body" : "error"   }},
          "warnings" : { "match" : { "body" : "warning" }}
        }
      }
    }
  }
}
```

&emsp;&emsp;上面的例子中，我们对日志信息进行解析。该聚合会构建两类日志信息的分桶，一个是包含`error`的，另一个是包含`warning`的。

&emsp;&emsp;响应：

```text
{
  "took": 9,
  "timed_out": false,
  "_shards": ...,
  "hits": ...,
  "aggregations": {
    "messages": {
      "buckets": {
        "errors": {
          "doc_count": 1
        },
        "warnings": {
          "doc_count": 2
        }
      }
    }
  }
}
```

##### Anonymous filters

&emsp;&emsp;可以将多个过滤条件作为一个filters数组。如下所示：

```text
GET logs/_search
{
  "size": 0,
  "aggs" : {
    "messages" : {
      "filters" : {
        "filters" : [
          { "match" : { "body" : "error"   }},
          { "match" : { "body" : "warning" }}
        ]
      }
    }
  }
}
```

&emsp;&emsp;响应中两个分桶的先后顺序跟请求中定义的顺序一样。这个例子的响应如下：

```text
{
  "took": 4,
  "timed_out": false,
  "_shards": ...,
  "hits": ...,
  "aggregations": {
    "messages": {
      "buckets": [
        {
          "doc_count": 1
        },
        {
          "doc_count": 2
        }
      ]
    }
  }
}
```

##### `Other` Bucket

&emsp;&emsp;可以设置参数`other_bucket`将不满足所有过滤条件的文档落入到一个桶中并添加到响应中。这个参数的值可以是：

- false：不计算`other` bucket
- true：如果请求中过滤条件指定了名字，那么在响应中`other bucket`的名字默认是`_other_`，如果请求中过滤条件没有指定名字（Anonymous filters），那么`other bucket`就是数组中的最后一个（上面的例子中，没有给过滤条件指定名字）

&emsp;&emsp;可以定义`other_bucket_key`参数指定`other_bucket`的名字，而不是使用默认的`_other_`（如果过滤条件没有指定名字，设置这个参数不会生效）。设置这个参数会默认将`other_bucket`设置为`true`。

&emsp;&emsp;下面的片段中`other bucket`的名字设置为`other_messages`：

```text
PUT logs/_doc/4?refresh
{
  "body": "info: user Bob logged out"
}

GET logs/_search
{
  "size": 0,
  "aggs" : {
    "messages" : {
      "filters" : {
        "other_bucket_key": "other_messages",
        "filters" : {
          "errors" :   { "match" : { "body" : "error"   }},
          "warnings" : { "match" : { "body" : "warning" }}
        }
      }
    }
  }
}
```

&emsp;&emsp;响应差不多是这样的：

```text
{
  "took": 3,
  "timed_out": false,
  "_shards": ...,
  "hits": ...,
  "aggregations": {
    "messages": {
      "buckets": {
        "errors": {
          "doc_count": 1
        },
        "warnings": {
          "doc_count": 2
        },
        "other_messages": {
          "doc_count": 1
        }
      }
    }
  }
}
```

#### Geohash grid aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-geohashgrid-aggregation.html)

#### Geohex grid aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-geohexgrid-aggregation.html)

#### Geotile grid aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-geotilegrid-aggregation.html)

#### Global aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-global-aggregation.html)

#### Histogram aggregation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-histogram-aggregation.html)

&emsp;&emsp;它属于multi-bucket Values Source aggregation，作用到从文档中提取出的数值类型或者数值类型范围值。他动态在值之间构建固定大小（bucket size，也就是间隔interval）。例如，如果文档中有一个名为价格的字段（数值），我们可以将这个聚合自动的配置为`5`的间隔（因为价格可以是`5$`）当执行聚合时，每一个文档中的价格会被评估并且下舍入（rounded down）到最近的分桶。比如说价格为`32`并且间隔为`5`，那么`32`被下舍入到`30`，因此"落入"到key为`30`的分桶中。正式点的表达就是，使用了以下的舌入函数：

```text
bucket_key = Math.floor((value - offset) / interval) * interval + offset
```

&emsp;&emsp;对于range values，一篇文档可以落入到多个分桶中。第一个分桶根据range value的下界值计算，计算方式跟单值是一样的。最后一个分桶根据range value的上界值计算。且range value中的所有被计入从首个桶到最后一个桶之间的所有桶内，包括这两个桶。

&emsp;&emsp;间隔`interval`必须是正整数，并且`offset`必须是`[0, interval)`之间的整数（左闭右开）。

&emsp;&emsp;下面的片段基于`price`字段以及间隔`50`进行分桶：

```text
POST /sales/_search?size=0
{
  "aggs": {
    "prices": {
      "histogram": {
        "field": "price",
        "interval": 50
      }
    }
  }
}
```

&emsp;&emsp;响应差不多应该是：

```text
{
  ...
  "aggregations": {
    "prices": {
      "buckets": [
        {
          "key": 0.0,
          "doc_count": 1
        },
        {
          "key": 50.0,
          "doc_count": 1
        },
        {
          "key": 100.0,
          "doc_count": 0
        },
        {
          "key": 150.0,
          "doc_count": 2
        },
        {
          "key": 200.0,
          "doc_count": 3
        }
      ]
    }
  }
}
```

##### Minimum document count

&emsp;&emsp;上面的响应中显示在`[100, 150)`的分桶中没有文档有这个价格。默认情况下会使用空的分桶来填充。不过有`min_doc_count`参数使得分桶中的文档数量必须高于该值，那么就不会返回空的分桶。

```text
POST /sales/_search?size=0
{
  "aggs": {
    "prices": {
      "histogram": {
        "field": "price",
        "interval": 50,
        "min_doc_count": 1
      }
    }
  }
}
```

&emsp;&emsp;响应：

```text

  ...
  "aggregations": {
    "prices": {
      "buckets": [
        {
          "key": 0.0,
          "doc_count": 1
        },
        {
          "key": 50.0,
          "doc_count": 1
        },
        {
          "key": 150.0,
          "doc_count": 2
        },
        {
          "key": 200.0,
          "doc_count": 3
        }
      ]
    }
  }
}
```

&emsp;&emsp;默认情况下，`histogram`根据数据自身情况返回所有的分桶，拥有最小值的文档决定了最小的分桶（第一个分桶，key最小的分桶），拥有最大值的文档决定了最大的分桶（最后一个分桶，key最大的分桶）。通常在要求返回空的分桶时，这样会造成一定的困惑，特别是在数据被过滤时。

&emsp;&emsp;要理解上面这段话，可以看下这个例子：

&emsp;&emsp;比如说你的请求中要求过滤出值为`0`到`500`的所有文档，另外你还想要使用间隔为`50`的histogram的来划分数据。因为你想要获取所有的分桶，包括空的分桶，所以你同时还指定了`min_doc_count: 0`。如果所有的文档的价格都大于`100`，那么第一个分桶的key将会是`100`。这可能会给你带来困惑，因为你也想要获取在`0-100`之间空的分桶。

&emsp;&emsp;通过`extended_bounds`参数，你可以指定`min`和`max`（即使某些区间没有对应满足的文档）来构建分桶。使用`extended_bounds`只有当`min_doc_count`为`0`时才有意义（因为如果`min_doc_count`大于`0`，不会返回空的分桶）。

&emsp;&emsp;注意的是`extended_bounds`不是用来过滤分桶的，也就说，如果`extended_bounds.min`大于文档中提取出的值，仍然会根据文档中的最小值来决定第一个分桶（`extended_bounds.max`小于文档中的最大值也是一样处理）。若要过滤分桶，应该在histogram中嵌套一个range `filter` aggregation并使用合适的`from/to`。

&emsp;&emsp;例子：

```text
POST /sales/_search?size=0
{
  "query": {
    "constant_score": { "filter": { "range": { "price": { "to": "500" } } } }
  },
  "aggs": {
    "prices": {
      "histogram": {
        "field": "price",
        "interval": 50,
        "extended_bounds": {
          "min": 0,
          "max": 500
        }
      }
    }
  }
}
```

&emsp;&emsp;分桶会根据返回的文档的值来聚合范围。也就是说响应中可能包含在query范围外的分桶。比如说，如果你的query查看大于100，并且你有50到150的文档，并且间隔为50，那么文档落入3个分桶中：50、100以及150。你可以理解为query跟aggregation的步骤是独立的。query用来选择文档，而聚合分桶不会考虑这些文档是否被query过滤。见[note on bucketing range fields](#Subtleties of bucketing range fields)查看更多信息以及例子。

&emsp;&emsp;`hard_bounds`对应于`extended_bounds`可以限制histogram中分桶的范围。在开放的[data ranges](#Range field types)中使用这个参数非常有用，因为它会导致大量的分桶。

&emsp;&emsp;例子：

```text
POST /sales/_search?size=0
{
  "query": {
    "constant_score": { "filter": { "range": { "price": { "to": "500" } } } }
  },
  "aggs": {
    "prices": {
      "histogram": {
        "field": "price",
        "interval": 50,
        "hard_bounds": {
          "min": 100,
          "max": 200
        }
      }
    }
  }
}
```

&emsp;&emsp;这个例子中尽管query中指定了`to 500`，histogram只有两个分桶，分别是100跟150，其他的分桶会被忽略，即使有文档会落入到这些分桶内并且应该呈现出来。

##### Order

&emsp;&emsp;默认情况下分桶按照他们的key升序排序，不过可以通过`order`来控制这个行为。它跟[Terms Aggregation](#Terms aggregation)中的`order`有相同的功能。

##### Offset

&emsp;&emsp;默认情况下，分桶的key从0开始，然后以间隔的等距一步步继续，例如，如果间隔是10，前三个桶（假设它们内部有数据）将是[0, 10)，[10, 20)，[20, 30)。可以通过使用offset来移动桶边界。

&emsp;&emsp;这可以通过一个例子最好地说明。如果有10个文档，其值范围从5到14，使用10作为间隔将导致两个各含5个文档的桶。如果使用额外的`offset`为5，那么将只有一个单一的桶[5, 15)，包含所有10个文档。

##### Response Format

&emsp;&emsp;默认情况下分桶是一个有序的数组。可以要求响应中用分桶的key值作为hash的key。

```text
POST /sales/_search?size=0
{
  "aggs": {
    "prices": {
      "histogram": {
        "field": "price",
        "interval": 50,
        "keyed": true
      }
    }
  }
}
```

&emsp;&emsp;响应：

```text
{
  ...
  "aggregations": {
    "prices": {
      "buckets": {
        "0.0": {
          "key": 0.0,
          "doc_count": 1
        },
        "50.0": {
          "key": 50.0,
          "doc_count": 1
        },
        "100.0": {
          "key": 100.0,
          "doc_count": 0
        },
        "150.0": {
          "key": 150.0,
          "doc_count": 2
        },
        "200.0": {
          "key": 200.0,
          "doc_count": 3
        }
      }
    }
  }
}
```

##### Missing value

&emsp;&emsp;`missing`参数定义了文档中缺失被聚合的字段时，如何进行聚合。默认这些文档会被忽略但也可以把这些文档看成有一个值。

```text
POST /sales/_search?size=0
{
  "aggs": {
    "quantity": {
      "histogram": {
        "field": "quantity",
        "interval": 10,
        "missing": 0 
      }
    }
  }
}

```

&emsp;&emsp;第7行，没有`quantity`字段的文档会落入相同的分桶中，并且把这些文档看成有`quantity`字段，并且值为`0`

##### Histogram fields（Histogram aggregation）

&emsp;&emsp;对于[histogram fields](#Histogram field type)进行histogram aggregation时，将计算每个间隔的总数。

&emsp;&emsp;比如对以下不同网络中已经预先聚合过的延迟指标进行histogram aggregation时：

```text
PUT metrics_index/_doc/1
{
  "network.name" : "net-1",
  "latency_histo" : {
      "values" : [1, 3, 8, 12, 15],
      "counts" : [3, 7, 23, 12, 6]
   }
}

PUT metrics_index/_doc/2
{
  "network.name" : "net-2",
  "latency_histo" : {
      "values" : [1, 6, 8, 12, 14],
      "counts" : [8, 17, 8, 7, 6]
   }
}

POST /metrics_index/_search?size=0
{
  "aggs": {
    "latency_buckets": {
      "histogram": {
        "field": "latency_histo",
        "interval": 5
      }
    }
  }
}
```

&emsp;&emsp;`histogram` aggregation会根据`values`累计每一个间隔（`counts`）的数量并返回以下结果：

```text
{
  ...
  "aggregations": {
    "prices": {
      "buckets": [
        {
          "key": 0.0,
          "doc_count": 18
        },
        {
          "key": 5.0,
          "doc_count": 48
        },
        {
          "key": 10.0,
          "doc_count": 25
        },
        {
          "key": 15.0,
          "doc_count": 6
        }
      ]
    }
  }
}
```

> IMPORTANT： Histogram aggregation是一种bucket aggregation，它将文档分配到桶中，而不是像metric aggregation那样对字段进行计算。每个桶代表了一组文档，可以在其上运行sub-aggregation。另一方面，[Histogram field](#Histogram field type)是一种预聚合字段，表示单个字段内的多个值：数值数据的桶和每个桶的项目/文档计数。这种在histogram聚合预期输入（期望原始文档）和Histogram field（提供摘要信息）之间的不匹配，限制了聚合的结果仅为每个桶的文档计数。
>因此，当在Histogram field上执行直方图聚合时，不允许进行sub-aggregation。

#### IP prefix aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-range-aggregation.html)

#### IP range aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-iprange-aggregation.html)

#### Missing aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-missing-aggregation.html)

&emsp;&emsp;它属于single bucket aggregation，它创建一个包含当前文档集上下文（根据query过滤后的文档集合）中所有缺少字段值的文档的桶（实际上，是缺少一个字段或已设置了配置的NULL值的文档）。这种聚合器通常与其他字段数据桶聚合器（如range aggregation）结合使用，以返回无法因缺少字段数据值而放置在任何其他桶中的所有文档的信息。

#### Multi Terms aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-multi-terms-aggregation.html)

&emsp;&emsp;它属于multi-bucket value source based aggregation，分桶自动构建（每一个分桶键为多个字段的字段值的组合）。multi terms aggregation跟[terms aggregation](#Terms aggregation)很类似，然而大多数情况下比terms aggregation慢并且会消耗更多的内存。因此如果经常需要使用固定的字段组合（mapping中定义一个runtime field），那么可以将其字段组合在索引时作为单独一个字段，然后使用terms aggregation。

#### Nested aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-nested-aggregation.html)

&emsp;&emsp;它属于特殊的single bucket aggregation，用于对嵌套文档（nested document）聚合。

#### Parent aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-parent-aggregation.html)

#### Random sampler aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-random-sampler-aggregation.html)

#### Range aggregation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-range-aggregation.html)

&emsp;&emsp;它属于multi-bucket value source based aggregation，允许用户定义一个范围集合，每一个范围代表一个分桶。在聚合过程中，从每个文档中提取出的值会根据分桶对应的范围进行检查，然后把文档分到这个桶内。注意的是在这个聚合中，每一个范围的`from`是包含并且`to`是不包含的。

&emsp;&emsp;例如：

```text
GET sales/_search
{
  "aggs": {
    "price_ranges": {
      "range": {
        "field": "price",
        "ranges": [
          { "to": 100.0 },
          { "from": 100.0, "to": 200.0 },
          { "from": 200.0 }
        ]
      }
    }
  }
}
```

&emsp;&emsp;响应：

```text
{
  ...
  "aggregations": {
    "price_ranges": {
      "buckets": [
        {
          "key": "*-100.0",
          "to": 100.0,
          "doc_count": 2
        },
        {
          "key": "100.0-200.0",
          "from": 100.0,
          "to": 200.0,
          "doc_count": 2
        },
        {
          "key": "200.0-*",
          "from": 200.0,
          "doc_count": 3
        }
      ]
    }
  }
}
```

##### Keyed Response

&emsp;&emsp;将`keyed`设置为`true`，分桶的key_as_string的值将作为key，响应中所有的分桶用hash展示而不是用数组：

```text
GET sales/_search
{
  "aggs": {
    "price_ranges": {
      "range": {
        "field": "price",
        "keyed": true,
        "ranges": [
          { "to": 100 },
          { "from": 100, "to": 200 },
          { "from": 200 }
        ]
      }
    }
  }
}
```

&emsp;&emsp;响应：

```text
{
  ...
  "aggregations": {
    "price_ranges": {
      "buckets": {
        "*-100.0": {
          "to": 100.0,
          "doc_count": 2
        },
        "100.0-200.0": {
          "from": 100.0,
          "to": 200.0,
          "doc_count": 2
        },
        "200.0-*": {
          "from": 200.0,
          "doc_count": 3
        }
      }
    }
  }
}

```

&emsp;&emsp;也可以为每一个范围自定义key：

```text
GET sales/_search
{
  "aggs": {
    "price_ranges": {
      "range": {
        "field": "price",
        "keyed": true,
        "ranges": [
          { "key": "cheap", "to": 100 },
          { "key": "average", "from": 100, "to": 200 },
          { "key": "expensive", "from": 200 }
        ]
      }
    }
  }
}
```

&emsp;&emsp;响应：

```text
{
  ...
  "aggregations": {
    "price_ranges": {
      "buckets": {
        "cheap": {
          "to": 100.0,
          "doc_count": 2
        },
        "average": {
          "from": 100.0,
          "to": 200.0,
          "doc_count": 2
        },
        "expensive": {
          "from": 200.0,
          "doc_count": 3
        }
      }
    }
  }
}
```

##### Script

&emsp;&emsp;如果文档中的数据不完全满足你聚合的目的，可以使用[runtime filed](#Runtime fields)。这个例子中，你如果要应用一个特殊的货币汇率：

```text
GET sales/_search
{
  "runtime_mappings": {
    "price.euros": {
      "type": "double",
      "script": {
        "source": """
          emit(doc['price'].value * params.conversion_rate)
        """,
        "params": {
          "conversion_rate": 0.835526591
        }
      }
    }
  },
  "aggs": {
    "price_ranges": {
      "range": {
        "field": "price.euros",
        "ranges": [
          { "to": 100 },
          { "from": 100, "to": 200 },
          { "from": 200 }
        ]
      }
    }
  }
}
```

##### Sub Aggregations

&emsp;&emsp;下面的例子中，不仅仅将文档分到桶内，还同时统计了在每一个价格区间内价格的统计信息。

```text
GET sales/_search
{
  "aggs": {
    "price_ranges": {
      "range": {
        "field": "price",
        "ranges": [
          { "to": 100 },
          { "from": 100, "to": 200 },
          { "from": 200 }
        ]
      },
      "aggs": {
        "price_stats": {
          "stats": { "field": "price" }
        }
      }
    }
  }
}
```

&emsp;&emsp;响应：

```text
{
  ...
  "aggregations": {
    "price_ranges": {
      "buckets": [
        {
          "key": "*-100.0",
          "to": 100.0,
          "doc_count": 2,
          "price_stats": {
            "count": 2,
            "min": 10.0,
            "max": 50.0,
            "avg": 30.0,
            "sum": 60.0
          }
        },
        {
          "key": "100.0-200.0",
          "from": 100.0,
          "to": 200.0,
          "doc_count": 2,
          "price_stats": {
            "count": 2,
            "min": 150.0,
            "max": 175.0,
            "avg": 162.5,
            "sum": 325.0
          }
        },
        {
          "key": "200.0-*",
          "from": 200.0,
          "doc_count": 3,
          "price_stats": {
            "count": 3,
            "min": 200.0,
            "max": 200.0,
            "avg": 200.0,
            "sum": 600.0
          }
        }
      ]
    }
  }
}
```

###### Histogram fields

&emsp;&emsp;对[Histogram fields](#Histogram field type)运行range aggregation会计算每一个配置的范围的`counts`数量。

&emsp;&emsp;这一过程不会在histogram之间进行插值。因此，可能存在一个范围位于两个histogram的值“之间”，导致该范围桶的文档计数为零。

&emsp;&emsp;例如，对存储不同网络的延迟指标（以毫秒为单位）的预聚合直方图的索引执行范围聚合。

```text
PUT metrics_index
{
  "mappings": {
    "properties": {
      "network": {
        "properties": {
          "name": {
            "type": "keyword"
          }
        }
      },
      "latency_histo": {
         "type": "histogram"
      }
    }
  }
}

PUT metrics_index/_doc/1?refresh
{
  "network.name" : "net-1",
  "latency_histo" : {
      "values" : [1, 3, 8, 12, 15],
      "counts" : [3, 7, 23, 12, 6]
   }
}

PUT metrics_index/_doc/2?refresh
{
  "network.name" : "net-2",
  "latency_histo" : {
      "values" : [1, 6, 8, 12, 14],
      "counts" : [8, 17, 8, 7, 6]
   }
}

GET metrics_index/_search?size=0&filter_path=aggregations
{
  "aggs": {
    "latency_ranges": {
      "range": {
        "field": "latency_histo",
        "ranges": [
          {"to": 2},
          {"from": 2, "to": 3},
          {"from": 3, "to": 10},
          {"from": 10}
        ]
      }
    }
  }
}
```

&emsp;&emsp;`range` aggregation根据`values`对将每一个范围内的`counts`进行累加并返回以下结果：

```text
{
  "aggregations": {
    "latency_ranges": {
      "buckets": [
        {
          "key": "*-2.0",
          "to": 2.0,
          "doc_count": 11
        },
        {
          "key": "2.0-3.0",
          "from": 2.0,
          "to": 3.0,
          "doc_count": 0
        },
        {
          "key": "3.0-10.0",
          "from": 3.0,
          "to": 10.0,
          "doc_count": 55
        },
        {
          "key": "10.0-*",
          "from": 10.0,
          "doc_count": 31
        }
      ]
    }
  }
}
```

> IMPORTANT： Histogram aggregation是一种bucket aggregation，它将文档分配到桶中，而不是像metric aggregation那样对字段进行计算。每个桶代表了一组文档，可以在其上运行sub-aggregation。另一方面，[Histogram field](#Histogram field type)是一种预聚合字段，表示单个字段内的多个值：数值数据的桶和每个桶的项目/文档计数。这种在histogram聚合预期输入（期望原始文档）和Histogram field（提供摘要信息）之间的不匹配，限制了聚合的结果仅为每个桶的文档计数。
>因此，当在Histogram field上执行range aggregation时，不允许进行sub-aggregation。


#### Rare terms aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-rare-terms-aggregation.html)

#### Reverse nested aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-reverse-nested-aggregation.html)

#### Sampler aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-sampler-aggregation.html)

#### Significant terms aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-significantterms-aggregation.html)

#### Significant text aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-significanttext-aggregation.html)

#### Terms aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-terms-aggregation.html)

&emsp;&emsp;它属于multi-bucket value source based aggregation，分桶自动构建，每一种字段值作为一个分桶key。

##### Execution hint

#### Variable width histogram aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-variablewidthhistogram-aggregation.html)

#### Subtleties of bucketing range fields
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-range-field-note.html)


### Metrics aggregations
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-metrics-stats-aggregation.html)

&emsp;&emsp;聚合家族中的这类聚合基于以某种方式从正在被聚合的文档中提取的值来计算指标。提取的值通常来自文档中的字段值，但也可以由脚本生成。

&emsp;&emsp;数值类的metric aggregation是一种特殊的聚合，它们输出的结果是数值类型的值。一些聚合的输出是单个数值类型的指标（比如`avg`）并且称为`single-value numeric metrics aggregation`，其他生成多个指标（比如`stats`）的聚合称为`multi-value numeric metrics aggregation`。这两者的区别在于，当这些聚合作为某些bucket aggregation的直接sub-aggregation时扮演为不同的角色（一些bucket aggregation允许你根据每个桶中的数值指标对返回的桶进行排序）

#### Avg aggregation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-metrics-avg-aggregation.html)

&emsp;&emsp;单值的指标聚合（Single-value metric aggregation）计算是从被聚合的文档中提取出数值的平均值。可以指定文档中数值类型的字段（numeric filed）或者[histogram字段](#Histogram field type)用于avg aggregation。

&emsp;&emsp;假设下面的文档描述的是学生的考试成绩（0~100分），我们可以使用下面的请求计算考试成绩平均值：

```text
POST /exams/_search?size=0
{
  "aggs": {
    "avg_grade": { "avg": { "field": "grade" } }
  }
}
```

&emsp;&emsp;上面的聚合会计算所有文档下的平均成绩。聚合的类型是`avg`并且`field`定义为一个数值类型的字段，计算这个字段的平均值。上述请求会返回下面的内容：

```text
{
  ...
  "aggregations": {
    "avg_grade": {
      "value": 75.0
    }
  }
}
```

&emsp;&emsp;聚合的名称avg_grade作为一个key，我们可以从返回的响应中读取这个字段的值获取聚合结果。

##### Script

&emsp;&emsp;如果说这次的考试难度非常大，你需要做一个成绩修正（grade correction）。在[runtime field](#Runtime fields)中进行"平均化"处理来得到一个修正后的平均值：

```text
POST /exams/_search?size=0
{
  "runtime_mappings": {
    "grade.corrected": {
      "type": "double",
      "script": {
        "source": "emit(Math.min(100, doc['grade'].value * params.correction))",
        "params": {
          "correction": 1.2
        }
      }
    }
  },
  "aggs": {
    "avg_corrected_grade": {
      "avg": {
        "field": "grade.corrected"
      }
    }
  }
}
```

##### Missing value

&emsp;&emsp;当文档缺失聚合字段时，`missing`参数定义了在这篇文档中聚合字段的值。默认情况下，它们会被忽略，但是可以将它们视为具有某个值的文档。

```text
POST /exams/_search?size=0
{
  "aggs": {
    "grade_avg": {
      "avg": {
        "field": "grade",
        "missing": 10     
      }
    }
  }
}
```

&emsp;&emsp;第7行，没有`grade`字段的文档被划分到相同分桶中并且认为`grade`的值为10。

##### Histogram fields

&emsp;&emsp;如果对[histogram](#Histogram field type)字段进行聚合计算，聚合的结果会结合`value`数组中的值以及对应在`count`数组中的数量进行计算。

&emsp;&emsp;例如，下面的索引存储了预先聚合好的、不同网络的延迟指标直方图：

```text
PUT metrics_index/_doc/1
{
  "network.name" : "net-1",
  "latency_histo" : {
      "values" : [0.1, 0.2, 0.3, 0.4, 0.5], 
      "counts" : [3, 7, 23, 12, 6] 
   }
}

PUT metrics_index/_doc/2
{
  "network.name" : "net-2",
  "latency_histo" : {
      "values" :  [0.1, 0.2, 0.3, 0.4, 0.5], 
      "counts" : [8, 17, 8, 7, 6] 
   }
}

POST /metrics_index/_search?size=0
{
  "aggs": {
    "avg_latency":
      { "avg": { "field": "latency_histo" }
    }
  }
}
```

&emsp;&emsp;在每一个histogram field中，avg aggregation会累加`value`数组以及对应`count`数组中的乘积值。最终计算所有histogram field的平均值，如下所示：

```text
{
  ...
  "aggregations": {
    "avg_latency": {
      "value": 0.29690721649
    }
  }
}
```

&emsp;&emsp;计算过程：

- 累加文档1中histogram field字段的值：(0.1 * 3) + (0.2 * 7) + (0.3 * 23) + (0.4 * 12) + (0.5 * 6) = 12.4，并且总数为3 + 7 + 23 + 12 + 6 = 46。
- 同理文档2中计算出的值跟数量分别为
  - (0.1 * 8) + (0.2 * 17) + (0.3 * 8) + (0.4 * 7) + (0.5 * 6) = 16.4和8 + 17 + 8 + 7 + 6 = 51
- 计算平均值：(12.4  + 16.4) / (46 + 51) = 0.29690721649

#### Boxplot aggregation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-metrics-boxplot-aggregation.html)

&emsp;&emsp;箱线图指标聚合（boxplot metric aggregation）计算的是从被聚合的文档中提取出的数值的箱线图数据。可以指定文档中数值类型的字段（numeric filed）或者[histogram字段](#Histogram field type)用于boxplot aggregation。

&emsp;&emsp;boxplot aggregation返回必要的信息用于成为一个箱线图：最小值（minimum）、最大值（maximum）、中位数（median、Q2）、下四分位数/第一四分位数（first quartile，25th percentile、Q1）、上四分位数/第三四分位数（third quartile，75th percentile、Q3）。

##### Syntax

&emsp;&emsp;单独的boxplot aggregation如下所示：

```text
{
  "boxplot": {
    "field": "load_time"
  }
}
```

&emsp;&emsp;用于表示加载时间（load time）的boxplot aggregation如下所示：

```text
GET latency/_search
{
  "size": 0,
  "aggs": {
    "load_time_boxplot": {
      "boxplot": {
        "field": "load_time" 
      }
    }
  }
}
```

&emsp;&emsp;第7行，`load time`字段必须是数值类型的字段。

&emsp;&emsp;响应如下所示：

```text
{
  ...

 "aggregations": {
    "load_time_boxplot": {
      "min": 0.0,
      "max": 990.0,
      "q1": 165.0,
      "q2": 445.0,
      "q3": 725.0,
      "lower": 0.0,
      "upper": 990.0
    }
  }
}
```

&emsp;&emsp;在这个例子中，下触须/下边缘（lower whisker）和上触须/上边缘（upper whisker）分别等同于最小值（min）和最大值（max）。一般情况下，这两个字段的值取值范围为：1.5\* IQR，IQR = Q3 - Q1，也就是分别最接近`Q1 - (1.5 * IQR)`和`Q3 + (1.5 * IQR)`的值。这是一个近似值（approximation value），所给出的数值可能并非实际从数据中观察到的值，但它们应该与实际值的误差范围相符。虽然箱线图聚合不直接返回异常值点（outlier point），但你可以通过检查lower whisker是否大于最小值或upper whisker是否小于最大值来判断是否存在任何一侧的异常值，然后直接查询它们。

##### Script

&emsp;&emsp;如果你需要将没有准确索引（indexed exactly）的数据用于创建箱线图，那你可以创建一个[runtime field](#Runtime fields)来调整这些数据。例如，如果你的加载时间（load time）的单位是毫秒，但是你想要使用秒作为单位来创建箱线图，那么可以使用runtime field来进行调整：

```text
GET latency/_search
{
  "size": 0,
  "runtime_mappings": {
    "load_time.seconds": {
      "type": "long",
      "script": {
        "source": "emit(doc['load_time'].value / params.timeUnit)",
        "params": {
          "timeUnit": 1000
        }
      }
    }
  },
  "aggs": {
    "load_time_boxplot": {
      "boxplot": { "field": "load_time.seconds" }
    }
  }
}
```

##### Boxplot values are (usually) approximate

&emsp;&emsp;箱线图指标使用的算法称为TDigest（见[Computing Accurate Quantiles using T-Digests](https://github.com/tdunning/t-digest/blob/main/docs/t-digest-paper/histo.pdf)）

> WARNNING：boxplot跟其他的percentile aggregation一样都是非确定性的（[non-deterministic](https://en.wikipedia.org/wiki/Nondeterministic_algorithm)）的，所以使用相同的数据得到的结果会有轻微的不同
> 

##### Compression

&emsp;&emsp;近似算法（approximate algorithm）必须平衡内存使用和估算准确性（estimate accurate），可以使用`compression`参数控制：

```text
GET latency/_search
{
  "size": 0,
  "aggs": {
    "load_time_boxplot": {
      "boxplot": {
        "field": "load_time",
        "compression": 200    
      }
    }
  }
}
```

&emsp;&emsp;第8行，compression参数控制内存使用和近似错误（approximate error，与实际值的差异）。

&emsp;&emsp;TDigest算法使用许多“节点”来近似百分位数 —— 可用的节点越多，准确性越高（并且与数据量成比例地增加内存占用）。compression参数将节点的最大数量限制为20 \* compression。

&emsp;&emsp;因此，通过增加compression的值，你可以在牺牲更多内存的代价下提高百分位数的准确性。更大的compression还会使算法变慢，因为底层的树状数据结构在大小上增长，导致操作成本更高。默认的compression为100。

&emsp;&emsp;一个“节点”大致使用32字节的内存，所以在最坏的情况下（大量的数据按顺序排序并按顺序到达），默认设置将产生一个大约64KB大小的TDigest。在实际应用中，数据往往更为随机，TDigest将使用更少的内存。

##### Missing value

&emsp;&emsp;当文档缺失聚合字段时，`missing`参数定义了在这篇文档中聚合字段的值。默认情况下，它们会被忽略，但是可以将它们视为具有某个值的文档。

```text
POST /exams/_search?size=0
{
  "aggs": {
    "grade_avg": {
      "avg": {
        "field": "grade",
        "missing": 10     
      }
    }
  }
}
```

&emsp;&emsp;第7行，没有`grade`字段的文档被划分到相同分桶中并且认为`grade`的值为10。

#### Cardinality aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-metrics-cardinality-aggregation.html)

&emsp;&emsp;它属于single-value aggregation。计算不同值的近似数量（approximate  count）。

&emsp;&emsp;假设你正在索引商店销售数据并且想要统计满足Query的不同的售卖的产品的数量：

```text
POST /sales/_search?size=0
{
  "aggs": {
    "type_count": {
      "cardinality": {
        "field": "type"
      }
    }
  }
}
```

&emsp;&emsp;响应：

```text
{
  ...
  "aggregations": {
    "type_count": {
      "value": 3
    }
  }
}
```

##### Precision control

&emsp;&emsp;这个聚合支持`precision_threshold`参数：

```text
POST /sales/_search?size=0
{
  "aggs": {
    "type_count": {
      "cardinality": {
        "field": "type",
        "precision_threshold": 100 
      }
    }
  }
}
```

&emsp;&emsp;第7行，`precision_threshold` 选项允许在内存与准确性之间进行权衡，并定义了一个独特的计数阈值，低于此阈值时，计数预计将接近准确。超过这个值，计数可能会变得有些模糊。支持的最大值是40000，高于这个数字的阈值将会产生与阈值为40000相同的效果。默认值是3000。

##### Counts are approximate

&emsp;&emsp;计算精确的计数需要将值加载到一个哈希集中并返回其大小。在处理高基数（high-cardinality）集合和/或大值时，这种方法不具可扩展性，因为所需的内存使用量以及在节点之间通信这些每个分片的集合会消耗太多集群资源。

&emsp;&emsp;这种基数聚合基于[HyperLogLog++](https://static.googleusercontent.com/media/research.google.com/fr//pubs/archive/40671.pdf)算法，该算法基于值的哈希进行计数，并具有一些有趣的特性：

- 可配置的精度，决定了如何权衡内存与准确性，
- 在低基数（low-cardinality）集合上具有优秀的准确性，
- 固定的内存使用：无论是数十个还是数十亿个唯一值，内存使用量只依赖于配置的精度。 

&emsp;&emsp;对于精度阈值`c`，我们使用的实现大约需要`c * 8`字节的内存。

&emsp;&emsp;下图显示了阈值前后误差如何变化：

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/cardinality_error.png">

&emsp;&emsp;对于所有3个阈值，计数在配置的阈值范围内都保持了准确性。虽然没有保证，但这很可能是常见情况。实际的准确性取决于具体的数据集。一般来说，大多数数据集显示出一致的良好准确性。还要注意，即使阈值低至100，当计数达到数百万个项目时，误差仍然非常低（如上图所示，为1-6%）。

&emsp;&emsp;HyperLogLog++算法依赖于哈希值前导零的数量，数据集中哈希的具体分布可以影响基数的准确性。

##### Pre-computed hashes

&emsp;&emsp;对于具有高基数的字符串字段，将字段值的哈希存储在索引中然后在此字段上运行cardinality aggregation可能会更快。这可以通过从客户端提供哈希值来完成，或者通过使用[mapper-murmur3](https://www.elastic.co/guide/en/elasticsearch/plugins/8.12/mapper-murmur3.html)插件让Elasticsearch为您计算哈希值来实现。

> NOTE：Pre-computed hashes通常只在非常大和/或高基数字段上有用，因为它节省了CPU和内存。然而，在数值字段上，哈希运算非常快速，并且存储原始值所需的内存与存储哈希值相比，需要的内存量相同或更少。在低基数的字符串字段上也是如此，特别是考虑到为了确保每个唯一值每个段最多只计算一次哈希，这些字段有一个优化。

##### Script

&emsp;&emsp;如果你要计算的目标是两个字段的组合值，可以使用[runtime field](#Runtime fields)

```text
POST /sales/_search?size=0
{
  "runtime_mappings": {
    "type_and_promoted": {
      "type": "keyword",
      "script": "emit(doc['type'].value + ' ' + doc['promoted'].value)"
    }
  },
  "aggs": {
    "type_promoted_count": {
      "cardinality": {
        "field": "type_and_promoted"
      }
    }
  }
}
```

##### Missing value

&emsp;&emsp;当文档缺失聚合字段时，`missing`参数定义了在这篇文档中聚合字段的值。默认情况下，它们会被忽略，但是可以将它们视为具有某个值的文档。

```text
POST /sales/_search?size=0
{
  "aggs": {
    "tag_cardinality": {
      "cardinality": {
        "field": "tag",
        "missing": "N/A" 
      }
    }
  }
}
```

&emsp;&emsp;第7行，没有`tag`字段的文档被划分到相同分桶中并且认为`tag`的值为`N/a`。


##### Execution hint

&emsp;&emsp;您可以通过不同的机制运行基数聚合：

- 直接使用字段值（`direct`）
- 使用字段的全局序数（global ordinals ），并在完成一个分片后解析这些值（`global_ordinals`）
- 使用段内序数值（segment ordinal values），并在每个段完成后解析这些值（`segment_ordinals`）

&emsp;&emsp;此外，还有两种基于启发式的模式。这些模式将导致Elasticsearch使用一些关于索引状态的数据来选择合适的执行方法。这两种启发式是：

- save_time_heuristic - 这是Elasticsearch 8.4及以后版本的默认设置。
- save_memory_heuristic - 这是Elasticsearch 8.3及以前版本的默认设置。

&emsp;&emsp;如果未指定，Elasticsearch将应用启发式来选择适当的模式。还要注意，对于某些数据（non-ordinal fields），`direct`是唯一的选项，在这些情况下将忽略提示。一般而言，不需要设置这个值。


#### Extended stats aggregation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-metrics-extendedstats-aggregation.html)

&emsp;&emsp;extended stats aggregation属于多值的指标聚合（multi-value metric aggregation），计算的是从被聚合的文档中提取出的数值的统计数据。

&emsp;&emsp;`extened_stats` aggregation 是 [stats aggregation](#Stats aggregation)的扩展版本，添加了例如`sum_of_squares`、`variance`、`std_deviation`、`std_deviation_bounds`这些额外的指标。

&emsp;&emsp;假设数据由代表学生考试成绩（0到100分）的文档组成：

```text
GET /exams/_search
{
  "size": 0,
  "aggs": {
    "grades_stats": { "extended_stats": { "field": "grade" } }
  }
}
```

&emsp;&emsp;上文的聚合基于所有的文档计算了考试成绩的统计数据。聚合的类型为`exteneed_stats`并且`field`定义了文档中的数值类型的字段并计算该字段的统计数据。上面中请求的响应如下所示：

&emsp;&emsp;`std_deviation`和`variance`是基于总体指标（population metric）计算的，所以它们的值分别跟`std_deviation_population`和`variance_population`相同。

```text
{
  ...

  "aggregations": {
    "grades_stats": {
      "count": 2,
      "min": 50.0,
      "max": 100.0,
      "avg": 75.0,
      "sum": 150.0,
      "sum_of_squares": 12500.0,
      "variance": 625.0,
      "variance_population": 625.0,
      "variance_sampling": 1250.0,
      "std_deviation": 25.0,
      "std_deviation_population": 25.0,
      "std_deviation_sampling": 35.35533905932738,
      "std_deviation_bounds": {
        "upper": 125.0,
        "lower": 25.0,
        "upper_population": 125.0,
        "lower_population": 25.0,
        "upper_sampling": 145.71067811865476,
        "lower_sampling": 4.289321881345245
      }
    }
  }
}
```

&emsp;&emsp;聚合的名称（上文中的`grades_stats`）可以作为key，使得返回的响应中作为key来获取聚合结果。

##### Standard Deviation Bounds

&emsp;&emsp;默认情况下，`extened_stats`指标会返回一个名为`std_deviation_bounds`的对象，它提供了一个区间，该区间是平均值加减两个标准偏差。使得可视化数据的变化。如果你想要一个不同的边界，例如三个标准偏差，你可以在请求中设置`sigma`的值:

```text
GET /exams/_search
{
  "size": 0,
  "aggs": {
    "grades_stats": {
      "extended_stats": {
        "field": "grade",
        "sigma": 3          
      }
    }
  }
}
```

&emsp;&emsp;第8行的，`sigma` 控制应显示均值的正负多少个标准偏差。

&emsp;&emsp;`sigma`可以是任何非负双精度值，这意味着你可以请求非整数值，如1.5。 0也是一个有效的值，但它将只返回平均值作为上下限（`uppper`和`lower`边界）。

&emsp;&emsp;上下限（`uppper`和`lower`边界）基于总体指标计算得来，所以它们的值分别跟`upper_population`和`lower_population`相同。

> NOTE：计算标准偏差和界限（bounds）需要数据服从正态分布
> 标准偏差及其边界默认会被显示，但它们并不总是适用于所有数据集。你的数据必须呈正态分布，这样这些指标才有意义。标准偏差背后的统计学假设数据是正态分布的，因此如果你的数据严重向左或向右偏斜，返回的值将是误导性的。

##### Script

&emsp;&emsp;如果你想要聚合的数据没有索引，可以使用[runtime field](#Runtime fields)假设我们发现我们一直在处理的这些成绩是来自一场超出学生水平的考试，我们想要对它进行纠正：

```text
GET /exams/_search
{
  "size": 0,
  "runtime_mappings": {
    "grade.corrected": {
      "type": "double",
      "script": {
        "source": "emit(Math.min(100, doc['grade'].value * params.correction))",
        "params": {
          "correction": 1.2
        }
      }
    }
  },
  "aggs": {
    "grades_stats": {
      "extended_stats": { "field": "grade.corrected" }
    }
  }
}
```

##### Missing value

&emsp;&emsp;当文档缺失聚合字段时，`missing`参数定义了在这篇文档中聚合字段的值。默认情况下，它们会被忽略，但是可以将它们视为具有某个值的文档。

```text
GET /exams/_search
{
  "size": 0,
  "aggs": {
    "grades_stats": {
      "extended_stats": {
        "field": "grade",
        "missing": 0        
      }
    }
  }
}
```

&emsp;&emsp;第7行，没有`grade`字段的文档被划分到相同分桶中并且认为`grade`的值为10。

#### Max aggregation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-metrics-max-aggregation.html)

&emsp;&emsp;单值的指标聚合（Single-value metric aggregation）计算是从被聚合的文档中提取出数值的最大值。

> NOTE：`min`和`max` aggregation是对用`double`表示的数据进行计算。因此，当计算long类型的数值并且绝对值大于等于2^53时，结果是一个近似值

&emsp;&emsp;基于所有的文档计算price的最大值：

```text
POST /sales/_search?size=0
{
  "aggs": {
    "max_price": { "max": { "field": "price" } }
  }
}
```

&emsp;&emsp;响应：

```text
{
  ...
  "aggregations": {
      "max_price": {
          "value": 200.0
      }
  }
}
```
&emsp;&emsp;由此可见，上文中聚合的名称`max_price`可以作为一个key，用于在返回的响应中获取聚合结果。

##### Script

&emsp;&emsp;相较于在一个字段上计算`max`，你可以通过[runtime field](#Runtime fields)执行更复杂的聚合。

```text
POST /sales/_search
{
  "size": 0,
  "runtime_mappings": {
    "price.adjusted": {
      "type": "double",
      "script": """
        double price = doc['price'].value;
        if (doc['promoted'].value) {
          price *= 0.8;
        }
        emit(price);
      """
    }
  },
  "aggs": {
    "max_price": {
      "max": { "field": "price.adjusted" }
    }
  }
}
```

##### Missing value

&emsp;&emsp;当文档缺失聚合字段时，`missing`参数定义了在这篇文档中聚合字段的值。默认情况下，它们会被忽略，但是可以将它们视为具有某个值的文档。

```text
POST /sales/_search
{
  "aggs" : {
      "grade_max" : {
          "max" : {
              "field" : "grade",
              "missing": 10       
          }
      }
  }
}
```

&emsp;&emsp;第7行，没有`grade`字段的文档被划分到相同分桶中并且认为`grade`的值为10。

##### Histogram fields

&emsp;&emsp;如果对[histogram](#Histogram field type)字段进行聚合计算，聚合的结果是`value`数组中所有元素的最大值。注意的是，这个直方图的`counts`数组会被忽略。

&emsp;&emsp;例如，下面的索引存储了预先聚合好的、不同网络的延迟指标直方图：

```text
PUT metrics_index
{
  "mappings": {
    "properties": {
      "latency_histo": { "type": "histogram" }
    }
  }
}

PUT metrics_index/_doc/1?refresh
{
  "network.name" : "net-1",
  "latency_histo" : {
      "values" : [0.1, 0.2, 0.3, 0.4, 0.5],
      "counts" : [3, 7, 23, 12, 6]
   }
}

PUT metrics_index/_doc/2?refresh
{
  "network.name" : "net-2",
  "latency_histo" : {
      "values" :  [0.1, 0.2, 0.3, 0.4, 0.5],
      "counts" : [8, 17, 8, 7, 6]
   }
}

POST /metrics_index/_search?size=0&filter_path=aggregations
{
  "aggs" : {
    "max_latency" : { "max" : { "field" : "latency_histo" } }
  }
}
```

&emsp;&emsp;`max` aggregation返回所有histogram字段中的最大值 ：

```text
{
  "aggregations": {
    "max_latency": {
      "value": 0.5
    }
  }
}
```


#### Min aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-metrics-min-aggregation.html#search-aggregations-metrics-min-aggregation-histogram-fields)

&emsp;&emsp;单值的指标聚合（Single-value metric aggregation）计算是从被聚合的文档中提取出数值的最小值。

> NOTE：`min`和`max` aggregation是对用`double`表示的数据进行计算。因此，当计算long类型的数值并且绝对值大于等于2^53时，结果是一个近似值

&emsp;&emsp;基于所有的文档计算price的最小值：

```text
POST /sales/_search?size=0
{
  "aggs": {
    "min_price": { "min": { "field": "price" } }
  }
}
```

&emsp;&emsp;响应：

```text
{
  ...

  "aggregations": {
    "min_price": {
      "value": 10.0
    }
  }
}
```
&emsp;&emsp;由此可见，上文中聚合的名称`min_price`可以作为一个key，用于在返回的响应中获取聚合结果。

##### Script

&emsp;&emsp;相较于在一个字段上计算`min`，你可以通过[runtime field](#Runtime fields)执行更复杂的聚合。

```text
POST /sales/_search
{
  "size": 0,
  "runtime_mappings": {
    "price.adjusted": {
      "type": "double",
      "script": """
        double price = doc['price'].value;
        if (doc['promoted'].value) {
          price *= 0.8;
        }
        emit(price);
      """
    }
  },
  "aggs": {
    "min_price": {
      "min": { "field": "price.adjusted" }
    }
  }
}
```

##### Missing value

&emsp;&emsp;当文档缺失聚合字段时，`missing`参数定义了在这篇文档中聚合字段的值。默认情况下，它们会被忽略，但是可以将它们视为具有某个值的文档。

```text
POST /sales/_search
{
  "aggs": {
    "grade_min": {
      "min": {
        "field": "grade",
        "missing": 10 
      }
    }
  }
}

```

&emsp;&emsp;第7行，没有`grade`字段的文档被划分到相同分桶中并且认为`grade`的值为10。

##### Histogram fields

&emsp;&emsp;如果对[histogram](#Histogram field type)字段进行聚合计算，聚合的结果是`value`数组中所有元素的最小值。注意的是，这个直方图的`counts`数组会被忽略。

&emsp;&emsp;例如，下面的索引存储了预先聚合好的、不同网络的延迟指标直方图：

```text
PUT metrics_index
{
  "mappings": {
    "properties": {
      "latency_histo": { "type": "histogram" }
    }
  }
}

PUT metrics_index/_doc/1?refresh
{
  "network.name" : "net-1",
  "latency_histo" : {
      "values" : [0.1, 0.2, 0.3, 0.4, 0.5],
      "counts" : [3, 7, 23, 12, 6]
   }
}

PUT metrics_index/_doc/2?refresh
{
  "network.name" : "net-2",
  "latency_histo" : {
      "values" :  [0.1, 0.2, 0.3, 0.4, 0.5],
      "counts" : [8, 17, 8, 7, 6]
   }
}

POST /metrics_index/_search?size=0&filter_path=aggregations
{
  "aggs" : {
    "min_latency" : { "min" : { "field" : "latency_histo" } }
  }
}
```

&emsp;&emsp;`min` aggregation返回所有histogram字段中的最小值 ：

```text
{
  "aggregations": {
    "min_latency": {
      "value": 0.1
    }
  }
}
```

#### Percentile ranks aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-metrics-percentile-rank-aggregation.html)

#### Percentiles aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-metrics-percentile-aggregation.html)

&emsp;&emsp;它属于multi-value metrics aggregation，对从文档中提出的数值类型的值计算一个或多个百分位数。可以对文档中数值类型或者[histogram field](#Histogram field type)的值计算。

&emsp;&emsp;百分位数展示了观察值中某一特定百分比的点。例如，第95百分位数是大于95%观察值的值。

&emsp;&emsp;百分位数常用于发现异常值。在正态分布中，第0.13和第99.87百分位数代表了距离平均值三个标准差。任何落在三个标准差之外的数据通常被认为是异常的。

&emsp;&emsp;当获取一系列百分位数时，它们可以用来估计数据分布并确定数据是否偏斜（skewed）、双峰（bimodal）等。

&emsp;&emsp;假设你的数据包含了网站加载时间。平均和中位数加载时间对管理员来说并不特别有用。最大值可能很有趣，但它很容易被单一的慢响应所偏斜。

&emsp;&emsp;我们看下代表加载时间的百分位数范围：

```text
GET latency/_search
{
  "size": 0,
  "aggs": {
    "load_time_outlier": {
      "percentiles": {
        "field": "load_time" 
      }
    }
  }
}
```

&emsp;&emsp;第7行，`load_time`字段必须是数值类型的字段。

&emsp;&emsp;默认情况下，`percentile` aggregation会生成一个百分位数的范围：`[ 1, 5, 25, 50, 75, 95, 99 ]`。上面对应的响应可能就是：

```text
{
  ...

 "aggregations": {
    "load_time_outlier": {
      "values": {
        "1.0": 10.0,
        "5.0": 30.0,
        "25.0": 170.0,
        "50.0": 445.0,
        "75.0": 720.0,
        "95.0": 940.0,
        "99.0": 980.0
      }
    }
  }
}
```

&emsp;&emsp;可见该聚合在一个默认的范围内计算每一个百分位数。如果我们假设响应时间的单位是毫秒，那么就能立刻观察到网页加载时间通常在10~725ms内，偶尔945-985ms。

（未完成）

#### Scripted metric aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-metrics-scripted-metric-aggregation.html)

##### Scope of scripts

#### Stats aggregation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-metrics-avg-aggregation.html)

&emsp;&emsp;stats aggregation属于多值的指标聚合（multi-value metric aggregation），计算的是从被聚合的文档中提取出的数值的统计数据。

&emsp;&emsp;统计数据包括`min`、`max`、`sum`、`count`和`avg`。

&emsp;&emsp;假设数据由代表学生考试成绩（0到100分）的文档组成：

```text
POST /exams/_search?size=0
{
  "aggs": {
    "grades_stats": { "stats": { "field": "grade" } }
  }
}
```

&emsp;&emsp;上文的聚合基于所有的文档计算了考试成绩的统计数据。聚合的类型为`stats`并且`field`定义了文档中的数值类型的字段并计算该字段的统计数据。上面中请求的响应如下所示：

```text
{
  ...

  "aggregations": {
    "grades_stats": {
      "count": 2,
      "min": 50.0,
      "max": 100.0,
      "avg": 75.0,
      "sum": 150.0
    }
  }
}
```

##### Script

&emsp;&emsp;如果你需要获取相较于单个字段、更复杂的统计数据，你可以在对一个[runtime field](#Runtime fields)进行聚合计算。

```text
POST /exams/_search
{
  "size": 0,
  "runtime_mappings": {
    "grade.weighted": {
      "type": "double",
      "script": """
        emit(doc['grade'].value * doc['weight'].value)
      """
    }
  },
  "aggs": {
    "grades_stats": {
      "stats": {
        "field": "grade.weighted"
      }
    }
  }
}
```

##### Missing value

&emsp;&emsp;当文档缺失聚合字段时，`missing`参数定义了在这篇文档中聚合字段的值。默认情况下，它们会被忽略，但是可以将它们视为具有某个值的文档。

```text
POST /exams/_search?size=0
{
  "aggs": {
    "grades_stats": {
      "stats": {
        "field": "grade",
        "missing": 0      
      }
    }
  }
}
```

&emsp;&emsp;第7行，没有`grade`字段的文档被划分到相同分桶中并且认为`grade`的值为10。

#### Sum aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-metrics-sum-aggregation.html)

#### Top metrics aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-metrics-top-metrics.html)

#### Value count aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-metrics-valuecount-aggregation.html#search-aggregations-metrics-valuecount-aggregation-histogram-fields)

### Pipeline aggregations
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline.html)

#### Average bucket aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline-avg-bucket-aggregation.html)

&emsp;&emsp;Average bucket aggregation是一种同级管道聚合（sibling pipeline aggregation），它计算在兄弟聚合（sibling aggregation）中指定指标的平均值。所指定的指标必须是数值型，且兄弟聚合必须是多桶聚合（multi-bucket aggregation）。

#### Bucket script aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline-bucket-script-aggregation.html)

&emsp;&emsp;Bucket script aggregation是一种父级管道聚合（parent pipeline aggregation），它通过执行一个脚本来对父多桶聚合中指定的度量进行每个桶的计算。指定的指标必须是数值型，且脚本必须返回一个数值结果。

#### Bucket count K-S test correlation aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-count-ks-test-aggregation.html)

&emsp;&emsp;

#### Bucket correlation aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-bucket-correlation-aggregation.html)

&emsp;&emsp;

#### Bucket selector aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline-bucket-selector-aggregation.html)

&emsp;&emsp;Bucket selector aggregation是一个父级管道聚合（parent pipeline aggregation），它通过执行一个脚本来决定当前的桶是否会保留在父多桶聚合中。指定的指标必须是数值型，脚本返回一个布尔值。如果脚本语言是`expression`，那么允许返回数值，其中0.0会被认为是假（false），所有其他值被认为是真（true）

#### Bucket sort aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline-bucket-sort-aggregation.html)

&emsp;&emsp;Bucket sort aggregation是一种父级管道聚合（parent pipeline aggregation），用于对其父多桶聚合中的桶进行排序。可以指定零个或多个排序字段以及相应的排序顺序。每个桶可以基于其\_key、\_count或子聚合进行排序。此外，还可以设置`from`和`size`参数来截断结果桶。

#### Change point aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-change-point-aggregation.html)

&emsp;&emsp;

#### Cumulative cardinality aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline-cumulative-cardinality-aggregation.html)

&emsp;&emsp;Cumulative cardinality aggregation是一种（parent pipeline aggregation），用于计算父直方图（或日期直方图）聚合中的累积基数。指定的指标必须是基数聚合（cardinality aggregation），并且封闭的直方图必须将`min_doc_count`设置为0（直方图聚合的默认设置）。累积基数聚合对于寻找“总新项目”很有用，例如，每天网站的新访客数量。它能帮助区分“新”访客和“重复”访客，了解每天的独特访客中有多少是“新”的。

#### Cumulative sum aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline-cumulative-sum-aggregation.html)

&emsp;&emsp;Cumulative sum aggregation是一种父级管道聚合（parent pipeline aggregation），它用于计算父直方图（或日期直方图）聚合中指定指标的累积和（累加每个分桶中的某个指标值）。指定的指标必须是数值型，并且封闭的直方图必须将`min_doc_count`设置为0（直方图聚合的默认设置）

#### Derivative aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline-derivative-aggregation.html)

&emsp;&emsp;

#### Extended stats bucket aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline-extended-stats-bucket-aggregation.html)

&emsp;&emsp;Extended stats bucket aggregation是一种同级管道聚合（sibling pipeline aggregation）可以计算指定指标在同级聚合的所有桶中的各种统计信息。这些指标必须是数值型，且相关的同级聚合必须是多桶聚合。与stats_bucket聚合相比，这种聚合提供了更多统计信息（如平方和、标准差等）。

#### Inference bucket aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline-inference-bucket-aggregation.html)

&emsp;&emsp;

#### Max bucket aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline-max-bucket-aggregation.html)

&emsp;&emsp;Max bucket aggregation是一种同级管道聚合（sibling pipeline aggregation），用于识别在同级聚合中指定指标值最大的桶，并输出该桶的值和键。指定的指标必须是数值型，且同级聚合必须是多桶聚合。

#### Min bucket aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline-min-bucket-aggregation.html)

&emsp;&emsp;Min bucket aggregation是一种同级管道聚合（sibling pipeline aggregation），用于识别在同级聚合中指定指标值最小的桶，并输出该桶的值和键。指定的指标必须是数值型，且同级聚合必须是多桶聚合。

#### Moving function aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline-movfn-aggregation.html)

&emsp;&emsp;

#### Moving percentiles aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline-moving-percentiles-aggregation.html)

&emsp;&emsp;

#### Normalize aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline-normalize-aggregation.html)

&emsp;&emsp;

#### Percentiles bucket aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline-percentiles-bucket-aggregation.html)

&emsp;&emsp;Percentiles bucket aggregation是一种同级管道聚合（sibling pipeline aggregation），用于计算在同级聚合中指定指标的所有桶的百分位数。指定的指标必须是数值型，且同级聚合必须是多桶聚合。

#### Serial differencing aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline-serialdiff-aggregation.html)

&emsp;&emsp;

#### Stats bucket aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline-stats-bucket-aggregation.html)

&emsp;&emsp;Stats bucket aggregation是一种同级管道聚合（sibling pipeline aggregation），可以计算指定指标在所有桶中的各种统计数据，包括平均值、最小值、最大值等。指标必须是数值型，且聚合必须是多桶聚合。

#### Sum bucket aggregation
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-aggregations-pipeline-sum-bucket-aggregation.html)

&emsp;&emsp;Sum bucket aggregation是一种同级管道聚合（sibling pipeline aggregation），用于计算在同级聚合中指定指标的所有桶的总和。所指定的指标必须是数值型，且同级聚合必须是多桶聚合。


## Scripting 
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-scripting.html)

&emsp;&emsp;通过编写脚本，你可以在Elasticsearch中计算自定义的表达式。比如，你可以使用脚本为一个字段返回字段值或者为一个query计算自定义的打分。

&emsp;&emsp;默认的脚本语言是[Painless](#Painless)。另外`lang`插件允许运行其他的余燕语言。你可以任何运行脚本的地方指定脚本语言。

#### Available scripting languages

&emsp;&emsp;Painless是专为Elasticsearch设计的，可以在脚本API中用于任何目的，并提供最大的灵活性。其他语言则不那么灵活，但对于特定的用途可能很有用。

| Language                                                     | Sandboxed                                                    | Required plugin | Purpose                         |
| ------------------------------------------------------------ | ------------------------------------------------------------ | --------------- | ------------------------------- |
| [`painless`](#Painless scripting language) | ![Yes](http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/icon-yes.png) | Built-in        | Purpose-built for Elasticsearch |
| [`expression`](#Lucene expressions language) | ![Yes](http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/icon-yes.png) | Built-in        | Fast custom ranking and sorting |
| [`mustache`](#Search templates) | ![Yes](http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/icon-yes.png) | Built-in        | Templates                       |
| [`java`](#Advanced scripts using script engines) | ![No](http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/icon-no.png) | You write it!   | Expert API                      |


### Painless scripting language
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-scripting-painless.html)

&emsp;&emsp;Painless是一种高性能、安全的脚本语言，专门为Elasticsearch设计。你可以在Elasticsearch支持脚本的任何地方安全地编写内联和存储脚本。

&emsp;&emsp;Painless提供了许多功能，围绕以下核心原则：

- Safety：确保集群的安全是至关重要的。为此，Painless使用了细粒度的允许列表，粒度细至类的成员。任何不在允许列表中的内容都会导致编译错误。请参阅[Painless API](https://www.elastic.co/guide/en/elasticsearch/painless/8.2/painless-api-reference.html)参考，以获取每个脚本上下文可用的完整类、方法和字段列表。
- Performance：Painless直接编译成JVM字节码，以利用JVM提供的所有可能优化。此外，Painless通常避免需要在运行时进行额外较慢检查的功能。
- Simplicity：Painless实现了一种对任何有基础编码经验的人都自然熟悉的语法。Painless使用Java语法的子集，并增加了一些改进，以增强可读性和去除样板代码（boilerplate）。

#### Start scripting

&emsp;&emsp;准备开始使用Painless编写脚本了吗，见[how to write your first script.](https://www.elastic.co/guide/en/elasticsearch/painless/8.2/painless-api-reference.html)。

&emsp;&emsp;如果你已经熟悉了Painless，可以查看[Painless Language Specification ](https://www.elastic.co/guide/en/elasticsearch/painless/8.2/painless-lang-spec.html)了解Painless语法和功能的详细介绍。

### How to write scripts
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-scripting-using.html)

&emsp;&emsp;无论在哪个API中编写脚本，总是按照下面的模式编写；你要指定脚本语言，提供脚本逻辑（或者脚本源，即通过标识符获取之前存储过的脚本），以及传入到脚本的参数：

```text
"script": {
    "lang":   "...",
    "source" | "id": "...",
    "params": { ... }
  }
```

- lang：指定要编写的脚本语言。默认是`Painless`
- source, id：在`source`中指定脚本自身内容作为内联脚本或者指定`id`，获取之前存储好的脚本。使用[stored script APIs ](#Stored script APIs)创建管理脚本。
- params：指定参数名称和传给脚本的参数值。使用[parameters](Use parameters in your script)而不是hard-code值来降低编译时间。

##### Write your first script

&emsp;&emsp;Painless是Elasticsearch的默认脚本语言。它既安全又高效，并为具有一点编程经验的人提供了自然的语法。

&emsp;&emsp;一个Painless脚本由一个或多个语句构成，并且在开头可选地包含一个或多个用户定义的函数。脚本必须至少包含一个语句。

&emsp;&emsp;[Painless execute API](https://www.elastic.co/guide/en/elasticsearch/painless/8.2/painless-execute-api.html)提供了使用简单的用户定义参数测试脚本并接收结果的能力。让我们从一个完整的脚本开始，并审查其组成部分。

&emsp;&emsp;首先，索引一个只有单个字段的文档，这样我们就有了一些可以操作的数据：

```text
PUT my-index-000001/_doc/1
{
  "my_field": 5
}
```

&emsp;&emsp;我们然后就可以编写一个脚本，并且在`my_field`上操作并且运行脚本，其作为查询的一部分。下面的请求使用了Search API中的[script_fields](#Script fields)参数来获取一个脚本结果。这个查询会发生很多事情，但我们会进行拆解然后一个一个的去理解。现在，你只需要知道脚本获取了`my_field`然后对这个字段进行操作。

```text
GET my-index-000001/_search
{
  "script_fields": {
    "my_doubled_field": {
      "script": { 
        "source": "doc['my_field'].value * params['multiplier']", 
        "params": {
          "multiplier": 2
        }
      }
    }
  }
}
```

&emsp;&emsp;第5行，`script`对象
&emsp;&emsp;第6行，`script`的source，也就是脚本本身的内容

&emsp;&emsp;`script`是一个标准JSON对象，在Elasticsearch中大多数APIs中定义。这个对象指定了`source`字段来提供脚本自身内容。这个脚本没有指定脚本语言，因此默认就是Painless。

##### Use parameters in your script

&emsp;&emsp;Elasticsearch首次遇到一个新脚本时，它会编译该脚本并将编译后的版本存储在缓存中。编译可能是一个繁重的过程。与其在脚本中hard-code，不如将它们作为在`params`参数中指定并传递。

&emsp;&emsp;例如，在之前的脚本中，我们本可以直接hard-code，并编写一个看似较为简单的脚本。我们可以直接获取`my_field`的第一个值，然后将其乘以`2`：

```text
"source": "return doc['my_field'].value * 2"
```

&emsp;&emsp;虽然这个方法可行，但这个解决方案相当不灵活。我们必须修改脚本源码以改变乘数，而且每次乘数改变时，Elasticsearch都必须重新编译脚本。

&emsp;&emsp;与其hard-code，不如使用`params`参数来使脚本灵活，并且在脚本运行时减少编译时间。现在，你可以更改乘数参数而无需Elasticsearch重新编译脚本。

```text
"source": "doc['my_field'].value * params['multiplier']",
"params": {
  "multiplier": 2
}
```

&emsp;&emsp;默认每5分钟可以编译最多150个脚本。对于ingest contexts，则默认没有限制：

```text
script.context.field.max_compilations_rate=100/10m
```

> IMPORTANT：如果你短时间内编译太多脚本，Elasticsearch会reject新的动态脚本（dynamic）并且抛出`circuit_breaking_exception`错误

##### Shorten your script

&emsp;&emsp;利用Painless固有的语法能力，你可以减少脚本中的冗长表达，使它们更加简洁。这里有一个我们可以简化的简单脚本：

```text
GET my-index-000001/_search
{
  "script_fields": {
    "my_doubled_field": {
      "script": {
        "lang":   "painless",
        "source": "doc['my_field'].value * params.get('multiplier');",
        "params": {
          "multiplier": 2
        }
      }
    }
  }
}
```

&emsp;&emsp;现在我们给出一个更简短的版本看下相比较之前的有哪些提升：

```text
GET my-index-000001/_search
{
  "script_fields": {
    "my_doubled_field": {
      "script": {
        "source": "field('my_field').get(null) * params['multiplier']",
        "params": {
          "multiplier": 2
        }
      }
    }
  }
}
```

&emsp;&emsp;这个版本的脚本移除了几个组成部分，并显著简化了语法：

- `lang`声明。因为Painless是默认语言，如果你正在编写Painless脚本，就不需要指定语言。
- `return`关键字。Painless自动使用脚本中的最后一条语句（在可能的情况下）在需要返回值的脚本上下文中产生返回值。
- `get`方法，被替换为方括号`[]`。Painless为Map类型使用了一种特定的快捷方式，允许我们使用方括号而不是较长的`get`方法。
- `source`中句末尾的分号。Painless在块的最后一句话不需要分号。然而，在其他情况下它确实需要分号来消除歧义。

在Elasticsearch支持脚本的任何地方使用这种简化的语法（abbreviated syntax），比如当你创建[runtime fields](#Map a runtime field)时。


##### Store and retrieve scripts

&emsp;&emsp;你可以通过[stored script APIs](#Stored script APIs)来保存和获取脚本。保存的脚本能降低编译时间使得查询更快。

> NOTE：不同于直接提供脚本自身内容，存储的脚本需要你使用`lang`参数显示的指定脚本语言

&emsp;&emsp;你可以使用[create stored script API](#Create or update stored script API)创建一个脚本。比如，下面的请求创建了一个名为`calculate-score`来存储脚本。

```text
POST _scripts/calculate-score
{
  "script": {
    "lang": "painless",
    "source": "Math.log(_score * 2) + params['my_modifier']"
  }
}
```

&emsp;&emsp;你可以通过[get stored script API](#Get stored script API)获取脚本。

```text
GET _scripts/calculate-score
```

&emsp;&emsp;如要在query中使用存储的脚本，需要在`script`中指定脚本的`id`。

```text
GET my-index-000001/_search
{
  "query": {
    "script_score": {
      "query": {
        "match": {
            "message": "some message"
        }
      },
      "script": {
        "id": "calculate-score", 
        "params": {
          "my_modifier": 2
        }
      }
    }
  }
}
```

&emsp;&emsp;第11行，存储的脚本`id`。

&emsp;&emsp;若要删除一个脚本，则提交一个[delete stored script API ](#Delete stored script API)请求。

```text
DELETE _scripts/calculate-score
```

##### Update documents with scripts

&emsp;&emsp;你可以使用[update API ](#Update API)使用一个脚本更新文档。脚本可以更新，删除或者跳过修改文档。update API还支持传递部分文档，这部分文档将被合并到现有文档中。

&emsp;&emsp;比如我们索引一篇文档：

```text
PUT my-index-000001/_doc/1
{
  "counter" : 1,
  "tags" : ["red"]
}
```

&emsp;&emsp;若要增加`counter`值，你可以使用下面的脚本提交一个更新请求：

```text
POST my-index-000001/_update/1
{
  "script" : {
    "source": "ctx._source.counter += params.count",
    "lang": "painless",
    "params" : {
      "count" : 4
    }
  }
}
```

&emsp;&emsp;同样地，你可以使用更新脚本新增一个tag到tags列表中。因为这是一个列表，因此即使已经存在也可以添加进去：

```text
POST my-index-000001/_update/1
{
  "script": {
    "source": "ctx._source.tags.add(params['tag'])",
    "lang": "painless",
    "params": {
      "tag": "blue"
    }
  }
}
```

&emsp;&emsp;你也可以从tags列表中移除一个tag。Java中的`remove`方法在Painless也可以使用。它根据数组下标来移除。若要防止可能的runtime error，你首先需要保证tags是存在的。如果列表中包含重复的tag，这个脚本只会移除一次。

```text
POST my-index-000001/_update/1
{
  "script": {
    "source": "if (ctx._source.tags.contains(params['tag'])) { ctx._source.tags.remove(ctx._source.tags.indexOf(params['tag'])) }",
    "lang": "painless",
    "params": {
      "tag": "blue"
    }
  }
}
```

&emsp;&emsp;你也可以从一篇文档中添加并且移除字段。例如下面的脚本中添加一个名为`new_field`的新字段：

```text
POST my-index-000001/_update/1
{
  "script" : "ctx._source.new_field = 'value_of_new_field'"
}
```

&emsp;&emsp;同样的也可以移除`new_field`。

```text
POST my-index-000001/_update/1
{
  "script" : "ctx._source.remove('new_field')"
}
```

&emsp;&emsp;你也可以根据脚本中执行的内容来修改操作。例如，下面的请求中，如果`tags`中包含`green`则删除该文档，否则什么也不做（`noop`）：

```text
POST my-index-000001/_update/1
{
  "script": {
    "source": "if (ctx._source.tags.contains(params['tag'])) { ctx.op = 'delete' } else { ctx.op = 'none' }",
    "lang": "painless",
    "params": {
      "tag": "green"
    }
  }
}
```

#### Scripts, caching, and search speed
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/scripts-and-search-speed.html)

#### Grokking grok
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/grok.html)


### Common scripting use cases
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/common-script-uses.html)

&emsp;&emsp;你可以编写一个脚本做大多数的事情，但有时候它也是一个麻烦。要想知道脚本能做哪些事情是比较有难度的。因此下面的例子争对了真正有帮助的常见例子进行说明：

- [Field extraction](#Field extraction)

#### Field extraction
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/scripting-field-extraction.html)

&emsp;&emsp;field extraction的目的简单：你的数据有很多携带了大量信息的字段，但你只是想要提取出部分内容。

&emsp;&emsp;你有两个选项可以使用：

- [Grok](#Grokking grok)是一种正则表达式方言，支持你可以复用的别名表达式。因为Grok基于正则表达式（regex），所以任何正则表达式在Grok中都是有效的。
- [Dissect](#Dissecting data)通过使用分隔符来定义匹配模式，从文本中提取结构化字段。与Grok不同，Dissect不使用正则表达式。

&emsp;&emsp;我们先从一个简单的例子开始，添加`@timestamp`和`message`字段作为索引字段添加到`my-index`索引的mapping中。为了保持灵活，`message`使用`wildcard`类型：

```text
PUT /my-index/
{
  "mappings": {
    "properties": {
      "@timestamp": {
        "format": "strict_date_optional_time||epoch_second",
        "type": "date"
      },
      "message": {
        "type": "wildcard"
      }
    }
  }
}
```

&emsp;&emsp;定义了你想要用来检索的字段mapping后，从你的日志数据中索引一些数据到Elasticsearch。下面的请求使用了[bulk API](#Bulk API)将原始的日志数据索引到`my-index`中。你可以使用少量的样例体验下runtime fields而不是使用你所有的日志数据。

```text
POST /my-index/_bulk?refresh
{"index":{}}
{"timestamp":"2020-04-30T14:30:17-05:00","message":"40.135.0.0 - - [30/Apr/2020:14:30:17 -0500] \"GET /images/hm_bg.jpg HTTP/1.0\" 200 24736"}
{"index":{}}
{"timestamp":"2020-04-30T14:30:53-05:00","message":"232.0.0.0 - - [30/Apr/2020:14:30:53 -0500] \"GET /images/hm_bg.jpg HTTP/1.0\" 200 24736"}
{"index":{}}
{"timestamp":"2020-04-30T14:31:12-05:00","message":"26.1.0.0 - - [30/Apr/2020:14:31:12 -0500] \"GET /images/hm_bg.jpg HTTP/1.0\" 200 24736"}
{"index":{}}
{"timestamp":"2020-04-30T14:31:19-05:00","message":"247.37.0.0 - - [30/Apr/2020:14:31:19 -0500] \"GET /french/splash_inet.html HTTP/1.0\" 200 3781"}
{"index":{}}
{"timestamp":"2020-04-30T14:31:22-05:00","message":"247.37.0.0 - - [30/Apr/2020:14:31:22 -0500] \"GET /images/hm_nbg.jpg HTTP/1.0\" 304 0"}
{"index":{}}
{"timestamp":"2020-04-30T14:31:27-05:00","message":"252.0.0.0 - - [30/Apr/2020:14:31:27 -0500] \"GET /images/hm_bg.jpg HTTP/1.0\" 200 24736"}
{"index":{}}
{"timestamp":"2020-04-30T14:31:28-05:00","message":"not a valid apache log"}
```

##### Extract an IP address from a log message (Grok)

&emsp;&emsp;如果你想要在查询结果中包含`clientip`，你可以在mapping中添加runtime field。下面的runtime脚本定义了一个grok pattern从`messsage`字段中提取出结构化的字段。

&emsp;&emsp;这个脚本匹配了`%{COMMONAPACHELOG}`的日志模式，它能理解Apache日志的结构。如果模式匹配了（`clientip != null`）。那么脚本会输出匹配到的IP地址。如果没有匹配只会返回字段值而不会出现问题

```text
PUT my-index/_mappings
{
  "runtime": {
    "http.clientip": {
      "type": "ip",
      "script": """
        String clientip=grok('%{COMMONAPACHELOG}').extract(doc["message"].value)?.clientip;
        if (clientip != null) emit(clientip); 
      """
    }
  }
}
```

&emsp;&emsp;第8行，这个条件保证了即使没有匹配到也不会输出任何信息

&emsp;&emsp;你可以定义一个简单的query来查询指定的IP地址并且返回相关的字段。在search API中使用`fields`参数可以返回`http.clientip`这类runtime field。

```text
GET my-index/_search
{
  "query": {
    "match": {
      "http.clientip": "40.135.0.0"
    }
  },
  "fields" : ["http.clientip"]
}
```

&emsp;&emsp;下面的响应中满足`http.clientip`的值为`40.135.0.0`。

```text
{
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index",
        "_id" : "Rq-ex3gBA_A0V6dYGLQ7",
        "_score" : 1.0,
        "_source" : {
          "timestamp" : "2020-04-30T14:30:17-05:00",
          "message" : "40.135.0.0 - - [30/Apr/2020:14:30:17 -0500] \"GET /images/hm_bg.jpg HTTP/1.0\" 200 24736"
        },
        "fields" : {
          "http.clientip" : [
            "40.135.0.0"
          ]
        }
      }
    ]
  }
}
```

##### Parse a string to extract part of a field (Dissect)

&emsp;&emsp;相比较上面的例子中使用一个log pattern进行匹配，你可以自定义一个dissect pattern来包含你想要丢弃的string。

&emsp;&emsp;比如说，日志数据开头有一个`message`字段。这个字段包含多个数据片段：

```text
"message" : "247.37.0.0 - - [30/Apr/2020:14:31:22 -0500] \"GET /images/hm_nbg.jpg HTTP/1.0\" 304 0"
```

&emsp;&emsp;你可以在runtime field定义一个dissect pattern提取出[HTTP response code](https://developer.mozilla.org/en-US/docs/Web/HTTP/Status)，也就是上面例子中的`304`。

```text
PUT my-index/_mappings
{
  "runtime": {
    "http.response": {
      "type": "long",
      "script": """
        String response=dissect('%{clientip} %{ident} %{auth} [%{@timestamp}] "%{verb} %{request} HTTP/%{httpversion}" %{response} %{size}').extract(doc["message"].value)?.response;
        if (response != null) emit(Integer.parseInt(response));
      """
    }
  }
}
```

&emsp;&emsp;你随后可以使用`http.response`这个runtime field检索指定的HTTP响应：

```text
ET my-index/_search
{
  "query": {
    "match": {
      "http.response": "304"
    }
  },
  "fields" : ["http.response"]
}
```

&emsp;&emsp;响应中包含了单个文档，其中HTTP响应为`304`：

```text
{
  "hits" : {
    "total" : {
      "value" : 1,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index",
        "_id" : "Sq-ex3gBA_A0V6dYGLQ7",
        "_score" : 1.0,
        "_source" : {
          "timestamp" : "2020-04-30T14:31:22-05:00",
          "message" : "247.37.0.0 - - [30/Apr/2020:14:31:22 -0500] \"GET /images/hm_nbg.jpg HTTP/1.0\" 304 0"
        },
        "fields" : {
          "http.response" : [
            304
          ]
        }
      }
    ]
  }
}
```

##### Split values in a field by a separator (Dissect)

&emsp;&emsp;如果你想要像上面的例子一样提取字段中部分内容，但是你想要在指定的值上进行切分。你可以使用一个dissect pattern来只提取你想要的信息，并且以某个制定的格式返回。

&emsp;&emsp;例如，比如Elasticsearch中有一些垃圾回收的日志数据，格式如下：

```text
[2021-04-27T16:16:34.699+0000][82460][gc,heap,exit]   class space    used 266K, capacity 384K, committed 384K, reserved 1048576K
```

&emsp;&emsp;你只想要提取`used`、`capacity`以及`committed`对应的数据。我们先索引一些包含日志数据的文档：

```text
POST /my-index/_bulk?refresh
{"index":{}}
{"gc": "[2021-04-27T16:16:34.699+0000][82460][gc,heap,exit]   class space    used 266K, capacity 384K, committed 384K, reserved 1048576K"}
{"index":{}}
{"gc": "[2021-03-24T20:27:24.184+0000][90239][gc,heap,exit]   class space    used 15255K, capacity 16726K, committed 16844K, reserved 1048576K"}
{"index":{}}
{"gc": "[2021-03-24T20:27:24.184+0000][90239][gc,heap,exit]  Metaspace       used 115409K, capacity 119541K, committed 120248K, reserved 1153024K"}
{"index":{}}
{"gc": "[2021-04-19T15:03:21.735+0000][84408][gc,heap,exit]   class space    used 14503K, capacity 15894K, committed 15948K, reserved 1048576K"}
{"index":{}}
{"gc": "[2021-04-19T15:03:21.735+0000][84408][gc,heap,exit]  Metaspace       used 107719K, capacity 111775K, committed 112724K, reserved 1146880K"}
{"index":{}}
{"gc": "[2021-04-27T16:16:34.699+0000][82460][gc,heap,exit]  class space  used 266K, capacity 367K, committed 384K, reserved 1048576K"}
```

&emsp;&emsp;我们再次观察下数据，里面有时间戳以及其他一些你不感兴趣的数据，然后才是`used`、`capacity`以及`committed`对应的数据.

```text
[2021-04-27T16:16:34.699+0000][82460][gc,heap,exit]   class space    used 266K, capacity 384K, committed 384K, reserved 1048576K
```

&emsp;&emsp;你可以在`gc`字段的数据中每个部分分配一个变量，然后只返回你想要的部分。用`{}`包含内容都是为一个变量。例如，变量`[%{@timestamp}][%{code}][%{desc}] `将会匹配前三个数据块，这些数据都在`[]`中。

```text
[%{@timestamp}][%{code}][%{desc}]  %{ident} used %{usize}, capacity %{csize}, committed %{comsize}, reserved %{rsize}
```

&emsp;&emsp;你的dissect pattern可以包含`used`、`capacity`、`committed`这些term而不是使用变量，因为你只想要返回这些term。当然你也可以分配变量给你想要返回的值，比如`%{usize}`、`%{csize}`、`%{comsize}`。在日志中的分隔符是逗号，因此你的dissect pattern同样需要使用这个分隔符。

&emsp;&emsp;现在你有一个dissect pattern，你可以在Painless 脚本呢中包含它并作为runtime field的一部分。这个脚本使用你这个dissect pattern将`gc`字段内容分割，然后返回你在`emit`中定义的丝逆袭。由于dissect使用了简单的语法，你只需要准确告知你想要的东西。

&emsp;&emsp;下面的pattern告诉dissect返回`used`这个term、空格、` gc.usize`的值以及一个逗号，以及其他你想要的数据。虽然这种pattern在生产环境中可能不那么有用，但它提供了大量的灵活性来实验和操作你的数据。在生产环境中，你可能只想使用`emit(gc.usize)`，然后对该值进行汇总或在计算中使用。

```text
emit("used" + ' ' + gc.usize + ', ' + "capacity" + ' ' + gc.csize + ', ' + "committed" + ' ' + gc.comsize)
```

&emsp;&emsp;我们将上面的内容一起放在查询请求中。使用[field option](#The fields option)，你可以检索`gc_size`这种runtime field类型的值。这个请求同样包含了一个bucket aggregation对你的数据分组。

```text
GET my-index/_search
{
  "runtime_mappings": {
    "gc_size": {
      "type": "keyword",
      "script": """
        Map gc=dissect('[%{@timestamp}][%{code}][%{desc}]  %{ident} used %{usize}, capacity %{csize}, committed %{comsize}, reserved %{rsize}').extract(doc["gc.keyword"].value);
        if (gc != null) emit("used" + ' ' + gc.usize + ', ' + "capacity" + ' ' + gc.csize + ', ' + "committed" + ' ' + gc.comsize);
      """
    }
  },
  "size": 1,
  "aggs": {
    "sizes": {
      "terms": {
        "field": "gc_size",
        "size": 10
      }
    }
  },
  "fields" : ["gc_size"]
}
```

&emsp;&emsp;响应中包含了来自`gc_size`的数据，跟你在dissect pattern中定义的格式完全一样。

```text
{
  "took" : 2,
  "timed_out" : false,
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "skipped" : 0,
    "failed" : 0
  },
  "hits" : {
    "total" : {
      "value" : 6,
      "relation" : "eq"
    },
    "max_score" : 1.0,
    "hits" : [
      {
        "_index" : "my-index",
        "_id" : "GXx3H3kBKGE42WRNlddJ",
        "_score" : 1.0,
        "_source" : {
          "gc" : "[2021-04-27T16:16:34.699+0000][82460][gc,heap,exit]   class space    used 266K, capacity 384K, committed 384K, reserved 1048576K"
        },
        "fields" : {
          "gc_size" : [
            "used 266K, capacity 384K, committed 384K"
          ]
        }
      }
    ]
  },
  "aggregations" : {
    "sizes" : {
      "doc_count_error_upper_bound" : 0,
      "sum_other_doc_count" : 0,
      "buckets" : [
        {
          "key" : "used 107719K, capacity 111775K, committed 112724K",
          "doc_count" : 1
        },
        {
          "key" : "used 115409K, capacity 119541K, committed 120248K",
          "doc_count" : 1
        },
        {
          "key" : "used 14503K, capacity 15894K, committed 15948K",
          "doc_count" : 1
        },
        {
          "key" : "used 15255K, capacity 16726K, committed 16844K",
          "doc_count" : 1
        },
        {
          "key" : "used 266K, capacity 367K, committed 384K",
          "doc_count" : 1
        },
        {
          "key" : "used 266K, capacity 384K, committed 384K",
          "doc_count" : 1
        }
      ]
    }
  }
}
```

### Accessing document fields and special variables
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-scripting-fields.html)

&emsp;&emsp;根据你脚本运行的位置，有不同的可以访问的变量以及文档字段。

#### Update scripts

&emsp;&emsp;在[update](#Update API)、[update-by-query](#Update By Query API)、[reindex](#Reindex API)中使用脚本时可以访问`ctx`变量：

- `ctx._source`：访问文档的[\_source](#\_source field)字段
- `ctx.op`：应用到文档的操作：`index`或`delete`
- `ctx._index`等等：访问文档的[metadata fields](#Metadata fields)，有些是只读的。

#### Search and aggregation scripts

&emsp;&emsp;除了每次搜索命中时执行一次的[script field](#Script fields)外，用于搜索和聚合的脚本将对可能匹配查询或聚合的每个文档执行一次。根据你拥有的文档数量，这可能意味着执行了数百万次或数十亿次：这些脚本需要快速执行！

&emsp;&emsp;字段值可以通过使用文档值（[doc-values](#Doc values)）、[\_source](#The document \_source)字段或[stored fields](#Stored fields（painless）)从脚本中访问，每种方式如下所述。

#### Accessing the score of a document within a script

&emsp;&emsp;在[function_score query](#Function score query)、[script-based sorting](#
Sort search results)、[aggregations](#Aggregations)中使用脚本时，可以访问`_score`变量，它代表了一篇文档的相关性打分。

&emsp;&emsp;下面的例子中在[function_score query ](#Function score query)中使用了脚本来修改每一篇文档的相关性打分：

```text
PUT my-index-000001/_doc/1?refresh
{
  "text": "quick brown fox",
  "popularity": 1
}

PUT my-index-000001/_doc/2?refresh
{
  "text": "quick fox",
  "popularity": 5
}

GET my-index-000001/_search
{
  "query": {
    "function_score": {
      "query": {
        "match": {
          "text": "quick brown fox"
        }
      },
      "script_score": {
        "script": {
          "lang": "expression",
          "source": "_score * doc['popularity']"
        }
      }
    }
  }
}
```

#### Doc values

&emsp;&emsp;目前为止在脚本中访问字段最快的方式就是使用`doc['field_name']`语法，它从[doc values](#doc_values)中提取字段值。Doc Values是列式存储，除了[analyzed text field](#Text type family)，其他类型字段mapping都是默认开启的。

```text
PUT my-index-000001/_doc/1?refresh
{
  "cost_price": 100
}

GET my-index-000001/_search
{
  "script_fields": {
    "sales_price": {
      "script": {
        "lang":   "expression",
        "source": "doc['cost_price'] * markup",
        "params": {
          "markup": 0.2
        }
      }
    }
  }
}
```

&emsp;&emsp;Doc-values可以返回"简单的"字段值比如数值、日期、地理值、term等等或者当字段是多值字段时返回一个数字。不能返回JSON对象。

> NOTE：**Missing fields**
> 如果`field`缺失的话，`doc['field']`会抛出异常。在`painless`中，可以首先通过`doc.containsKey('field')`进行检查使得安全访问`doc`。不过无法在`expression`中检查mapping中的字段在文档中是否存在

> NOTE：**Doc Values and text fields**
> `doc['field']`语法同样可以用于[analyzed text fields](##Text type family)，前提是开启了[fielddata](#fielddata mapping parameter-1)。**注意的是**，开启后会将所有的term加载到JVM堆中，对于内存和CPU的开销都很大。在脚本中访问`text`字段往往没有什么意义

#### The document \_source

&emsp;&emsp;可以通过`_source.field_name`语法访问文档的[\_source](#\_source field)。`_source`作为map-of-maps的方式加载，因此在对象字段（object field）中的属性也可以访问。比如`_source.name.first`。

> IMPORTANT：**Prefer doc-values to\_source**
> 访问`_source`往往比doc-values要慢，`_source`字段在每一个结果中用于返回一些字段时有优化，而doc values在许多文档中访问某个字段的值时有优化。
> 
> 在查询结果中为前10个命中生成[script field](#Retrieve selected fields from a search)时使用`_source`比较合适，而在其他查询和聚合用例中，通常使用doc valus更合适。

&emsp;&emsp;比如：

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "first_name": {
        "type": "text"
      },
      "last_name": {
        "type": "text"
      }
    }
  }
}

PUT my-index-000001/_doc/1?refresh
{
  "first_name": "Barry",
  "last_name": "White"
}

GET my-index-000001/_search
{
  "script_fields": {
    "full_name": {
      "script": {
        "lang": "painless",
        "source": "params._source.first_name + ' ' + params._source.last_name"
      }
    }
  }
}
```

#### Stored fields

&emsp;&emsp;`Stored fields`也即是mapping参数定义了["store"\: true](#store(mapping parameter))，可以使用`_fields['field_name'].value`或者`_fields['field_name']`的语法。

```text
PUT my-index-000001
{
  "mappings": {
    "properties": {
      "full_name": {
        "type": "text",
        "store": true
      },
      "title": {
        "type": "text",
        "store": true
      }
    }
  }
}

PUT my-index-000001/_doc/1?refresh
{
  "full_name": "Alice Ball",
  "title": "Professor"
}

GET my-index-000001/_search
{
  "script_fields": {
    "name_with_title": {
      "script": {
        "lang": "painless",
        "source": "params._fields['title'].value + ' ' + params._fields['full_name'].value"
      }
    }
  }
}
```

> TIP：**Stored vs \_source**
> `_source`字段其实是一个特殊的stored field，因此性能上跟其他的stored fields是相似的。`_source`可以访问原始的文档内容（包括原始文档中某些因为空值而在索引过程中被移除，简单的标量值在索引后变成单值的数组）
> 
> 只有在`_srouce`特别大时使用stored fields才有意义，因为这样只会加载指定的较小的stored field而不是加载整个`_source`

## Data management
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/data-management.html)

&emsp;&emsp;你在Elasticsearch中存储的数据通常是下面类别中的一种：

- Content：你想要查询的items集合，比如产品目录。
- Time series data：不断产生并且带有时间戳的数据流，比如日志（log entry）。

&emsp;&emsp;Content可能会经常更新，但是Content的价值随着时间的流逝仍然是相对不变的。你可能想要快速的获取items，并且不关心这些数据的新旧。

&emsp;&emsp;时序数据（Time series data）随着时间的流逝（over time）不断的增长（accumulate），所以你需要平衡存储开销和数据的价值的策略。这些数据随着时间的推移（as it ages），变的不重要并且很少被访问。所以你可以将数据移动到更低成本，更低性能的硬件中。对于最旧的数据（for your oldest data），最重要的是你仍然可以访问它，但是如果查询时间很长也是可以接受的。

&emsp;&emsp;Elasticsearch可以基于下面的内容帮你管理你的数据：

- 在data node上根据不同的性能属性定义[multiple tiers](#Data tiers)
- 根据你的性能要求以及[index lifecycle management](#ILM: Manage the index lifecycle)的保留策略（retention policy）自动的在不同的data tiers中进行转化。
- 利用[searchable snapshots](#Searchable snapshots)将数据存储在一个remote repository中， 为你的较旧的数据提供弹性。同时降低了操作开销并且保持查询性能。
- 在较低性能的硬件上执行数据的异步查询[asynchronous searches](#Long-running searches)

### ILM: Manage the index lifecycle
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-lifecycle-management.html)

&emsp;&emsp;你可以通过配置index lifecycle management（ILM），根据你的性能、弹性（resiliency）、保留（retention）需求来自动的管理索引。你可以使用ILM用于：

- 你可以在一个索引中文档数量达到某个大小时使用新的索引（spin up a new index）
- 按照每天、每周、每月创建一个新的索引并且归档之前的索引
- 删除过时的（stale）索引来强制执行数据保留标准

&emsp;&emsp;你可以通过Kidddbana Management或者ILM接口来创建管理索引生命周期策略。当你使用Elastic Agent、Beats、或者Logstash时会自动创建默认的ILM策略。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/index-lifecycle-policies.png">

> Tips: 使用[snapshot lifecycle policies](#Automate snapshots with SLM)自动备份你的索引以及管理snapshot。


- [Tutorial: Customize built\-in policies](#Tutorial: Customize built-in ILM policies)
- [Tutorial: Automate rollover](#Tutorial: Automate rollover with ILM)
- [Overview](#ILM overview)
- [Concepts](#ILM concepts)
- [Configure a lifecycle policy](#Configure a lifecycle policy)
- [Migrate index allocation filters to node roles](#Migrate index allocation filters to node roles)
- [Troubleshooting index lifecycle management errors](#Troubleshooting index lifecycle management errors)
- [Start and stop index lifecycle management](#Start and stop index lifecycle management)
- [Manage existing indices](#Manage existing indices)
- [Skip rollover](#Skip rollover)
- [Restore a managed data stream or index](#Restore a managed data stream or index)
- [Index lifecycle management APIs](#Index lifecycle management APIs)
- [Index lifecycle actions](#Index lifecycle actions)

### Tutorial: Customize built-in ILM policies
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/example-using-index-lifecycle-policy.html)

&emsp;&emsp;Elasticsearch包含了下列内置的ILM策略

- logs
- metris
- synthetics

&emsp;&emsp;Elastic Agent使用这些策略为data streams管理backing索引。这个教程展示如何基于你的性能、弹性（resiliency）、保留（retention）需求使用Kibana的**Index Lifecycle Policies**来自定义策略。

#### Scenario

&emsp;&emsp;你想要将日志文件发送给Elasticsearch集群用于可视化和分析数据，这些数据有下列的保留需求（retention requirement）

- 当write index达到50GB或者30天，滚动（roll over）到一个新的索引
- 在滚动后，在hot tier保留30天
- 30天以后滚动
  - 将索引移动到warm tier
  - 将副本分片数量设置为1
  - [Force merge](#Force merge API)索引的段来释放被删除的文档占用的空间
- 在滚动90后天删除索引

#### Prerequisites

&emsp;&emsp;要完成这个教程，你需要准备下面的内容:

- 集群中有hot和warm tier
  - Elasticsearch Service：在Elasticsearch Service上的部署默认有一个hot tier。若要添加一个warm tier，点击你的部署并点击**Add capacity**

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/tutorial-ilm-ess-add-warm-data-tier.png">

- Self-managed cluster：赋予节点`data_hot`和`data_warm`的角色，见[Data tiers.](#Data tiers)。

&emsp;&emsp;例如，在warm tier中的每一个节点的`elasticsearch.yml`中包含`data_warm`的节点角色：

```text
node.roles: [ data_warm ]
```

&emsp;&emsp;安装Elastic Agent并配置为将日志发送到你的Elasticsearch集群。

#### View the policy

&emsp;&emsp;Elastic Agent使用index pattern为`logs-*-*`的data streams来存储日志监控数据。内置的`logs`ILM策略自动的为data stream管理backing indices。

&emsp;&emsp;若要在Kibana中查看`logs`策略：

1. 打开菜单并进入**Stack Management > Index Lifecycle Policies**
2. 选择**logs**策略

&emsp;&emsp;`logs`策略使用了默认推荐的rollover：当前write index的大小达到50GB或者30天后就开始写入到新的索引。

&emsp;&emsp;若要查看或者修改rollover设置，点击hot阶段的**Advanced settings **。然后关闭**Use recommended defaults**来展示rollover设置。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/tutorial-ilm-hotphaserollover-default.png">

#### Modify the policy

&emsp;&emsp;默认的`logs`策略设计为防止生成很多小的按日创建的索引。你可以修改这个策略来满足你的性能要求以及资源使用管理。

1. 激活warm阶段并点击**Advanced settings**
   1. 设置**Move data into phase when**为**30 days old**。这个索引在滚动后的30天移动到warm阶段
   2. 开启**Set replicas**并修改**Number of replicas**为**1**
   3. 开启**Force merge data**并设置**Number of segments**为**1**


<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/tutorial-ilm-modify-default-warm-phase-rollover.png"> 

2. 在warm阶段，点击trash 图标来开启delete阶段。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/tutorial-ilm-enable-delete-phase.png"> 

&emsp;&emsp;在delete阶段，将**Move data into phase when**设置为**90 days old**。这个索引在滚动后30天被删除。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/tutorial-ilm-delete-rollover.png"> 

3. 点击**Save Policy**

### Tutorial: Automate rollover with ILM
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/getting-started-index-lifecycle-management.html#manage-time-series-data-without-data-streams)

&emsp;&emsp;当你不停的将带有时间戳的文档索引到Elasticsearch中，你通常会使用[data stream](#Data streams)，使得你可以周期性的滚动（roll over）到一个新的索引。能让你实现一个 hot-warm-cold的架构，为你的最新数据满足性能要求，控制随着时间增加的成本，执行保留策略，并且能最大力度利用你的数据（get the most out of  your data）。

> TIP：Data streams最适合用于[append-only](#Append-only)的用例。如果你需要跨多个索引经常更新或者删除现有的文档，我们建议你使用一个index alias和index template。你仍然可以用ILM来管理并且rollover alias indices。直接跳到[Manage time series data without data streams](#Manage time series data without data streams)。

&emsp;&emsp;若要使用ILM自动的rollover并且管理一个data stream，你可以：

1. [Create a lifecycle policy](#Create a lifecycle policy)来定义合适的阶段和动作
2. [Create an index template](#Create an index template to create the data stream and apply the lifecycle policy)来创建data stream以及应用ILM策略， 为backing index配置索引设置和mapping
3. [Verify indices are moving through the lifecycle phases ](#Check lifecycle progress) as expected

&emsp;&emsp;见[rollover](#Rollover（concept）)了解rolling indices。

> IMPORTANT：When you enable index lifecycle management for Beats or the Logstash Elasticsearch output plugin, lifecycle policies are set up automatically. You do not need to take any other actions. You can modify the default policies through [Kibana Management](#Tutorial: Customize built-in ILM policies) or the ILM APIs.

#### Create a lifecycle policy

&emsp;&emsp;生命周期策略指定了索引生命周期的阶段以及在每一个阶段中需要执行的动作。一个生命周期可以有最多5个阶段：`hot`，`warm`， `cold`，`frozen`，`delete`。

&emsp;&emsp;例如，你可能定义了一个`timeseries_policy`，它有两个阶段：

- `hot`阶段定义了一个rollover动作：当索引的大小达到为50GB（`max_primary_shard_size`）或者达到30天（`max-age`）就进行滚动（roll over）
- `delete`阶段定义了在滚动后的90天（`min_age`）就移除索引

> NOTE：`min_age`是相对于滚动的时间而不是索引的创建时间。

&emsp;&emsp;你可以通过Kibana或者[create or update policy ](#Create or update lifecycle policy API) API来创建策略。若要通过Kibana创建策略，打开菜单并进入**Stack Management > Index Lifecycle Policies**，点击**Create policy**。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/create-policy.png">

&emsp;&emsp;API例子：

```text
PUT _ilm/policy/timeseries_policy
{
  "policy": {
    "phases": {
      "hot": {                                
        "actions": {
          "rollover": {
            "max_primary_shard_size": "50GB", 
            "max_age": "30d"
          }
        }
      },
      "delete": {
        "min_age": "90d",                     
        "actions": {
          "delete": {}                        
        }
      }
    }
  }
}
```

&emsp;&emsp;第5行，`min_age`默认值是`0ms`，所以新的索引会马上进入到`hot`阶段
&emsp;&emsp;第8行，满足任意一个后就触发`rollover`动作
&emsp;&emsp;第14行，滚动后90天将索引移动到`delete`阶段
&emsp;&emsp;第16行，当索引进入到delete阶段后触发`delete`动作

#### Create an index template to create the data stream and apply the lifecycle policy

&emsp;&emsp;若要建立一个data stream， 首先创建一个index template来定制生命周期策略。因为模板用于data stream，所以它必须包括`data_stream`的定义。

&emsp;&emsp;例如，你可能创建一个名为`timeseries_template`的模板用于一个名为`timeseries`的data stream。

- `index.lifecycle.name`指定生命周期策略的名称，并应用到data stream中。

&emsp;&emsp;你可以使用Kibana创建模板向导程序来添加模板。在Kibana中，打开菜单并进入**Stack Management > Index Management**。在**Index Templates**页，点击**Create template**。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/create-index-template.png">

&emsp;&emsp;向导程序调用[create or update index template API](#Create or update index template API)并使用你指定的选项创建index template。

&emsp;&emsp;API例子：

```text
PUT _index_template/timeseries_template
{
  "index_patterns": ["timeseries"],                   
  "data_stream": { },
  "template": {
    "settings": {
      "number_of_shards": 1,
      "number_of_replicas": 1,
      "index.lifecycle.name": "timeseries_policy"     
    }
  }
}
```

&emsp;&emsp;第3行，当文档索引到`timeseries`中时应用这个模板
&emsp;&emsp;第8行，使用ILM策略的名称来管理data stream

#### Create the data stream

&emsp;&emsp;索引一篇文档到[index template](#Index templates)中定义的名称或定义在`index_patterns`中的wildcard pattern中。只要现有的data stream，索引或者index alias没有使用这个名称，那么索引请求会自动的创建单个backing index和其对应的data stream。Elasticsearch自动的将请求的文档索引到backing index中，这个backing index作为这个流的[write index](#Write index)。

&emsp;&emsp;例如，下面的请求创建了名为`timeseries`的data stream并且首先创建了first generation名为`.ds-timeseries-2099.03.08-000001`的backing index。

```text
POST timeseries/_doc
{
  "message": "logged the request",
  "@timestamp": "1591890611"
}
```

&emsp;&emsp;当生命周期策略中的rollover条件满足后，`rollover`动作就会：

- 创建名为`.ds-timeseries-2099.03.08-000002`的second generation的backing index。因为它是名为`timeseries` 的data stream中的backing index，名为`timeseries_template`的模板会将配置应用到这个新的索引
- 由于它是名为`timeseries`  的data stream中最新的索引，最新创建的backing index `.ds-timeseries-2099.03.08-000002`成为data stream的write index

&emsp;&emsp;每次rollover 条件满足后就重复上面的处理过程。你可以跨data stream中所有的backing indices进行查询。由名为`timeseries`的data steam，名为`timeseries_policy`的策略管理。写操作被路由到当前的write index。读操作由所有的backing indices处理。

#### Check lifecycle progress

&emsp;&emsp;若要获得管理中的索引（managed index）的状态，你可以使用ILM explain API，能让你获得：

- index正处于的阶段以及进入这个阶段的时间
- 当前正在执行的动作和步骤
- 是否有错误发生或者处理被阻塞

&emsp;&emsp;例如，下面的请求获取了名为`timeseries` 的data stream的backing indices的信息：

```text
GET .ds-timeseries-*/_ilm/explain
```

&emsp;&emsp;下面的响应显示data stream的first generation backing index正在等待`hot`阶段中的`rollover`动作。它仍在这个状态并且ILM继续调用`check-rollove-ready`直到rollover的条件满足：

```text
{
  "indices": {
    ".ds-timeseries-2099.03.07-000001": {
      "index": ".ds-timeseries-2099.03.07-000001",
      "index_creation_date_millis": 1538475653281,
      "time_since_index_creation": "30s",        
      "managed": true,
      "policy": "timeseries_policy",             
      "lifecycle_date_millis": 1538475653281,
      "age": "30s",                              
      "phase": "hot",
      "phase_time_millis": 1538475653317,
      "action": "rollover",
      "action_time_millis": 1538475653317,
      "step": "check-rollover-ready",            
      "step_time_millis": 1538475653317,
      "phase_execution": {
        "policy": "timeseries_policy",
        "phase_definition": {                    
          "min_age": "0ms",
          "actions": {
            "rollover": {
              "max_primary_shard_size": "50gb",
              "max_age": "30d"
            }
          }
        },
        "version": 1,
        "modified_date_in_millis": 1539609701576
      }
    }
  }
}
```

&emsp;&emsp;第6行，age of the index，用于跟`max_age`计算出rollover的时间
&emsp;&emsp;第8行，管理这个索引的策略名称
&emsp;&emsp;第10行，age of the indexed，用于转移到下一个阶段（在这个例子中跟age of the index一样）
&emsp;&emsp;第15行，ILM在这个索引上正在执行的步骤
&emsp;&emsp;第19行，当前阶段的定义（`hot`阶段的定义）

#### Manage time series data without data streams

&emsp;&emsp;尽管[data streams](#Data streams)是用于扩展和管理时序数据的一种便捷方式，它们被设计为用于append-only。我们认识到存在一些用例需要就地（in place）更新或者删除数据并且data streams不支持直接删除跟更新请求，所以Index APIs需要直接用于data stream的backing indice上。

&emsp;&emsp;在这些情况下，你可以使用index alias来管理包含时序数据的索引并且周期性的roll override到一个新的索引。

若要使用ILM并使用index alias来自动的rollover以及管理时序数据，你需要：

1. 创建一个生命周期策略，定义合适的阶段和动作，见[Create a lifecycle policy](#Create a lifecycle policy)
2. [Create an index template](#Create an index template to apply the lifecycle policy)将策略应用到每一个新的索引
3. [Bootstrap an index](#Bootstrap the initial time series index with a write index alias)作为最开始的write index
4. [Verify indices are moving through the lifecycle phases](#Check lifecycle progress) as expected

#### Create an index template to apply the lifecycle policy

&emsp;&emsp;若要在rollover上自动的将生命周期策略应用到新的write index上，那么在用于创建新索引的index template中指定策略。

&emsp;&emsp;例如，你可能创建了一个`timeseries_template`的模板，它将应用到索引名匹配了`timeseries-*`index pattern的索引。

&emsp;&emsp;若要自动的rollover，那么在模板中配置两个ILM设置：

- `index.lifecycle.name`指定生命周期策略的名称，该策略应用到匹配了index pattern的新的索引上
- `index.lifecycle.rollover_alias`指定了index alias，当rollover动作出发后，它将被roll over

&emsp;&emsp;你可以使用Kibana创建模板向导程序来添加模板。若要访问向导程序，打开菜单并且进入**Stack Management > Index Management**。在**Index Templates**页面，点击**Create template**。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/create-template-wizard.png">

&emsp;&emsp;创建模板请求：

```text
PUT _index_template/timeseries_template
{
  "index_patterns": ["timeseries-*"],                 
  "template": {
    "settings": {
      "number_of_shards": 1,
      "number_of_replicas": 1,
      "index.lifecycle.name": "timeseries_policy",      
      "index.lifecycle.rollover_alias": "timeseries"    
    }
  }
}
```

&emsp;&emsp;第3行，模板将应用到索引名以`timeseries-`开头的索引上
&emsp;&emsp;第8行，生命周期策略的名称，将应用到每一个新的索引上
&emsp;&emsp;第9行，alias的名称用于引用这些索引，要求策略使用rollover动作。

#### Bootstrap the initial time series index with a write index alias

&emsp;&emsp;你需要引导一个最开始的索引并且为你模板中的rollover alias将这个索引指为write index。这个索引的名称必须匹配模板中的index pattern并且以数字结尾。在滚动时，这个数字会递增用于生成新的索引的名称。

&emsp;&emsp;例如，下面的请求创建了一个名为`timeseries-000001`的索引，并且让它作为名为 `timeseries` 的alias的write index。

```text
PUT timeseries-000001
{
  "aliases": {
    "timeseries": {
      "is_write_index": true
    }
  }
}
```

&emsp;&emsp;当满足了rollover的条件，`rollover`动作就会：

- 创建一个名为`timeseries-000002`的新的索引。这个名称匹配了`timeseries-*` pattern，所以`timeseries_template`模板中的设置会应用到这个新的索引上
- 将这个新的索引指为write index并且让引导索引（`timeseries-000001`）变成只读

#### Check lifecycle progress(index)

&emsp;&emsp;对管理中的索引获取其状态信息类似于data stream，见[check progress section ](#Check lifecycle progress)了解更多信息。唯一的不同是 indices namespace，so retrieving the progress will entail the following api call:

```text
GET timeseries-*/_ilm/explain
```

### Index management in Kibana
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-mgmt.html)

&emsp;&emsp;Kibana的**Index Management **功能是一种简单方便的用于管理你集群的索引，[data streams](#Data streams)，[index template](#Index templates)的方法。实践良好的（practising）索引管理可确保以最具成本效益的方式正确存储数据。

#### What you’ll learn

&emsp;&emsp;你将会学习到如何：

- 查看和编辑索引设置（index setting）
- 查看索引的mapping和statistic
- 执行index-level的操作，例如refresh
- 查看和管理data stream
- 创建index template自动的配置新的data stream和索引

#### Required permissions

&emsp;&emsp;如果你使用了Elasticsearch security feature，则需要下面的[security privileges](#Security privileges)：

- `monitor` cluster privilege 用于访问Kibana的**Index Management**功能
- `iew_index_metadata`和`manage` index privilege用于查看data stream和索引数据
- `manage_index_templates` cluster privilege 用于管理index template

&emsp;&emsp;若要在Kibana中添加这些privilege，进入**Stack Management > Security > Roles**。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/management_index_labels.png">

&emsp;&emsp;**Index Management**页面包含了你的索引概览。索引名旁边的标签（badge）告知这个索引是一个[follower index](#Create follower API)，[rollup index](#Get rollup index capabilities API)，还是一个[frozen](#Unfreeze index API)。

&emsp;&emsp;点击某个标签后将只显示这个标签类型的索引。你也可以用搜索框过滤索引。

&emsp;&emsp;你也可以下钻每一个索引来查看[index settings](#Index Settings)，[mapping](#Mapping)和statistic。在这个视图中，你也可以编辑index settings。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/management_index_details.png">

#### Perform index-level operations

&emsp;&emsp;使用**Manage**菜单执行index-level的操作。这个菜单在索引详情视图中可见，或者当你在索引概览页面选择一个或多个索引时可见。菜单包含了下面的动作：

- [Close index](#Close index API)
- [Force merge index](#Force merge API)
- [Refresh index](#Refresh API)
- [Flush index](#Flush API)
- [Delete index](#Delete index API)
- [Add lifecycle policy](#Configure a lifecycle policy)

#### Manage data streams

&emsp;&emsp;**Data Streams**视图中列出了你的data steams并且能让你测试或者删除它们。

&emsp;&emsp;若要查看data stream的更多的信息，比如它当前的generation或者当前的索引生命周期策略，那么点击流的名称。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/management_index_data_stream_stats.png">

&emsp;&emsp;若要查看backing index的信息，点击该索引即可。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/management_index_data_stream_backing_index.png">

#### Manage index templates

&emsp;&emsp;**Index Templates**视图中列出了你的模板以及测试，编辑，克隆和删除。对某个index template的更改不会影响现有的索引。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/management-index-templates.png">

&emsp;&emsp;如果你还没有任何的模板，你可以使用**Create template**向导程序创建模板。

##### Try it: Create an index template

&emsp;&emsp;在这个教程中，你将会创建一个模板然后使用它配置两个新的索引。

- **Step 1. Add a name and index pattern**

1. 在**Index Templates**视图中，打开**Create template**的向导程序

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/management_index_create_wizard.png">

2. 在**Name**中，输入`my-indeex-template`
3. 设置**Index pattern**为`my-index-*`，使得模板匹配这种Index pattern的索引
4. 不填写**Data stream**，**Priority**，**Version**以及**\_meta field**

- **Step 2. 添加settings，mappings和aliases**

1. 添加[component templates](#Create or update component template API)到你的index template中

&emsp;&emsp;Component templates是预先配置好的mappings, index settings, and aliases的集合，你可以跨多个Index template使用。标签指出了某个component template是否包含mappings（M），Index settings（S），aliases（A），或者是三者都有。

&emsp;&emsp;Component templates是可选的，对于这个教程，不要添加任何的component templates。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/management_index_component_template.png">

2. 定义Index settings（可选）。对于这个教程，可以不填
3. 定义一个mapping，它包含一个名为`geo`类型为[object](#Object field type)字段，以及名为`coordinates`类型为[geo_point](#Geopoint field type)的子字段（child filed）

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/management-index-templates-mappings.png">

&emsp;&emsp;或者你可以点击**Load JSON**，用JSON形式定义mapping：

```text
{
  "properties": {
    "geo": {
      "properties": {
        "coordinates": {
          "type": "geo_point"
        }
      }
    }
  }
}
```

&emsp;&emsp;你可以在**Dynamic templates**和**Advanced options**页面创建额外的mapping配置。对于这个教程，不创建任何额外的mappings。

4. 定义名为`my-index`的alias：

```text
{
  "my-index": {}
}
```

5. 在review 页面，检查整体的配置。如果一切看起来没有问题就点击**Create template**。

- **Step 3. Create new indices**

&emsp;&emsp;你现在可以使用你的index template来创建新的索引

1. 索引下面的文档创建两个索引：`my-index-000001`和`my-index-000002`

```text
POST /my-index-000001/_doc
{
  "@timestamp": "2019-05-18T15:57:27.541Z",
  "ip": "225.44.217.191",
  "extension": "jpg",
  "response": "200",
  "geo": {
    "coordinates": {
      "lat": 38.53146222,
      "lon": -121.7864906
    }
  },
  "url": "https://media-for-the-masses.theacademyofperformingartsandscience.org/uploads/charles-fullerton.jpg"
}

POST /my-index-000002/_doc
{
  "@timestamp": "2019-05-20T03:44:20.844Z",
  "ip": "198.247.165.49",
  "extension": "php",
  "response": "200",
  "geo": {
    "coordinates": {
      "lat": 37.13189556,
      "lon": -76.4929875
    }
  },
  "memory": 241720,
  "url": "https:\//theacademyofperformingartsandscience.org\/people/type:astronauts/name:laurel-b-clark/profile"
}
```

2. 使用[get index API](#Get index API)查看新索引的配置，这个索引使用了你之前创建的index template。

```text
GET /my-index-000001,my-index-000002
```

### ILM overview
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/overview-index-lifecycle-management.html)

&emsp;&emsp;你可以创建以及应用（apply）索引生命周期管理（ILM：index lifecycle management）并依据性能、弹性（resiliency）、保留（retention）需求来管理你的索引。

&emsp;&emsp;索引生命周期策略能触发下列的动作：

- Rollover：当前索引占用大小、包含的文档数量达到某个值，或者达到某个寿命（age）
- Shrink：减少索引的主分片数量
- Force merge：触发[force merge](#Force merge API)来降低索引的分片中的段的数量。
- Delete：永久的移除索引中的所有数据和元数据

&emsp;&emsp;ILM使得在热-暖-冷数据架构中管理索引变得更加容易。这种架构在处理比如日志（logs）、指标（metrics）这类时序数据中非常常见。

&emsp;&emsp;你可以这么指定：

- 当达到分片大小、包含的文档数量、寿命的最大值时滚动（rollover）到一个新的索引
- 当索引不再更新，可以减少主分片的数量
- 通过force merge永久的移除被标记为删除的文档
- 将索引移动到性能较低的硬件中
- 当可用性（availability）没那么重要时，可以减少分片的数量
- 当索引可以被安全的移除时

&emsp;&emsp;例如，当你将成群的ATMs的指标数据写入到Elasticsearch中时，可以定义下面的策略：

- 当索引主分片的大小达到50GB时，滚动到新的索引中
- 将老的索引（old index）移到[warm phase](#Index lifecycle)，标记为只读，并且收缩（shrink）为单个分片。
- 7天以后把索引移到[cold phase](#Index lifecycle)，并且移到成本较低的硬件中。
- 当达到30天的保留周期时就删除索引。


>IMPORTANT：想要使用ILM必须保证每个节点的版本号是一致的。在一个版本号混在的集群中使用ILM可能可以创建并应用策略，但是不能保证可以按照预期运行。在集群中尝试使用一个策略时，如果其对应的动作（action）不是所有节点都支持的话会引发错误。

### ILM concepts
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-concepts.html)

- [Index lifecycle](#Index lifecycle)
- [Rollover](#Rollover（concept）)
- [Policy updates](#Lifecycle policy updates)

#### Index lifecycle
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-index-lifecycle.html)

&emsp;&emsp;ILM定义了五个生命周期阶段：

- Hot：这个索引的更新、查询操作非常的活跃
- Warm：这个索引不再更新，并且仍然会被查询
- Cold：这个索引不再更新并且很少（infrequent）被查询。索引中的数据仍然可以被搜索到，但是允许查询速度较慢
- Frozen：这个索引不再更新并且几乎（rare）不会被查询。索引中的数据仍然可以被搜索到，但是允许查询速度特别慢
- Delete：不再需要这个索引，可以被移除

&emsp;&emsp;一个索引的生命周期策略指定了哪个阶段是适用的（applicable）、在每一个阶段执行哪些动作、什么时候转变阶段（transition between phases）。

&emsp;&emsp;在你创建索引时可以手动应用一个生命周期策略。对于时序索引（time series indices），你需要用index template在时序中创建新的索引，并且将生命周期策略关联到index template。当（rollover）索引时，手动应用的策略不会自动应用到新索引。

&emsp;&emsp;如果你使用Elasticsearch的安全功能。ILM将以最后更新策略的用户的身份执行操作。ILM only has the [roles](#Defining roles) assigned to the user at the time of the last policy update。

##### Phase transitions

&emsp;&emsp;ILM根据生命周期中的寿命（age）来移动索引（move index）。你可以在每一个阶段设置一个寿命最小值（maximum age）来控制转变的时机（the timing of transition）。对于一个索引移动到另一个阶段来说，当前阶段的所有动作都必须已经完成并且the index must be older than the minimum age of the next phase。下一个阶段配置的最小寿命必须高于当前阶段。比如说warm阶段的寿命设置为10天，那么下一个阶段，即cold阶段的寿命要么不设置，要么设置成一个大于等于10天的值。

&emsp;&emsp;寿命最小值默认值为0，意味着当前阶段所有的动作都完后就马上转变到下一个阶段。

&emsp;&emsp;如果索引有未分配的分片，并且[cluster health status](#Cluster health API)的状态为yellow，仍然可以根据生命周期策略转移到下一个阶段。然后由于Elasticsearch只会在状态为green的集群上执行相关的clean up工作，所以可能会有不可预料的副作用。

&emsp;&emsp;为了避免不断增加的磁盘使用量和可靠性问题，请及时处理集群运行状况问题。

##### Phase execution

&emsp;&emsp;ILM控制了一个阶段中所有动作的执行顺序以及执行步骤来执行必要的索引操作。

&emsp;&emsp;当索引进入到一个阶段，ILM缓存索引元数据（index metadata）中的阶段定义（phase definition）。这使得不会因为更新策略将索引置入一个用于无法退出阶段的状态。如果可以安全的应用变更，ILM会更新已经缓存的阶段定义。如果不能，则使用缓存的定义继续执行下去。

&emsp;&emsp;ILM会周期的运行检查某个索引是否满足策略规则（policy criteria）并执行必要的步骤。为了避免race condition，ILM可能需要执行多次才能执行所有的步骤，每一个步骤都需要完成一个动作。例如，如果ILM检测到一个索引满足滚动规则，它开始执行这个步骤并且要求完成这个滚动动作。如果它到达了一个点使得不能安全的进行入到下一步骤，那么终止执行。下一次ILM运行后，它会继续执行（pick up execution where it left off）。这意味着即使`indices.lifecycle.poll_interval`设置为10分钟并且索引满足滚动规则。也有可能需要20分钟来完成滚动。

##### Phase actions

&emsp;&emsp;ILM支持每一个阶段中以下的动作，ILM有序执行这些动作。

- Hot
  - [Set Priority](#Set priority)
  - [Unfollow](#Unfollow)
  - [Rollover](#Rollover（action）)
  - [Read-Only](#Read only)
  - [Shrink](#Shrink)
  - [Force Merge](#Force merge)
  - [Searchable Snapshot](#Searchable snapshot)

- Warm
  - [Set Priority](#Set priority)
  - [Unfollow](#Unfollow)
  - [Read-Only](#Read only)
  - [Allocate](#Allocate)
  - [Migrate](#Migrate)
  - [Shrink](#Shrink)
  - [Force Merge](#Force merge)

- Cold
  - [Set Priority](#Set priority)
  - [Unfollow](#Unfollow)
  - [Read-Only](#Read only)
  - [Searchable Snapshot](#Searchable snapshot)
  - [Allocate](#Allocate)
  - [Migrate](#Migrate)

- Frozen
  - [Searchable Snapshot](#Searchable snapshot)

- Delete
  - [Wait For Snapshot](#Wait for snapshot)
  - [Delete](#Delete)

#### Rollover（concept）
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-rollover.html)

&emsp;&emsp;当索引日志或者指标这类时序数据的时候，你不能往单个索引中无限的写入。为了能满足你的索引和查询性能要求以及资源使用的管理，你往一个索引中写入数据直到满足阈值，然后再创建一个新的索引，并开始写到这个索引中。使用 rolling indices可以让你：

- 优化active index，在高性能的`hot`节点上有high ingest rates
- 优化`warm`节点上的查询性能
- 将旧的，很少被访问的数据移动到低成本的`code`节点上
- 根据你的保留策略（retention policy），通过删除整个索引的方式来删除数据

&emsp;&emsp;我们建议使用[data streams](#Create data stream API)来管理时序数据。Data streams自动的跟踪write index并且保持配置最小化。

&emsp;&emsp;每一个data stream要求一个[index template](#Index templates)，模板中包括：

- 用于data stream的名称或者通配符（`*`)
- data stream的timestamp字段。这个字段必须是[date](#Date field typec)或者[date_nanos](#Date nanoseconds field type)的date类型，每一篇索引到data stream的文档都要包含这个字段
- mappings和settings要应用到每一个创建的[backing index](#Backing indices)_上

&emsp;&emsp;Data streams为append-only data设计，data stream name可以用于作为操作（read，write，shrink等等）目标。如果你的用例要求就地（in place）更新数据。你可以使用[index aliases](#Aliases)来管理你的时序数据。然而需要一些配置步骤以及概念：

- 一个`index template`为序列中的每一个新索引指定了settings。你通常要通过有多少hot node就使用多少分片的方式来为提取（ingestion）优化
- 一个`index alias`引用了整个的索引集合
- 单个索引指为（designate）`write index`。这里处理所有的写请求的active index。On each rollover，新的索引会作为write index

##### Automatic rollover

&emsp;&emsp;ILM能让你自动的基于索引大小（index size）、文档数量（document count），寿命（age）滚动（roll over）到一个新的索引。当触发了一个rollover，就会创建一个新的索引。write alias会被更新指向新的索引，接下来的更新都会写入到新的索引中。

> TIP：基于大小，文档数量，或者寿命的滚动比基于时间（time-based）的更可取（prefer）。在任意的时间（arbitrary time）进行rolling over经常会生成小的索引，对性能和资源使用有负面的影响。

#### Lifecycle policy updates
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/update-lifecycle-policy.html)

&emsp;&emsp;你可以通过修改当前的策略或者切换到一个不同的策略来改变索引的生命周期或者rolling indices的收集的管理。

&emsp;&emsp;为了保证更新策略后不会让某个索引进入到某个状态后无法退出当前的阶段，因此在进入阶段后，阶段定义（phase definition）缓存在index metadata中。如果能安全的应用变更，ILM就更新缓存的阶段定义。如果不能，使用缓存的定义继续阶段的执行。

&emsp;&emsp;当索引进入到下一个阶段，它会使用更新后的策略中的阶段定义。

##### How changes are applied

&emsp;&emsp;当某个策略最开始应用到某个索引时，索引获取到了最新的策略版本号。如果你更新了策略，策略的版本号将被变更，ILM能检测侧到索引正在使用一个较早版本的策略，这个策略需要更新。

&emsp;&emsp;对`min_age`的更改不会propagate到缓存的定义。修改一个阶段的`min_age`不会影响当前正在执行那个阶段的索引。

&emsp;&emsp;例如，如果你创建了一个策略包含了一个hot 阶段并且没有指定`min_age`。当应用策略后，索引马上进入到这个阶段。如果你随后更新了策略并指定了值为一天的`min_age`。那不会影响已经进入到hot阶段的索引。策略更新后，新创建的索引只有等到一天后才能进入hot阶段。

##### How new policies are applied

&emsp;&emsp;当你应用一个策略来管理索引，索引会使用上一个策略来完成当前的策略。索引会在进入到下一个阶段时使用新的策略。

### Index lifecycle actions
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-actions.html)

| [Allocate](#Allocate) | 将分片移到性能特征（performance characteristic）不同的节点上并减少副本的数量 |
| :------: | ---- |
| [Delete](#Delete) | 永久移除索引 |
| [Force Merge](#Force merge（action）) | 减少索引中段的数量并且清楚（purge）被删除的文档。索引被置为只读 |
| [Migrate](#Migrate) | 将索引的分片迁移到当前ILM阶段对应的数据层（[data tier](#Data tiers)）中 |
| [Read-Only](#Read only) | 阻塞索引的写入操作 |
| [Rollover](#Rollover（action）) | 为rollover alias移除作为write index的索引，并开始为新索引建立索引 |
| [Searchable Snapshot](#Searchable snapshot) | 在配置好的仓库中添加一个被管理的索引的快照，挂载这个快照使其成为一个可以用于搜索的快照。 |
| [Set Priority](#Set priority) | 降低索引在生命周期中的优先级，以确保首先恢复hot索引 |
| [Shrink](#Shrink) | 收缩到新的索引中并减少主分片的数量 |
| [Unfollow](#Unfollow) | 将一个follower index转化为常规索引。会在执行rollover、shrink、searchable snapshot动作前自动执行该动作 |
| [Wait For Snapshot](#Wait for snapshot) | 保证在删除索引前已经创建好了snapshot |



#### Allocate
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-allocate.html)

&emsp;&emsp;可以在warm、cold阶段使用该动作（action）。

&emsp;&emsp;更新（update）索引设置来变更（change）哪些节点允许存放（host）索引分片以及变更副本分片（replica）的数量。

&emsp;&emsp;不允许在hot阶段执行这个allocate动作。索引最初的分配必须通过手动或者[index template](#Index templates)实现。

&emsp;&emsp;你可以配置这个动作来同时修改分配规则（allocation rules）和副本数量，只修改分配规则或者只修改副本数量。见[Scalability and resilience](#Scalability and resilience: clusters, nodes, and shards)了解更多Elasticsearch如何使用副本来实现伸缩性（scaling）的信息。见[index-level shard allocation filtering](#Index-level shard allocation filtering)了解更多Elasticsearch如何控制指定索引的分片分配。

##### Options

&emsp;&emsp;你必须指定副本的数量或者至少包含一个`include`、`exclude`、`require`选项。空的allocate动作是非法的。

&emsp;&emsp;见[Index-level shard allocation filtering](#Index-level shard allocation filtering)了解更多使用自定义属性进行分片分配的信息。

- number_of_replicas：（Optional,integer）索引的副本分片数量。
- total_shards_per_node：（Optional,integer）在单个Elasticsearch节点上索引的分片数量最大值
- include：（Optional,object）分配一个索引到一些节点，这些节点至少包含一个指定的自定义的属性。
- exclude：（Optional,object）分配一个索引到一些节点，这些节点不包含任何自定义的属性。
- require：（Optional,object）分配一个索引到一些节点，这些节点包含所有自定义的属性。

##### Example

&emsp;&emsp;下面的策略中的allocate动作将索引的副本分片数量更改为2。在任何单个节点上放置的索引分片不超过200个。否则不改变索引分配规则。

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "warm": {
        "actions": {
          "allocate" : {
            "number_of_replicas" : 2,
            "total_shards_per_node" : 200
          }
        }
      }
    }
  }
}
```

###### Assign index to nodes using a custom attribute

&emsp;&emsp;下面策略中的allocate动作将索引分配到一些节点，这些节点需要包含值为`hot`或者`warm`的`box_type`属性。

&emsp;&emsp;为了指明（designate）一个节点的`box_type`，你可以在节点配置（node configuration）中设置这个自定义的属性。例如说，在`elasticsearch.yml`中设置` node.attr.box_type: hot`。见[Enabling index-level shard allocation filtering](#Index-level shard allocation filtering)了解更多信息。

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "warm": {
        "actions": {
          "allocate" : {
            "include" : {
              "box_type": "hot,warm"
            }
          }
        }
      }
    }
  }
}
```

###### Assign index to nodes based on multiple attributes

&emsp;&emsp;allocate动作可以基于多个节点属性将索引分配到节点。下面的allocate动作基于`box_type`和`storage`这两个节点属性进行索引的节点分配。

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "cold": {
        "actions": {
          "allocate" : {
            "require" : {
              "box_type": "cold",
              "storage": "high"
            }
          }
        }
      }
    }
  }
}
```

######  Assign index to a specific node and update replica settings

&emsp;&emsp;下面策略中的allocate动作会将索引的副本分片数量更新为1个并且索引会被分配到带有值为`cold`的`box_type`属性的节点中。

&emsp;&emsp;为了指明（designate）一个节点的`box_type`，你可以在节点配置（node configuration）中设置这个自定义的属性。例如说，在`elasticsearch.yml`中设置` node.attr.box_type: cold`。见[Enabling index-level shard allocation filtering](#Index-level shard allocation filtering)了解更多信息。

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "warm": {
        "actions": {
          "allocate" : {
            "number_of_replicas": 1,
            "require" : {
              "box_type": "cold"
            }
        }
        }
      }
    }
  }
}
```

#### Delete
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-delete.html)

&emsp;&emsp;可以在delete阶段使用该动作。

&emsp;&emsp;永久移除索引。

##### Options

- delete_searchable_snapshot：（Optional, Boolean）删除在上一个阶段生成可以用于搜索的快照（searchable snapshot）。默认值为true。这个适用于任意上一个阶段中使用了[searchable snapshot](#Searchable snapshot)这个动作。

##### Example

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "delete": {
        "actions": {
          "delete" : { }
        }
      }
    }
  }
}
```

#### Force merge（action）
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-forcemerge.html)

&emsp;&emsp;可以在hot、warm阶段使用该动作。

&emsp;&emsp;[Force merge](#Force merge API)将索引合并到指定数量的[segments](#Index segments API)中，这个动作会另索引变为[read-noly](#Dynamic index settings)。


>NOTE：forcemerge这个动作属于best effort。这个动作有可能在一些分片正在分配时执行，在这种情况下这些分片不会进行合并。

&emsp;&emsp;在`hot`阶段使用`forcemerge`动作需要有`rollover`动作。如果没有配置`rollover`动作，ILM会reject这个策略。

##### Performance considerations

&emsp;&emsp;Force merge是一个资源密集型（resource-intensive）的操作。如果一次性触发太多的force merge，将会对你的集群造成负面的影响。这种情况经常发生在你将一个包含force merge动作的ILM策略应用到现有的（existing）索引中。如果这些索引满足`min_age`规则（criteria），它们会马上被处理并经历多个阶段。你可以通过提高`min_age`或者设置`index.lifecycle.origination_date`来修改计算索引寿命的方式来阻止这种情况。

&emsp;&emsp;如果发生了force merge任务队列堆积，你可能需要增加force merge线程池的大小，使得索引可以并行的进行force merge。你可以通过配置`thread_pool.force_merge.size`  进行更改[cluster setting](#Cluster get settings API)。

>IMPORTANT：这种操作会造成持续性的（cascading）性能影响，监控好集群的性能并且提高线程池的大小慢慢的减少堆积。

&emsp;&emsp;Force merge动作会根据索引处于的阶段在对应的节点执行。处于`hot`阶段的forcemerge会在潜在较快的hot节点执行，同时对提取（ingestion）影响更大。处于`warm`阶段的forcemerge会在warm节点执行，执行时间会潜在较长，但是不会影响`hot`层的提取（ingestion）。

##### Option

- max_num_segemnts：（Required, integer）合并后的段的数量。通过设置为1来实现完全的合并（fully merge）。
- inde_codec：（Optional, string）用来对文档存储的codec。唯一可以设置的值是`best_compression`，使用了 [DEFLATE](https://en.wikipedia.org/wiki/Deflate)实现较高的压缩率但是较低的[存储字段](https://www.amazingkoala.com.cn/Lucene/suoyinwenjian/2020/1013/索引文件之fdx&&fdt&&fdm)的性能。不指定该参数则使用默认的[LZ4 codec](https://www.amazingkoala.com.cn/Lucene/yasuocunchu/2019/0226/LZ4)。

>WARNING：如果使用`best_compression`，ILM将在force merge之前先[close](#Close index API)并且[re-open](#Open index API)。当关闭后，索引的读写操作将不可见。

##### Example

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "warm": {
        "actions": {
          "forcemerge" : {
            "max_num_segments": 1
          }
        }
      }
    }
  }
}
```


#### Migrate
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-migrate.html)

&emsp;&emsp;可以在warm、cold阶段使用该动作。

&emsp;&emsp;通过更新索引设置[index.routing.allocation.include.\_tier\_preference](#index.routing.allocation.include.\_tier\_preference)将索引移动到当前阶段对应的[data tier](#Data tiers)中。ILM自动在warm和cold阶段注入（inject）该动作。如果要阻止自动迁移（migration），你可以显示指定migrate动作，并且将enabled参数设置为`false`。

&emsp;&emsp;If the cold phase defines a searchable snapshot action the migrate action will not be injected automatically in the cold phase because the managed index will be mounted directly on the target tier using the same [\_tier\_preference](#index.routing.allocation.include.\_tier\_preference) infrastructure the migrate actions configures。

&emsp;&emsp;在`warm`阶段，如果[index.routing.allocation.include.\_tier\_preference](#index.routing.allocation.include.\_tier\_preference)设置为`data_warm`，`data_hot`，会将索引移动到[warm tier](#Warm tier)的节点上。如果没有一个节点是warm tier，那么就移动到 [hot tier](#Hot tier)。

&emsp;&emsp;在`cold`阶段，如果[index.routing.allocation.include.\_tier\_preference](#index.routing.allocation.include.\_tier\_preference)设置为`data_cold`，`data_warm`，`data_hot`，会将索引移动到[Cold tier](#Cold tier)的节点上。如果没有一个节点是cold tier，那么就移动到warm tier，否则移动到hot tier。

&emsp;&emsp;`frozen`阶段不允许执行migrate动作。这个阶段会直接将searchable snapshot挂载到`data_frozen`,`data_cold`,`data_warm`,`data_hot`中的一个。先移动到[frozen tier](#Frozen tier)，如果没有一个节点是frozen tier，则移动到cold tier，否则移动到warm tier，最终移动移到hot tier。

&emsp;&emsp;hot阶段不允许执行migrate动作。最初索引的分配是自动[执行](#Data tier index allocation)的，可以通过手动或者[index template](#Index templates)配置。

##### Option

- enabled：（Optional, Boolean）用于控制是否在migrate阶段自动的执行迁移动作。默认值是true。

##### Example

&emsp;&emsp;下面的策略中，ILM将索引迁移到warm node前先通过[allocate](#Allocate)动作减少副本的数量。

> NOTE：显示指定开启migrate动作是不需要的，ILM会自动执行migrate动作除非你关闭迁移。

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "warm": {
        "actions": {
          "migrate" : {
          },
          "allocate": {
            "number_of_replicas": 1
          }
        }
      }
    }
  }
}
```

##### Disable automatic migration

&emsp;&emsp;下面的策略中关闭了migrate动作并且将这个索引分配那些配置了`rack_id`的值为`one`或者`two`的节点。

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "warm": {
        "actions": {
          "migrate" : {
           "enabled": false
          },
          "allocate": {
            "include" : {
              "rack_id": "one,two"
            }
          }
        }
      }
    }
  }
}
```

#### Read only
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-readonly.html)

&emsp;&emsp;可以在hot、warm、cold阶段使用该动作。

&emsp;&emsp;使得索引变成只读，不再允许写入和元数据变更（metadata changes）操作。

&emsp;&emsp;在`hot`阶段使用`readonly`动作必须同时配置`rollover`动作。如果没有配置`rollover`动作，ILM会reject这个策略。

##### Options

&emsp;&emsp;无

##### Example

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "warm": {
        "actions": {
          "readonly" : { }
        }
      }
    }
  }
}
```

#### Rollover（action）
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-rollover.html)

&emsp;&emsp;可以在hot阶段使用该动作。

&emsp;&emsp;当现有的索引满足一个或者多个滚动（rollover）条件后滚动到一个新的索引。

> IMPORTANT：如果rollover动作应用在一个[follower index](#Create follower API)，执行策略前需要等待leader index滚动结束（或者[otherwise marked complete](#Skip rollover)），然后通过[unfollow](#Unfollow)动作将follower index转化为一个常规索引（regular index）。

&emsp;&emsp;滚动对象可以是[data stream](#Data Streams)或者[index alias](#Aliases)。当滚动对象是数据流（data stream）时，新的索引会变成数据流的write index并且提高它的generation。

&emsp;&emsp;要滚动一个index alias，alias和它的的write index必须满足下面的条件：

- 索引名称必须满足这个pattern `^.*-\d+$`，例如（my-index-00001）
- `index.lifecycle.rollover_alias`必须配置为alias进行滚动
- 索引必须是alias的[write index](#Write index（Alias）)

&emsp;&emsp;例如如果`my-index-000001`是名为`my_data`的alias。那么必须配置下面的设置：

```text
PUT my-index-000001
{
  "settings": {
    "index.lifecycle.name": "my_policy",
    "index.lifecycle.rollover_alias": "my_data"
  },
  "aliases": {
    "my_data": {
      "is_write_index": true
    }
  }
}
```

##### Options

&emsp;&emsp;你必须至少指定一个rollover条件。没有条件的rollover动作是非法的。

- max_age：（Optional,[time units](#Time units)）达到在创建索引后开始流逝的时间（elapsed time）最大值后触发滚动动作。总是从索引的创建时间开始计算流逝的时间，即使索引的原先的数据配置为自定义的数据。比如设置了[index.lifecycle.parse_origination_date](#index.lifecycle.parse_origination_date) 或者 [index.lifecycle.origination_date ](#index.lifecycle.origination_date)。
- max_docs：（Optional,integer）当达到指定的文档数量最大值时触发滚动。上一次refresh后新增的文档不在文档计数中。副本分片中的文档不在文档计数中。
- max_size：（Optional,[byte units](#Byte size units)）当索引达到一定的大小时触发滚动。指的是索引中所有主分片的大小总量。副本分片的数量不会参与统计。

  > TIP：可以通过[\_cat indices API](#cat indices API)查看当前索引的大小。`pri.store.size`值显示了所有主分片的大小总量。

- max_primary_shard_size：（Optional,[byte units](#Byte size units)）当索引中的最大的主分片的大小达到某个值就触发滚动。它是索引中主分片大小的最大值。跟`max_size`一样，副本分片则忽略这个参数。

  > TIP：可以通过[\_cat shard API](#cat shards API)查看当前分片的大小。`store`值显示了每一个分片的大小，`prirep`值指示了一个分片是主分片还是副本分片。

- max_primary_shard_docs：（Optional,integer）当索引中最大的主分片的文档数量达到某个值就触发滚动。这是索引中主分片中的文档数量最大值。跟`max_doc`一样，副本分片则忽略这个参数。

  > TIP：可以通过[\_cat shard API](#cat shards API)查看当前分片的大小。`doc`值显示了每个分片中的文档数量。

##### Example

###### Roll over based on largest primary shard size

&emsp;&emsp;下面的例子中，当最大的主分片的大小达到50gb则滚动这个索引。

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover" : {
            "max_primary_shard_size": "50GB"
          }
        }
      }
    }
  }
}
```

###### Roll over based on index size

&emsp;&emsp;下面的例子中，当索引大小至少100g则滚动这个索引。

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover" : {
            "max_size": "100GB"
          }
        }
      }
    }
  }
}
```

###### Roll over based on document count

&emsp;&emsp;下面的例子中，当索引中包含了至少了100000000篇文档后滚动这个索引。

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover" : {
            "max_docs": 100000000
          }
        }
      }
    }
  }
}
```

###### Roll over based on document count of the largest primary shard

&emsp;&emsp;在这个例子中，当这个索引中最大的主分片中的文档数量至少有一千万时滚动这个索引。

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover" : {
            "max_primary_shard_docs": 10000000
          }
        }
      }
    }
  }
}
```

###### Roll over based on index age

&emsp;&emsp;在这个例子中，索引在创建后已经至少过了7天后滚动这个索引。

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover" : {
            "max_age": "7d"
          }
        }
      }
    }
  }
}
```

###### Roll over using multiple conditions

&emsp;&emsp;当你指定了多个滚动条件时，任何一个条件满足都会触发滚动。这个例子中，要么索引在创建后已经至少过了7天后或者当索引大小至少100g时触发滚动。

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover" : {
            "max_age": "7d",
            "max_size": "100GB"
          }
        }
      }
    }
  }
}
```

###### Rollover condition blocks phase transition

&emsp;&emsp;只有至少一个滚动条件满足才会完成rollover动作，这意味着任何接下来的阶段在滚动成功前都会被阻塞住。

&emsp;&emsp;例如，下面的策略在索引滚动后就删除索引。在索引创建后的一天内不会去删除这个索引。

```text
PUT /_ilm/policy/rollover_policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_size": "50G"
          }
        }
      },
      "delete": {
        "min_age": "1d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```


#### Searchable snapshot
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-searchable-snapshot.html)

&emsp;&emsp;可以在hot、cold、frozen阶段使用该动作。

&emsp;&emsp;在配置好的仓库中对被管理的索引（managed index）生成一个快照并且挂载它作为一个[searchable snapshot](#Searchable snapshots)。如果这个索引是[data stream](#Data Streams)的一部分，被挂载的索引将替换流中的原来的索引（original index）。

&emsp;&emsp;`searchable_snapshot`动作需要数据层（[data tiers](#Data tiers)），这个动作使用[index.routing.allocation.include.\_tier\_preference](#index.routing.allocation.include.\_tier\_preference)参数来挂载不同阶段对应的数据层（data tiers）的索引。在frozen阶段，这个动作会挂载到frozen层中前缀为`partial-`的[partially mounted index](#Partially mounted index)。在其他阶段，这个动作会挂载到对应数据层中前缀为`restored-`的[fully mounted index](#Fully mounted index)。

> WARNING：不要同时在hot跟cold阶段包含`searchable_snapshot`动作。这样会导致在cold阶段索引无法自动迁移（migrate）到cold tier。

&emsp;&emsp;如果在hot阶段使用了`searchable_snapshot`动作，那么接下来的阶段中不能包含`shrink`或者`forcemerge`动作。

&emsp;&emsp;这个动作不能在数据流的write index上执行。尝试这种操作会导致失败。为了可以将数据流中的索引转化为searchable snapshot，可以先[manually roll over](#Manually roll over a data stream)数据流，这将创建一个新的write index。因为这个索引不再是流的write index，所以当前动作可以将其转化为searchable snapshot。使用一个策略，在这个策略中的hot阶段使用[rollover](#Rollover)动作可以避免这种情况以及不需要为未来被管理的索引作手动滚动。

> IMPORTANT：挂载并且重分配searchable snapshot的分片涉及到从snapshot仓库中拷贝分片内容。This may incur different costs from the copying between nodes that happens with regular indices。这些开销通常很低，但是在有些环境可能会很高。见[Reduce costs with searchable snapshots](#Reduce costs with searchable snapshots)了解更多内容。

&emsp;&emsp;默认情况下，snapshot会在delete阶段被[delete](#Delete)动作删除，可以通过在delete动作中设置`delete_searchable_snapshot`为`false`的方式来保留这个snapshot。

##### Options

- snapshot_repository：（Required, string）存储snapshot的[repository](#Register a snapshot repository)。
- force_merge_index：（Optional, Boolean）将被管理的索引（managed index）强制合并到一个段。默认值为true。如果被管理的索引已经使用前面的[force merge](#Force merge)动作强制合并过了，那么`searchable snapshot`中的强制合并操作将不会执行。

  >NOTE：forcemerge这个动作属于best effort。这个动作有可能在一些分片正在分配时执行，在这种情况下这些分片不会进行合并。如果不是所有的分片都执行了forcemerge，那么searchable_snapshot动作会继续执行。

&emsp;&emsp;合并操作在`searchable_snapshot`动作之前执行。如果在`hot`阶段使用`searchable_snapshot`动作，force merge将在hot节点上执行。如果在`cold`阶段使用`searchable_snapshot`动作，force merge将在`hot`或者`warm`阶段执行。

##### Example

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "cold": {
        "actions": {
          "searchable_snapshot" : {
            "snapshot_repository" : "backing_repo"
          }
        }
      }
    }
  }
}
```

#### Set priority
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-set-priority.html#ilm-set-priority)

&emsp;&emsp;可以在hot、warm、cold阶段使用该动作。

&emsp;&emsp;一旦策略进入hot、warm或cold阶段，就设置索引的[priority](#Index recovery prioritization)。节点重启后，优先级高的索引会在优先级低的索引之前恢复。

&emsp;&emsp;通常来说在hot阶段的索引应该有最高的优先级并且在cold阶段的索引应该有最低的优先级。例如：hot阶段的值为100，warm阶段的值为50，cold阶段的值为0。没有设置的话则优先级的默认值为1。

##### Options

- priority：（Required, integer）索引的优先值。该值必须不小于0.设置为`null`则移除优先值。

##### Example

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "warm": {
        "actions": {
          "set_priority" : {
            "priority": 50
          }
        }
      }
    }
  }
}
```

#### Shrink
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-shrink.html)

&emsp;&emsp;可以在hot、warm阶段使用该动作。

&emsp;&emsp;将源索引（source index）设置为[read-only](#index.blocks.read_only)并且收缩（shrink）到一个新的索引中，这个索引有很少的主分片（fewer primary shards）。生成的索引名称为`shrink-<random-uuid>-<original-index-name>`。这个动作对应于 [shrink API](#Shrink index API)。

&emsp;&emsp;在`shrink`动作执行后，那些指向源索引的aliases会指向收缩后的索引（shrunken index）。如果ILM在一个数据流（data stream）的[backing index](#Backing indices)上执行收缩操作时，收缩后的索引会替代流中的源索引。你不能在一个写索引（write index）上执行`shrink`动作。

&emsp;&emsp;如果要在`hot`阶段使用`shrink`动作，必须同时配置`rollover`动作。如果没有配置rollover动作，ILM会reject这个策略。

&emsp;&emsp;shrink动作会移除索引的`index.routing.allocation.total_shards_per_node`设置，意味着将取消限制。这个操作能保证索引的所有分片都被拷贝到同一个节点上。This setting change will persist on the index even after the step completes。

> IMPORTANT：如果收缩动作在[follower index](#Create follower API)上使用，执行策略前需要等待leader index滚动结束（或者[otherwise marked complete](#Skip rollover)），然后在执行shrink动作前先通过[unfollow](#Unfollow)动作将follower index转化为一个常规索引（regular index）。

##### Shrink options

- number_of_shards：（Optional,integer）收缩后分片的数量。必须是源索引的分片数量的因子值（factor）。这个参数跟下文的`max_primary_shard_size`冲突，只能设置一个值。
- max_primary_shard_size：（Optional,[byte units](#Byte size units)）目标索引（target index）的主分片大小的最大值。用来找到目标索引的最适宜的（optimum）分片大小。当设置了这个参数后，目标索引中每一个分片大小（shard's storage）不会超过这个参数值。目标索引的分片数量将仍然是源索引（source index）的因子值（factor）。如果这个参数值比源索引中的分片大小还要小，那么目标索引中的分片数量将和源索引中的分片数量相等。比如说参数设置为50gb，如果源索引有60个主分片，一共100gb，那么目标索引将有2个主分片，每个分片的大小为50gb；如果源索引有60个主分片，一共1000gb，那么目标索引将有20个主分片，每个分片的大小为50gb，如果源索引有60个主分片，一共4000gb，那么目标索引仍然有60个主分片。这个参数跟上文中的`number_of_shards`存在冲突，只能选择一个对其设置。

##### Example

###### Set the number of shards of the new shrunken index explicitly

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "warm": {
        "actions": {
          "shrink" : {
            "number_of_shards": 1
          }
        }
      }
    }
  }
}
```

###### Calculate the optimal number of primary shards for a shrunken index

&emsp;&emsp;下面的策略使用`max_primary_shard_size`参数基于源索引中的存储大小自动的计算新的收缩后（shrunken）的索引的主分片的数量。

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "warm": {
        "actions": {
          "shrink" : {
            "max_primary_shard_size": "50gb"
          }
        }
      }
    }
  }
}
```

###### Shard allocation for shrink

&emsp;&emsp;在`shrink`动作执行期间，ILM将源索引的主分片分配到一个节点。在收缩完索引后，ILM基于你的分配规则将收缩后的索引分片分配给合适的节点。

&emsp;&emsp;这些分配步骤会因为下面的一些原因而导致失败，包括：

- 在`shrink`动作执行期间，一个节点被移除了
- 没有节点有足够的磁盘空间来存放源索引的分片
- Elasticsearch因为分配规则发生冲突而无法重新分配收缩后的索引

&emsp;&emsp;当分配步骤的其中一步失败后，ILM会等待[index.lifecycle.step.wait_time_threshold](#index.lifecycle.step.wait_time_threshold)，默认值为12小时。这个阈值会周期性让集群去解决任何导致分配失败的问题。

&emsp;&emsp;如果过了这个周期性的字段值时间并且ILM还没有收缩完索引，ILM会尝试将源索引的主分片分配到其他节点。如果ILM收缩完索引但是不能在这个周期性的字段值时间内重新分配收缩后的索引，ILM会删除收缩后的索引并且重新尝试整个`shink`动作。

#### Unfollow
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-unfollow.html#ilm-unfollow)

&emsp;&emsp;可以在hot、warm、cold、frozen阶段使用该动作。

&emsp;&emsp;将一个[CCR](#Cross-cluster replication APIs) follower index转变为常规索引（regular index），使得shrink、rollover、以及searchable snapshot动作可以在follower index上安全的执行。你可以在生命周期中移动follower index时直接使用unfollow。该动作不会对非follower index产生影响，阶段中执行该动作只是继续做下一个动作。

> NOTE：如果是应用在follower index上，这个动作会被[rollover](#Rollover)、[shrink](#Shrink)以及[searchable snapshot](#Searchable snapshot)动作自动触发。

&emsp;&emsp;这个动作会一直等到安全的将一个follower index转化为常规索引才会执行，必须满足下面的条件：

- lead index必须已经将`index.lifecycle.indexing_complete`设置为`true`。如果leader index使用[rollover](#Rollover)动作滚动结束，这个设置会自动完成。并且也可以使用[index settings API](#Update index settings API)来手动设置。
- leader index上的所有操作都已经在follower index上执行。这个保证了当索引转变结束后不会丢失任何的操作。

&emsp;&emsp;一旦满足条件，unfollow动作将执行下面的操作：

- Pauses indexing following for the follower index.
- Closes the follower index.
- Unfollows the leader index.
- Opens the follower index (which is at this point is a regular index).

##### Options

&emsp;&emsp;无

##### Example

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "unfollow" : {}
        }
      }
    }
  }
}
```

#### Wait for snapshot
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-wait-for-snapshot.html)

&emsp;&emsp;可以在delete阶段使用该动作。

&emsp;&emsp;在移除索引前等待指定的[SLM](#Automate snapshots with SLM)策略执行结束。使得被删除的索引的snapshot是可见的。

##### Options

- policy：（Required, string）SLM策略的名称，delete动作执行前需要等待执行这个策略。

##### Example

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "delete": {
        "actions": {
          "wait_for_snapshot" : {
            "policy": "slm-policy-name"
          }
        }
      }
    }
  }
}
```

### Configure a lifecycle policy
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/set-up-lifecycle-policy.html)

&emsp;&emsp;为了让ILM能够管理一个索引，必须在索引设置 `index.lifecycle.name`中指定一个合法的策略。

&emsp;&emsp;若要为[rolling indices](#Rollover（concept）)配置一个生命周期策略，你需要创建一个策略然后添加到[index template](#Index templates)中。

&emsp;&emsp;若要使用策略来管理一个不进行roll over的索引，你可以在创建索引的时候指定一个生命周期策略，或者将一个策略应用到已存在的索引上。

&emsp;&emsp;ILM 策略存储在全局cluster state中，当你[take the snapshot](#Create a snapshot)时，你可以将`include_global_state`设置为`true`，使得在snapshot中包含策略。当存储snapshot后，会存储全局state中的所有策略并且任意相同名称的本地策略会被覆盖。

> IMPORTANT：
When you enable index lifecycle management for Beats or the Logstash Elasticsearch output plugin, the necessary policies and configuration changes are applied automatically. You can modify the default policies, but you do not need to explicitly configure a policy or bootstrap an initial index.

#### Create lifecycle policy

&emsp;&emsp;若要从Kibana中创建一个生命周期策略，打开菜单然后跳转到**Stack Management > Index Lifecycle Policies**。点击 **Create policy**。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/create-policy.png">

&emsp;&emsp;为策略指定生命周期策略的阶段以及每一个阶段中的动作（action）。

&emsp;&emsp;[create or update policy](#Create or update lifecycle policy API)被调用后，策略被添加到Elasticsearch 集群中。

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_primary_shard_size": "25GB" 
          }
        }
      },
      "delete": {
        "min_age": "30d",
        "actions": {
          "delete": {} 
        }
      }
    }
  }
}
```

&emsp;&emsp;第8行，当索引大小达到25G后进行滚动（roll over）
&emsp;&emsp;第16行，在滚动后的30天后删除索引

#### Apply lifecycle policy with an index template

&emsp;&emsp;若要使用策略来触发rollover动作，你需要在index template中配置策略用于创建每一个新的索引。你指定策略的名称和alias用于引用rolling indices。

&emsp;&emsp;你可以使用Kibana创建模板向导程序来创建一个模板。若要访问向导程序，打开菜单并且进入**Stack Management > Index Management**。在**Index Template**页面，点击**Create template**

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/create-template-wizard-my_template.png">

&emsp;&emsp;向导程序调用了[create or update index template API](#Create or update index template API)将模板添加到集群。

```text
PUT _index_template/my_template
{
  "index_patterns": ["test-*"], 
  "template": {
    "settings": {
      "number_of_shards": 1,
      "number_of_replicas": 1,
      "index.lifecycle.name": "my_policy", 
      "index.lifecycle.rollover_alias": "test-alias" 
    }
  }
}
```

&emsp;&emsp;第3行，使用这个模板用于所有索引名以`test-`开头的新的索引
&emsp;&emsp;第8行，应用`my_policy`到使用这个模板创建的索引上
&emsp;&emsp;第9行，定义一个index alias用于引用被`my_policy`管理的索引

#### Create an initial managed index

&emsp;&emsp;当为你自己的rolling indices设置策略时，你需要手动创建由策略管理的第一个索引，并且授权（designate）这个索引为write index。

> IMPORTANT：
When you enable index lifecycle management for Beats or the Logstash Elasticsearch output plugin, the necessary policies and configuration changes are applied automatically. You can modify the default policies, but you do not need to explicitly configure a policy or bootstrap an initial index.

&emsp;&emsp;索引的名称必须匹配定义在index template中的pattern并且以数字结尾。递增这个数字来生成由rollover动作创建的索引的名称。

&emsp;&emsp;例如，下面的请求创建了一个名为`test-00001`的索引。因为它匹配到了`my_template`中的index pattern，Elasticsearch自动的从这个应用这个模板中的设置。

```text
PUT test-000001
{
  "aliases": {
    "test-alias":{
      "is_write_index": true 
    }
  }
}
```

&emsp;&emsp;第5行，为这个alias设置最初的索引作为write index

&emsp;&emsp;现在你可以开始将数据索引到生命周期策略中指定的rollover alias中。对于这个样例策略`my_policy`，rollover的动作是在最初的索引大小超过25GB后就触发动作。ILM随后为`test-alias`创建一个新的索引并作为write index。

#### Apply lifecycle policy manually

&emsp;&emsp;你可以通过Kibana或者[update settings API](#Update index settings API)在创建索引时指定一个策略或者将某个策略应用到现有的索引上。在你应用某个策略后，ILM会马上开始管理索引。

> IMPORTANT：不要手动应用一个使用rollover动作的策略。使用rollover的策略必须由[index template](#Apply lifecycle policy with an index template)应用。否则当rollover动作闯将一个新的索引时不能carry forward这个策略。

&emsp;&emsp;`index.lifecycle.name`这个设置用于指定索引的一个策略。

```text
PUT test-index
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 1,
    "index.lifecycle.name": "my_policy" 
  }
}
```

&emsp;&emsp;第6行，为索引指定生命周期策略

#### Apply a policy to multiple indices

&emsp;&emsp;当你使用[update settings API](#Update index settings API)时，你可以使用索引名的通配符将某个策略应用到多个索引中。

> WARNING：当心无意中（inadvertent）匹配到了你不想修改的索引。

```text
PUT mylogs-pre-ilm*/_settings 
{
  "index": {
    "lifecycle": {
      "name": "mylogs_policy_existing"
    }
  }
}
```

&emsp;&emsp;第1行，更新所有以`mylogs-pre-ilm`开头的索引。

#### Switch lifecycle policies

&emsp;&emsp;若要切换某个索引的生命周期策略，可以按照下面的步骤：

1. 使用[remove policy API](#Remove policy from index API)移除现有的策略。目标是一个data stream或者alias，移除它的所有索引的策略。

```text
POST logs-my_app-default/_ilm/remove
```

2. 移除策略的API会移除索引中所有的ILM metadata并且不会考虑索引的生命周期状态。这会让索引处于一个不希望（undesired）的状态。

&emsp;&emsp;例如，[forcemerge](#Force merge)动作会在重新打开一个index前先关闭它。在`forcemerge`期间移除一个索引的ILM的策略会让索引永久的（indefinite）关闭。

&emsp;&emsp;移除策略后，使用[get index API ](#Get index API)检查索引的状态。目标是一个data stream或者alias来获得所有它的索引的状态。

```text
GET logs-my_app-default
```

&emsp;&emsp;随后你可以按需来变更索引。比如你可以使用[open index API](#Open index API)重新打开任何被关闭的索引。

```text
POST logs-my_app-default/_open
```

3. 使用[update settings API](#Update index settings API)分配（assign）一个新的策略。目标是一个data stream或者alias的所有索引

> WARNING：首先移除现有的策略再分配一个新的策略。否则会导致[phase execution](#Phase execution)出现故障。

```text
PUT logs-my_app-default/_settings
{
  "index": {
    "lifecycle": {
      "name": "new-lifecycle-policy"
    }
  }
}
```

### Migrate index allocation filters to node roles
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/migrate-index-allocation-filters.html)

&emsp;&emsp;如果你正在一个[hot-warm-cold](https://www.elastic.co/blog/implementing-hot-warm-cold-in-elasticsearch-with-index-lifecycle-management)的架构中使用自定的节点属性（custom node attribution）和[attribute-based allocation filters](#Index-level shard allocation filtering)将索引移动到[data tiers](#Data tiers)。我们建议你转而使用内置的节点属性并且自动的[data tier allocation](#Data tier index allocation)。使用节点角色能让ILM自动的将索引在data tiers间移动。

> NOTE：尽管我们建议在hot-warm-cold架构中使用自动化的data tier allocation来管理你的数据，你依然可以出于其他的目的使用 attribute-based allocation filters来控制分片的分配。

&emsp;&emsp;Elasticsearch Service和Elastic Cloud Enterprise能自动的执行迁移（migration）。对于self-managed 的部署，你需要手动的更新你的配置，ILM策略，以及索引来切换到节点角色。

#### Automatically migrate to node roles on Elasticsearch Service or Elastic Cloud Enterprise

&emsp;&emsp;在Elasticsearch Service和Elastic Cloud Enterprise中，如果你使用默认部署模板中的节点属性，you will be prompted to switch to node roles when you：

- 更新到Elasticsearch 7.10或者更高
- 部署一个warm，cold，或者frozen data tier
- [Enable autoscaling](https://www.elastic.co/guide/en/cloud/current/ec-autoscaling.html)

&emsp;&emsp;这些动作自动的更新你的集群配置和ILM 策略以使用节点角色。另外，更新到7.14或者更高版本后，每当部署应用任何配置更改时，都会自动更新ILM策略。

&emsp;&emsp;如果你使用自定义的index template，在自动迁移完成后检查这个模板并移除每一个[attribute-based allocation filters](#Index-level shard allocation filtering)。

> NOTE：自动迁移后你不需要再执行任何的步骤。下文手动的步骤只有在你不允许自动迁移或者自己管理部署时才需要。

#### Migrate to node roles on self-managed deployments

&emsp;&emsp;若要切换到（switch to）使用节点角色：

1. 分配（assign）[data node](#Assign data nodes to a data tier)到合适的data tier
2. 从你的ILM中[Remove the attribute-based allocation settings ](#Remove custom allocation settings from existing ILM policies)
3. 在新的索引上[Stop setting the custom hot attribute ](#Stop setting the custom hot attribute on new indices)
4. 更新现有的索引用于[set a tier preference](#Troubleshooting index lifecycle management errors)

##### Assign data nodes to a data tier

&emsp;&emsp;为每一个data node配置一个合理的角色，分配给这些节点一个或者多个data tiers：`data_hot`, `data_content`, `data_warm`, `data_cold`, 或者 `data_frozen`。节点也可以有其他的[role](#Node roles)。默认情况下，新的节点配置为有所有的角色。

&emsp;&emsp;当你添加一个data tier到一个Elasticsearch Service deployment，一个或多个节点会被自动的配置对应的角色。你可以显示的通过[Update deployment API](https://www.elastic.co/guide/en/cloud/current/ec-api-deployment-crud.html#ec_update_a_deployment)修改Elasticsearch Service deployment中的节点的角色。用合适的`node_roles`代替节点的`node_type`配置。例如，下面的配置将节点添加到hot和content tiers，并让这个节点作为一个ingest node，remote 和transform node。

```text
"node_roles": [
  "data_hot",
  "data_content",
  "ingest",
  "remote_cluster_client",
  "transform"
],
```

&emsp;&emsp;如果你直接管理自己的集群，在每个节点的`elasticsearch.yml`中配置合适的角色。例如下面的设置将节点配置为一个位于hot和content tier的data-only的节点。

```text
node.roles [ data_hot, data_content ]
```

##### Remove custom allocation settings from existing ILM policies

&emsp;&emsp;更新每一个生命周期中[allocate](#Allocate)动作来移除attribute-base allocation settings。ILM会在每一个阶段中inject一个[migrate](#Migrate)动作来自动的在data tiers上转移（transition）索引。

&emsp;&emsp;如果allocate动作不设置副本分片（replica）的数量，则移除整个allocate动作。（空的allocate动作是非法的）

> IMPORTANT：
The policy must specify the corresponding phase for each data tier in your architecture. Each phase must be present so ILM can inject the migrate action to move indices through the data tiers. If you don’t need to perform any other actions, the phase can be empty. For example, if you enable the warm and cold data tiers for a deployment, your policy must include the hot, warm, and cold phases.

##### Stop setting the custom hot attribute on new indices

&emsp;&emsp;当你创建了一个data stream，它的第一个backing index自动的被分配到`data_hot`节点。同样的，当你直接创建了一个索引，自动的分配到`data_content`节点。

&emsp;&emsp;在[Elasticsearch Service deployments](https://www.elastic.co/cloud/elasticsearch-service/signup?baymax=docs-body&elektra=docs)，移除index template `cloud-hot-warm-allocation-0`，它会在所有节点上设置hot shard allocation attribution。

```text
DELETE _template/.cloud-hot-warm-allocation-0
```

&emsp;&emsp;如果你正在使用一个自定义的模板，更新这个模板来移除用于分配新的索引到hot tier的[attribute-based allocation filters ](#Index-level shard allocation filtering)。

&emsp;&emsp;To completely avoid the issues that raise when mixing the tier preference and custom attribute routing setting we also recommend updating all the legacy, composable, and component templates to remove the [attribute-based allocation filters](#Index-level shard allocation filtering) from the settings they configure.

##### Set a tier preference for existing indices

&emsp;&emsp;ILM通过在每一个阶段中inject一个[migrate](#Migrate)动作来自动的将管理的索引转移到可用的data 

&emsp;&emsp;为了能让ILM将现有的索引移动到data tier，更新下面的index settings：

- 通过将设置置为`null`的方式来移除自定义的allocation filter
- 设置[tier preference](#index.routing.allocation.include.\_tier\_preference)

&emsp;&emsp;例如，如果在你的旧模板中设置属性`data`的值为`hot`将分片分配到hot tier，那么将属性`data`设置为`null`并且设置`_tier_preference`为`data_hot`。

```text
PUT my-index/_settings
{
  "index.routing.allocation.require.data": null,
  "index.routing.allocation.include._tier_preference": "data_hot"
}
```

&emsp;&emsp;对于已经从hot阶段转移出去的索引，tier preference必须包含了一个合适的回滚tier（fallback tier）使得期望用于分配的（prefer）tier如果不可见的话也能分配它。例如为已经在warm阶段中的索引指定一个回滚tier。

```text
PUT my-index/_settings
{
  "index.routing.allocation.require.data": null,
  "index.routing.allocation.include._tier_preference": "data_warm,data_hot"
}
```

&emsp;&emsp;如果索引已经在cold阶段，回滚tier可以是cold，warm和hot阶段。

&emsp;&emsp;如果索引同时配置了`_tier_preference`和`require.data`，但是`_tier_preference` 是outdated（比如说，节点属性的配置比配置的`_tier_preference`更"colder"）。那migration必须要移除`require.data`属性并且将`_tier_preference`更新为正确的tier。

&emsp;&emsp;例如，有以下路由配置的索引：

```text
{
  "index.routing.allocation.require.data": "warm",
  "index.routing.allocation.include._tier_preference": "data_hot"
}
```

&emsp;&emsp;路由配置应该修复为：

```text
PUT my-index/_settings
{
  "index.routing.allocation.require.data": null,
  "index.routing.allocation.include._tier_preference": "data_warm,data_hot"
}
```

&emsp;&emsp;这种情况有可能发生在默认是data tiers的系统中。例如ILM使用了存储的节点属性并将管理的索引从hot阶段转移到warm阶段，在这种情况下，节点属性的配置指出了索引应该被分配到的正确的tier。

### Troubleshooting index lifecycle management errors
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-lifecycle-error-handling.html)

&emsp;&emsp;当ILM执行一个生命周期策略时，有可能在某个步骤中执行必要的索引操作时候发生错误。发生错误后，ILM将索引移动到`ERROR`。如果ILM不能自动的解决错误，执行过程将被暂停直到你解决了策略，索引，或者集群的问题才能继续。

&emsp;&emsp;例如你可能有一个`shink-index`的策略：一旦索引的寿命达到5天，其分片数量收缩到4个：

```text
PUT _ilm/policy/shrink-index
{
  "policy": {
    "phases": {
      "warm": {
        "min_age": "5d",
        "actions": {
          "shrink": {
            "number_of_shards": 4
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;不会有任何的障碍阻止你应用`shrink-index`策略到一个新的索引上，尽管这个索引只有两个分片：

```text
PUT /my-index-000001
{
  "settings": {
    "index.number_of_shards": 2,
    "index.lifecycle.name": "shrink-index"
  }
}
```

&emsp;&emsp;五天后，ILM试图将索引`my-index-000001`的分片数从两个分片收缩到四个分片。因为[shrink](#Shrink)动作不会增加分片的数量，所以这个操作会失败，ILM将`my-index-000001`移动到`ERROR`。

&emsp;&emsp;你可以使用[ILM Explain API](#Explain lifecycle API)来获取发生错误的相关信息：

```text
GET /my-index-000001/_ilm/explain
```

&emsp;&emsp;上述请求返回如下的信息：

```text
{
  "indices" : {
    "my-index-000001" : {
      "index" : "my-index-000001",
      "managed" : true,
      "index_creation_date_millis" : 1541717265865,
      "time_since_index_creation": "5.1d",
      "policy" : "shrink-index",                
      "lifecycle_date_millis" : 1541717265865,
      "age": "5.1d",                            
      "phase" : "warm",                         
      "phase_time_millis" : 1541717272601,
      "action" : "shrink",                      
      "action_time_millis" : 1541717272601,
      "step" : "ERROR",                         
      "step_time_millis" : 1541717272688,
      "failed_step" : "shrink",                 
      "step_info" : {
        "type" : "illegal_argument_exception",  
        "reason" : "the number of target shards [4] must be less that the number of source shards [2]"
      },
      "phase_execution" : {
        "policy" : "shrink-index",
        "phase_definition" : {                  
          "min_age" : "5d",
          "actions" : {
            "shrink" : {
              "number_of_shards" : 4
            }
          }
        },
        "version" : 1,
        "modified_date_in_millis" : 1541717264230
      }
    }
  }
}
```

&emsp;&emsp;第8行，用于管理索引的策略：`shrink-index`
&emsp;&emsp;第10行，索引寿命：5.1天
&emsp;&emsp;第11行，索引当前位于`warm`阶段
&emsp;&emsp;第13行，当前的动作：`shrink`
&emsp;&emsp;第15行，索引当前位于`ERROR`步骤
&emsp;&emsp;第17行，执行失败时位于`shrink`步骤
&emsp;&emsp;第19行，错误类型和错误的描述
&emsp;&emsp;第24行，`shrink-index`策略中当前阶段的定义
&emsp;&emsp;若要解决这个问题，你可以更新策略：在5天后将索引收缩为单个分片：

```text
PUT _ilm/policy/shrink-index
{
  "policy": {
    "phases": {
      "warm": {
        "min_age": "5d",
        "actions": {
          "shrink": {
            "number_of_shards": 1
          }
        }
      }
    }
  }
}
```

#### Retrying failed lifecycle policy steps

&emsp;&emsp;一旦你解决位于`ERROR`步骤的索引的问题，你可能需要显示的告知ILM进行重试：

```text
POST /my-index-000001/_ilm/retry
```

&emsp;&emsp;ILM接下来尝试重新运行刚刚失败的步骤。你可以使用[ILM Explain API](#Explain lifecycle API)查看处理进程。

#### Common ILM errors

&emsp;&emsp;以下是一些常见在`ERROR`步骤中报出的错误以及解决方案。

> TIP：
Problems with rollover aliases are a common cause of errors. Consider using [data streams](#Data streams) instead of managing rollover with aliases.

##### Rollover alias [x] can point to multiple indices, found duplicated alias [x] in index template [z]

&emsp;&emsp;index template的`index.lifecycle.rollover_alias`指定了目标rollover alias。你需要在[bootstrap the initial index](#Bootstrap the initial time series index with a write index alias)时显示的配置一次这个alias。rollover动作才能管理设置并更新alias来对接下来的索引进行[roll over](#Rollover API)。

##### index.lifecycle.rollover_alias [x] does not point to index [y]

&emsp;&emsp;要么索引使用了错误的alias或者alias不存在。

&emsp;&emsp;检查[index setting](#Get index settings API)的`index.lifecycle.rollover_alias`。若要查看alias配置的内容，可以使用[\_cat/aliases](#cat aliases API)。

##### Setting [index.lifecycle.rollover_alias] for index [y] is empty or not defined

&emsp;&emsp;必须为rollover动作配置`index.lifecycle.rollover_alias`。

&emsp;&emsp;更新index settings来设置`index.lifecycle.rollover_alias`。

##### Alias [x] has more than one write index [y,z]

&emsp;&emsp;对于某一个alias，只能有一个索引可以被指派（designate）为write index。

&emsp;&emsp;使用[aliases ](#Aliases API) API，将除了某一个索引以外的其他索引都设置`is_write_index:false`

##### index name [x] does not match pattern ^\.\*-\\d+

&emsp;&emsp;为了rollover动作可以工作，索引名必须匹配regex pattern "^.\*-\d+"。最常见的问题是索引名不包含尾随数字（trailing digits）。例如，`my-index`不匹配pattern要求。

&emsp;&emsp;索引名称尾部追加一个数值，例如`my-index-000001`。

##### CircuitBreakingException: [x] data too large, data for [y]

&emsp;&emsp;说明集群达到了资源限制。

&emsp;&emsp;在继续设置ILM前，你需要采取方式来缓和（alleviate）资源问题。见[Circuit breaker errors](#Circuit breaker errors)了解更多信息。

##### High disk watermark [x] exceeded on [y]

&emsp;&emsp;说明cluster的磁盘空间快满了。当你的ILM中没有设置从hot节点滚动到warm节点时可能会发生。

&emsp;&emsp;考虑添加节点，更新你的硬件，或者删除不要的索引。

##### security_exception: action [\<action-name>] is unauthorized for user [\<user-name>] with roles [\<role-name>], this action is granted by the index privileges [manage_follow_index,manage,all]

&emsp;&emsp;说明ILM动作不能执行的原因是因为用户没有合适的privilege。在更新完ILM策略后用户的privilege可能被drop。ILM动作是以最后一个修改策略的用户运行的。创建或者修改策略的用户应该有这个策略中所有操作的权限。

### Start and stop index lifecycle management
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/start-stop-ilm.html)

&emsp;&emsp;默认情况下，ILM服务处于`RUNNING`的状态并且管理所有有生命周期策略的索引。你可以停止index lifecycle management为所有的索引暂停管理操作。例如，当执行定期维护或者对集群作变更且会对ILM动作的执行造成影响时关闭ILM。

> IMPORTANT：When you stop ILM, SLM operations are also suspended. No snapshots will be taken as scheduled until you restart ILM. In-progress snapshots are not affected

#### Get ILM status

&emsp;&emsp;使用[Get Status API](#Get index lifecycle management status API)查看当前ILM服务的状态：

```text
GET _ilm/status
```

&emsp;&emsp;正常条件下，响应会显示ILM处于`RUNNING`：

```text
{
  "operation_mode": "RUNNING"
}
```

#### Stop ILM

&emsp;&emsp;若要停止ILM服务并且暂停所有生命周期策略的执行，使用[Stop API](#Stop index lifecycle management API)：

```text
POST _ilm/stop
```

&emsp;&emsp;ILM服务将所有的策略运行到某个点使得可以安全的停止。当ILM服务正在关闭中，状态API会显示ILM处于`STOPPING`模式：

```text
{
  "operation_mode": "STOPPING"
}
```

&emsp;&emsp;一旦所有的策略处于一个安全的停止点，ILM就进入`STOPPED`模式：

```text
{
  "operation_mode": "STOPPED"
}
```

#### Start ILM

&emsp;&emsp;若要启动ILM并且恢复执行策略，使用[Start API](#Start index lifecycle management API)。这将ILM服务置为`RUNNING`的状态，ILM开始从离开的地方执行策略。

```text
POST _ilm/start
```

### Manage existing indices
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-with-existing-indices.html)

&emsp;&emsp;如果你使用了Curator或者其他机制来管理周期性（periodic）的索引，那么迁移到ILM时有两个选项：

- 设置index template来使用ILM策略管理你的新索引。一旦ILM正在管理你当前的write index，你可以应用一个合适的策略到你的旧索引
- reindex到一个ILM-managed index

> NOTE：Starting in Curator version 5.7, Curator ignores ILM managed indices.

#### Apply policies to existing time series indices

&emsp;&emsp;转移（transition）管理你周期性的索引的最简单的方式就是使用ILM ，[configure an index template](#Apply lifecycle policy with an index template)，将生命周期策略应用到新的索引上。一旦你正在写入的索引由ILM管理，你可以对你的旧索引[manually apply a policy](#Apply a policy to multiple indices)。

&emsp;&emsp;为你的旧索引定义一个不同的策略，策略中omit rollover动作。Rollover用于管理新的数据，所以不适用旧数据。

&emsp;&emsp;注意的是对现有的索引应用策略后，每一个阶段的`min_age`会跟索引的创建时间作比较，所以有可能会立即处理多个阶段。如果你的策略属于资源密集型的操作比如force merge，当切换到ILM时你不会想要让很多的索引马上一下子都执行这些操作。

&emsp;&emsp;你可以为现有的索引指定不同的`min_age`，或者设置[index.lifecycle.origination_date](#index.lifecycle.origination_date)来控制索引寿命（age）的计算。

&emsp;&emsp;Once all pre-ILM indices have been aged out and removed, you can delete the policy you used to manage them。

> NOTE：If you are using Beats or Logstash, enabling ILM in version 7.0 and onward sets up ILM to manage new indices automatically. If you are using Beats through Logstash, you might need to change your Logstash output configuration and invoke the Beats setup to use ILM for new data.

#### Reindex into a managed index

&emsp;&emsp;另一种[applying policies to existing indices](#Manage existing periodic indices with ILM)就是将你的数据reindex到一个ILM-managed index中。如果创建了数据很小导致过多的（excessive）分片数量的周期性索引或者一直往相同的索引中写入导致大分片以及性能问题，那么你可能就想要reindex。

&emsp;&emsp;首先，你需要设置新的ILM-managed 索引：

1. 更新index template来包含必要的ILM 设置
2. 引导（bootstrap）一个最初的索引作为write index
3. 停止往旧索引中写入数据并且使用指向引导索引的alias来索引新的文档

&emsp;&emsp;若要reindex到ILM管理的索引中：

1. 如果你不想要将新旧索引混合到ILM-managed index中，那么先停止索引新的文档。将新旧索引混合到一个索引是安全，但是混合的索引需要被保留直到你准备删除新的数据。
2. 降低ILM的拉取间隔时间（poll interval）来保证索引的大小在等待rollover的检查时不会增长的太大。默认情况下，ILM每十分钟会检查要执行哪些动作：

```text
PUT _cluster/settings
{
  "persistent": {
    "indices.lifecycle.poll_interval": "1m" 
  }
}
```
&emsp;&emsp;第4行，每隔一分钟就检查下例如rollover的ILM动作是否要执行。

3. 使用[reindex API](#Reindex API)来reindex你的数据。如果你想要根据写入的时间有序的拆分数据，你可以运行多个reindex请求。

> IMPORTANT：Documents retain their original IDs. If you don’t use automatically generated document IDs, and are reindexing from multiple source indices, you might need to do additional processing to ensure that document IDs don’t conflict. One way to do this is to use a [script](#Modify documents during reindexing) in the reindex call to append the original index name to the document ID.

```text
POST _reindex
{
  "source": {
    "index": "mylogs-*" 
  },
  "dest": {
    "index": "mylogs", 
    "op_type": "create" 
  }
}
```

&emsp;&emsp;第4行，匹配现有的索引。为新的索引使用前缀使得index pattern更加简单
&emsp;&emsp;第7行，alias指向了你的bootstrapped index
&emsp;&emsp;第8行，如果多个文档有相同的ID就暂停reindex。这种做法是推荐的，能防止在不同的源索引中出现ID相同的文档时覆盖文档的问题


4. reindex完成后，将ILM的拉取时间间隔（poll interval）设置回默认的值，避免master node上没必要的负载：

```text
PUT _cluster/settings
{
  "persistent": {
    "indices.lifecycle.poll_interval": null
  }
}
```

5. 使用相同的alias恢复索引新的数据。

&emsp;&emsp;使用这个alias进行查询时会获取你的新数据跟所有的reindex的数据。

6. 一旦你已经验证了在新的managed index中所有的reindex数据都是可用的，你就可以安全的移除旧的索引。

### Skip rollover
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/skipping-rollover.html)

&emsp;&emsp;当`index.lifecycle.indexing_complete`设置为`true`，ILM不会在这个索引上执行rollover动作，即使满足了rollover的标准。当rollover成功完成后，ILM会自动的设置该参数。

&emsp;&emsp;如果你需要在正常的生命周期策略中有一个例外以及更新alias来强制rollover，但是想要ILM能继续的管理索引，那你可以手动的设置该参数来跳过rollover。如果你使用了rollover API，那就不需要手动的配置这个设置。

&emsp;&emsp;如果索引的生命周期策略被移除了，这个设置也会被移除。

> IMPORTANT：当`index.lifecycle.indexing_complete`设置为`true`，ILM会核实（verify）这个索引不再是`index.lifecycle.rollover_alias`中指定的write index。如果索引仍然是write index或者rollover alias没有设置，这个索引就被移动到[ERROR step](#Troubleshooting index lifecycle management errors)。

&emsp;&emsp;例如，如果你需要在一个series中更改新索引的名称同时还要保留根据配置的策略生成的之前的索引的数据，你可以：

1. 为新的index pattern创建一个新的模板并且使用新的策略
2. 引导最初的索引（bootstrap the initial index）
3. 使用[aliases API](#Aliases API)为alias修改引导索引为write index
4. 在旧的索引上将`index.lifecycle.indexing_complete`设置为`ture`告知它不需要被rollover

&emsp;&emsp;ILM继续使用你现有的策略来管理旧的索引。新的索引根据新的模板进行命名并且根据相同的策略进行管理without interruption。

### Restore a managed data stream or index
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-lifecycle-and-snapshots.html)

### Data tiers
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/data-tiers.html)

&emsp;&emsp;数据层（data tier）是具有相同数据角色（[data role](#Data node)），通常享有相同硬件配置（hardware profile ）的节点集合。

- [Content tier](#Content tier)节点处理例如产品目录内容的索引和查询负载
- [Hot tier](#Hot tier)节点处理例如logs或者metrics这些时序（time series）数据的索引负载，并且保存（hold）你最近最常访问的数据
- [Warm tier](#Warm tier)节点保存最近less-frequently的访问并且很少（rarely）需要更新的时序数据
- [Cold tier](#Cold tier)节点保留infrequent的访问并且一般不更新的时序数据。为了节省空间，你可以在cold tier上保留[fully mounted indices](#Fully mounted index)的[searchable snapshots](#Searchable snapshots)。这些fully mounted indices会消除（eliminate）对副本分片的需求，相较于常规索引（regular index）能降低50%的磁盘空间
- [Frozen tier](#Frozen tier)节点保留很少（rarely）访问并且从不更新的时序数据。 Fronze tier只存储 [partially mounted indices](#Partially mounted index)的[searchable snapshots](#Searchable snapshots)。This extends the storage capacity even further — by up to 20 times compared to the warm tier.

&emsp;&emsp;当你直接往指定索引中写入文档，这些文档将无期限（indefinitely）的一直保留（remain on）在content ties节点上。

&emsp;&emsp;当你往[data stream](#Data streams)中写入文档，这些文档最开始会常驻（reside on）在hot tier节点上。你可以根据性能、弹性（resiliency）、数据保留（data retention）的要求，通过配置[index lifecycle management](#ILM: Manage the index lifecycle) 策略自动的将文档转移到hot ties、warm ties以及cold ties。

#### Content tier

&emsp;&emsp;存储在content tier上的数据通常是iterm的集合比如说产品目录（product catalog）或者文章归档（article archive）。跟时序数据不同的是，这些内容的价值随着时间的流逝相对保持不变的，所以根据这些数据的寿命（age）将它们移到性能不同的数据层是不合理的。Content data通常有长时间保留（retention）的要求，并且也希望无论这些数据的寿命的长短，总是能很快的检索到。

&emsp;&emsp;Content tier是必须要有的（required）。系统索引以及其他不是data stream的索引都会被自动分配到content tier。

#### Hot tier

&emsp;&emsp;hot tier是Elasticsearch中时序数据的入口点（entry point），并且保留最近，最频繁搜索的时序数据。hot tier节点上的读写速度都需要很快，要求更多的硬件资源和更快的存储（SSDs）。出于弹性目的，hot tier上的索引应该配置一个或多个副本分片。

&emsp;&emsp;hot tier是必须要有的。[data stream](#Data streams)中新的索引会被自动分配到hot tier。

#### Warm tier

&emsp;&emsp;一旦时序数据的访问频率比最近索引的数据（recently-indexed data）低了，这些数据就可以移到warm tier。warm tier通常保留最近几周的数据。允许更新数据，但是infrequent。warm tier节点不需要像hot tier一样的快。出于弹性目的，warm tier上的索引应该配置一个或多个副本分片。

#### Cold tier

&emsp;&emsp;当你不再经常（regular）搜索时序数据了，那可以将它们从warm tier移到cold tier。数据仍然可以被搜索到，在这一层的通常会被优化成较低的存储开销而不是查询速度。

&emsp;&emsp;为了更好的节省存储（storage saveing），你可以在cold tier保留[fully mounted indices](#Fully mounted index)的[searchable snapshots](#Searchable snapshots)。跟常规索引（regular index）不同的是，这些fully mounted indices不需要副本分片来满足可靠性（reliability），一旦出现失败事件，可以从底层（underlying）snapshot中恢复。这样可以潜在的减少一般的本地数据存储开销。snapshot仓库要求在cold tier使用fully mounted indices。Fully mounted indices只允许读取，不能修改。

&emsp;&emsp;另外你可以使用cold tier存储常规索引并且使用副本分片的方式，而不是使用searchable snapshot，这样会帮你在较低成本的硬件上存储较老的索引，但是相较于warm tier不会降低磁盘空间。

#### Frozen tier

&emsp;&emsp;一旦数据不需要或者很少（rare）被查询，也许就可以将数据从cold tier移到frozen tier，where it stays for the rest of its life。

&emsp;&emsp;frozen tier需要用到snapshot repository。frozen tier使用[partially mounted indices](#Partially mounted index)的方式存储以及从snapshot repository中载入数据。这样仍然可以让你搜索frozen数据并且可以减少本地储存（local storage）和操作开销（operation cost）。因为Elasticsearch必须有时从snapshot repository中提取（fetch）数据，在frozen tier的查询速度通常比cold  tier慢。

#### Configure data tiers on Elasticsearch Service or Elastic Cloud Enterprise

&emsp;&emsp;Elastic Cloud部署中默认配置了包含hot、content data的一个共享层（shared tier）。这层是必须要有的并且不能被移除。

&emsp;&emsp;要增加一个warm。cold。或者frozen tier，你可以创建这么一个部署（deployment）：

1. 在**Create deployment**页面，点击 **Advanced Settings**
2. 点击**Add capacity**增加任意数据层
3. 页面底部点击**Create deployment**保存你的更改

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ess-advanced-config-data-tiers.png">

&emsp;&emsp;如果要移除一个数据层，见[Disable a data tier](https://www.elastic.co/guide/en/cloud/current/ec-disable-data-tier.html)。

#### Configure data tiers for self-managed deployments

&emsp;&emsp;对于自己管理的部署，每个节点的[data role](#Data node)配置在`elasticsearch.yml`中。例如，集群中性能最高的节点应该同时分配hot和content tier：

```text
node.roles: ["data_hot", "data_content"]
```

> NOTE：我们强烈建议你在frozen tier中使用[ dedicated nodes](#Frozen data node)。


#### Data tier index allocation

&emsp;&emsp;当你创建一个索引时，Elasticsearch默认设置[index.routing.allocation.include.\_tier\_preference](#index.routing.allocation.include.\_tier\_preference)为`data_content`来自动的在content tier上分配索引分片。

&emsp;&emsp;当Elasticsearch创建一个索引，该索引作为 data stream的一部分时，Elasticsearch默认设置[index.routing.allocation.include.\_tier\_preference](#index.routing.allocation.include.\_tier\_preference)为`data_hot`来自动的在hot tier上分配索引分片。

&emsp;&emsp;你可以显示指定[index.routing.allocation.include.\_tier\_preference](#index.routing.allocation.include.\_tier\_preference)的值，不使用（opt out of）默认的tier-based的分配方式。

#### Automatic data tier migration

&emsp;&emsp;ILM使用[migrate](#Migrate)动作自动的在可见的数据层间进行索引的转变，默认情况下，每一层都会自动注入这个动作。你可以通过 `"enabled"`:`false`显示指定关闭自动迁移。例如你正在使用[allocate](#Allocate)动作手动的指定分配规则。

## Monitor a cluster
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/monitor-elasticsearch-cluster.html)

&emsp;&emsp;Elastic Stack monitoring功能提供了一种可以随时了解（keep a pulse）Elasticsearch集群健康跟性能的方法。

- [Overview](#Monitoring overview)
- [How it works](#How monitoring works)
- [Monitoring in a production environment](#Monitoring in a production environment)
- [Collecting monitoring data with Metricbeat](#Collecting Elasticsearch monitoring data with Metricbeat)
- [Collecting log data with Filebeat](#Collecting Elasticsearch log data with Filebeat)
- [Configuring indices for monitoring](#Configuring indices for monitoring)
- [Legacy collection methods](#Collecting monitoring data using legacy collectors)
- [Troubleshooting](#Troubleshooting monitoring)

### Monitoring overview
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/monitoring-overview.html)

&emsp;&emsp;当你监控一个集群时，你会从集群中的Elasticsearch nodes，Logstash nodes，Kibana和Beats中收集数据。你也可以[use Filebeat to collect Elasticsearch logs](#Collecting Elasticsearch log data with Filebeat)。

&emsp;&emsp;所有的检测指标（monitoring metrics）都存储在Elasticsearch中，使得你可以很容易的在Kibana中可视化这些数据。默认情况下，监控指标存储在本地索引中。

> TIP: 在生产中，我们强烈建议使用分开的监控集群（a separate monitoring cluster）。防止发生因生产集群（production cluster）中断而影响访问监控数据的问题。同样的也是防止因为监控活动对生产集群的性能产生影响。基于同样的原因，我们也建议使用一个分开的Kibana实例来观察监控数据（monitoring data）

&emsp;&emsp;你可以使用Metricbeatt将收集到的Elasticsearch，Kibana，Logstash和Beats的数据直接发送给你的监控集群，而不是通过生产集群进行路由。下图中展示的是一个经典的生产集群和监控集群分离的监控架构：

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/architecture.png">

&emsp;&emsp;如果你有特定的许可证，你可以从多个生产集群中将数据路由到一个监控集群中。见https://www.elastic.co/subscriptions 了解了解更多关于订阅级别的差异。

> IMPORTANT： 通常来说，监控集群和被监控的集群应该使用相同的版本。监控集群不能监控高版本的生产集群。If necessary, the monitoring cluster can monitor production clusters running the latest release of the previous major version.。

### How monitoring works
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/how-monitoring-works.html)

&emsp;&emsp;基于它们的持久的UUID（persistent UUID），每一个Elasticsearch node，Logstash node，Kibana实例和Beat实例都被认为是唯一的。在节点或者实例启动时，这个值会被写入到`path.data`中。

&emsp;&emsp;Monitoring document只是普通的JSON格式的document，通过每一个ELastic Stack 组件在一个指定的收集间隔（collection interval）构建。如果你想要更改这些索引的模板，见[Configuring indices for monitoring](#Configuring indices for monitoring)。

&emsp;&emsp;MetricBeat用于收集监控数据并且发送给监控集群。

&emsp;&emsp;见下面的内容学习如何收集监控数据：

- [Legacy collection methods](#Collecting monitoring data using legacy collectors)
- [Collecting monitoring data with Metricbeat](#Collecting Elasticsearch monitoring data with Metricbeat)
- [Monitoring Kibana](https://www.elastic.co/guide/en/kibana/8.2/xpack-monitoring.html)
- [Monitoring Logstash](https://www.elastic.co/guide/en/logstash/8.2/configuring-logstash.html)
- Monitoring Beats:
  - [Auditbeat](https://www.elastic.co/guide/en/beats/auditbeat/8.2/monitoring.html)
  - [Filebeat](https://www.elastic.co/guide/en/beats/filebeat/8.2/monitoring.html)
  - [Functionbeat](https://www.elastic.co/guide/en/beats/functionbeat/8.2/monitoring.html)
  - [Heartbeat](https://www.elastic.co/guide/en/beats/heartbeat/8.2/monitoring.html)
  - [Metricbeat](https://www.elastic.co/guide/en/beats/metricbeat/8.2/monitoring.html)
  - [Packetbeat](https://www.elastic.co/guide/en/beats/packetbeat/8.2/monitoring.html)
  - [Winlogbeat](https://www.elastic.co/guide/en/beats/winlogbeat/8.2/monitoring.html)

### Monitoring in a production environment
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/monitoring-production.html)

&emsp;&emsp;在生产中，你应该将监控数据发送到到一个分开的监控集群（a separate monitoring cluster）中，使得即使有些你监控的节点不在了也能对历史的数据可见。例如，你可以使用Metricbeat将Kibana，Elasticsearch，Logstash，和Beats的监控数据发送到监控集群中。

> IMPORTANT：Metricbeat是一种收集和发送监控数据到监控集群的一种方法。
> 如果你之前配置了legacy collection method，你应该迁移到使用Metricbeat collection。要么使用Metricbeat collection，要么使用legacy collection method，不要同时使用。
> 见[Collecting monitoring data with Metricbeat](#Collecting Elasticsearch monitoring data with Metricbeat)了解更多信息

&emsp;&emsp;如果你至少有一个Gold Subscription，可以使用一个专门的监控集群让你从一个central location中监控多个集群。

&emsp;&emsp;在一个分开的集群中存储监控数据，你需要：

1. 建立（set up）你想要用于作为监控集群的Elasticsearch集群。例如，你可以建立两个节点分别名为`es-mon-1`和`es-mon-2`的集群。

> IMPORTANT：最理想的情况是监控集群跟生产集群的运行在相同版本的Elastic Stack上。然而8.x的最新版本的监控集群也能跟主版本号（major version）相同的生产集群工作正常。8.x版本的监控集群也能跟7.x的最新版本的生产集群工作正常。
> 

&emsp;&emsp;a. （Optional）核实好在监控集群上的监控数据的收集功能是关闭的。默认情况下，`xpack.monitoring.collection.enabled`的设置为`false`。

&emsp;&emsp;例如，你可以使用下面的API查看以及更改设置：

```text
GET _cluster/settings

PUT _cluster/settings
{
  "persistent": {
    "xpack.monitoring.collection.enabled": false
  }
}
```

&emsp;&emsp;b. 如果监控集群上开启了Elasticsearch Security功能，创建好有权限发送以及查询监控数据的用户。

> NOTE: 如果你计划使用Kibana查看监控数据。在Kibana服务和监控集群上的用户名密码都必须要合法。

- 如果你计划使用Metricbeat收集Elasticsearch或者Kibana的数据，创建有`remote_monitoring_collector`内建角色（built-in）的用户和有[remote_monitoring_agent](#remote_monitoring_agent)内建角色的用户。或者是`remote_monitoring_user`的内建用户（[built-in user](#Built-in users)）
- 如果你计划使用HTTP exproters通过你的生产集群来路由数据，创建一个用户并拥有[remote_monitoring_agent](#remote_monitoring_agent)的内建角色。

&emsp;&emsp;例如，下面的请求创建一个`remote_monitor`用户，它拥有`remote_monitoring_agent`的角色：

```text
POST /_security/user/remote_monitor
{
  "password" : "changeme",
  "roles" : [ "remote_monitoring_agent"],
  "full_name" : "Internal Agent For Remote Monitoring"
}
```

&emsp;&emsp;或者，使用`remote_monitoring_user`[ built-in user](#Built-in users)。

2. 配置你的生产集群来收集数据并且发送到监控集群：
   - [Metricbeat collection methods](#Collecting Elasticsearch monitoring data with Metricbeat)
   - [Legacy collection methods](#Collecting monitoring data using legacy collectors). 
3. （Optional）[Configure Logstash to collect data and send it to the monitoring cluster](#Monitoring Logstash)。
4. （Optional）配置Beats来收集数据并发送到监控集群
   - [Auditbeat](https://www.elastic.co/guide/en/beats/auditbeat/8.2/monitoring.html)
   - [Filebeat](https://www.elastic.co/guide/en/beats/filebeat/8.2/monitoring.html)
   - [Heartbeat](https://www.elastic.co/guide/en/beats/heartbeat/8.2/monitoring.html)
   - [Metricbeat](https://www.elastic.co/guide/en/beats/metricbeat/8.2/monitoring.html)
   - [Packetbeat](https://www.elastic.co/guide/en/beats/packetbeat/8.2/monitoring.html)
   - [Winlogbeat](https://www.elastic.co/guide/en/beats/winlogbeat/8.2/monitoring.html)

5. （Optional）配置Kivana来收集数据并发送到监控集群
   - [Metricbeat collection methods](https://www.elastic.co/guide/en/kibana/8.2/monitoring-metricbeat.html)
   - [Legacy collection methods](https://www.elastic.co/guide/en/kibana/8.2/monitoring-kibana.html)

6. （Optional）创建一个专用的Kibana实例用于监控。而不是使用单个即要访问你的生产集群也要访问你的监控集群的Kibana。

> NOTE: 如果你使用SAML, Kerberos, PKI, OpenID Connect, or token authentication providers登陆Kibana，那么必须使用专用的Kibana。security token是cluster-specific的，因此你不能使用单个Kibana实例同时连接生产和监控集群。

- （Optional）关闭Kibana中监控数据的收集功能，在`kibana.yml`文件中，将`xpack.monitoring.kibana.collection.enabled`设置为`false`。见[Monitoring settings in Kibana](https://www.elastic.co/guide/en/kibana/8.2/monitoring-settings-kb.html)了解更多设置信息。

7. [Configure Kibana to retrieve and display the monitoring data](https://www.elastic.co/guide/en/kibana/8.2/monitoring-data.html)。


### Collecting Elasticsearch monitoring data with Metricbeat
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/configuring-metricbeat.html)

### Collecting Elasticsearch log data with Filebeat
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/configuring-filebeat.html)

### Configuring indices for monitoring
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/config-monitoring-indices.html)

&emsp;&emsp;[Index templates](#Create or update index template API)用于对索引进行配置来存储从集群中收集的监控数据。

&emsp;&emsp;你可以使用`_template`API查询监控数据的模版：

```text
GET /_template/.monitoring-*
```

&emsp;&emsp;默认情况下，只会在模板中为监控索引（monitoring Index）配置一个分片以及一个副本。你可以添加自己的模版信息来覆盖默认配置：

1. 设置template pattern来匹配现有的`.monitoring-{product}-7-*`索引
2. 将template的`order`设置为`1`，使得默认模版先于你的模版，默认模版的`order`为`0`
3. 在`settings`区域中设置`number_of_shards` and/or `number_of_replicas`

&emsp;&emsp;下面的例子中，配置了5个分片以及2个副本分片：

```text
PUT /_template/custom_monitoring
{
  "index_patterns": [".monitoring-beats-7-*", ".monitoring-es-7-*", ".monitoring-kibana-7-*", ".monitoring-logstash-7-*"],
  "order": 1,
  "settings": {
    "number_of_shards": 5,
    "number_of_replicas": 2
  }
}
```

> IMPORTANT：只设置settings区域中的`number_of_shards` 以及`number_of_replicas`。覆盖其他的监控模版信息会导致你的监控仪表盘停止正常工作。

### Collecting monitoring data using legacy collectors
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/collecting-monitoring-data.html)

### Troubleshooting monitoring
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/monitoring-troubleshooting.html)


## Roll up or transform your data
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/data-rollup-transform.html)

&emsp;&emsp;Elasticsearch提供了下面的方法来处理（manipulate）你的数据：

- [Rolling up your historical data](#Rolling up historical data)
- [Transforming data](#Transforming data)

### Rolling up historical data
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/xpack-rollup.html)

>WARNING：这个功能目前处于技术预览阶段，未来的版本中可能会进行更改或移除。Elastic会努力修复任何问题，但是技术预览中的功能不受官方正式发布（GA）功能的支持服务级别协议（SLA）约束。

&emsp;&emsp;保留历史数据用于分析是非常实用的，但由于归档大量的数据带来的财政开销（financial cost）而不会保留。因此保留时间由财政现实（financial realities）决定而不是根据历史数据的用处。

&emsp;&emsp;Elastic Stack的rollup功能提供一个汇总（summarize ）和存储历史数据的方法，使得历史数据仍然可以用于分析，但只有原始数据（raw data）所需的存储开销的一小部分（a fraction of the storage cost）。

- [Overview](#Rollup overview)
- [Getting started](#Getting started with rollups)
- [API quick reference](#Rollup API quick reference)
- [Understanding rollup grouping](#Understanding groups)
- [Rollup aggregation limitations](#Rollup aggregation limitations)
- [Rollup search limitations](#Rollup search limitations)

#### Rollup overview
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rollup-overview.html)

> WARNING：这个功能目前处于技术预览阶段，未来的版本中可能会进行更改或移除。Elastic会努力修复任何问题，但是技术预览中的功能不受官方正式发布（GA）功能的支持服务级别协议（SLA）约束。

&emsp;&emsp;基于时间（time-based）（主要由时间戳标识的文档）的数据经常有相关的保留策略（retention policy）来管理数据的成长。比如说，你的系统可能生成每秒生成500篇文档，那么一天会有4300万篇文章，一年有接近16亿篇文档。

&emsp;&emsp; 然而分析师和数据科学家可能希望你能存储无限量的数据。时间是无穷无尽的，所以你的存储需求将持续增长并且没有尽头。因此保留策略通常由随时间推移的存储成本的简单计算决定以及公司愿意为保留历史数据而花的钱。通常这些策略会在几个月或者几年后删除数据。

&emsp;&emsp; 存储成本是一个固定数量（Storage cost is a fixed quantity）。It takes X money to store Y data。但是一段数据的效用（utility）会随着时间发生变化。在当前时间内毫秒颗粒度的传感器的数据是非常有用，在几个星期前的数据相对有点用，但是几个月前就几乎没什么用了。

&emsp;&emsp; 虽然存储十年前的一毫秒传感器数据的成本是固定的，但传感器读数的价值会随着时间而减小。这些数据并不是没有用，它可以很容易地为实用的分析（useful analysis）做出贡献，但是价值的降低通常会导致被删除而不是使用固定的存储开销来保留它们。

##### Rollup stores historical data at reduced granularity

&emsp;&emsp;That’s where Rollup comes into play。Rollup 功能将旧的高粒度（high-granularity）汇总为降低粒度的格式，以便长期存储。通过将数据roll up到单个summary document中，相较于原始数据（raw data），历史数据可以更好的被压缩。

&emsp;&emsp;比如说一个每天生成4300万文档的系统，每一秒的数据对实时分析是很实用的，但是查看超过 10 年数据的历史分析可能只在更大的时间间隔内起作用，例如每小时或每天的趋势。

&emsp;&emsp;如果我们把4300篇文档按小时进行汇总，我们就可以节省大量的空间。Rollup功能会对历史数据进行自动的汇总。

&emsp;&emsp;见[Create Job API](#Create rollup jobs API)详细了解Rollup的设置跟配置。

##### Rollup uses standard Query DSL

&emsp;&emsp;Rollup功能提供了新的search endpoint（`_rollup_search` vs 标准的`/_search`），这个endpoint知道如何查询rolled-up的数据。重要的是，这个endpoint接受100%普通的（normal）Elasticsearch Query DSL。你的应用不需要学习新的DSL来inspect历史数据，很容易重新使用现有的查询跟dashboard。

&emsp;&emsp;这个功能也是有一些限制。不是所有的查询和聚合都支持的，一些查询功能（高亮）被禁用了并且可以使用的字段（available fields）取决于Rollup的配置。更多的限制见[Rollup Search limitations](#Rollup search limitations)。

&emsp;&emsp;But if your queries, aggregations and dashboards only use the available functionality, redirecting them to historical data is trivial。

##### Rollup merges "live" and "rolled" data

&emsp;&emsp;Rollup的另一个实用的功能是可以在同一个查询中同时查询"live"实时数据和历史的"rollup"数据。

&emsp;&emsp;比如说你的系统保留了一个月的原始数据（raw data）。一个月后，这些数据被rollup到历史汇总（historical summarizes）数据中并且原始数据会被删除。

&emsp;&emsp;如果你想查询原始数据，你只能查看最近一个月的数据，如果你想查看rollup数据，你只能查看一个月前的数据。RollupSearch endpoint支持在同一时间同时进行查询。这个查询会从两个数据源获取并且将结果合并到一起。如果"live"和"rollup"的数据发生重叠，那么选择"live"数据，因为它更准确。

##### Rollup is multi-interval aware

&emsp;&emsp;最后，Rollup能够智能地利用可用的最佳间隔。如果你使用过其他产品的汇总功能（summarizing Feature），你会发现其局限性。比如如果配置了按天为间隔的rollup，那么你只能基于按天来进行查询或者出图表。如果你需要每月间隔，则必须显示的（explicit）创建另一个存储每月平均值的汇总。

&emsp;&emsp;Rollup功能可以通过按照最小的可用的间隔进行存储，并且依次来进行处理。如果你rollup了按天的数据，那么可以执行按天或者更长间隔的查询（按周、按月、按年等等）而不用显示的去配置一个新的rollup任务。这有助于缓解（alleviate）汇总系统的主要缺点之一：相对于原始数据的灵活性的降低。

#### Rollup API quick reference
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rollup-api-quickref.html#rollup-api-quickref)

> WARNING：这个功能目前处于技术预览阶段，未来的版本中可能会进行更改或移除。Elastic会努力修复任何问题，但是技术预览中的功能不受官方正式发布（GA）功能的支持服务级别协议（SLA）约束。

&emsp;&emsp;大多数rollup endpoint都有下面的前缀：

```text
/_rollup/
```

##### /job/

- [PUT /\_rollup/job/](#Create rollup jobs API): Create a rollup job
- [GET /\_rollup/job](#Get rollup jobs API): List rollup jobs
- [GET /\_rollup/job/](#Get rollup jobs API): Get rollup job details
- [POST /\_rollup/job//\_start](#Start rollup jobs API): Start a rollup job
- [POST /\_rollup/job//\_stop](#Stop rollup jobs API): Stop a rollup job
- [DELETE /\_rollup/job/](#Delete rollup jobs API): Delete a rollup job

##### /data/

- [GET /\_rollup/data//\_rollup\_caps](#Get rollup job capabilities API): Get Rollup Capabilities
- [GET //\_rollup/data/](#Get rollup index capabilities API): Get Rollup Index Capabilities

##### /<index\_name>/

- [GET //\_rollup\_search](#Rollup search): Search rollup data

#### Getting started with rollups
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rollup-getting-started.html)

> WARNING：这个功能目前处于技术预览阶段，未来的版本中可能会进行更改或移除。Elastic会努力修复任何问题，但是技术预览中的功能不受官方正式发布（GA）功能的支持服务级别协议（SLA）约束。

&emsp;&emsp;你需要创建一个或多个"Rollup jobs"来使用rollup功能。这些job会在后台持续不断的运行，将你指定的index或者indices进行rollup操作。那些被rolled的文档会被放到二级索引（secondary index）中。

&emsp;&emsp;比如你有一些按天记录传感器数据的索引（daily indices）（比如说`sensor-2017-01-01`, `sensor-2017-01-02`）。示例文档如下所示：

```text
{
  "timestamp": 1516729294000,
  "temperature": 200,
  "voltage": 5.2,
  "node": "a"
}
```

##### Creating a rollup job

&emsp;&emsp;我们想要将文档rollup到按小时汇总（hourly summarize）。这将允许我们生成间隔为1个小时或者更大间隔的报告和dashboard。rollup job如下所示：

```text
PUT _rollup/job/sensor
{
  "index_pattern": "sensor-*",
  "rollup_index": "sensor_rollup",
  "cron": "*/30 * * * * ?",
  "page_size": 1000,
  "groups": {
    "date_histogram": {
      "field": "timestamp",
      "fixed_interval": "60m"
    },
    "terms": {
      "fields": [ "node" ]
    }
  },
  "metrics": [
    {
      "field": "temperature",
      "metrics": [ "min", "max", "sum" ]
    },
    {
      "field": "voltage",
      "metrics": [ "avg" ]
    }
  ]
}
```

&emsp;&emsp;我们给这个job命名为"sensor"（在url中: `PUT _rollup/job/sensor`），并且告诉这个job我们要对 `sensor-*`进行rollup。这个job会找到匹配`sensor-*` pattern的所有索引。Rollup会进行汇总并存储到名为`sensor_rollup`的索引中。

&emsp;&emsp;`cron`参数用来控制什么时候以及什么频率来激活这个job。当一个rollup job的cron计划激活后，这个job将对上一次job结束任务后产生的新的数据（new worth of data）执行rollup。所以你如果配置了一个每30秒就执行一次的cron，该作业将处理最近 30 秒的数据，这些数据被索引到 sensor\-* 索引中。

&emsp;&emsp;如果你配置了每天在午夜时执行rollup的任务，这个job将会处理过去24小时的数据。选择主要取决于偏好，具体取决于你希望汇总的“实时”程度，以及你是否希望连续处理或将其移至非高峰时间。

&emsp;&emsp;接下来是`groups`的设置。Essentially, we are defining the dimensions that we wish to pivot on at a later date when querying the data。这个job中的grouping允许我们在`timestamp`字段上执行`date_histogram`操作，按照一小时的间隔进行rollup。同样允许我们在`node`字段上进行terms aggregation。

> Date histogram interval vs cron schedule
> 你可能注意到了job中的cron配置为每30秒执行一次，但是date_histogram配置为按照60分钟的间隔执行rollup，它们之间是什么关系呢？
> date_histogram控制保存的数据（saved data）的颗粒度。数据会按照小时的间隔（hourly interval）执行rollup，并且你无法执行更细的颗粒度（finer granularity）的查询。cron只是简单的查找是不是有新的数据可以用于rollup。每隔30秒，cron会看下有没有new hour’s worth of data可以用于rollup，如果没有，这个job则go back to sleep。
> 通常来说，在一个较大的间隔（1h）中定义一个这么小的cron（30s）是不合理的，因为大部分cron激活后会马上go back to sleep。但这也不会有什么问题，job会正确处理这种情况。

&emsp;&emsp;在定义好了这些数据中会生成哪些groups后，你下一步的配置是应该收集哪些metric。默认情况下，只会收集每一个group的`doc_counts`。为了使rollup更加具有实用性，你一般经常会收集例如averages, mins, maxes等一些metric信息。在这个例子中需要收集的metric是相对简单的（fairly straightforward）：我们想要保留`temperature`字段的min/max/sum，以及`voltage`字段的average。

> Averages aren’t composable?!
> 如果你之前使用过rollup，那么使用Average的时候要注意了。如果average用于计算10分钟的间隔，通常来说average就不能用于更大间隔的计算。你不能用简单的10分钟的average * 6 来计算以一个小时为间隔的average。the average of averages is not equal to the total average。
> 由于这个原因，其他的系统会试图omit计算average的能力或者存储多个不同的时间间隔来支持更多的灵活的查询。
> 然而在rollup功能中会根据定义的时间间隔来保存对应的`count`和`sum`。使得当时间间隔大于或等于定义的时间间隔后我们可以重新来计算。这就给予了使用最小的存储开销来实现最大化的灵活性。所以你不用担心average的精确度（no average of averages here!）。

&emsp;&emsp;见[Create rollup jobs](#Create rollup jobs API)了解更多关于job语法的信息。

&emsp;&emsp;在你执行上文中的命令，并创建了job后，你会收到下面的响应：

```text
{
  "acknowledged": true
}
```

##### Starting the job

&emsp;&emsp;job创建后，它会处于未激活状态（inactive state），在开始处理数据（这样使得随后你可以临时的暂停这个job而不需要删除它）前，job需要被启动。

&emsp;&emsp;执行这个命令来启动job：

```text
POST _rollup/job/sensor/_start
```

##### Searching the rolled results

&emsp;&emsp;在job运行并处理了一些数据后，我们就可以通过[Rollup search](#Rollup search) endpoint来做一些查询了。Rollup功能被设计为你可以使用你习惯了的Query DSL进行查询，只不过是在rolled up的数据上进行查询。

&emsp;&emsp;例如执行这个查询：

```text
GET /sensor_rollup/_rollup_search
{
  "size": 0,
  "aggregations": {
    "max_temperature": {
      "max": {
        "field": "temperature"
      }
    }
  }
}
```

&emsp;&emsp;这是一个计算`temperature`字段最大值的简单聚合查询。但需要你注意的是，这是在`sensor_rollup`索引上进行查询而不是原始的`sensor-*`索引。还有一个你要注意的是使用了`_rollup_search`这个endpoint。Otherwise the syntax is exactly as you’d expect。

&emsp;&emsp;如果你执行了这个查询，那么会收到一个跟普通的聚合（normal aggregation）一样的响应：

```text
{
  "took" : 102,
  "timed_out" : false,
  "terminated_early" : false,
  "_shards" : ... ,
  "hits" : {
    "total" : {
        "value": 0,
        "relation": "eq"
    },
    "max_score" : 0.0,
    "hits" : [ ]
  },
  "aggregations" : {
    "max_temperature" : {
      "value" : 202.0
    }
  }
}
```

&emsp;&emsp;这里唯一需要注意的是Rollup的查询结果中是没有`hits`结果的，因为我们不再从原始的，live data中进行查询。其他部分都是相同的语法结构。

&emsp;&emsp;There are a few interesting takeaways here。首先，尽管数据按小时的间隔（hourly interval）并按照node的名称进行分组，但是这个query我们只计算了所有文档中temperature的最大值。job中配置的`groups`在查询不是必要的元素（mandatory element），它们只是你可以用于分组的额外的维度。其次，这个查询和响应跟普通的DSL是一模一样的，使得更容易集成与dashboard和应用中。

&emsp;&emsp;最后，我们可以使用分组的字段来构造一个更加复杂的查询：

```text
GET /sensor_rollup/_rollup_search
{
  "size": 0,
  "aggregations": {
    "timeline": {
      "date_histogram": {
        "field": "timestamp",
        "fixed_interval": "7d"
      },
      "aggs": {
        "nodes": {
          "terms": {
            "field": "node"
          },
          "aggs": {
            "max_temperature": {
              "max": {
                "field": "temperature"
              }
            },
            "avg_voltage": {
              "avg": {
                "field": "voltage"
              }
            }
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;这个查询对应的响应是：

```text
{
   "took" : 93,
   "timed_out" : false,
   "terminated_early" : false,
   "_shards" : ... ,
   "hits" : {
     "total" : {
        "value": 0,
        "relation": "eq"
     },
     "max_score" : 0.0,
     "hits" : [ ]
   },
   "aggregations" : {
     "timeline" : {
       "meta" : { },
       "buckets" : [
         {
           "key_as_string" : "2018-01-18T00:00:00.000Z",
           "key" : 1516233600000,
           "doc_count" : 6,
           "nodes" : {
             "doc_count_error_upper_bound" : 0,
             "sum_other_doc_count" : 0,
             "buckets" : [
               {
                 "key" : "a",
                 "doc_count" : 2,
                 "max_temperature" : {
                   "value" : 202.0
                 },
                 "avg_voltage" : {
                   "value" : 5.1499998569488525
                 }
               },
               {
                 "key" : "b",
                 "doc_count" : 2,
                 "max_temperature" : {
                   "value" : 201.0
                 },
                 "avg_voltage" : {
                   "value" : 5.700000047683716
                 }
               },
               {
                 "key" : "c",
                 "doc_count" : 2,
                 "max_temperature" : {
                   "value" : 202.0
                 },
                 "avg_voltage" : {
                   "value" : 4.099999904632568
                 }
               }
             ]
           }
         }
       ]
     }
   }
}
```

&emsp;&emsp;除了使用更加复杂的查询（(date histogram and a terms aggregation, plus an additional average metric），值得你注意的是这里的date_histogram使用了`7d`作为时间间隔而不是配置中的`60m`。

##### Conclusion

&emsp;&emsp;这一章的内容提供了简明扼要的关于Rollup所能提供的核心功能。在剩余的章节中你可以学习到更多在设置Rollup时的tips和内容。你可以通过[REST API](#Rollup API quick reference)概览下目前可用的功能。 

#### Understanding groups
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rollup-understanding-groups.html)

> WARNING：这个功能目前处于技术预览阶段，未来的版本中可能会进行更改或移除。Elastic会努力修复任何问题，但是技术预览中的功能不受官方正式发布（GA）功能的支持服务级别协议（SLA）约束。

&emsp;&emsp;为了能提供灵活性（flexibility），Rollup的定义会基于以后的数据查询对应的query。通常来说，系统会要求管理员在配置rollup时要决定好metric以及时间间隔。比如说按小时的间隔（hourly interval）并且是`cpu_time`的average。这种方式存在局限性。在以后的日子里，管理员可能想要查询按小时的间隔并且是`cpu_time`的average同时还要根据`host_name`分组，这时候就做不到了。

&emsp;&emsp;当然管理员可以在按小时的间隔的基础上对`[hour, host]`这么一对来配置rollup，但是随着分组字段数量的增加，其需要更多的配置。另外`[hour, host]`的配置只能在按小时的间隔上才有用，按天、按周、按月的rollup都需要新的配置。

&emsp;&emsp;相较于要求管理员提前为rollup做好决策，Elasticsearch中的Rollup job的配置则是基于哪些groups可能会在将来被用于查询。例如下面的这个配置：

```text
"groups" : {
  "date_histogram": {
    "field": "timestamp",
    "fixed_interval": "1h",
    "delay": "7d"
  },
  "terms": {
    "fields": ["hostname", "datacenter"]
  },
  "histogram": {
    "fields": ["load", "net_in", "net_out"],
    "interval": 5
  }
}
```

&emsp;&emsp;允许在`timestamp`字段上使用`date_histogram`。在`hostname`和`datacenter`字段上使用`terms aggregation`，并且可以在`load`、`net_in`、`net_out`的任意一个字段上使用`histograms`。

&emsp;&emsp;更重要的是，这些 agg/fields可以任何的组合：

```text
"aggs" : {
  "hourly": {
    "date_histogram": {
      "field": "timestamp",
      "fixed_interval": "1h"
    },
    "aggs": {
      "host_names": {
        "terms": {
          "field": "hostname"
        }
      }
    }
  }
}
```

&emsp;&emsp;跟下面的这个聚合都是合法的：

```text
"aggs" : {
  "hourly": {
    "date_histogram": {
      "field": "timestamp",
      "fixed_interval": "1h"
    },
    "aggs": {
      "data_center": {
        "terms": {
          "field": "datacenter"
        }
      },
      "aggs": {
        "host_names": {
          "terms": {
            "field": "hostname"
          }
        },
        "aggs": {
          "load_values": {
            "histogram": {
              "field": "load",
              "interval": 5
            }
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;需要你注意的是第二个aggregation is not only substantially larger, it also swapped the position of the terms aggregation on "hostname"，说明了聚合的先后顺序对rollup没有影响。在对数据进行rollup时`date_histogram`是必须的，但是在查询阶段不是必须的（尽管经常会被使用）。例如下面是个可以用于Rollup Search的合法聚合查询：

```text
"aggs" : {
  "host_names": {
    "terms": {
      "field": "hostname"
    }
  }
}
```

&emsp;&emsp;最后，在对一个job配置`groups`时，考虑好在以后的查询会用到哪些字段用于分组，那么将这些字段都添加到配置中就可以了。因为Rollup Search允许任意的先后顺序或者分组的字段的组合。你只需要决定好某个字段是否会在以后用于聚合，以及你可能希望使用某一种聚合方式（terms、histogram等等）。

##### Calendar vs fixed time intervals

&emsp;&emsp;每一个rollup-job必须有一个date histogram并且定义一个间隔。Elasticsearch可以解析（understand） [calendar and fixed time intervals](#Date histogram aggregation)，fix time intervals解析（understand）起来相对简单；`60s`意味着60秒。但是`1M`是什么意思呢？一个月代表的时间取决于不同的月份，一些月份可能大于或者小于某些月份。这个calendar time的例子的持续的时间取决于上下文（ This is an example of calendar time and the duration of that unit depends on context）。Calendar unitis同样受到润秒（leap-seconds）、闰年（leap-years）等等的影响。

&emsp;&emsp;rollup生成的bucket可以使用calendar或者fixed intervals，所以上文中提到的影响是重要。并且这个问题会限制后续的查询。见[Requests must be multiples of the config](#Rollup search limitations)。

&emsp;&emsp;我们建议坚持（sticking with）使用fixed time intervals，因为它更容易解析（understand）并且在查询时更具弹性。它会在闰事件（leap-event）期间在你的数据中引入一些drift，你将不得不考虑固定数量（30 天）的月份，而不是实际的日历长度。However, it is often easier than dealing with calendar units at query time。

&emsp;&emsp;units的倍数总是fixed（Multiples of units are always "fixed"）。比如说`2h`也是一个固定数值（fixed quantity）`7200`秒。单个units是fixed还是calendar取决于unit：

|    Unit     | Calendar |       Fixed        |
| :---------: | :------: | :----------------: |
| millisecond |    NA    | 1ms, 10ms, etc |
|   second    |    NA    |  1s, 10s, etc  |
|   minute    |    1m    |  2m, 10m, etc  |
|    hour     |    1h    |  2h, 10h, etc  |
|     day     |    1d    |  2d, 10d, etc  |
|    week     |    1w    |         NA         |
|    month    |    1M    |         NA         |
|   quarter   |    1q    |         NA         |
|    year     |    1y    |         NA         |


&emsp;&emsp;对于一些单位（units）可是同时是fixed和calendar，你最好使用更小的单位来描述数量（quantity）。比如说你想要一个fixed day（not a calendar day），你应该指定为`24h`而不是`1d`。同样的，如果你想要一个fixed hours，应该指定为`60m`而不是`1h`。因为单个数量（single quantity）意味着calendar time，并且会在以后限制通过calendar time进行查询。

##### Grouping limitations with heterogeneous indices

&emsp;&emsp;以前在 Rollup 如何处理具有heterogeneous mappings（multiple, unrelated/non-overlapping mappings）的索引时存在限制。 当时的建议是为每个数据“类型”配置一个单独的作业。 例如，你可以为已启用的每个 Beats module配置一个单独的作业（一个用于`process`，另一个用于`filesystem`等）

&emsp;&emsp;由于在内部实现细节中。如果单个"merge" job被使用后会导致文档的统计可能会不正确，因此给出了上文中的建议。

&emsp;&emsp;这个局限性已经被缓和（alleviate）了，从 6.4.0 开始，现在将所有rollup配置组合到一个job中被认为是最佳实践。

&emsp;&emsp;例如，如果你的索引中有这两类的文档：

```text
{
  "timestamp": 1516729294000,
  "temperature": 200,
  "voltage": 5.2,
  "node": "a"
}
```

&emsp;&emsp;以及：

```text
{
  "timestamp": 1516729294000,
  "price": 123,
  "title": "Foo"
}
```

&emsp;&emsp;最佳实践是将它们组合成一个涵盖这两种文档类型的rollup job，如下所示：

```text
PUT _rollup/job/combined
{
  "index_pattern": "data-*",
  "rollup_index": "data_rollup",
  "cron": "*/30 * * * * ?",
  "page_size": 1000,
  "groups": {
    "date_histogram": {
      "field": "timestamp",
      "fixed_interval": "1h",
      "delay": "7d"
    },
    "terms": {
      "fields": [ "node", "title" ]
    }
  },
  "metrics": [
    {
      "field": "temperature",
      "metrics": [ "min", "max", "sum" ]
    },
    {
      "field": "price",
      "metrics": [ "avg" ]
    }
  ]
}
```

##### Doc counts and overlapping jobs

&emsp;&emsp;以前在“overlapping”的job配置上存在文档计数问题，这同样是由于内部实现细节导致的。 如果有两个rollup job保存到同一个索引，其中一个job是另一个job的“subset”，则对于某些aggregation arrangements，文档计数可能不正确。

&emsp;&emsp;此问题也已在 6.4.0 中消除。

#### Rollup aggregation limitations
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rollup-agg-limitations.html)

> WARNING：这个功能目前处于技术预览阶段，未来的版本中可能会进行更改或移除。Elastic会努力修复任何问题，但是技术预览中的功能不受官方正式发布（GA）功能的支持服务级别协议（SLA）约束。

&emsp;&emsp;对字段进行rollup up/aggregated存在一些限制。这一章强调了你需要注意的主要的限制（major limitation）。

##### Limited aggregation components

&emsp;&emsp;Rollup功能允许使用对字段使用下面的aggregation进行分组：

- Date Histogram aggregation
- Histogram aggregation
- Terms aggregation

&emsp;&emsp;下列是numeric Field可以指定的metric：

- Min aggregation
- Max aggregation
- Sum aggregation
- Average aggregation
- Value Count aggregation

#### Rollup search limitations
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rollup-search-limitations.html)

> WARNING：这个功能目前处于技术预览阶段，未来的版本中可能会进行更改或移除。Elastic会努力修复任何问题，但是技术预览中的功能不受官方正式发布（GA）功能的支持服务级别协议（SLA）约束。

&emsp;&emsp;当我们觉得Rollup功能非常的灵活（flexible）时，汇总数据（summarize data）的本质是有局限的。一旦live data（raw data）被丢弃后，你总是会丢失一些灵活性。

##### Only one rollup index per search

&emsp;&emsp;在使用[Rollup search](#Rollup search)时，`index`参数可以接受一个或者多个索引，可以是普通的（regular）、non-rollup、rollup的混合索引。然而只能指定一个rollup索引。使用`index`参数的规则如下所示：

- 至少要指定一个index/index-pattern。可以是一个rollup或者non-rollup索引。不允许omit index参数或者使用`\_all`
- 可以指定多个 non-rollup索引
- 只能指定一个rollup索引。如果指定了多个则会抛出异常
- 可以使用index-pattern，但是如果匹配到了超过一个rollup索引则会抛出异常

&emsp;&emsp;导致这些局限的原因是内部逻辑会根据给定的query决策出一个"best" job。如果在单个索引中存储了10个job。这个索引将覆盖不同的完整度（degree of completeness）和时间间隔的原数据（source data），这个query需要判断使用哪个job来进行查询。错误的决策可能会导致不准确的聚合（inaccurate aggregation）结果（比如over-counting doc counts, or bad metrics）。不管怎么样（needless to say），在技术上这是对编码的一种挑战。

&emsp;&emsp;为了简化这个问题，我们限制为每次只能对一个rollup index进行查询（which may contain multiple jobs）。In the future we may be able to open this up to multiple rollup jobs。

##### Can only aggregate what’s been stored

&emsp;&emsp;一个也许是非常明显的限制就是只能对存储在rollup中的数据进行聚合操作。如果你没有在rollup job中配置`price`字段的metric，那么你不能在任何query或者aggregation中使用`price`字段。

&emsp;&emsp;比如说在下面的query中，`temperature`字段已经存储在一个rollup job中，但是没有存储它的`avg`这个metric信息。意味着不能使用`avg`这个功能：

```text
GET sensor_rollup/_rollup_search
{
  "size": 0,
  "aggregations": {
    "avg_temperature": {
      "avg": {
        "field": "temperature"
      }
    }
  }
}
```

&emsp;&emsp;在响应中会告诉你不能对这个字段做这次聚合操作，因为没有一个rollup job中包含这个字段的`avg`的metric信息：

```text
{
  "error": {
    "root_cause": [
      {
        "type": "illegal_argument_exception",
        "reason": "There is not a rollup job that has a [avg] agg with name [avg_temperature] which also satisfies all requirements of query.",
        "stack_trace": ...
      }
    ],
    "type": "illegal_argument_exception",
    "reason": "There is not a rollup job that has a [avg] agg with name [avg_temperature] which also satisfies all requirements of query.",
    "stack_trace": ...
  },
  "status": 400
}
```

##### Interval granularity

&emsp;&emsp;Rollup会以某个粒度进行存储，即`date_histogram`在配置中的定义。这意味着你在query/aggregation rollup data时，只能使用比rollup配置中相等或者更大的间隔（interval）。

&emsp;&emsp;比如说如果数据按小时间隔（hourly interval）进行rollup。[Rollup search](#Rollup search)可以在按任意小时的间隔或者更大的间隔进行聚合。小于一个小时间隔的查询都将抛出异常，因为不存在粒度更细（finer granularity）的数据。

> Requests must be multiples of the config
Perhaps not immediately apparent，在aggregation请求中指定的间隔必须是配置中的间隔的倍数。如果一个job中rollup配置了`3d`时间间隔。那么你只能query/aggregation 3的倍数的时间间隔（`3d`、`6d`、`9d`等等）。
> 不是倍数的时间间隔无法正常工作。因为rollup data无法与aggregation生成的桶完全“overlap”，从而导致不正确的结果。
> 基于这个原因，如果没有找到配置中的时间间隔的倍数将会抛出异常。

&emsp;&emsp;由于RollupSearch endpoint可以使用配置中的时间间隔的倍数（ "upsample" intervals） ，所以不需要根据不同倍数的间隔（hourly、daily等等）来配置多个job。建议按需配置最小粒度的时间间隔，使得RollupSearch可以根据需要来向上调整时间间隔（upsample）。

&emsp;&emsp;也就是说，如果在单个索引中配置了多个不同时间间隔的job，search endpoint会识别（identify）并且使用最大间隔的job来进行查询。

##### Limited querying components

&emsp;&emsp;Rollup功能允许在查询请求中带有query，但局限于只能使用一部分query，目前可用的query如下所示：

- Term Query
- Terms Query
- Range Query
- MatchAll Query
- Any compound query (Boolean, Boosting, ConstantScore, etc)

&emsp;&emsp;这些query同样只能使用在rollup job的`group`中配置的字段。如果你想要过滤出keyword类型的`hostname`字段，那么这个字段必须在rollup job中配置`term` grouping。

&emsp;&emsp;如果你尝试使用一个不支持的query，或者query引用了一个没有在rollup job中配置的字段，那么会抛出一个异常。我们预计随着时间推移会增加更多的query支持。

##### Timezones

&emsp;&emsp;Rollup documents存储在`date_histogram`中的时区里。如果没有指定时区，那么默认rollup的timestamp是UTC。


### Transforming data
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/transforms.html)

&emsp;&emsp;Transforms可以让你将现有的（existing）Elasticsearch索引转换（Convert）到汇总索引中（summarized indice），使得能提供一个新的insight和analytics。比如说你可以使用transforms将你的数据pivot到entity-centric indices中，它汇总了用户的行为（he behavior of users）或者是sessions，或者是其他实体（other entities）。或者你可以使用transforms在具有某个唯一键的所有文档中查找最新文档。

- [Overview](#Transform overview)
- [Setup](#Set up transforms)
- [When to use transforms](#When to use transforms)
- [Generating alerts for transforms](#Generating alerts for transforms)
- [Transforms at scale](#Working with transforms at scale)
- [How checkpoints work](#How transform checkpoints work)
- [API quick reference](#API quick reference)
- [Tutorial: Transforming the eCommerce sample data](#Tutorial: Transforming the eCommerce sample data)
- [Examples](#Transform examples)
- [Painless examples](#Painless examples for transforms)
- [Troubleshooting](#Troubleshooting transforms)
- [Limitations](#Transform limitations)

#### Transform overview
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/transform-overview.html)

&emsp;&emsp;你可以使用下面两个方法中的一种来transform你的数据：[pivot](#Pivot transforms)或者[latest](#Latest transforms)。

> IMPORTANT：transforms不会破坏你的源索引（leave your  source index intact）。transformed data会创建一个新的专用的索引。
> 相较于Kibana中的可用参数，Transforms可以通过APIs来提供更多的配置选项。参考[API documentation](#Transform APIs)了解transform所有的配置选项。

&emsp;&emsp;Transforms是持久性（persistent）的任务，存储在cluster state使得节点发生故障也具有弹性（resilient）。参考[How checkpoints work](#How transform checkpoints work)和[Error handling](#Error handling)了解更多关于transforms背后的机制（machinery）。

##### Pivot transforms

&emsp;&emsp;你可以使用transforms将你的数据pivot到entity-centric indices中。通过transforming和汇总（summarizing）你的数据，可以以另一种有趣的方式对其进行可视化和分析。

&emsp;&emsp;许多的Elasticsearch索引的组织方式是事件流（stream of events），每一个事件是一个独立的文档，比如说单个商品的购买信息。Transforms能让你对这些数据进行汇总，汇总成一个组织化的（organized）、更友好的用于分析（analysis-friendly）格式。比如说你可以将某个用户购买的所有商品进行汇总。

&emsp;&emsp;Transforms让你定义一个pivot，它是一个特征集合（a set of features）将索引transform到一个不同的，更加易于提取（digestible）的格式。Pivoting会将汇总后的数据输出到一个新的索引。

&emsp;&emsp;要定义一个pivot，你首先要选择一个或者多个字段，你将使用这些字段对你的数据进行分组。你可以选择可以用于分类的字段（categorical field）以及numerical fields用于分组。如果你使用了numerical fields，则使用你指定的间隔对这些字段的字段值进行分桶。

&emsp;&emsp;第二步就是决定好你想要如何对分组的数据进行聚合。When using aggregations, you practically ask questions about the index。聚合的类型很有多，每一种都有其作用和输出。见[Create transform](#Create transform API)了解更多关于支持的聚合和分组依据字段的内容。

&emsp;&emsp;作为一个可选的步骤，你也可以增加一个query来进一步限制聚合的范围。

&emsp;&emsp;transform会分页处理source index query对应的数据并且对此执行一个composite aggregation。聚合的输出会存储到一个destination index中。transform每次查询source index时，都会创建一个checkpoint。你可以决定是否希望transform执行一次还是连续执行。一个batch transform属于单个操作并且只有一个checkpoint。连续的transforms会不断的增加以及处理（increment and process）新提取的（ingest）source data的checkpoint。

&emsp;&emsp;比如你正在运行一个售卖衣服的网上商城（webshop）。每个订单会创建一个文档，文档中包含一个唯一的订单ID、订单名称和订单中产品的分类、价格、数量，订单的准确时间和一些客户的信息（名称，性别，地址等等）。你的数据集中包含了过去一年的交易。

&emsp;&emsp;如果你想查看上一财年不同类别的销售额，那么定义一个transform，这个transform根据产品分类（女鞋、男鞋等等）和订单时间进行分组。使用过去一年的时间作为订单时间的时间间隔（interval），然后根据订单数量增加一个sum aggregation。输入结果是一个entity-centric index，这个index中显示了过去一年每一类产品卖出的商品的数量。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/pivot-preview.png">

##### Latest transforms

&emsp;&emsp;你可以使用 latest transform将最近的文档拷贝到新的索引中（You can use the latest type of transform to copy the most recent documents into a new index）。你必须指定一个或者多个字段作为一个唯一键用于对数据进行分组，同时还要一个时间字段（date field）来进行时间排序（sort the data chronologicallly）。比如说你可以使用这种类型的transform来跟踪每一个用户最新付款的时间或者每一台服务器最新的事件（the latest event for each host）。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/pivot-preview.png">

&emsp;&emsp;跟pivot transforms一样，latest transform也可以允许一次或者持续运行。它会在source index的data上执行一个composite aggregation并将输出存储到destination index。如果transform持续运行，新的唯一键会自动的添加到destination index上，现有键（existed key）的最新的文档会在每一个checkpoint时自动的更新。

##### Performance considerations

&emsp;&emsp;Transforms在source index上执行聚合并将结果存储到destination index中。因此它所需要的时间跟资源不会比aggregation和index这两个操作花费的时间少。

&emsp;&emsp;如果你一定要对大量的历史数据执行Transform，那么它最初会占用大量的资源—特别是在第一个检查点期间。

&emsp;&emsp;为了有更好的性能，你需要保证对search aggregation和queries进行了优化并且只对必要的进行Transform处理。考虑好你是否能将一个source query对应的数据进行Transform操作，降低处理的数据范围。同样要考虑好集群是否有足够的资源同时处理composite aggregation的查询和索引Transform输出的结果。

&emsp;&emsp;如果你想要分散集群的压力（spread out the impact on your cluster）（slower Transform带来的压力）。你可以调整执行查询跟索引请求的速率。在你[create](#Create transform API)或者[update ](#Update transform API)你的Transform时设置`docs_per_second`进行限制。如果想要计算当前的速率，根据[get transform stats API](#Get transform statistics API)中使用下面的信息：

&emsp;&emsp;`documents_processed / search_time_in_ms * 1000`

#### Set up transforms
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/transform-setup.html)

##### Requirements overview

&emsp;&emsp;若要使用Transform，你必须：

- 至少要有一个[transform node](#Transform node)
- management features visible in the Kibana space, and
- security privileges that:
  - grant use of transforms, and
  - grant access to source and destination indices

##### Security_privileges

&emsp;&emsp;用户的安全权限的分配对用户使用transform会有影响。考虑下面的两个类别：

- [Elasticsearch API user](#Elasticsearch API user)：通过Elasticsearch APIs使用Elasticsearch client、cURL、或者kibana Dev Tools访问transforms。这个场景要求Elasticsearch安全权限（security privileges）
- [Kibana user](#Kibana user)：在Kibana中使用transforms。这种场景要求Kibana feature privileges和Elasticsearch安全权限（security privileges）

###### Elasticsearch API user

&emsp;&emsp;为了能管理transform，你必须满足下面的所有要求：

- `transform_admin` built-in role或者`manage_transform` cluster privileges
- 源索引（source index）上`read`和`view_index_metadata`的索引权限（index privilege）
- 目前索引（destination index）上的`create_index`、`index`、`manage`和`read`的索引权限。一旦配置了`retention_policy`，也需要有目标索引上的`delete`索引权限。

&emsp;&emsp;如果是仅仅要读取下transforms的配置跟状态，你必须要有：

- `transform_user` built-in role or `monitor_transform` cluster privileges

&emsp;&emsp;可以参考[Built-in roles](#Built-in roles) and [Security privileges](#Security privileges)了解更多关于Elasticsearch roles and privileges的信息。

###### Kibana user

##### Kibana spaces

#### When to use transforms
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/transform-usage.html)

&emsp;&emsp;Elasticsearch的aggregation是一个强有力并且非常灵活的功能，可以让你汇总（summarize）并且获取出你的数据中的complex insight。你可以在一个busy website上汇总每天请求网页的数量这类复杂的事情，并可以按地理位置和浏览器类型细分（break down）。 如果你使用相同的数据集来尝试计算一些东西比如说计算页面访问的会话（visitor web session）的平均时间，然而，这样很快会出现OOM。

&emsp;&emsp;为什么会发生OOM呢？网页会话（web session）的持续时间是behavior attribute的一个例子，它不被记录于任何一条日志中。只有从weblogs中找到每一个session的第一条跟最后一条才能衍生出来。这种衍生（derivation）数据要求复杂的查询表达式以及很多的内存来连接（connect）所有指向的数据。如果你有一个持续处理的后台程序将相关的事件融合（fuse）到entity-centric并汇总到另一个索引，你就可以得到一个更实用、连贯的图（joined-up picture）。这个新索引有时被称为数据帧（data frame）。

&emsp;&emsp;当出现下面的需求时，你应该要使用transforms而不是aggregation：

- 你需要一个复杂的特征索引（feature index）而不是top-N的集合
  - 在机器学习中，你通常需要一个复杂的behavioral feature的集合而不是top-N。 如果你要预测客户流失（customer churn），你可能需要观察例如上一周网页访问数量、销售量、或者邮件发出的数量（the number of emails sent）。Elastic Stack的机器学习功能（machine learning feature）会创建基于多维特性空间（multi-dimensional feature space）的模型，就可以受益于由transforms创建的全特征索引（full feature index）
  - 这个场景同样可以应用于跨一个或多个聚合结果进行查询。聚合结果可以被排序或者过滤，但是会受到返回的分桶数量上限的约束（constraint），见[limitations to ordering](#Terms aggregation)和[filtering by bucket selector](#Bucket selector aggregation)。如果你想要查询所有的聚合结果，你需要创建完整的数据帧（data frame）。如果你需要根据多个字段对聚合结果进行排序或者过滤，那么transforms就特别的有用。
- 你需要通过pipeline aggregation对聚合结果进行排序
  - [Pipeline aggregations](#Pipeline aggregations)不能用于排序。技术上来说这是因为pipeline aggregation是在所有的聚合已经完成后的reduce phase期间运行的。如果你创建了一个transforms，你可以有效的对多个数据进行传递。
- 你想要创建summary tables来优化查询
  - 如果你有一个high level dashboard并且会被大量的用户访问并且在一个很大的数据集上使用复杂的聚合查询，这时候创建一个transforms来缓存结果可能会比较会更有效。因此，每个用户都不需要运行聚合查询

#### Generating alerts for transforms
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/transform-alerts.html)

>WARNING：该功能处于beta测试阶段，可能会发生变化。设计跟代码相较于官方的GA（general availability）功能是不成熟的，原样的提供了这个功能但没有任何的保证（warranty）。Beta功能不受官方GA功能的支持SLA的限制。

&emsp;&emsp;Kibana的告警功能（alerting feature）中支持transforms的规则，它会基于某些条件来检查连续的transforms的运行状况（the healthy of continuous transforms）。如果满足规则中的条件，会创建一条规则并且触发相关的action。您可以创建一个规则来检查连续的transforms是否已经启动，如果没有启动，则通过电子邮件通知你。参考[Alerting](https://www.elastic.co/guide/en/kibana/8.2/alerting-getting-started.html#alerting-getting-started)了解更多关于Kibana alerting feature的内容。

&emsp;&emsp;目前可用的transforms规则如下所示：

###### Transform health

&emsp;&emsp;监控transforms的运行状况并在发生操作问题（operational issue）时发出警报。

##### Creating a rule

&emsp;&emsp;你可以在**Stack Management > Rules and Connectors**下创建transforms的规则。

&emsp;&emsp;在**Create rule**的窗口中，给规则起一个名称并且提供一个可选的tag。指定检查transforms运行状态是否发生变化的时间间隔。你也通过选择`Notify`来指定一个通知选项。只要在检查间隔期间满足配置的条件，alert就会保持活动状态。当在下一个interval中没有满足条件，`Recovered` action group会被调用并且告警状态改为`OK`。参考[general rule details](https://www.elastic.co/guide/en/kibana/8.2/create-and-manage-rules.html#defining-rules-general-details)了解更多的内容细节。

&emsp;&emsp;在Stack Monitoring下选择Transform health rule type：

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/transform-rule.png">

###### Transform health

&emsp;&emsp;选择一个或者多个transforms。你可以使用特殊的符号（`*`）将规则应用到所有的transforms。Transforms created after the rule are automatically included。

&emsp;&emsp;下文中的transform运行状态检查是可用的并且是默认开启的：

- Transform is not started

&emsp;&emsp;相关的transforms如果没有启动或者它没有索引任何的数据时会发出通知。建议在通知的消息中包含必要的action来解决这个错误。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/transform-check-config.png">

&emsp;&emsp;在创建规则的最后一步中，[define the actions](#Defining actions)来执行满足条件后的后续动作。

##### Defining actions

&emsp;&emsp;通过选择一个connector type将你的规则和内建集成（built-in integrations）的action联系起来。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/transform-alert-actions.png">

&emsp;&emsp;比如说你可以选择`Slack`作为一个connector type，配置为将消息发送到一个你选择频道中。你可以选择一个index connector type，配置为往你指定的索引中写入JSON对象（JSON object）。当然也可以自定义通知消息。消息中可以有一系列的变量可选，例如 transform ID, description, transform state等等。

&emsp;&emsp;在保存完配置后，你可以在`Rules and Connectors`列表中看到这条规则，你可以检查其状态并且概览其配置信息。

&emsp;&emsp;告警的名称总是跟关联的触发的transforms的ID是一样的。你可以在列出各个警报的规则页面上关闭特定的transforms的通知。你可以在**Rules and Connectors**中根据规则的名称打开通知。

#### Working with transforms at scale
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/transform-scale.html)

&emsp;&emsp;Transforms将现有的Elasticsearch索引转化到汇总索引中（summarized indices），使得有机会用于new insight和数据分析。transforms中的查询和索引操作都使用了标准的Elasticsearch功能，所以在大规模（at scale）使用transforms时需要考虑的事项跟使用标准的Elasticsearch查询是相似的（similar）。如果你遇到了性能问题（performance issue），可以先从明确好瓶颈区域（bottleneck areas）（查询、索引、运行或者存储），然后再看下本章节中提高的需要考虑的相关事项来提高性能。本章节的内容还有助于理解transforms是如何工作的，因为根据transforms是在连续模式下运行还是在批处理模式下（in continuous mode or in batch）运行，应用了不同的考虑事项。

&emsp;&emsp;在这一章节中，你可以学习到：

- 理解配置选项对transforms性能的影响

###### Prerequisites:

&emsp;&emsp;阅读这些指导内容的前提是你有一个想要进行调整（tune）的transforms，并且你对熟悉了下面的内容：

- [How transforms work](#Transform overview)
- [How to set up transforms](#Set up transforms)
- [How transform checkpoints work in continuous mode](#How transform checkpoints work)

&emsp;&emsp;下面的考虑事项（consideration）没有先后顺序（not sequential），每一条的编号只是作为navigation而已。你可以任意的采纳（take action）一个或者多个考虑事项。大多数的建议同时适用于continuous和batch transforms。如果只适用于一种transforms类型，会在描述中高亮说明。

&emsp;&emsp;在每一条建议末尾的圆括号中的关键字说的是当遇到这类瓶颈区域（bottleneck area）可能可以通过这条建议来提高性能。

##### Measure transforms performance

&emsp;&emsp;为了可以优化transforms的性能，首先从明确主要工作的地方开始（start by identifying the areas where most work is being done）。在Kibana 页面中，`Transforms`中的`Stats`里包含了下面三个主要的信息：Indexing、searching、和processing time（或者你可以使用[transforms stats API](#Get transform statistics API)）。例如，如果结果中显示在查询时花费的时间比例最高（highest proportion of time），那么优先优化transforms中的查询。Transforms同样有[Rally support](https://esrally.readthedocs.io/en/stable/)，使得在必要时通过transforms进行配置来检查运行时的性能。如果你已经优化了关键的部分（crucial factor），但是仍然有性能问题，那么你可能需要考虑是否要提高硬件的性能。

##### 1. Optimize frequency (index)

&emsp;&emsp;在一个continuous transform中，配置项`frequency`用来设置周期检查source index变更的时间间隔。如果检测到了变更，source index会被搜索，并将变更应用（apply）到destination index。基于于你的用例，你可能会降低应用变更的频率。通过将`frequency`设置为一个较高的值（最大值是一个小时），工作负载（workload）可以随着时间推移被分散，代价是数据更新频率降低了（less up-to-date）。

##### 2. Increase the number of shards of the destination index (index)

&emsp;&emsp;基于destination index的大小，你可能需要提高分片的数量。transforms在创建destination index时默认只使用一个分片。可以在启动transforms前先创建好索引来覆盖索引设置。参考[Scalability and resilience](#Scalability and resilience: clusters, nodes, and shards)了解分片的数量会如何影响扩展性跟弹性（scalability 和 resilience）。

>TIP：使用[Preview transform](#Preview transform API)来查看transforms创建destination index时使用的设置（settings）。你可以拷贝并且调整索引配置并且在transforms启动前创建好索引。

##### 3. Profile and optimize your search queries (search)

&emsp;&emsp;如果你定义了一个查询source index的query，确保这个query是高效的。使用Kibana中的`Search Profiler`和`Dev Tools`来获取每一个独立的组件在查询请求中详细的时间开销信息（timing information）。或者你可以使用[Profile](#Profile API)。其结果可以给你一个insight，让你了解查询请求在low level是如何执行的，并理解为什么一些请求很慢。接着你就可以采取操作进行优化。

&emsp;&emsp;Transforms执行的是标准的Elasticsearch查询请求。有很多的方法来编写Elasticsearch query，并且有些query比其他的query更加的高效。参考[Tune for search speed](#Tune for search speed)了解更多关于调节（tune）Elasticsearch性能的内容。

#### How transform checkpoints work
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/transform-checkpoints.html)

&emsp;&emsp;transformas每次检查完（examine）source index会创建或者更新destination index，并生成一个checkpoint。

&emsp;&emsp;如果transforms只运行一次，逻辑上只会生成一个checkpoint。如果transforms持续运行，它会在提取（ingest）和转化（transform）source data时创建checkpoint。ransform的`sync`属性通过指定一个时间类型的字段（time fields）来配置checkpoint。

&emsp;&emsp;要创建一个checkpoint，continuous transform会：

1. 检查source index的变更
   - transform使用一个周期性的定时器（periodic timer） 对source index进行检查。基于transforms中定义的`frequency`这个时间间隔属性来完成检查。
   - If the source indices remain unchanged or if a checkpoint is already in progress then it waits for the next timer
   - 如果发生了变更则创建一个checkpoint。
2. 明确哪个entities and/or time buckets发生了变更
   - transforms在查询后会检查在上一次和最新的检查点之间哪些entities or time buckets发生了变更。transform会用变更的值，使用较少的操作来同步source和destination index，而不是full  re-run。
3. 将变更更新到destination index（the data frame）中
   - transform将变更应用到destination index中相关的新的或者发生变更的entities or time buckets中。The set of changes can be paginated。transform执行一个composite aggregation，类似batch transform operation。然而它会基于之前的步骤添加query filters来减少工作量（amount of work）。应用（apply）后所有的变更后，checkpoint就完成了。

&emsp;&emsp;checkpoint的处理会涉及到集群上的Indexing和search动作（activity）。在开发transforms这个功能时，我们已经尝试控制其性能。我们决定让transform花更长的时间来完成，而不是快速完成并优先（take  precedence in）消耗资源。也就是说，集群仍然需要足够的资源来支持composite aggregation搜索以及索引其结果。

>TIP：如果集群因为transform导致性能下降（unsuitable performance degradation），可以停止使用transform，并且参考[Performance considerations](#Performance considerations)。

##### Using the ingest timestamp for syncing the transform

&emsp;&emsp;在大多数的情况下，强烈建议提取（ingest）source index中的时间戳用于同步transform。这是一种能让transform区分发生变更最好的方法（the optimal way）。如果你的数据源（data source）遵循[ECS standard](https://www.elastic.co/guide/en/ecs/8.2/ecs-reference.html)，那你已经有一个[event.ingested](https://www.elastic.co/guide/en/ecs/8.2/ecs-event.html#field-event-ingested)字段，这时候就可以将`event.ingested`作为transform的`sync.time.field`属性。

&emsp;&emsp;如果你没有`event.ingested`或者没有生成（populated）。你可以通过一个ingest pipeline进行设置。创建一个ingest pipeline可以使用[ingest pipeline API](#Create or update pipeline API)或者通过Kibana中的`Stack Management > Ingest Pipelines`。使用[set processor](#Set processor)来设置这个字段并关联提取的timestamp的值。

```text
PUT _ingest/pipeline/set_ingest_time
{
  "description": "Set ingest timestamp.",
  "processors": [
    {
      "set": {
        "field": "event.ingested",
        "value": "{{{_ingest.timestamp}}}"
      }
    }
  ]
}
```

&emsp;&emsp;你在创建ingest pipeline之后，将其应用（apply）到transform的source index中。这个pipeline会将提取出的时间戳的添加到每篇文档的`event.ingested`字段中。在通过[create transform API](#Create transform API)创建一个新的transform或者通过[update transform API](#Update transform API)更新现有的transform时来配置transform的`sync.time.field`属性。

&emsp;&emsp;参考[Add a pipeline to an indexing request](#Add a pipeline to an indexing request)和[Ingest pipelines](#Ingest pipelines)了解更多关于如何使用ingest pipeline的内容。

##### Change detection heuristics

&emsp;&emsp;当transform以continuous mode运行时，它不会的更新新的数据到destination index中。transform使用一组称为change detection的启发式方法以更少的操作更新destination index。

&emsp;&emsp;在这个例子中，根据host name对数据进行了分组。Change detection会检测哪些host name发生了变更。host A、C和G，只更新与这些host有关的文档，而不更新存储关于hostB、D或任何其他未更改的主机的信息的文档。

&emsp;&emsp;`date_histogram`这种通过时间分桶的场景也可以使用这种启发式方法。Change detection会检测哪些时间分桶发生了变更并且只更新发生变更的时间分桶。

##### Error handling

&emsp;&emsp;transform中发生故障往往是在Indexing或者searching时。To increase the resiliency of transforms, the cursor positions of the aggregated search and the changed entities search are tracked in memory and persisted periodically。

&emsp;&emsp;Checkpoint的故障可以根据下面的方式分类：

- Temporary failures：checkpoint会进行重试。如果连续10次出现错误（10 consecutive failure occur），transform会有一个失败的状态。比如说当分片发生故障并且请求只能返回部分数据时。
- Irrecoverable failures：transform会马上失败。比如说source index没有找到时。
- Adjustment failures：transform基于adjusted settings进行重试。比如说如果在composite aggregation时发生了 parent circuit breaker memory errors 。transform会收到部分的结果（partial result）。aggregated search在重试时会进行更少数量的分桶。重试会使用在transform中定义的`frequency`这个时间间隔。如果重试的查询达到了要使用最小数量分桶的地步，则会发生一个不可恢复的错误。

&emsp;&emsp;如果在一个节点上transform发生故障。transform会从最近的一次 persisted cursor position重启恢复。恢复的操作可能执行一些已经完成过的工作，但是能保证数据一致性。

#### API quick reference
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/transform-api-quickref.html)

&emsp;&emsp;所有transform的endpoint都有这个前缀：

```text
_transform/
```

- [Create transforms](#Create transform API)
- [Delete transforms](#Delete transform API)
- [Get transforms](#Get transform statistics API)
- [Get transforms statistics](#Get transform statistics API)
- [Preview transforms](#Preview transform API)
- [Reset transforms](#Reset transform API)
- [Start transforms](#Start transform API)
- [Stop transforms](#Stop transforms API)
- [Update transforms](#Update transform API)

&emsp;&emsp;见[Transform APIs](#Transform APIs)查看所有的接口。

#### Tutorial: Transforming the eCommerce sample data
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ecommerce-transforms.html)

&emsp;&emsp;[Transforms](#Transforming data)能让你获取Elasticsearch的索引，然后进行转化（transform）并存放到另一个索引中。我们将使用[Kibana sample data](https://www.elastic.co/guide/en/kibana/8.2/add-sample-data.html)来演示如何使用transform来pivot以及summarize。

1. 先验证下你的环境是否设置正确以便使用transform。如果开启了Elasticsearch Security Feature，你需要一个有预览和创建transform权限的用户来完成此次教程。你同样需要有source index和destination index这些指定的索引权限。见[Setup](#Set up transforms)。
2. 选择source index
   -  在这个例子中， 我们将使用电子商务订单的样例数据(eCommerce orders sample data)。如果你不熟悉`kibana_sample_data_ecommerce`这个索引，使用Kibana中的**Revenue**了解下。考虑下你能从这些电子商务数据中获得哪些insight
3. 选出transform的pivot类型，并使用各种用于分组和聚合的选项
   - 目前有两种类型的transform，但首先我们先要`Pivoting`你的数据，也就是至少使用一个字段进行分组（apply）并应用一个聚合。你可以先预览转化后的数据（transformed data）， so go ahead and play with it! 你也可以开启histogram chart来更好的了解你的数据分布。
   - 比如说你可能想要根据产品ID进行分组并且统计每一个产品的销售额和平均价格。或者你可能想要查看每一个客户的行为，计算每一个客户的总消费以及它们为多少种产品付费。或者你可能想要将货币和地区考虑进去。What are the most interesting ways you can transform and interpret this data?

&emsp;&emsp;进入Kibana中的**Management > Stack Management > Data > Transforms**，使用向导（use the wizard）来创建一个transform：

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ecommerce-pivot1.png">

&emsp;&emsp;根据客户ID进行分组并添加一个或多个aggregation来了解每一个客户的订单。比如说让我们计算下购买的产品数量，购买花费总额、最贵以及最便宜的两个订单、订单的总数。通过在`total_quantity`和`taxless_total_price`上使用[sum aggregation](#Sum aggregation)，在`total_quantity`上使用[max aggregation](#Max aggregation)，在`order_id`上使用[cardinality aggregation](#Cardinality aggregation)。 

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ecommerce-pivot2.png">

> TIP: 如果你只对数据的部分子集感兴趣，你可以使用[query](#query)。在这个例子中，我们对数据进行了过滤，使得只关心货币类型为欧元的数据。或者你可以根据货币进行分组。如果你想要使用更加复杂的查询，你可以从[saved search](https://www.elastic.co/guide/en/kibana/8.2/save-open-search.html)中创建你的数据帧（data frame）。

&emsp;&emsp;你也可以使用[preview transforms API](#Preview transform API)。

```text
POST _transform/_preview
{
  "source": {
    "index": "kibana_sample_data_ecommerce",
    "query": {
      "bool": {
        "filter": {
          "term": {"currency": "EUR"}
        }
      }
    }
  },
  "pivot": {
    "group_by": {
      "customer_id": {
        "terms": {
          "field": "customer_id"
        }
      }
    },
    "aggregations": {
      "total_quantity.sum": {
        "sum": {
          "field": "total_quantity"
        }
      },
      "taxless_total_price.sum": {
        "sum": {
          "field": "taxless_total_price"
        }
      },
      "total_quantity.max": {
        "max": {
          "field": "total_quantity"
        }
      },
      "order_id.cardinality": {
        "cardinality": {
          "field": "order_id"
        }
      }
    }
  }
}
```

4. 如果你满意这个预览数据，那么就可以创建这个transform了。
   a. 提供一个transform ID，destination index的名称以及描述信息（可选）。如果destination index不存在，则会在transform启动的时候创建。
   b. 先确定好transform要运行一次还是一直运行（once or continuous）。由于样例数据是不会发生变更，那就让transform运行一次。If you want to try it out, however, go ahead and click on **Continuous mode**。你必须选择一个字段，transform就可以根据这个字段检查是否发生了变更。一般情况下，使用ingest timestamp域是个不错的主意。在这个例子中，你可以使用`order_filed`字段。
   c. 获取你可以配置一个retention policy来应用到你的transform中。选择一个date字段用于明确destination index中的旧文档并且提供一个最大的寿命（age）。destination index中超过这个寿命的文档会被移除。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ecommerce-pivot3.png">

&emsp;&emsp;在Kibana中，在你完成创建transform前，你可以复制preview transform API request 到你的粘贴板。这些信息对你之后想要手动创建destination index会有一定的帮助。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ecommerce-pivot4.png">

&emsp;&emsp;你也可以使用[create transforms API.](#Create transform API)。

```text
PUT _transform/ecommerce-customer-transform
{
  "source": {
    "index": [
      "kibana_sample_data_ecommerce"
    ],
    "query": {
      "bool": {
        "filter": {
          "term": {
            "currency": "EUR"
          }
        }
      }
    }
  },
  "pivot": {
    "group_by": {
      "customer_id": {
        "terms": {
          "field": "customer_id"
        }
      }
    },
    "aggregations": {
      "total_quantity.sum": {
        "sum": {
          "field": "total_quantity"
        }
      },
      "taxless_total_price.sum": {
        "sum": {
          "field": "taxless_total_price"
        }
      },
      "total_quantity.max": {
        "max": {
          "field": "total_quantity"
        }
      },
      "order_id.cardinality": {
        "cardinality": {
          "field": "order_id"
        }
      }
    }
  },
  "dest": {
    "index": "ecommerce-customers"
  },
  "retention_policy": {
    "time": {
      "field": "order_date",
      "max_age": "60d"
    }
  }
}
```

5. 可选：创建destination index

   - 如果destination index不存在，会在transform启动时候创建。pivot transform会根据source index和transform aggregation为destination index推测出mapping。如果destination index中的一些字段是从脚本中衍生出来的（比如说你使用了[scripted_metrics](#Scripted metric aggregation)或者[bucket_scripts](#Bucket script aggregation)），它们是通过[dynamic mappings](#Dynamic mapping)创建的。你可以使用preview transform API预览下destination index中的mapping。在Kibana中，如果你拷贝了API请求，粘贴到控制台，然后参考下API响应中的`generated_dest_index`。

>NOTE：相较于Kibana中可用的选项，通过APIs能够为transform提供更多的用于配置的选项。例如你可以通过调用[Create transform](#Create transform API)来为`dest`设置一个ingest pipeline。参考[documentation](#Transform APIs)了解更多关于transform配置信息 。

&emsp;&emsp;API example:

```text
{
  "preview" : [
    {
      "total_quantity" : {
        "max" : 2,
        "sum" : 118.0
      },
      "taxless_total_price" : {
        "sum" : 3946.9765625
      },
      "customer_id" : "10",
      "order_id" : {
        "cardinality" : 59
      }
    },
    ...
  ],
  "generated_dest_index" : {
    "mappings" : {
      "_meta" : {
        "_transform" : {
          "transform" : "transform-preview",
          "version" : {
            "created" : "8.0.0"
          },
          "creation_date_in_millis" : 1621991264061
        },
        "created_by" : "transform"
      },
      "properties" : {
        "total_quantity.sum" : {
          "type" : "double"
        },
        "total_quantity" : {
          "type" : "object"
        },
        "taxless_total_price" : {
          "type" : "object"
        },
        "taxless_total_price.sum" : {
          "type" : "double"
        },
        "order_id.cardinality" : {
          "type" : "long"
        },
        "customer_id" : {
          "type" : "keyword"
        },
        "total_quantity.max" : {
          "type" : "integer"
        },
        "order_id" : {
          "type" : "object"
        }
      }
    },
    "settings" : {
      "index" : {
        "number_of_shards" : "1",
        "auto_expand_replicas" : "0-1"
      }
    },
    "aliases" : { }
  }
}
```

6. 启动transform

>TIP：尽管资源的使用会基于集群负载进行调整，在transform启动后会增加集群中Indexing和search的负载。如果你遇到了过多的负载，那你可以停止transform。

&emsp;&emsp;你可以在Kibana中启动、停止、重置以及管理transform：

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/manage-transforms.png">

&emsp;&emsp;或者你也可以使用[start transforms](#Start transform API), [stop transforms](#Stop transforms API) 和 [reset transforms](#Reset transform API)这些APIs。

&emsp;&emsp;如果你reset了transform，所有的checkpoint，states和destination index（如果是通过transform创建的）都会被删除。如果transform创建好了那么它就可以开始运行了。

&emsp;&emsp;API example:

```text
POST _transform/ecommerce-customer-transform/_start
```

>TIP：如果你选择了Batch transform，这是一个single operation并且只有一个checkpoint。当这个transform完成后你不能重启它。 Continuous transforms的不同点在于，当提取了新的source 数据后，它持续的增加和处理checkpoint。

7. 查看新的索引中的数据

&emsp;&emsp;例如在Kibana中使用`Discover`这个应用：

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ecommerce-results.png">

8. 可选：使用`latest`方法创建另一个transform

&emsp;&emsp;这个方法在destination index中产生最新的文档的每一个唯一键的值。例如你可能想找到每一个客户或者每一个国家和地区的最新排序（根据`order_date`进行排序）。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ecommerce-latest1.png">

API example：

```text
POST _transform/_preview
{
  "source": {
    "index": "kibana_sample_data_ecommerce",
    "query": {
      "bool": {
        "filter": {
          "term": {"currency": "EUR"}
        }
      }
    }
  },
  "latest": {
    "unique_key": ["geoip.country_iso_code", "geoip.region_name"],
    "sort": "order_date"
  }
}
```

>TIP：如果destination index不存在，则会在第一次启动transform时创建。跟pivot transforms不同的是，latest transform不会在创建index的时推测mapping定义。它使用的是动态索引（dynamic index）。如果要使用显示的（explicit）mapping。在你启动transform前创建好索引。

9. 如果你不想要保留某个transform，你可以在Kibana中删除它或者使用使用[delete transform API](#Delete transform API)进行删除。默认情况下，当你删除了一个transform，它的destination index和Kibana index pattern会保留。

&emsp;&emsp;现在你对Kibana样例数据创建了简单的transform，并且考虑了你的数据中可能的用例。见[When to use transforms](#When to use transforms)和[Examples](#Transform examples)了解更多的想法。

#### Transform examples
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/transform-examples.html)

&emsp;&emsp;这些例子展示的是如何使用transform从你的数据中衍生（derive）出有用的insight。所有的例子都使用了[Kibana sample datasets](https://www.elastic.co/guide/en/kibana/8.2/add-sample-data.html)。至于更详细的，分步骤的例子，见[Tutorial: Transforming the eCommerce sample data](#Tutorial: Transforming the eCommerce sample data)。

- [Finding your best customers](#Finding your best customers)
- [Finding air carriers with the most delays](#Finding air carriers with the most delays)
- [Finding suspicious client IPs](#Finding suspicious client IPs)
- [Finding the last log event for each IP address](#Finding the last log event for each IP address)
- [Finding client IPs that sent the most bytes to the server](#Finding client IPs that sent the most bytes to the server)
- [Getting customer name and email address by customer ID](#Getting customer name and email address by customer ID)

##### Finding your best customers

&emsp;&emsp;这个例子中使用了电子商务订单的样例数据集来找出在一个假想的网上商城中消费最多的客户。让我们使用`pivot`类型的transform，这样我们的destination index中会包含订单的数量，订单的总价，每一个商品的总数，每一个订单的平均价格以及每一个客户购买的产品数量。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/transform-ex1-1.jpg">

&emsp;&emsp;或者你可以使用[preview transform](#Preview transform API)和[create transform API](#Create transform API)。

&emsp;&emsp;API example

```text
POST _transform/_preview
{
  "source": {
    "index": "kibana_sample_data_ecommerce"
  },
  "dest" : { 
    "index" : "sample_ecommerce_orders_by_customer"
  },
  "pivot": {
    "group_by": { 
      "user": { "terms": { "field": "user" }},
      "customer_id": { "terms": { "field": "customer_id" }}
    },
    "aggregations": {
      "order_count": { "value_count": { "field": "order_id" }},
      "total_order_amt": { "sum": { "field": "taxful_total_price" }},
      "avg_amt_per_order": { "avg": { "field": "taxful_total_price" }},
      "avg_unique_products_per_order": { "avg": { "field": "total_unique_products" }},
      "total_unique_products": { "cardinality": { "field": "products.product_id" }}
    }
  }
}
```

&emsp;&emsp;第6行，transform中的destination index，在`_preview`中可以忽略
&emsp;&emsp;第10行，使用了两个`group_by`。这意味着transform包含了由`user`和`customer_id`组成的列。在此数据集中，这两个字段都是唯一的。通过将两者都包含在transform中，它为最终结果提供了更多的context。

> NOTE: 在上面的例子中，简练的 （condense）JSON 格式对pivot对象有更好的可读性

&emsp;&emsp;preview transform API能让你提前（in advance）的看到transform生成出的一些样例结果布局：

```text
{
  "preview" : [
    {
      "total_order_amt" : 3946.9765625,
      "order_count" : 59.0,
      "total_unique_products" : 116.0,
      "avg_unique_products_per_order" : 2.0,
      "customer_id" : "10",
      "user" : "recip",
      "avg_amt_per_order" : 66.89790783898304
    },
    ...
    ]
  }
```

&emsp;&emsp;这个transform使得更容易的回答下面的问题：

- 哪个客户消费最多
- 在每个订单中，哪个客户消费最多
- 哪个客户经常下单购买
- 哪个客户购买的不同产品的数量是最少的

&emsp;&emsp;当然也可以只使用聚合来回答上面的问题，但是transform可以让我们持久化这些数据作为一个customer centric index。这个索引使我们能够用于大规模（at scale）分析数据，并提供更大的灵活性，从以客户为中心的角度探索和navigate 数据。在某些情况下，它甚至可以使创建可视化变得非常简单。

##### Finding air carriers with the most delays

&emsp;&emsp;这个例子使用了航空样例数据来找出哪个航空公司延误次数最多。首先通过一个query filter从source data中筛选掉所有被取消的航班。然后将数据转化（transform）为包含了根据航空公司对航班编号分组去重，延误时间求和，飞行时间求和。最终使用了一个[bucket_script](#Bucket script aggregation)确定实际延误的飞行时间百分比。

```text
POST _transform/_preview
{
  "source": {
    "index": "kibana_sample_data_flights",
    "query": { 
      "bool": {
        "filter": [
          { "term":  { "Cancelled": false } }
        ]
      }
    }
  },
  "dest" : { 
    "index" : "sample_flight_delays_by_carrier"
  },
  "pivot": {
    "group_by": { 
      "carrier": { "terms": { "field": "Carrier" }}
    },
    "aggregations": {
      "flights_count": { "value_count": { "field": "FlightNum" }},
      "delay_mins_total": { "sum": { "field": "FlightDelayMin" }},
      "flight_mins_total": { "sum": { "field": "FlightTimeMin" }},
      "delay_time_percentage": { 
        "bucket_script": {
          "buckets_path": {
            "delay_time": "delay_mins_total.value",
            "flight_time": "flight_mins_total.value"
          },
          "script": "(params.delay_time / params.flight_time) * 100"
        }
      }
    }
  }
}
```
&emsp;&emsp;第5行，从数据源中筛选出没有被取消的航班
&emsp;&emsp;第13行，transform中的destination index，在`_preview`中可以忽略
&emsp;&emsp;第17行，根据`Carrier`字段进行分组，这个字段的字段值是航空公司的名称
&emsp;&emsp;第24行，这个`bucket_script`会对聚合返回的结果进行计算。在这个特定的例子中，它会计算出延误占飞行时间的比例。

&emsp;&emsp;预览可以让你看到在新的索引中将含有每家航空公司的这样的数据：

```text
{
  "preview" : [
    {
      "carrier" : "ES-Air",
      "flights_count" : 2802.0,
      "flight_mins_total" : 1436927.5130677223,
      "delay_time_percentage" : 9.335543983955839,
      "delay_mins_total" : 134145.0
    },
    ...
  ]
}
```

&emsp;&emsp;这个transform使得更容易的回答下面的问题：

- 哪家航空公司的延误时间占飞行时间的比例最大？

> NOTE: 此数据是虚构的（fictional），并不反映任何特色目的地或出发地机场的实际延误或航班统计数据

##### Finding suspicious client IPs

&emsp;&emsp;这个例子使用网页样例数据集来识别出有问题的（suspicious）client ip。转化（transform）后的数据所在的索引中包含了流量的总和（sum of bytes）以及每个客户机IP的不同url、代理、按位置和地理目的地传入的请求的数量。还使用了filter aggregation来统计每一个client IP收到的不同HTTP类型响应的数量。最终下面的例子将网页日志数据转化为一个entity centric index，这个entity就是`cientip`。

```text
PUT _transform/suspicious_client_ips
{
  "source": {
    "index": "kibana_sample_data_logs"
  },
  "dest" : { 
    "index" : "sample_weblogs_by_clientip"
  },
  "sync" : { 
    "time": {
      "field": "timestamp",
      "delay": "60s"
    }
  },
  "pivot": {
    "group_by": {  
      "clientip": { "terms": { "field": "clientip" } }
      },
    "aggregations": {
      "url_dc": { "cardinality": { "field": "url.keyword" }},
      "bytes_sum": { "sum": { "field": "bytes" }},
      "geo.src_dc": { "cardinality": { "field": "geo.src" }},
      "agent_dc": { "cardinality": { "field": "agent.keyword" }},
      "geo.dest_dc": { "cardinality": { "field": "geo.dest" }},
      "responses.total": { "value_count": { "field": "timestamp" }},
      "success" : { 
         "filter": {
            "term": { "response" : "200"}}
        },
      "error404" : {
         "filter": {
            "term": { "response" : "404"}}
        },
      "error5xx" : {
         "filter": {
            "range": { "response" : { "gte": 500, "lt": 600}}}
        },
      "timestamp.min": { "min": { "field": "timestamp" }},
      "timestamp.max": { "max": { "field": "timestamp" }},
      "timestamp.duration_ms": { 
        "bucket_script": {
          "buckets_path": {
            "min_time": "timestamp.min.value",
            "max_time": "timestamp.max.value"
          },
          "script": "(params.max_time - params.min_time)"
        }
      }
    }
  }
}
```

&emsp;&emsp;第6行，transform中的destination index
&emsp;&emsp;第9行，将transform配置为持续运行。它使用`timestamp`字段来同步source index和destination index。最晚60秒进行一次同步
&emsp;&emsp;第16行，根据`clientip`对数据进行分组
&emsp;&emsp;第26行，Filter aggregation统计了`response`字段中成功响应（200）的数量。接下来的两个aggregation根据错误码、某个范围内的响应码（response code）统计了错误的响应。
&emsp;&emsp;第40行，`bucket_script`基于聚合结果统计了`clientip`访问的持续时间

&emsp;&emsp;在你创建transform之后，你必须启动它：

```text
POST _transform/suspicious_client_ips/_start
```

&emsp;&emsp;此后不久（Shortly thereafter），第一个结果会出现在destination index中：

```text
GET sample_weblogs_by_clientip/_search
```

&emsp;&emsp;下面展示的是你的数据中某一个client ip的结果：

```text
  "hits" : [
      {
        "_index" : "sample_weblogs_by_clientip",
        "_id" : "MOeHH_cUL5urmartKj-b5UQAAAAAAAAA",
        "_score" : 1.0,
        "_source" : {
          "geo" : {
            "src_dc" : 2.0,
            "dest_dc" : 2.0
          },
          "success" : 2,
          "error404" : 0,
          "error503" : 0,
          "clientip" : "0.72.176.46",
          "agent_dc" : 2.0,
          "bytes_sum" : 4422.0,
          "responses" : {
            "total" : 2.0
          },
          "url_dc" : 2.0,
          "timestamp" : {
            "duration_ms" : 5.2191698E8,
            "min" : "2020-03-16T07:51:57.333Z",
            "max" : "2020-03-22T08:50:34.313Z"
          }
        }
      }
    ]
```

> NOTE: 跟其他的Kibana数据集一样，网页日志样例数据集的时间戳跟你加载时的时间有关，包括有些数据的时间是未来时间。一旦这些时间超前的数据成为以前的时间，continuous transform在一定的时间点就会处理它们。如果你是在一段时间之前加载的网页日志样例数据集，你可以卸载并重新加载，时间戳就会发生变更。

&emsp;&emsp;这个transform使得更容易的回答下面的问题：

- 哪个client IP传输最多的数据？
- 哪个client IP跟最多不同数量的URl进行了交互
- 哪个client IP有high error state
- 哪个client IP跟最多不同数量的国家进行了交互

##### Finding the last log event for each IP address

&emsp;&emsp;这个例子使用网页日志样例数据集来找出一个IP地址最后的那条日志。我们使用continuous模式下的`latest`类型的transform。它从最新的source data中拷贝出每一个唯一的key对应的最新文档信息到destination index中，并且在新的数据进入到source data后，将其更新到destination index中。

&emsp;&emsp;选择`clientip`作为唯一的key。根据这个字段对数据进行分组。选择`timestamp`字段作为时间类型的字段用来对数据进行排序。对于continuous mode，指定一个时间类型的字段用于识别在source index中新的文档，以及用于周期性检查是否发生变更的间隔时间。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/transform-ex4-1.jpg">

&emsp;&emsp;让我们假设下，我们对只在最近的log中出现的IP地址对应的文档感兴趣。你可以定义一个保留策略（retention policy）并且指定一个date字段用于计算文档的寿命（age）。这个例子使用了相同的date字段用于排序。然后设置一个文档的最大寿命，超过你设定的值的文档会从destination index中移除。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/transform-ex4-2.jpg">

&emsp;&emsp;这个transform创建的destination index中包含了每一个client ip最新的登陆时间。因为transform是continuous模式，所以当新的数据进入到source index后会更新destination index的值。最终每一篇超过30天（基于保留策略）的文档都会从destination index中移除

```text
PUT _transform/last-log-from-clientip
{
  "source": {
    "index": [
      "kibana_sample_data_logs"
    ]
  },
  "latest": {
    "unique_key": [ 
      "clientip"
    ],
    "sort": "timestamp" 
  },
  "frequency": "1m", 
  "dest": {
    "index": "last-log-from-clientip"
  },
  "sync": { 
    "time": {
      "field": "timestamp",
      "delay": "60s"
    }
  },
  "retention_policy": { 
    "time": {
      "field": "timestamp",
      "max_age": "30d"
    }
  },
  "settings": {
    "max_page_search_size": 500
  }
}
```

&emsp;&emsp;第9行，指定用于对数据分组的字段
&emsp;&emsp;第12行，指定用于对数据排序的date字段
&emsp;&emsp;第14行，设置时间间隔用于周期性检查source index中是否发生了变化
&emsp;&emsp;第18行，这里面的date字段和delay的设置用于同步source index跟destination index
&emsp;&emsp;第24行，为transform指定保留策略，超过配置的值的文档会从destination index中移除

&emsp;&emsp;创建transform之后并启动：

```text
POST _transform/last-log-from-clientip/_start
```

&emsp;&emsp;transform处理了数据后，可以从destination index中进行查询：

```text
GET last-log-from-clientip/_search
```

&emsp;&emsp;下面展示的是你的数据中某一个client ip的结果：

```text
{
  "_index" : "last-log-from-clientip",
  "_id" : "MOeHH_cUL5urmartKj-b5UQAAAAAAAAA",
  "_score" : 1.0,
  "_source" : {
    "referer" : "http://twitter.com/error/don-lind",
    "request" : "/elasticsearch",
    "agent" : "Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)",
    "extension" : "",
    "memory" : null,
    "ip" : "0.72.176.46",
    "index" : "kibana_sample_data_logs",
    "message" : "0.72.176.46 - - [2018-09-18T06:31:00.572Z] \"GET /elasticsearch HTTP/1.1\" 200 7065 \"-\" \"Mozilla/4.0 (compatible; MSIE 6.0; Windows NT 5.1; SV1; .NET CLR 1.1.4322)\"",
    "url" : "https://www.elastic.co/downloads/elasticsearch",
    "tags" : [
      "success",
      "info"
    ],
    "geo" : {
      "srcdest" : "IN:PH",
      "src" : "IN",
      "coordinates" : {
        "lon" : -124.1127917,
        "lat" : 40.80338889
      },
      "dest" : "PH"
    },
    "utc_time" : "2021-05-04T06:31:00.572Z",
    "bytes" : 7065,
    "machine" : {
      "os" : "ios",
      "ram" : 12884901888
    },
    "response" : 200,
    "clientip" : "0.72.176.46",
    "host" : "www.elastic.co",
    "event" : {
      "dataset" : "sample_web_logs"
    },
    "phpmemory" : null,
    "timestamp" : "2021-05-04T06:31:00.572Z"
  }
}
```

&emsp;&emsp;这个transform使得更容易的回答下面的问题：

- 与特定IP地址相关的最近的日志事件是什么?

##### Finding client IPs that sent the most bytes to the server

&emsp;&emsp;这个例子使用了网页日志样例数据集来找出每小时往服务器发送最多字节的client Ip。这个例子使用了带有[top_metrics aggregation](#Top metrics aggregation) 的`pivot`类型的transform。

&emsp;&emsp;在date字段上使用[date histogram](#\_Date histogram)并指定一个小时作为时间间隔对数据进行分组。在`bytes`字段上使用[max aggregation](#Max aggregation)获取发送到服务的最大数值总和。如果不用`max aggregation`，可以返回发送字节最多的client ip，但是获取不到字节的数值总和。在`top_metrics`属性中，指定`clientip`和`geo.src`，然后根据`byte`字段进行降序排序。这个transform返回发送数据最多的client ip和用对应位置的2个字母的ISO代码（2-letter ISO code of the corresponding location）。

```text
POST _transform/_preview
{
  "source": {
    "index": "kibana_sample_data_logs"
  },
  "pivot": {
    "group_by": { 
      "timestamp": {
        "date_histogram": {
          "field": "timestamp",
          "fixed_interval": "1h"
        }
      }
    },
    "aggregations": {
      "bytes.max": { 
        "max": {
          "field": "bytes"
        }
      },
      "top": {
        "top_metrics": { 
          "metrics": [
            {
              "field": "clientip"
            },
            {
              "field": "geo.src"
            }
          ],
          "sort": {
            "bytes": "desc"
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;第7行，使用时间间隔为1个小时的date histogram对数据进行分组
&emsp;&emsp;第16行，计算`bytes`字段的最大值
&emsp;&emsp;第22行，指定top document中的`clientip`和`geo.src`以及排序方法（`bytes`的值最大的文档）

&emsp;&emsp;上面的例子返回的结果大致是：

```text
{
  "preview" : [
    {
      "top" : {
        "clientip" : "223.87.60.27",
        "geo.src" : "IN"
      },
      "bytes" : {
        "max" : 6219
      },
      "timestamp" : "2021-04-25T00:00:00.000Z"
    },
    {
      "top" : {
        "clientip" : "99.74.118.237",
        "geo.src" : "LK"
      },
      "bytes" : {
        "max" : 14113
      },
      "timestamp" : "2021-04-25T03:00:00.000Z"
    },
    {
      "top" : {
        "clientip" : "218.148.135.12",
        "geo.src" : "BR"
      },
      "bytes" : {
        "max" : 4531
      },
      "timestamp" : "2021-04-25T04:00:00.000Z"
    },
    ...
  ]
}
```

##### Getting customer name and email address by customer ID

&emsp;&emsp;这个例子使用电子商务样例数据集，基于客户ID创建一个entity-centric index，并通过`top_metrics aggregation`获取客户名称、email地址。

&emsp;&emsp;根据`customer_id`进行分组，在`top_metrics aggregation`中添加`metirc`，包括`email`，`ustomer_first_name.keyword`，以及`customer_last_name.keyword`。用`order_date`字段对`top_metrics`进行降序排序。API大致如下：

```text
POST _transform/_preview
{
  "source": {
    "index": "kibana_sample_data_ecommerce"
  },
  "pivot": {
    "group_by": { 
      "customer_id": {
        "terms": {
          "field": "customer_id"
        }
      }
    },
    "aggregations": {
      "last": {
        "top_metrics": { 
          "metrics": [
            {
              "field": "email"
            },
            {
              "field": "customer_first_name.keyword"
            },
            {
              "field": "customer_last_name.keyword"
            }
          ],
          "sort": {
            "order_date": "desc"
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;第7行，在`customer_id`字段上使用`terms aggregation`对数据进行分组
&emsp;&emsp;第16行，根据时间降序排序返回指定的字段（email和名称）

&emsp;&emsp;API返回结果大致如下：

```text
 {
  "preview" : [
    {
      "last" : {
        "customer_last_name.keyword" : "Long",
        "customer_first_name.keyword" : "Recip",
        "email" : "recip@long-family.zzz"
      },
      "customer_id" : "10"
    },
    {
      "last" : {
        "customer_last_name.keyword" : "Jackson",
        "customer_first_name.keyword" : "Fitzgerald",
        "email" : "fitzgerald@jackson-family.zzz"
      },
      "customer_id" : "11"
    },
    {
      "last" : {
        "customer_last_name.keyword" : "Cross",
        "customer_first_name.keyword" : "Brigitte",
        "email" : "brigitte@cross-family.zzz"
      },
      "customer_id" : "12"
    },
    ...
  ]
}
```

#### Painless examples for transforms
（8,2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/transform-painless-examples.html)

&emsp;&emsp;这些例子展示的是如何在transform中使用painless。你可以在[Painless guide](https://www.elastic.co/guide/en/elasticsearch/painless/8.2/painless-guide.html)中了解更多关于Painless scripting language的内容。

- [Getting top hits by using scripted metric aggregation](#Getting top hits by using scripted metric aggregation)
- [Getting time features by using aggregations](#Getting time features by using aggregations)
- [Getting duration by using bucket script](#Getting duration by using bucket script)
- [Counting HTTP responses by using scripted metric aggregation](#Counting HTTP responses by using scripted metric aggregation)
- [Comparing indices by using scripted metric aggregations](#Comparing indices by using scripted metric aggregations)
- [Getting web session details by using scripted metric aggregation](#Getting web session details by using scripted metric aggregation)

>NOTE: 
>
>尽管下面的例子中的上下文是transform的用例，但是它内部的painless脚本可以在Elasticsearch的其他查询聚合中使用。
>
>下面的所有的例子都使用了脚本，但是transform无法推测出脚本生成的用于mapping的字段。Transform不会在destination index中创建这些字段的mapping，意味着是动态映射的（dynamic mapped）。如果你要显示的mapping，你可以在启动Transform前创建好destination index。

##### Getting top hits by using scripted metric aggregation

&emsp;&emsp;下面的脚本片段（snippet）说的是如何找到最新的文档，换句话说就是找到timestamp是最新的文档。从技术视角看，在Transform中使用[Scripted metric aggregation](#Scripted metric aggregation)也能达到[Top hits](#Top hits aggregation)的功能。

```text
"aggregations": {
  "latest_doc": {
    "scripted_metric": {
      "init_script": "state.timestamp_latest = 0L; state.last_doc = ''", 
      "map_script": """ 
        def current_date = doc['@timestamp'].getValue().toInstant().toEpochMilli();
        if (current_date > state.timestamp_latest)
        {state.timestamp_latest = current_date;
        state.last_doc = new HashMap(params['_source']);}
      """,
      "combine_script": "return state", 
      "reduce_script": """ 
        def last_doc = '';
        def timestamp_latest = 0L;
        for (s in states) {if (s.timestamp_latest > (timestamp_latest))
        {timestamp_latest = s.timestamp_latest; last_doc = s.last_doc;}}
        return last_doc
      """
    }
  }
}
```

&emsp;&emsp;第4行，`init_script`在`state`对象中创建了一个long类型的`timestamp_latest`和string类型的`last_doc`
&emsp;&emsp;第5行，`map_script`中基于文档中的`timestamp`定义了`current_date`，并且跟`state.timestamp_latest`作比较。最终从shard中返回`state.last_doc`。通过使用`new HashMap(...)`复制了source document，这个操作对于将完整的source object从一个阶段传递给下一个阶段是非常重要的
&emsp;&emsp;第11行，`combine_script`从每一个shard中返回`state`
&emsp;&emsp;第12行，`reduce_script`遍历每一个shard返回的`s.timestamp_latest`的值并且返回带有最新timestamp（last_doc）的文档。在响应中，top hit（也就是`latest_doc`）嵌套在`latest_doc`下

&emsp;&emsp;见[scope of scripts](#Scope of scripts)了解更多的脚本信息

&emsp;&emsp;你可以使用类似的方式来获取last value：

```text
"aggregations": {
  "latest_value": {
    "scripted_metric": {
      "init_script": "state.timestamp_latest = 0L; state.last_value = ''",
      "map_script": """
        def current_date = doc['@timestamp'].getValue().toInstant().toEpochMilli();
        if (current_date > state.timestamp_latest)
        {state.timestamp_latest = current_date;
        state.last_value = params['_source']['value'];}
      """,
      "combine_script": "return state",
      "reduce_script": """
        def last_value = '';
        def timestamp_latest = 0L;
        for (s in states) {if (s.timestamp_latest > (timestamp_latest))
        {timestamp_latest = s.timestamp_latest; last_value = s.last_value;}}
        return last_value
      """
    }
  }
}
```

##### Getting time features by using aggregations

&emsp;&emsp;下面的这个片段（snippet）用来展示如何在transform中使用Painless提取time的功能。片段中使用的`@timestamp`字段被定义为了date类型。

```java
"aggregations": {
  "avg_hour_of_day": { 
    "avg":{
      "script": { 
        "source": """
          ZonedDateTime date =  doc['@timestamp'].value; 
          return date.getHour(); 
        """
      }
    }
  },
  "avg_month_of_year": { 
    "avg":{
      "script": { 
        "source": """
          ZonedDateTime date =  doc['@timestamp'].value; 
          return date.getMonthValue(); 
        """
      }
    }
  },
 ...
}
```

&emsp;&emsp;第2行，aggregation的名称
&emsp;&emsp;第4行，包含了一个painless脚本，该脚本将返回小时
&emsp;&emsp;第6行，基于文档中的timestamp字段设置一个`date`
&emsp;&emsp;第7行，从`date`中返回小时
&emsp;&emsp;第12行，aggregation的名称
&emsp;&emsp;第14行，包含了一个painless脚本，该脚本将返回月份
&emsp;&emsp;第16行，基于文档中的timestamp字段设置一个`date`
&emsp;&emsp;第17行，从`date`中返回月份


##### Getting duration by using bucket script

&emsp;&emsp;这个例子展示的是如何从一个数据日志中通过使用[bucket script](#Bucket script aggregation)并根据client ip进行分组获取session的持续时间。这个例子使用了 Kibana中的样例网页日志集。

```text
PUT _transform/data_log
{
  "source": {
    "index": "kibana_sample_data_logs"
  },
  "dest": {
    "index": "data-logs-by-client"
  },
  "pivot": {
    "group_by": {
      "machine.os": {"terms": {"field": "machine.os.keyword"}},
      "machine.ip": {"terms": {"field": "clientip"}}
    },
    "aggregations": {
      "time_frame.lte": {
        "max": {
          "field": "timestamp"
        }
      },
      "time_frame.gte": {
        "min": {
          "field": "timestamp"
        }
      },
      "time_length": { 
        "bucket_script": {
          "buckets_path": { 
            "min": "time_frame.gte.value",
            "max": "time_frame.lte.value"
          },
          "script": "params.max - params.min" 
        }
      }
    }
  }
}
```

&emsp;&emsp;第25行，为了定义session的时长，我们使用 bucket script
&emsp;&emsp;第27行， bucket script是脚本变量的一个映射（map）并且关联的路径是你在脚本中会使用的变量。在这个例子中，`min`跟`max`这两个变量跟`time_frame.gte.value`和`time_frame.lte.value`形成映射关系
&emsp;&emsp;第31行，最终脚本对session的开始跟结束时间执行减法操作并返回了session的持续时间

##### Counting HTTP responses by using scripted metric aggregation

&emsp;&emsp;你可以在transform中使用scripted metric aggregation来计算网页日志数据中不同的HTTP响应类型。你也可以使用 filter aggregations来达到类似的功能，见[Finding suspicious client IPs](#Finding suspicious client IPs)了解更多详细的例子。

&emsp;&emsp;下面的例子中假设HTTP响应码是文档中keyword类型的`response`字段的字段值。

```text
"aggregations": { 
  "responses.counts": { 
    "scripted_metric": { 
      "init_script": "state.responses = ['error':0L,'success':0L,'other':0L]", 
      "map_script": """ 
        def code = doc['response.keyword'].value;
        if (code.startsWith('5') || code.startsWith('4')) {
          state.responses.error += 1 ;
        } else if(code.startsWith('2')) {
          state.responses.success += 1;
        } else {
          state.responses.other += 1;
        }
        """,
      "combine_script": "state.responses", 
      "reduce_script": """ 
        def counts = ['error': 0L, 'success': 0L, 'other': 0L];
        for (responses in states) {
          counts.error += responses['error'];
          counts.success += responses['success'];
          counts.other += responses['other'];
        }
        return counts;
        """
      }
    },
  ...
}
```

&emsp;&emsp;第1行，这个transform的`aggregations`对象包含了所有的aggregation
&emsp;&emsp;第2行，`scripted_metric`  aggregation对象
&emsp;&emsp;第3行，`scripted_metric`在网页日志数据上执行了分布式操作来统计指定的HTTP响应码类型（error，success，其他）
&emsp;&emsp;第4行，`init_scriptc`在`state`对象中创建了带有三个long类型属性（`error`, `success`, `other`）的`responses`数组。
&emsp;&emsp;第5行，`map_script`基于文档中的`response.keyword`定义了`code`。然后基于response中的第一个数字来统计errors, successes, 和其他responses类型
&emsp;&emsp;第15行，`combine_script`从每一个shard中返回`state.responses`
&emsp;&emsp;第16行，`reduce_script`创建了一个`counts`数组，带有`error`,，`success`，`other`三个属性。然后遍历从每一个shard返回的`response`，将不同的相应类型分配给`counts`的对应属性。error responses to the error counts, success responses to the success counts, and other responses to the other counts。最终返回`counts`数组。

##### Comparing indices by using scripted metric aggregations

&emsp;&emsp;这个例子展示的是如何在transform中使用一个scripted metric aggregation来比较两个索引的内容

```text
POST _transform/_preview
{
  "id" : "index_compare",
  "source" : { 
    "index" : [
      "index1",
      "index2"
    ],
    "query" : {
      "match_all" : { }
    }
  },
  "dest" : { 
    "index" : "compare"
  },
  "pivot" : {
    "group_by" : {
      "unique-id" : {
        "terms" : {
          "field" : "<unique-id-field>" 
        }
      }
    },
    "aggregations" : {
      "compare" : { 
        "scripted_metric" : {
          "map_script" : "state.doc = new HashMap(params['_source'])", 
          "combine_script" : "return state", 
          "reduce_script" : """ 
            if (states.size() != 2) {
              return "count_mismatch"
            }
            if (states.get(0).equals(states.get(1))) {
              return "match"
            } else {
              return "mismatch"
            }
            """
        }
      }
    }
  }
}
```

&emsp;&emsp;第4行，在`source`对象中引用的索引，它们将会进行相互比较
&emsp;&emsp;第13行，`dest`索引中包含了比较的结果
&emsp;&emsp;第20行，`group_by`字段必须是文档中的唯一键
&emsp;&emsp;第25行，`scripted_metric` aggregation对象
&emsp;&emsp;第27行，`map_script`在`state`对象中定义了一个`doc`，通过`new HashMap(...)`拷贝了原文档，这个操作对于将完整的source object从一个阶段传递给下一个阶段是非常重要的
&emsp;&emsp;第28行，`combine_script`从每一个分片中返回`state`
&emsp;&emsp;第29行，`reduce_script`检查索引的大小是否一样，如果不一样，那么返回`count_mismatch`。然后遍历两个索引的所有值，如果值相等，返回`match`，否则返回`mismatch`

##### Getting web session details by using scripted metric aggregation

&emsp;&emsp;这个例子展示的是如何从一个transform中衍生（derive）出多个功能。让我们先看下原数据：

```text
{
  "_index":"apache-sessions",
  "_type":"_doc",
  "_id":"KvzSeGoB4bgw0KGbE3wP",
  "_score":1.0,
  "_source":{
    "@timestamp":1484053499256,
    "apache":{
      "access":{
        "sessionid":"571604f2b2b0c7b346dc685eeb0e2306774a63c2",
        "url":"http://www.leroymerlin.fr/v3/search/search.do?keyword=Carrelage%20salle%20de%20bain",
        "path":"/v3/search/search.do",
        "query":"keyword=Carrelage%20salle%20de%20bain",
        "referrer":"http://www.leroymerlin.fr/v3/p/produits/carrelage-parquet-sol-souple/carrelage-sol-et-mur/decor-listel-et-accessoires-carrelage-mural-l1308217717?resultOffset=0&resultLimit=51&resultListShape=MOSAIC&priceStyle=SALEUNIT_PRICE",
        "user_agent":{
          "original":"Mobile Safari 10.0 Mac OS X (iPad) Apple Inc.",
          "os_name":"Mac OS X (iPad)"
        },
        "remote_ip":"0337b1fa-5ed4-af81-9ef4-0ec53be0f45d",
        "geoip":{
          "country_iso_code":"FR",
          "location":{
            "lat":48.86,
            "lon":2.35
          }
        },
        "response_code":200,
        "method":"GET"
      }
    }
  }
}
```

&emsp;&emsp;通过使用`sessionid`字段对数据进行分组，你可以通过session列举事件并且使用scripted metric aggregation获得session的更多详细内容。

```text
POST _transform/_preview
{
  "source": {
    "index": "apache-sessions"
  },
  "pivot": {
    "group_by": {
      "sessionid": { 
        "terms": {
          "field": "apache.access.sessionid"
        }
      }
    },
    "aggregations": { 
      "distinct_paths": {
        "cardinality": {
          "field": "apache.access.path"
        }
      },
      "num_pages_viewed": {
        "value_count": {
          "field": "apache.access.url"
        }
      },
      "session_details": {
        "scripted_metric": {
          "init_script": "state.docs = []", 
          "map_script": """ 
            Map span = [
              '@timestamp':doc['@timestamp'].value,
              'url':doc['apache.access.url'].value,
              'referrer':doc['apache.access.referrer'].value
            ];
            state.docs.add(span)
          """,
          "combine_script": "return state.docs;", 
          "reduce_script": """ 
            def all_docs = [];
            for (s in states) {
              for (span in s) {
                all_docs.add(span);
              }
            }
            all_docs.sort((HashMap o1, HashMap o2)->o1['@timestamp'].toEpochMilli()compareTo(o2['@timestamp']-toEpochMilli()));
            def size = all_docs.size();
            def min_time = all_docs[0]['@timestamp'];
            def max_time = all_docs[size-1]['@timestamp'];
            def duration = max_time.toEpochMilli() - min_time.toEpochMilli();
            def entry_page = all_docs[0]['url'];
            def exit_path = all_docs[size-1]['url'];
            def first_referrer = all_docs[0]['referrer'];
            def ret = new HashMap();
            ret['first_time'] = min_time;
            ret['last_time'] = max_time;
            ret['duration'] = duration;
            ret['entry_page'] = entry_page;
            ret['exit_path'] = exit_path;
            ret['first_referrer'] = first_referrer;
            return ret;
          """
        }
      }
    }
  }
}
```

&emsp;&emsp;第8行，使用`sessionid`对数据进行分组
&emsp;&emsp;第14行，aggregation统计了每一个session中`apache.access.path`的种类以及`apache.access.url`的数量。
&emsp;&emsp;第27行，`init_script`在`state`对象中创建了数组类型的`doc`
&emsp;&emsp;第28行，`map_script`使用文档中的timestamp定义了一个`span`数组，以及根据文档中对应的字段定义了`referrer`、`url`。然后将`span`数组的值添加到`doc`对象中。
&emsp;&emsp;第36行，`combine_script`从每一个分片中返回`state.docs`
&emsp;&emsp;第37行，`reduce_script`定义的多个对象例如`min_time`，`max_time`，`duration`都是基于文档中的字段。然后声明一个`ret` 对象，使用`new HashMap ()`将source document拷贝到这个对象中。接着脚本在`ret`中定义了`first_time`，`last_time`，`duration`，这些字段的值基于脚本中的之前的一些值，最终返回`ret`

&emsp;&emsp;最终返回的结果大致如下：

```text
{
  "num_pages_viewed" : 2.0,
  "session_details" : {
    "duration" : 100300001,
    "first_referrer" : "https://www.bing.com/",
    "entry_page" : "http://www.leroymerlin.fr/v3/p/produits/materiaux-menuiserie/porte-coulissante-porte-interieure-escalier-et-rambarde/barriere-de-securite-l1308218463",
    "first_time" : "2017-01-10T21:22:52.982Z",
    "last_time" : "2017-01-10T21:25:04.356Z",
    "exit_path" : "http://www.leroymerlin.fr/v3/p/produits/materiaux-menuiserie/porte-coulissante-porte-interieure-escalier-et-rambarde/barriere-de-securite-l1308218463?__result-wrapper?pageTemplate=Famille%2FMat%C3%A9riaux+et+menuiserie&resultOffset=0&resultLimit=50&resultListShape=PLAIN&nomenclatureId=17942&priceStyle=SALEUNIT_PRICE&fcr=1&*4294718806=4294718806&*14072=14072&*4294718593=4294718593&*17942=17942"
  },
  "distinct_paths" : 1.0,
  "sessionid" : "000046f8154a80fd89849369c984b8cc9d795814"
},
{
  "num_pages_viewed" : 10.0,
  "session_details" : {
    "duration" : 343100405,
    "first_referrer" : "https://www.google.fr/",
    "entry_page" : "http://www.leroymerlin.fr/",
    "first_time" : "2017-01-10T16:57:39.937Z",
    "last_time" : "2017-01-10T17:03:23.049Z",
    "exit_path" : "http://www.leroymerlin.fr/v3/p/produits/porte-de-douche-coulissante-adena-e168578"
  },
  "distinct_paths" : 8.0,
  "sessionid" : "000087e825da1d87a332b8f15fa76116c7467da6"
}
```

#### Troubleshooting transforms
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/transform-troubleshooting.html)

&emsp;&emsp;使用这个章节中的信息来解决一些常用的问题。

&emsp;&emsp;For issues that you cannot fix yourself … we’re here to help。如果你是有support contract的现有用户，可以在[Elastic Support portal](https://support.elastic.co/customers/s/login/)中创建一个ticket，或者在[Elastic forum](https://discuss.elastic.co/)发帖。

&emsp;&emsp;如果你遇到了transform的问题，你可以从下面的文件和API中获取更多的信息。

- 在`.transform-notifications-read`存储了轻量级的审计信息，基于`transform_id`进行查询。
- [get transform statistics API](#Get transform statistics API)提供了transform状态和失败的信息
- 如果transform作为一个任务存在，你可以使用[task management API](#Task management API)来收集任务信息（task information）。例如`GET _tasks?actions=data_frame/transforms*&detailed`。通常来说，在transform启动后或者处于失败状态时会存在这个任务。
- 正在运行transform的节点上的Elasticsearch日志可能也含有有用的信息。你可以从通知消息中确认是哪个节点。或者如果task存在，你可以通过`get transform statistics API`获取信息。更多信息见[Logging](#Logging)。

#### Transform limitations
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/transform-limitations.html)

&emsp;&emsp;下文中的局限性和已知问题适用于8.2.3版本的Elastic transform功能。局限性按下面几种进行分类：

- [Configuration limitations](#Configuration limitations)适用于transform的配置过程
- [Operational limitations](#Operational limitations)适用于影响运行中的transform的行为
- [Limitations in Kibana](#Limitations in Kibana)适用于通过用户接口（user interface）配置的transform

##### Configuration limitations

###### Field names prefixed with underscores are omitted from latest transforms

&emsp;&emsp;如果你使用了`latest`类型的transform并且source index中有已下划线开头的字段名，它们会被认为是内部字段（internal fileds）。在destination index中这些字段会被omit。

###### Transforms support cross-cluster search if the remote cluster is configured properly

&emsp;&emsp;如果你使用[cross-cluster search](#Search across clusters)，远程的集群（remote cluster）必须支持你transform中的查询和聚合。Transform会验证它们的配置。如果你使用了cross-cluster search并且验证失败，确保远程的集群支持你使用的查询和聚合。

###### Using scripts in transforms

 只要是聚合中支持的脚本，那么在任意case下，在transform中使用也是支持的。然而在transform中使用脚本需要考虑下面的一些因素（factor）：

- 当字段是由脚本生成的时，Transform不能推测其index mapping。在这种情况下，你可以在创建 transform前先在destination index中创建好索引
- Scripted fields可能会增加transform的运行时间
- 当你使用脚本中定义的`group_by`进行分组时，transform无法为此query进行优化，如果在脚本中使用了，会收到警告消息（warning message）

###### Deprecation warnings for Painless scripts in transforms

&emsp;&emsp;如果transform使用了Painless脚本，并且使用弃用的语法（deprecated syntax），transform的预览或启动时不会显示warning。然而 由于运行中的查询可能是资源密集型过程，所以不可能在转换期间为了弃用警告进行检查。因此任何因为使用了弃用的语法而引起的弃用警告都不会在Upgrade Assistant中出现。

###### Transforms perform better on indexed fields

&emsp;&emsp;transform使用用户定义的时间字段（time filed）进行排序，这个字段会被经常访问。如果时间字段是一个[runtime filed](#Runtime fields)，在查询期间对于计算字段值的性能影响会显著的降低transform。在使用transform时使用一个indexed field作为时间字段。

###### Continuous transform scheduling limitations

&emsp;&emsp;continuous transform会周期的检查source data是否发生变化。这个定时功能目前局限于一个basic periodic timer，`frequency`可以是1s到1h范围的值。默认值是1m。它被设计为run litter and often。当基于你的提取率（ingest rate）为计时器选择了一个`frequency`时，同时也要考虑在集群中使用transform search/index操作对其他用户的影响。Also note that retries occur at `frequency ` interval。

##### Operational limitations

###### Aggregation responses may be incompatible with destination index mappings

&emsp;&emsp;当pivot transform首先启动后，它会为destination index推测出mapping。这种处理基于source index中的字段的类型以及使用的aggregation。如果字段是从[scripted_metrics](#Scripted metric aggregation)或者[bucket_scripts](#Bucket script aggregation)中衍生（derive）出来的，会使用[dynamic mappings](#Dynamic mapping)。在有些情况下（in some instances），推测出来的mapping（deduced mapping）可能跟真正的数据不兼容。比如说可能会发生numeric overflow或者动态映射出的字段同时包含numbers和strings。如果你觉得发生了上述的问题，可以检查Elasticsearch的日志。

&emsp;&emsp;你可以使用[preview transform API预览推测出的mapping。见API response中`generated_dest_index`对象。

&emsp;&emsp;如果有必要的话，你可以在transform启动前，使用[create index API](#Create index API)先创建一个自定义的destination index使得可以自定义mapping。由于推测的mapping不能被index template overwrite，所以就使用create index API来自定义mapping。index template只能应用于（apply to）脚本中使用dynamic mapping衍生出的字段。

###### Batch transforms may not account for changed documents

&emsp;&emsp;Batch transform使用了[composite aggregation](#Composite aggregation)使得所有的分桶可以有高效的分页（efficient pagination）。composite aggregation不支持search context，因此如果source data发生了变更（删除，更新，添加）并且同时Batch data frame正在处理中，那么结果中可能不会包括这些变更。

###### Continuous transform consistency does not account for deleted or updated documents

&emsp;&emsp;尽管在提取（ingest）到new data后，transform中的处理允许重新转化（transform）计算，但它同样有一些局限。

&emsp;&emsp;发生变更的entities只有在它们的时间字段同时发生变化并且处于更改检查的操作范围内时才能被识别到（identified）。这是因为从设计原则上来讲，是根据new data提供的时间戳进行提取的。

&emsp;&emsp;如果满足source index pattern的索引被删除了，例如当删除了历史的基于时间的索引（historical time-based），那么composite aggregation在持续的checkpoint处理中会搜索到不同的source data，entities只存在于被删除的索引中，并且不会从destination index中移除。

&emsp;&emsp;取决于你的使用用例，你可能希望在有删除（deletion）后重新创建transform。或者你的使用用例能容忍historical archiving，你可能希望在你的聚合中包含一个max ingest timestamp。This will allow you to exclude results that have not been recently updated when viewing the destination index。

###### Deleting a transform does not delete the destination index or Kibana index pattern

&emsp;&emsp;当使用`DELETE _transform/index`删除一个transform时，destination index和Kibana index pattern都不会删除，这些都需要各自进行删除。

###### Handling dynamic adjustment of aggregation page size

&emsp;&emsp;在开发transform时，更倾向于control而不是性能。在设计方面的考虑中，更愿意让transform在后台花费更长的时间进行处理而不是快速完成并且优先资源消费（take precedence in resource consumption）。

&emsp;&emsp;Composite aggregations are well suited for high cardinality data enabling pagination through results。当composite aggregation查询时发生了[circuit breaker](#Circuit breaker settings)的内存异常时，我们会降低请求中分桶的数量并重试。这个circuit breaker是基于集群中的所有活动（activity）计算的，而不仅仅是来自transform的活动，因此它可能只是一个临时的资源可用性问题（resource availability issue）。

&emsp;&emsp;For a batch transform, the number of buckets requested is only ever adjusted downwards. The lowering of value may result in a longer duration for the transform checkpoint to complete. For continuous transforms, the number of buckets requested is reset back to its default at the start of every checkpoint and it is possible for circuit breaker exceptions to occur repeatedly in the Elasticsearch logs.

&emsp;&emsp;The transform retrieves data in batches which means it calculates several buckets at once. Per default this is 500 buckets per search/index operation. The default can be changed using max_page_search_size and the minimum value is 10. If failures still occur once the number of buckets requested has been reduced to its minimum, then the transform will be set to a failed state.

###### Handling dynamic adjustments for many terms

&emsp;&emsp;对于每个checkpoint，会检查entities自上一次检查后发生的变化。发生变更的entities作为一个[terms query](#Terms query)提供给transform的composite aggregation，one page at a time。然后更新被应用到destination index中。

&emsp;&emsp;`max_page_search_size`定义了页的`size`，它同样用于定义composite aggregation查询返回的分桶数量。默认值是500，最小值是10.

&emsp;&emsp;index setting中的`index.max_terms_count`定义了在一个terms query中term的最大数量。默认值是65536。如果`max_page_search_size`超过了`index.max_terms_count`，transform将失败。

&emsp;&emsp;使用更小的`max_page_search_size`的值会导致transform的checkpoint完成的时间花费更长。

###### Handling of failed transforms

&emsp;&emsp;失败的transform仍然会作为一个persistent task并且应该要做适当的处理。要么删除它或者在解决导致失败的原因后重启。

&emsp;&emsp;当使用API删除一个失败的transform时，先使用`_stop?force=true`停止transform，然后再删除它。

###### Continuous transforms may give incorrect results if documents are not yet available to search

&emsp;&emsp;一篇文档在被索引后，在它对搜索可见前有一段很小的延迟。

&emsp;&emsp;一个continuous transform 会周期性的检查entities是否发生变化，其检查的时间范围是上一次检查的时间到 （`now` 减去 `sync.time.delay`）。时间窗口的移动不会有重叠的。如果一个最近索引的文档的时间戳在检查时间窗口内，但是这个文档对搜索不可见，那么这个entity不会被更新。

&emsp;&emsp;如果使用了一个`sync.time.field` 字段来表示数据的提取时间（data ingest time）并且使用了一个 zero second或者一个非常小的`sync.time.delay`。那么更有可能会发生这个问题。

###### Support for date nanoseconds data type

&emsp;&emsp;如果你的数据使用了[date nanosecond data type](#Date nanoseconds field type)，aggregation仍然是millionsecond的粒度。这个局限同样会影响transform中的聚合。

###### Data streams as destination indices are not supported

&emsp;&emsp;transform会在destination index中更新数据，要求能够往destination index写入。[Data Streams](#Data Streams)被设计为append-only，意味着你不能直接往data stream中发送更新或者删除请求。因为这个原因，data streams不支持作为transform的destination index。

###### ILM as destination index may cause duplicated documents

&emsp;&emsp;不建议[ILM](#ILM: Manage the index lifecycle)用于作为transform 的destination index。Transform在当前destination index中更新文档，不能删除ILM之前使用过的索引中的文档。在rollover中，使用transform结合ILM可能会导致重复的文档。

&emsp;&emsp;如果你使用ILM来获得time-based索引，请考虑使用[Date index name](#Date index name processor)。如果你的transform包含基于`date_hisotram`的`group_by`，这个处理工作不会有重复的文档。

##### Limitations in Kibana

###### Transforms are visible in all Kibana spaces

&emsp;&emsp;[Spaces](https://www.elastic.co/guide/en/kibana/8.2/xpack-spaces.html)允许你组织你的source、destination index 以及Kibana中其他保存的对象（saved object）并且只能看到属于你的空间的objects。然而transform是一个长期运行的任务并且是在集群层进行管理的，因此不会将其可见范围限制到某些空间（certain spaces）。Space awareness can be implemented for a data view under **Stack Management > Kibana** which allows privileges to the transform destination index。

###### Up to 1,000 transforms are listed in Kibana

&emsp;&emsp;transform的管理页面最多只能显示1000个transform。

###### Kibana might not support every transform configuration option

&emsp;&emsp;可能有些配置选项在transform API中可见但是在Kibana中不支持。参考[documentation](#Transform APIs)浏览所有的可配置的选项。


## Set up a cluster for high availability
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/high-availability.html)

&emsp;&emsp;你的数据对你来说很重要。让数据保持安全以及可见性对Elasticsearch来说很重要。有时候你的集群可能会经历硬件错误或者断电。为了帮助你对付这类事件，Elasticsearch提供了一些功能实现高可用。

- 通过适当的规划，可以对一个集群[design for resilience](#Designing for resilience)以应对许多通常出错的事情，从单个节点或网络连接的丢失到区域范围的中断（outage），比如断电。
- 你可以使用[cross-cluster replication](#Cross-cluster replication)让远程的follower cluster拥有副本数据（replicate data）。follower cluster可能是一个不同的数据中心或者相对于leader cluster在不同的陆地上（continent）。follower cluster一方面扮演热备（hot standby）的角色，时刻为leader cluster发生灾难事件时做好故障转移（fail over），另一个方面也扮演了geo-replica来为附近的客户服务。
- 对付数据丢失的最后防线就是对你的集群[take regular snapshots](#Create a snapshot
) ，这样你可以在任何地方有需要时恢复完整的数据。

### Designing for resilience
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/high-availability-cluster-design.html)

&emsp;&emsp;分布式系统比如Elasticsearch都会设计为即使它们中的一些组件失效了也能工作。只要有足够多连接正常的节点能担负起它们的任务职责，Elasticsearch集群仍能正常的操作，即使一些节点不可用或者丢失连接。

&emsp;&emsp;再小的具有弹性的集群也要有以下的限制。所有的Elasticsearch集群要求：

- 一个[elected master node](#Quorum-based decision making)节点
- 每个[role](#Node)至少有一个节点
- 每一个[shard](#Scalability and resilience: clusters, nodes, and shards)至少要有一个副本

&emsp;&emsp;弹性集群需要为每个必需的集群组件提供冗余。这意味着弹性集群要求：

- 至少有三个具有master资格的节点
- 每一个角色至少有两个节点
- 每一个分片至少有2个拷贝（一个主分片和一个或多个副本，除非索引是[searchable snapshot index](#Searchable snapshots)）

&emsp;&emsp;一个有弹性的集群需要三个具有master节点的资格，这样如果其中一个节点失败，那么剩下的两个仍然形成多数（form a majority），并且可以成功举行选举。

&emsp;&emsp;同样的，每一个节点角色的冗余意味着如果某个有特定角色的节点发生故障后，其他的节点能承担起对应的职责。

&emsp;&emsp;最终，一个弹性的集群中至少包每一个分片有两个拷贝。如果一个拷贝发生故障，另一个拷贝能接管。在发生故障后，Elasticsearch会自动的在剩余的节点上重建分片拷贝来恢复集群的健康。

&emsp;&emsp;故障会暂时减少集群的总容量。 此外，在发生故障后，集群必须执行额外的后台活动才能将自身恢复到健康状态。 即使某些节点发生故障，你也应该确保您的集群有能力处理您的工作负载。

&emsp;&emsp;基于你的需求以及预算，一个Elasticsearch集群可以由一个或者上百个或者这个范围内的节点组成。当你设计一个较小的集群时，你通过应该集中考虑单节点故障的问题。对于较大集群的设计，需要考虑同一个时间多个节点发生故障的问题。下面的链接介绍了在不同规模的集群上构建弹性（build resilience）的一些建议：

- [Resilience in small clusters](#Resilience in small clusters)
- [Resilience in larger clusters](#Resilience in larger clusters)

#### Resilience in small clusters
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/high-availability-cluster-small-clusters.html)

&emsp;&emsp;在较小的集群上，最重要的是单节点故障对应的弹性。这个章节给出了一些指导：对于单节点故障如何尽可能保证集群的弹性。

##### One-node clusters

&emsp;&emsp;如果你的集群由一个节点组成，这个节点必须做所有的事情。对这种场景的推荐是，Elasticsearch默认将所有的角色给这个节点。

&emsp;&emsp;单个节点的集群是不具备弹性的。如果这个节点发生了故障，集群将停止工作。因为在这个单节点的集群中没有replica，你无法存储冗余数据。然而默认情况下`green` [cluster health](#Cluster health API)状态至少需要一个replica。为了你的集群能显示一个`green`状态，可以对每一个索引通过[index.number_of_replicas](#index.number_of_replicas)设置为`0`。

&emsp;&emsp;如果节点发生故障，你可能需要从[snapshot](#Snapshot module-1)中恢复会丢失索引数据的较老的拷贝。

&emsp;&emsp;由于对于任何的故障不具备弹性，我们不建议在生产中使用单节点的集群。

##### Two-node clusters

&emsp;&emsp;如果你有两个节点，我们建议这两个节点都是数据节点。你应该保证在两个节点上都保留着冗余数据，通过在每一个索引上设置[index.number_of_replicas](#index.number_of_replicas)为`1`，这些索引不是一个[ searchable snapshot index](#Searchable snapshots)，以上都是默认的行为但是可以通过[index template](#Index templates
)覆盖。[Auto-expand relicas](#index.auto_expand_replicas)能达到同样的目的，但是不建议在这种规模的集群上使用这个功能。

&emsp;&emsp;我们建议在这两个节点的其中一个节点上设置`node.master: false`使得这个节点不是[master-eligible](# Master-eligible node)。这样你可以知道哪个节点会被选为集群的master节点。这个集群容忍不是master-eligible的节点的丢失。如果你不在其中一个节点上设置`node.master:false`，两个节点都是master-eligible。这意味着这两个节点要求master的选举。由于如果任一节点不可用，选举将失败，因此你的集群无法可靠地容忍任一节点的丢失。

&emsp;&emsp;默认情况下，每一个节点会有一个角色。我们建议你除了master-eligible外给这两个节点赋予所有的角色。如果一个节点发生了故障，另一个节点可以接管任务。

&emsp;&emsp;你应该避免把客户请求只发送给其中一个节点。如果你这么做了并且这个节点发生了故障，这个请求将不会收到响应即使剩余的节点本身是一个健康的集群。理想情况下你应该在这两个节点间做好请求的平衡。一个好方法是在客户端配置所有节点的地址，这样你可以弹性的负载均衡的请求集群中的节点。

&emsp;&emsp;由于对于任何的故障不具备弹性，我们不建议在生产中使用只有两个节点的集群。

##### Two-node clusters with a tiebreaker

&emsp;&emsp;因为master节点的选举基于多数（majority-based）。上文中的两个节点的集群容忍某一个节点可以发生故障而不是另一个。你不可能配置一个两个节点的集群，并且可以容忍任意一个节点可以发生故障，因为理论上是不可能。你可能期望任意一个节点发生故障后，Elasticsearch可以选出剩余的节点作为master，因为没法区分出远程节点发生故障还是仅仅发生了连接问题。如果两个节点都可以进行各自的选举，连接的丢失会导致[split-brain](https://en.wikipedia.org/wiki/Split-brain_(computing))问题从而丢失数据。Elasticsearch会避免这种情况并且通过不让任何节点作为master节点来保护你的数据，直到节点保证拥有最新的集群状态并且集群中没有其他的master节点。这种方式会导致集群中没有master节点直到恢复连接。

&emsp;&emsp;你可以通过添加第三个节点并且让这个三个节点都是master-eligible来解决这个问题。一次[master election](#Quorum-based decision making)只要求三个master-eligible中的两个节点。这意味着集群可以容忍任意单个节点的丢失。原先的两个节点相互之间丢失连接后，这个第三个节点扮演了一个tiebreaker。你可以将这个额外的节点作为一个 [dedicated voting-only master-eligible node](#Voting-only master-eligible node)来降低这个节点的资源配置，即dedicated tiebreaker。因为这个节点没有其他的角色，一个dedicated tiebreaker 节点不需要像其他两个节点一样的powerful。这个节点不会用于执行任何搜索或者客户端的请求，并且不能选举为集群的master节点。

&emsp;&emsp;原先的两个节点不能是[voting-only master-eligible nodes](#Voting-only master-eligible node)因为一个弹性的集群至少需要3个master-eligible 节点，至少有两个节点不能是 voting-only master-eligible nodes。如果三个节点中的两个是 voting-only master-eligible nodes那么选出的master节点必须是第三个节点，但这个节点会成为单点故障（single point of failure）。

&emsp;&emsp;我们建议给两个non-tiebreaker的节点赋予所有的角色。通过创建冗余信息保证集群中的任何任务都有节点接管。

&emsp;&emsp;你应该避免把客户请求发送到dedicated tiebreaker节点。你也应该避免把客户请求只发送给其他两个节点中的一个节点。如果你这么做了并且这个节点发生了故障，这个请求将不会收到响应，及时剩余的节点本身是一个健康的集群。理想情况下你应该在这两个节点间做好请求的平衡。一个好方法是在客户端配置所有节点的地址，这样你可以弹性的负载均衡的请求集群中的节点。[Elastic Cloud](https://www.elastic.co/cloud/elasticsearch-service/signup?baymax=docs-body&elektra=docs) 服务提供这种负载均衡。

&emsp;&emsp;两个节点加上一个额外的tiebreaker节点的集群是可以在生产中使用的最小的集群。

##### Three-node clusters

&emsp;&emsp;如果你有三个节点，我们建议这三个节点都是[data nodes](#Data node)，不是[ searchable snapshot index](#Searchable snapshots)的索引都应该至少有一个replica。默认情况下节点是data node。你可以让一些索引拥有两个replica，这样每一个节点都有一个分片的拷贝。你应该将每一个节点配置为[master-eligible](#Master-eligible node)，这样任意两个节点都可以选择出一个master并且不需要第三个节点的参与。默认情况下所有的节点都是master-eligible。在丢失单个节点的情况这个集群仍然是弹性的。

&emsp;&emsp;你应该避免把客户请求只发送给其中一个节点。如果你这么做了并且这个节点发生了故障，这个请求将不会收到响应即使剩余的节点本身是一个健康的集群。理想情况下你应该在这两个节点间做好请求的平衡。一个好方法是在客户端配置所有节点的地址，这样你可以弹性的负载均衡的请求集群中的节点。[Elastic Cloud](https://www.elastic.co/cloud/elasticsearch-service/signup?baymax=docs-body&elektra=docs) 服务提供这种负载均衡。

##### Clusters with more than three nodes

&emsp;&emsp;一旦你的集群增长超过3个节点，你可以指定节点给予特定的责任，根据需要独立的衡量节点需要的资源。你可以有许多例如 [data nodes](#Data node), [ingest nodes](#Ingest pipelines), [machine learning nodes](#Machine learning node)等等的节点。根据你的需求来支持你的工作量。当你的集群增长到更大后，我们建议你为每一个角色指定专用的节点。这样可以为每一个任务来独立的衡量对应的资源。

&emsp;&emsp;然而，一个好的实践就是限制master-eligible的节点数量到三个。Master节点不需要像其他类型的节点一样去衡量，因为总是从它们三个中选取一个作为master节点。如果有太多的master-eligible节点，那么完成master的选举需要较长的时间才能完成。在大型的集群中，我们建议你配置一些节点为dedicated master-eligible节点，并且避免往这些专用的节点中发送请求。如果master-eligible节点因为额外的工作可能会导致你的集群可能变得不稳定，这些额外的工作可以交给其他节点处理。

&emsp;&emsp;你可以将master-eligible节点中的一个节点为[voting-only node](#Voting-only master-eligible node)，这样它永远不会被选为master节点。比如说你有两个专用的master 节点以及一个第三个节点，这个节点同时是data note以及voting-only master-eligible节点。第三个节点将会用于在master选举中扮演tiebreaker并且自身不会成为master。

##### Summary

&emsp;&emsp;只要满足下面的条件，集群在丢失任何节点后依然是弹性的：

- [cluster health status](#Cluster health API)是`green`
- 至少有两个数据节点
- 每一个不是[searchable snapshot index](#Searchable snapshots)的索引至少每一个分片都有一个replica，in addition to the primary
- 集群中至少有三个master-eligible节点，只要这些节点中至少有两个不是voting-only master-eligible nodes
- 客户端配置为将其请求发送到多个节点，或者配置为使用负载均衡器在一组适当的节点之间平衡请求，[Elastic Cloud](https://www.elastic.co/cloud/elasticsearch-service/signup?baymax=docs-body&elektra=docs) 服务提供这种负载均衡

#### Resilience in larger clusters
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/high-availability-cluster-design-large-clusters.html)

&emsp;&emsp;节点间共享一些基础设施（infrastructure），比如网络互联（network interconnect）、电源供应（power supply）并不罕见。如果是这样的话，你应该计划好为这些基础设施发生故障做好准备并且保证发生故障后不会影响太多的节点。常见的实践是将共享一些基础设施的节点分组为不同的区域（zone）并且为整个zone发生故障做好计划。

&emsp;&emsp;Elasticsearch期望节点之间的连接是可靠的、低延迟以及有足够的带宽。许多Elasticsearch任务要求在节点间多次往返（multiple round-trips）。缓慢或者不可靠的互联（interconnect）对集群的性能和稳定有着很明显的影响。

&emsp;&emsp;比如说，每次往返增加几毫秒的延迟会迅速累积成明显的性能损失（performance penalty）。一个不可靠的网络可能会有频繁的网络分区（network partition）。Elasticsearch 将尽快从网络分区自动恢复，但你的集群可能在分区期间部分不可用，并且需要花费时间和资源来重新同步任何丢失的数据并在分区恢复后重新平衡自身。从故障中恢复可能会设计节点之间拷贝大量的数据，所以恢复的时间往往取决于可用的带宽。

&emsp;&emsp;如果将你的集群按照区域（zone）进行划分，区域内（within）的节点网络通常比区域间（between）的更高。保证区域间的网络连接质量足够的高。将所有的区域放在一个数据中心（data center）并且每一个区域有自己独立的电源供应和其他基础设置是最好的方式。你也可以把集群扩张（stretch）到附近的数据中心，只要数据中心间的网络互联足够好。

&emsp;&emsp;没法指定一个健康运行的Elasticsearch所需要的最低网络性能。理论上，即使节点间的往返延迟有上百毫秒的延迟，集群也能正常工作。在实践中，缓慢的网络会导致很差的集群性能。另外缓慢的网络通常是不可靠的并且会发生网络分区导致间断性的不可用。

&emsp;&emsp;如果多个数据中心间相隔很远（further apart）或者连接不好并且还要希望数据可见，可以在每一个数据中心内部署一个额外的集群，使用 [cross-cluster search](#Search across clusters)或者 [cross-cluster replication](#Cross-cluster replication)来连接集群。这些功能使得，相较于集群内，即使集群间有更低的可靠性以及性能也能较好的运行。

&emsp;&emsp;在丢失了一个区域内所有的节点后，一个设计合理的集群可能会正常运行（functional），只是大大的降低了capacity（significantly reduced capacity）。当出现这个故障后你需要提供（provision）额外的节点恢复到可接受的性能。

&emsp;&emsp;对于整个区域发生后故障的弹性问题，在一个或多个区域中有每一个分片的拷贝是很重要，可以通过将[data node](#Data node)放置在多个区域中并且配置[shard allocation awareness](#Shard allocation awareness) 实现弹性。你也应该保证客户端的请求会发送给一个或多个区域。

&emsp;&emsp;你需要考虑到所有的节点角色并且保证每一个角色冗余的分配到两个或多个区域中。例如，如果你使用[ingest pipelines](#Ingest pipelines)或者machine learning，你应该在两个或者多个区域中有ingest node或者machine learning node。然而master-eligible node的位置更需要关注因为一个弹性的集群需要三个master-eligible node中的两个才能工作正常。下面的章节展开介绍了多个区域中如何放置master-eligible node。

##### Two-zone clusters

&emsp;&emsp;如果你有两个区域，在每一个区域中应该有不同数量的master-eligible node，这样拥有更多节点的区域中将包含大多数（majority）并且在丢失其他区域后也能够survive。比如说如果你有三个master-eligible node，那么可以将所有的master-eligible node 放在一个区域中或者你把其中两个master-eligible node放到一个区域，第三个放到另一个区域中。你不能在每一个区域中放置相同数量的master-eligible node。每一个区域没有拥有它自己区域内的大多数。因此，在丢失任意一方的区域后，另一方的区域可能没法survive。

##### Two-zone clusters with a tiebreaker

&emsp;&emsp;因为master节点的选举基于多数（majority-based）。上文中的two-zones可以容忍某个某一个节点可以发生故障而不是另一个。你不可能配置一个two-zone cluster，并且可以容忍任意一个区域可以发生故障，因为理论上是不可能。你可能期望任意一个区域发生故障后，Elasticsearch可以从剩余的区域中选出一个节点作为master，因为没法区分出远程区域（remote zone）发生故障还是仅仅发生了连接问题。如果两个区域都可以进行各自的选举，连接的丢失会导致[split-brain](https://en.wikipedia.org/wiki/Split-brain_(computing))问题从而丢失数据。Elasticsearch会避免这种情况并且通过不让任意区域中的节点作为master节点来保护你的数据，直到节点保证拥有最新的集群状态并且集群中没有其他的master节点。这种方式会导致集群中没有master节点直到恢复连接。

&emsp;&emsp;你可以在每一个区域中添加一个master-eligible node并且在一个独立的第三个区域中额外单独添加一个master-eligible node。原先的两个区域相互之间丢失连接后，这个第三个节点扮演了一个tiebreaker。你可以将这个额外的节点作为一个 [dedicated voting-only master-eligible node](#Voting-only master-eligible node)，即dedicated tiebreaker。因为这个节点没有其他的角色，一个dedicated tiebreaker 节点不需要像其他两个节点一样的powerful。这个节点不会用于执行任何搜索或者客户端的请求，并且不能选举为集群的master节点。

&emsp;&emsp;你应该使用[shard allocation awareness](#Shard allocation awareness) 来保证每个区域中的每一个分片都有一份拷贝。这意味着任意一个区域发生故障后所有的数据仍然是可见的。

&emsp;&emsp;所有的master-eligible node，包括voting-only node，相较于集群中的其他节点，它们需要相当快速的持久存储以及一个可靠的低延迟的网络连接，因为它们处于[publishing cluster state updates](#Publishing the cluster state)的关键路径（critical path）上。

##### Clusters with three or more zones

&emsp;&emsp;如果你有三个区域，那么你应该在每一个区域都放置一个 master-eligible node。如果你有超过三个区域，那么从中选出三个区域并且在这三个区域都分别放置一个 master-eligible node。这意味着其中一个区域发生故障后仍然可以选出一个master node。

&emsp;&emsp;你的索引应该至少有一个副本来防止节点发生故障，除非这些索引是[searchable snapshot indices](#Searchable snapshots)。你也应该使用[shard allocation awareness](#Shard allocation awareness) 来限制每一个区域中每一个分片的副本数量。比如说你配置了一个索引有一个或者两个副本，那么allocation awareness会保证主分片的副本在不同的区域中。这意味着即使一个区域发生了故障，分片的副本仍然是可见的，分片的可见性不会受到这种故障的影响。

##### Summary

&emsp;&emsp;只要满足下面的条件，集群在丢失任何区域后依然是弹性的：

- [cluster health status](#Cluster health API)是`green`
- 至少有两个区域有数据节点
- 每一个不是[searchable snapshot index](#Searchable snapshots)的索引至少每一个分片都有一个replica，in addition to the primary
- 配置Shard allocation awareness使得不让一个分片的所有副本都集中在一个区域中
- 集群中至少有三个master-eligible节点，只要这些节点中至少有两个不是voting-only master-eligible nodes，并且它们分在至少三个不同的区域中
- 客户端配置为将其请求发送到多个节点，或者配置为使用负载均衡器在一组适当的节点之间平衡请求，[Elastic Cloud](https://www.elastic.co/cloud/elasticsearch-service/signup?baymax=docs-body&elektra=docs) 服务提供这种负载均衡

### Cross-cluster replication
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/xpack-ccr.html)

&emsp;&emsp;通过cross-cluster replication，你可以跨集群复制索引使得：

- 数据中心断电后仍可以继续处理查询请求
- 防止搜索量（search volume）影响索引吞吐量
- 根据用户的地理位置（geo-proximity）来降低查询的延迟

&emsp;&emsp;Cross-cluster replication使用主动-被动模式（active-passive mode），往leader index写入数据，然后数据被复制到只读的follower index中。在集群中创建一个follower index之前，需要先配置一个包含leader index的远程集群（remote cluster）。

&emsp;&emsp;当leader index收到写入请求时，follower index会从远程的leader index中拉取（pull）变更。你可以手动创建follower index，也可以配置auto-follow 模式为时序索引（time series indices）自动创建follower index。

&emsp;&emsp;你可以配置双向（bi-directional）或者单向的（uni-directional）cross-cluster replication集群：

- 在uni-directional配置中，一个集群只包含leader index，其他集群只包含follower index
- 在bi-directional配置中，每一个集群同时包含leader index和follower index

&emsp;&emsp;在uni-directional中，包含follower index的集群中的Elasticsearch版本必须跟远程集群的Elasticsearch版本相同或者更新。如果版本更新，那么参考下面表格中的兼容性。

&emsp;&emsp;版本兼容性：


|                    | Local cluster |      |         |      |      |      |          |      |         |
| :----------------: | :-----------: | :--: | :-----: | :--: | :--: | :--: | :------: | :--: | ------- |
| **Remote cluster** |    5.0–5.5    | 5.6  | 6.0–6.6 | 6.7  | 6.8  | 7.0  | 7.1–7.16 | 7.17 | 8.0–8.2 |
|      5.0–5.5       |       √       |  √   |    ×    |  ×   |  ×   |  ×   |    ×     |  ×   | ×       |
|        5.6         |       √       |  √   |    √    |  √   |  √   |  ×   |    ×     |  ×   | ×       |
|      6.0–6.6       |       ×       |  √   |    √    |  √   |  √   |  ×   |    ×     |  ×   | ×       |
|        6.7         |       ×       |  √   |    √    |  √   |  √   |  √   |    ×     |  ×   | ×       |
|        6.8         |       ×       |  √   |    √    |  √   |  √   |  √   |    √     |  √   | ×       |
|        7.0         |       ×       |  ×   |    ×    |  √   |  √   |  √   |    √     |  √   | ×       |
|      7.1–7.16      |       ×       |  ×   |    ×    |  ×   |  √   |  √   |    √     |  √   | ×       |
|        7.17        |       ×       |  ×   |    ×    |  ×   |  √   |  √   |    √     |  √   | √       |
|      8.0–8.2       |       ×       |  ×   |    ×    |  ×   |  ×   |  ×   |    ×     |  √   | √       |


##### Multi-cluster architectures

&emsp;&emsp;在Elastic Stack中使用cross-cluster replication来构造多个多集群架构：

- [Disaster recovery](#Disaster recovery and high availability)说的是一旦主集群（primary cluster）发生了故障，辅助集群（secondary cluster）可以作为一个热备（hot backup）提供服务。
- [Data locality](#Data locality)用于在靠近应用程序服务器（和用户）的地方维护数据集的多个副本，并减少代价高昂的延迟
- [Centralized reporting](#Centralized reporting)用于最大限度地减少查询多个地理分布式（geo-distribution） Elasticsearch 集群时的网络流量（network traffic）和延迟，或防止辅助集群（secondary cluster）上的查询负载影响该集群上的索引写入性能。

&emsp;&emsp;观看[cross-cluster replication webinar](https://www.elastic.co/cn/webinars/replicate-elasticsearch-data-with-cross-cluster-replication-ccr)来了解更多关于下文中用例的内容，然后在你的电脑上[set up cross-cluster replication]来允许webinar中的demo。

###### Disaster recovery and high availability

&emsp;&emsp;Disaster recovery为你的任务关键型（mission-critical ）应用程序提供承受数据中心或区域中断的容错能力。这个用例是最常见的基于cross-cluster replication的部署方式。你可以通过不同的架构来提供集群的disaster recovery和高可用：

- Single disaster recovery datacenter
- Multiple disaster recovery datacenters
- Chained replication
- Bi-directional replication

1. Single disaster recovery datacenter

&emsp;&emsp;在这个配置中，生产中心（production datacenter）中的数据拷贝到灾难恢复中心（disaster recovery datacenter）或者说灾备中心。由于follower index复制了leader index，所以如果生产中心不可用时候可以使用灾难恢复中心的数据。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ccr-arch-disaster-recovery.png">

2. Multiple disaster recovery datacenters

&emsp;&emsp;你可以将一个数据中心的数据复制到多个数据中心。在复制到两个数据中心后，当主数据中心（primary datacenter）发生故障后，这种配置可以提供disaster recovery和高可用。

&emsp;&emsp;下图中，数据中心A的数据复制到了数据中心B和数据中心C，这两个数据中心拥有从数据中心A上的leader index复制的索引，并且是只读的。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ccr-arch-multiple-dcs.png">

3. Chained replication

&emsp;&emsp;你可以跨多个数据中心进行数据复制并形成一个复制链（replication chain）。下图中，数据中心A包含了leader index，数据中心B复制了数据中心A的数据，数据中心C复制了数据中心B上的follower index。这些数据中心间的连接形成了一个链式的复制模式（chained replication pattern）。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ccr-arch-chain-dcs.png">

4. Bi-directional replication

&emsp;&emsp;在[bi-directional replication](https://www.elastic.co/blog/bi-directional-replication-with-elasticsearch-cross-cluster-replication-ccr)中，所有集群有权（access）访问所有的数据，所有集群拥有用于写入的索引并且不用手动实现故障转移（failover）。应用程序可以写入到每一个数据中心的本地索引中并且可以跨多个索引（across multiple indices）来读取全局的所有的信息（a global view of all information）。

&emsp;&emsp;当一个集群或者数据中心不可用时，这种配置不需要手动的干预。在下面图中，当数据中心A发生不可用后，你可以继续使用数据中心B并且不需要手动的进行故障转移。当数据中心A重新上线后（come online），集群间就会恢复复制。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ccr-arch-bi-directional.png">

&emsp;&emsp;这种配置对于index-only，不会对文档进行更新的场景十分有用。在这个配置中，通过Elasticsearch索引的数据是不会变更的（immutable）。客户端只跟所在数据中心的集群通信，不要跟其他数据中心的集群有联系（Clients are located in each datacenter alongside the Elasticsearch cluster, and do not communicate with clusters in different datacenters）。

###### Data locality

&emsp;&emsp;把数据放在靠近应用程序或者用户的地方能降低延迟以及响应时间。这种方法在Elasticsearch中进行数据复制时同样适用。比如你可以将产品目录（product catalog）和reference dataset复制到全世界20个或者更多的数据中心中，使得最小化数据和应用服务之间的距离。

&emsp;&emsp;下图中，一个数据中心的数据复制到了三个额外的数据中心。每个数据中心位于各自的区域（region）。Central DC中包含了leader index，其他三个数据中心在各自的区域中包含follower index。这个配置将数据放在了靠近应用的地方。

###### Centralized reporting

&emsp;&emsp;当查询需要跨一个很大的网络的时候其效率很低下的，这时候使用一个中心化的报告集群（centralized reporting cluster）是非常有用的。在这个配置中，你将许多较小的集群中的数据拷贝到centralized reporting cluster。这个配置使你能够将数据从区域中心（regional hubs）复制到中央集群（central cluster），使得你可以在本地运行所有报告。

<img src="http://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ccr-arch-central-reporting.png">

##### Replication mechanics

&emsp;&emsp;尽管你可以在索引层（index level）来设置[cross-cluster replication](#Tutorial Set up cross-cluster replication)，而Elasticsearch是在分片级别（shard level）实现复制。当创建了一个follower index，这个索引中的每一个分片会从对应的leader index中的分片拉取变更。这意味着follower index跟leader index有相同的分片数量。在leader index上的所有操作都会被follower index复制，比如创建、更新、删除一篇文档的操作。这些请求会通过leader shard（primary或者replica）发送。

&emsp;&emsp;当一个follower shard发送一个读请求，leader shard会根据在创建follower index时候配置的read parameter来响应每一个新的操作。如果没有新的操作，leader shard会根据配置的超时时间等待新的操作的发生，等待超时后，leader shard会响应follower shard没有新的操作。follower shard在更新shard statistics后会马上再发一个读请求给leader shard。这种通信模型确保远程集群和本地集群之间的网络连接持续使用，避免被防火墙等外部源强行终止。

&emsp;&emsp;如果读请求失败了，则会检查（inspect）失败的原因。如果失败的原因认为是可以恢复的（比如说网络问题），那么 follower shard会进行循环重试。否则follower shard会暂停[until you resume it](#Pause and resume replication（manage）)。

###### Processing updates

&emsp;&emsp;你不能手动的更改follower index的mappings或者aliases。如果进行变更，需要去更改leader index。因为follower index是只读的，所有配置中对follower index的写操作都会被reject。

> NOTE：Although changes to aliases on the leader index are replicated to follower indices, write indices are ignored. Follower indices can’t accept direct writes, so if any leader aliases have `is_write_index` set to `true`, that value is forced to `false`.

&emsp;&emsp;例如你在数据中心A中添加了一篇名为`doc_1`的文档并且复制到了数据中心B。如果客户连接到了数据中心B并且尝试更新`doc_1`，那这次请求会失败。要想更新`doc_1`，必须连接数据中心A并且在leader index上进行更新。

&emsp;&emsp;当follower shard从leader shard收到操作请求时，follow shard会将这些操作放到一个writer buffer中，follower shard会基于这个writer buffer执行bulk write请求。当writer buffer超过配置的上限后，则不会发送读请求。这个配置提供back-pressure机制，使得writer buffer不再full以后就恢复读请求。

&emsp;&emsp;你可以在[creating the follower index](#Tutorial Set up cross-cluster replication)时进行配置来管理如何复制leader index的操作。

&emsp;&emsp;leader index上的index mapping的变更会马上复制到follower index上。这种行为同样发生在index settings上。除了一些leader index的本地设置，比如说修改leader index的分片数量是不会复制到follower index的，follower index不会收到（retrieve）这种设置。

&emsp;&emsp;如果在leader index上应用了一个non-dynamic的设置（settings）并且follower index也需要这个设置，follower index会先关闭，然后应用这个设置，最后重新打开。在这个期间，follower index无法被读取并且不能复制写操作。

##### Initializing followers using remote recovery

&emsp;&emsp;当创建了一个follower index，你只有等这个follower index完全的初始化结束后才能使用它。`remote recovery process`会在follower node上创建一个新的分片并且将leader node上的主分片数据拷贝到这个新的分片中。

&emsp;&emsp;Elasticsearch使用这个`remote recovery process`来引导（bootstrap）follower index使用来自leader index的数据。`remote recovery process`将leader index当前状态的副本提供给follower index，即使由于Lucene中合并中的段使得leader上完整的历史更改还不可用。

&emsp;&emsp;remote recovery是网络密集型（network intensive）的处理过程：它将所有的Lucene的段文件从leader集群传到follower集群。follower在leader集群中的主分片上初始化一个recovery session，随后并发的向leader请求文件块（file chunks）。默认情况下，并发的请求five 1MB file chunks。默认的行为设计为：支持leader和follower集群之间具有高网络延迟（high network latency）。

> TIP：你可以修改[dynamic remote recovery settings](#Remote recovery settings)来限制数据传输速率，并通过remoterecovery管理资源消费。

&emsp;&emsp;使用集群中的[recovery API](#cat recovery API)以及follower index来获取处理中的remote recovery信息。由于Elasticsearch使用[snapshot and restore](#Snapshot and restore)来实现remote recovery，运行中的remote recovery在recovery API被标记为`snapshot`类型。

##### Replicating a leader requires soft deletes

&emsp;&emsp;Cross-cluster replication的工作方式是通过replay leader index的主分片上执行的各个写操作的历史记录。Elasticsearch需要保留[history of these operations](#History retention)，这样就可以让follower shard任务去拉取这些操作。保留这些操作的底层机制是`soft deletes`。

&emsp;&emsp;当删除/更新现有的文档时会发生soft delete。保留的soft delete有一定的限制，这个限制是可配置的。操作历史保留在leader shard上并且对follower shard task可见，用于replay历史操作。

&emsp;&emsp;[index.soft_deletes.retention_lease.period](#index.soft_deletes.retention_lease.period)这个设置定义了分片历史的驻留最大时间，这个设置决定了你的follower index可以离线的最长时间，默认12个小时。如果分片拷贝在其保留租约（retention lease）到期后恢复，但是缺失的操作仍然在leader index上可用，那Elasticsearch会建立一个新的租约并拷贝缺失的操作。然而Elasticsearch不能保证retain unleased operations，所以有可能一些缺失的操作被leader丢弃了并且现在完全不可用了。如果发生了这种情况，follower就不能自动恢复，那你必须[recreate it](#Recreate a follower index)。

&emsp;&emsp;如果要作为leader index，它必须开启soft delete。在Elasticsearch 7.0.0之后，新创建的索引默认开启soft delete。

> IMPORTANT： Cross-cluster replication不能用于Elasticsearch 7.0.0或者更早版本中现有的索引，因为soft delete是关闭的。你必须[reindex](#Reindex API)你的数据并且开启soft delete。

##### Use cross-cluster replication

&emsp;&emsp;下面的内容提供了关于如何配置以及使用cross-cluster replication的更多信息：

- [Set up cross-cluster replication](#Tutorial: Set up cross-cluster replication)
- [Manage cross-cluster replication](#Manage cross-cluster replication)
- [Manage auto-follow patterns](#Manage auto-follow patterns)
- [Upgrading clusters](#Upgrading clusters using cross-cluster replication)

#### Tutorial Set up cross-cluster replication
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ccr-getting-started-tutorial.html)

&emsp;&emsp;使用本章节的指南来设置两个数据中心集群的cross-cluster replication（CCR）。跨数据中心复制（replicate）你的数据能提供以下的好处：

- 将数据靠近用户或者应用服务来降低延迟很响应时间
- 为你的mission-critical application提供可承受数据中心或区域中断的容忍度

&emsp;&emsp;在这一章节，你将会学习到：

- 如何用leader index配置一个[remote cluster](#Remote clusters)
- 如何在一个本地集群中创建一个follower index
- 如何创建一个auto-follow pattern来自动的follow时序索引，remote cluster会周期性的创建这些索引

&emsp;&emsp;你可以手动创建follower index来复制（replicate）remote cluster上的指定的索引，或者配置auto-follow patterns来复制 rolling time series indices。

> TIP：如果你要在cloud上跨集群复制数据，你可以[configure remote clusters on Elasticsearch Service](https://www.elastic.co/guide/en/cloud/current/ec-enable-ccs.html)，然后你就可以[search across clusters](#Search across clusters)以及设置CCR。

##### Prerequisites

&emsp;&emsp;若要完成这个教程，你需要：

- 本地集群上的`manage` 集群特权（cluster privilege）
-  集群的许可证以及CCR。[Activate a free 30-day trial](https://www.elastic.co/guide/en/kibana/8.2/managing-licenses.html)
-  remote cluster上有你想要复制的数据。这个教程使用了电子商务订单的样例数据。[Load sample data](https://www.elastic.co/guide/en/kibana/8.2/get-started.html#gs-get-data-into-kibana)
-  本地集群中，拥有`master` [node role](#Node)的所有节点必须还要有[remote_cluster_client](#Node)角色。本地的集群至少还要一个有`data` 和`remote_cluster_client`角色的节点。个别用于coordinating replication的任务的规模（scale）取决于本地集群中拥有`data` 和`remote_cluster_client`角色的节点数量

##### Connect to a remote cluster

&emsp;&emsp;若要将remote cluster（Cluster A）上的索引复制到本地集群（Cluster B），你需要将Cluster A配置为Cluster B的remote 。

<img src="https://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ccr-tutorial-clusters.png">

&emsp;&emsp;若要在Kibana中的Stack Management中配置一个remote cluster：

1. 从侧边导航栏选择**Remote Clusters**
2. 指定remote cluster（Cluster A）的Elasticsearch endpoint URL，或者IP地址，hostname以及transport端口（默认值`9300`）。例如`cluster.es.eastus2.staging.azure.foundit.no:9400`或者`192.168.1.1:9300`

&emsp;&emsp;你也可以使用[cluster update settings API ](#Cluster update settings API)来添加一个remote cluster：

```text
PUT /_cluster/settings
{
  "persistent" : {
    "cluster" : {
      "remote" : {
        "leader" : {
          "seeds" : [
            "127.0.0.1:9300" 
          ]
        }
      }
    }
  }
}
```

&emsp;&emsp;第8行，指定remote cluster的hostname以及transport 端口。

&emsp;&emsp;你可以通过下面的方式验证本地集群是否成功连接了remote cluster。

```text
GET /_remote/info
```

&emsp;&emsp;API响应显示本地集群连接了一个名为`leader`的remote cluster。

```text
{
  "leader" : {
    "seeds" : [
      "127.0.0.1:9300"
    ],
    "connected" : true,
    "num_nodes_connected" : 1, 
    "max_connections_per_cluster" : 3,
    "initial_connect_timeout" : "30s",
    "skip_unavailable" : false,
    "mode" : "sniff"
  }
}
```

&emsp;&emsp;第7行，本地集群连接的remote cluster中节点的数量

##### Enable soft deletes on leader indices

&emsp;&emsp;index若要能被follow，在创建时必须开启[soft deletes](#Replicating a leader requires soft deletes)。如果索引没有开启soft deletes，你必须reindex并且使用新的索引作为leader index。Elasticsearch 7.0.0及以后的版本默认在新的索引上开启soft deletes。

##### Configure privileges for cross-cluster replication

&emsp;&emsp;CCR在remote cluster和本地集群（local cluster）上要求有不同的cluster和index privilege。使用下面的请求创建remote cluster和本地集群（local cluster）上不同的角色，然后创建一个拥有这些角色的用户。

###### Remote cluster

&emsp;&emsp;拥有leader index的remote cluster上，CCR role要求`read_ccr` cluster privilege，以及leader index上的`monitor`和`read` privilege。

> NOTE：如果请求出现了[on behalf of other users](#Submitting requests on behalf of other users)的问题，用户需要有remote cluster上的`run_as` privilege。

&emsp;&emsp;下面的请求在remote cluster上创建一个`remote-replication` 角色：

```text
POST /_security/role/remote-replication
{
  "cluster": [
    "read_ccr"
  ],
  "indices": [
    {
      "names": [
        "leader-index-name"
      ],
      "privileges": [
        "monitor",
        "read"
      ]
    }
  ]
}
```

###### Local cluster

&emsp;&emsp;拥有follower index的本地集群，`remote-replication` 角色要求`manage_ccr` cluster privilege，以及follower index上的`monitor`，`read`，`write`，以及`manage_follow_index` privilege。

&emsp;&emsp;下面的请求在本地集群上创建了一个`remote-replication`角色：

```text
POST /_security/role/remote-replication
{
  "cluster": [
    "manage_ccr"
  ],
  "indices": [
    {
      "names": [
        "follower-index-name"
      ],
      "privileges": [
        "monitor",
        "read",
        "write",
        "manage_follow_index"
      ]
    }
  ]
}
```

&emsp;&emsp;在一个集群上创建`remote-replication`角色后，在本地集群上使用[create or update users API](#Create or update users API)上创建一个用户并且赋予`remote-replication`角色。例如，下面的请求赋予名为`cross-cluster-user`的用户一个`remote-replication`角色：

```text
POST /_security/user/cross-cluster-user
{
  "password" : "l0ng-r4nd0m-p@ssw0rd",
  "roles" : [ "remote-replication" ]
}
```

> NOTE：你只需要在本地集群上创建这个用户

##### Create a follower index to replicate a specific index

&emsp;&emsp;当你创建了一个follower index，要将remote cluster和leader index引用到你的remote cluster中。

&emsp;&emsp;若要从Kibana中的Stack Management中创建一个follower index：

1. 侧边导航栏选择**Cross-Cluster Replication**，然后选择**Follower Indices**
2. 选择你想要复制（replicate）的包含leader index的cluster（Cluster A）
3. 输入leader index的名称，如果你按照的是以下的教程，那么就填入`kibana_sample_data_ecommerce`
4. 为你的follower index输入一个名称，比如`follower-kibana-sample-data`

&emsp;&emsp;Elasticsearch使用[remote recovery process](#Initializing followers using remote recovery)来初始化follower，即将leader index中的Lucene段文件传输到follower index中。索引状态被更改为**Paused**。在remote recovery process结束，follower index开始使用并且状态更改为**Active**。

&emsp;&emsp;当你在leader index中索引文档时，Elasticsearch会将这些文档复制到follower index中。

<img src="https://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/ccr-follower-index.png">

&emsp;&emsp;你也可以使用[create follower API](#Create follower API)来创建follower index。在你创建一个follower index后，你必须引用remote cluster以及leader index。

&emsp;&emsp;在初始化follower请求时，请求响应在[remote recovery process](#Initializing followers using remote recovery)完成前就会返回，若要等待完成后再返回，那么在你的请求中添加参数`wait_for_active_shards`。

```text
PUT /server-metrics-follower/_ccr/follow?wait_for_active_shards=1
{
  "remote_cluster" : "leader",
  "leader_index" : "server-metrics"
}
```

&emsp;&emsp;使用[get follower stats API](#Get follower stats API)来观察复制状态（status of replication）。

##### Create an auto-follow pattern to replicate time series indices

&emsp;&emsp;你可以使用[auto-follow patterns](#Manage auto-follow patterns)自动的为rolling time series indices创建新的follower。只要remote cluster上的新的索引名称匹配到了auto-follow pattern，对应的follower index就会添加到local cluster中。

&emsp;&emsp;auto-follow pattern用于指定你想要从remote cluster中复制的索引，可以指定一个或者多个。

&emsp;&emsp;若要在Kibana的Stack Management中创建一个auto-follow pattern：

1. 侧边导航栏选择**Cross Cluster Replication **，然后选择**Auto-follow patterns**
2. 为auto-follow pattern输入一个名称，比如`beats`
3. 选择你想要复制（replicate）的包含leader index的remote cluster，例子中的场景是Cluster A
4. 输入一个或者多个index pattern来确认你想要从remote cluster中复制的索引。例如输入`metricbeat-* packetbeat-*`会为Metricbeat和Packetbeat自动创建follower
5. 输入以**Follower**作为follower index名称的前缀，这样能让你更容易区分出复制的索引

&emsp;&emsp;匹配pattern的新索引在remote cluster中创建，Elasticsearch会自动的将它们复制到本地follower index中。

<img src="https://www.amazingkoala.com.cn/uploads/Elasticsearch/8.2/auto-follow-patterns.png">

&emsp;&emsp;你可以使用[create auto-follow pattern API](#Create auto-follow pattern API)来配置auto-follow patterns。

```text
PUT /_ccr/auto_follow/beats
{
  "remote_cluster" : "leader",
  "leader_index_patterns" :
  [
    "metricbeat-*", 
    "packetbeat-*" 
  ],
  "follow_index_pattern" : "{{leader_index}}-copy" 
}
```

&emsp;&emsp;第6行，自动follow新的Metricbeat索引
&emsp;&emsp;第7行，自动follow新的Packetbeat索引
&emsp;&emsp;第9行，follower index的名称继承于leader index 并且添加了`copy`后缀

#### Manage cross-cluster replication
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ccr-managing.html#ccr-pause-replication)

&emsp;&emsp;使用下面的内容来管理cross-cluster replication任务，例如inspecting replication progress、pausing and resuming replication, recreating a follower index, and terminating replication。

&emsp;&emsp;若要开始使用CCR，访问Kibana然后进入**Management > Stack Management**。在侧边导航栏选择**Cross-Cluster Replication**。

##### Inspect replication statistics

&emsp;&emsp;若要查看某个follower index的复制过程以及详细的分片统计数据（shard statistics）。根据上文中的内容进入**Cross-Cluster Replication**然后选择**Follower indices**。

&emsp;&emsp;选择你想要查看的follower index详情的名称，滑出（slide out）的面板会先显示follower index的设置和replication statistics，包括follower shard的读写操作。

&emsp;&emsp;若要显示更多的详细的统计数据，点击**View in Index Management**，在index management中选择follower index的名称，查看follower index的详细统计数据。

&emsp;&emsp;API example：

- 使用[get follower stats API](#Get follower stats API)查看在分片级别的复制过程，这个API能提供follower shard管理的读写操作，还会报出可以重试的异常以及需要用户干预的异常。

##### Pause and resume replication（manage）

&emsp;&emsp;若要停止或者恢复leader index的复制（replication），根据上文中的内容进入**Cross-Cluster Replication**然后选择**Follower indices**

&emsp;&emsp;选择你想要停止的follower index然后选择**Manage > Pause Replication**，follower index的状态变更为Paused。

&emsp;&emsp;若要恢复复制，选择follower index然后选择**Resume replication**

&emsp;&emsp;API example：

&emsp;&emsp;你可以使用[pause follower API](#Pause follower API)停止复制并随后使用[resume follower API](#Resume follower API)恢复复制。一前一后（in tandem）使用这两个API可以让你调整follower shard task的读写参数，如果你最开始的配置不太适用于你的用例。

##### Recreate a follower index

&emsp;&emsp;当更新/删除一篇文档时，底层操作会在Lucene的索引中保留一段时间，这个时间通过参数[index.soft_deletes.retention_lease.period](#index.soft_deletes.retention_lease.period)定义。你可以在[leader index](#Cross-cluster replication)上配置这个设置。

&emsp;&emsp;当follower index开始后，它会向leader index需求一个retention lease，这个lease要求leader不允许prune一个soft delete，直到follower告知它已经收到了该操作或者lease到期。

&emsp;&emsp;如果follower index远远落后于（fall sufficiently）leader并且不能复制操作，Elasticsearch会报出`indices[].fatal_exception`的错误，若要解决这个问题，需要重新创建一个follower index。当新的follower index创建后，[remote recovery process](#Cross-cluster replication)开始从leader中复制Lucene的段文件。

> IMPORTANT：重新创建follower index属于破坏性的行为（destructive action），集群中包含follower index的所有Lucene段文件都会被删除

&emsp;&emsp;若要重新创建follower index，根据上文中的内容进入**Cross-Cluster Replication**然后选择**Follower indices**。

&emsp;&emsp;选择follower index和pause replication。当follower index的状态更改为Paused，重新选择follower index，然后unfollower leader index。

&emsp;&emsp;follower index会被转化为standard index，并且不再Cross-Cluster Replication 页面上显示。

&emsp;&emsp;在侧边导航栏，选择**Index Management**，选择上一步的follower index然后关闭它们。

&emsp;&emsp;随后你就可以[recreate the follower index](#Create a follower index to replicate a specific index)，重新进行复制。

&emsp;&emsp;API example：

&emsp;&emsp;使用[pause follow API](#Pause follower API)来停止复制，然后关闭follower index并重新创建：

```text
POST /follower_index/_ccr/pause_follow

POST /follower_index/_close

PUT /follower_index/_ccr/follow?wait_for_active_shards=1
{
  "remote_cluster" : "remote_cluster",
  "leader_index" : "leader_index"
}
```

##### Terminate replication

&emsp;&emsp;你可以unfollower leader index，然后将follower index转化为一个standard index。根据上文中的内容进入**Cross-Cluster Replication**然后选择**Follower indices**。

&emsp;&emsp;选择follower index和pause replication。当follower index的状态更改为Paused，重新选择follower index，然后unfollower leader index。

&emsp;&emsp;follower index会被转化为standard index，并且不再Cross-Cluster Replication 页面上显示。

&emsp;&emsp;选择**Index Management**，选择上一步的follower index然后关闭它们。

&emsp;&emsp;API example：

&emsp;&emsp;你可以使用[unfollow API](#Unfollow API)来停止复制，这个API会将follower index转化为standard index（non-follower）。

#### Manage auto-follow patterns
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ccr-auto-follow.html)

&emsp;&emsp;若要复制（replication）时序索引，你可以配置一个auto-follow pattern，使得新创建的索引被自动复制。只要remote cluster上新的索引名称匹配到auto-follow pattern，对应的follower index就会添加到本地集群（local cluster）中。

> NOTE：Auto-follow patterns只会匹配remote cluster上打开的索引（open index），并且索引的所有的主分片都已经启动（start）。Auto-follow patterns不会匹配[closed indices](#Open index API)和[searchable snapshots](#Searchable snapshots)来用于CCR。避免使用auto-follow pattern匹配带有[read or write block](#Index block settings)的索引，这些block会阻止follower index执行复制操作。

&emsp;&emsp;你也可以为data streams创建auto-follow pattern。当remote cluster上创建了一个新的backing index并且auto-follow pattern匹配到了data stream的名称，那index和data stream会被自动的follow。如果你在创建auto-follow pattern之后再创建data stream，那么所有的backing index都会被follow。

&emsp;&emsp;通过CCR从remote cluster复制过来的data streams受到local rollovers的保护，可以使用[promote data stream API](#Promote data stream API)来将这些data streams变成regular data streams。

&emsp;&emsp;Auto-follow patterns在[Index lifecycle management](#ILM: Manage the index lifecycle)中特别有用，因为它在包含leader index的集群上会不断的创建新的索引。

&emsp;&emsp;若要在Kibana的Stack Management中使用cross-cluster replication auto-follow patterns，从侧边导航栏选择**Cross-Cluster Replication**然后选择**Auto-follow patterns**。

##### Create auto-follow patterns

&emsp;&emsp;当你[create an auto-follow pattern](#Create an auto-follow pattern to replicate time series indices)时，你可以对单个集群配置一个pattern集合。当在remote cluster上创建一个索引时，索引名会匹配pattern集合中的一个，随后就会在local cluster中配置一个follower index。follower index使用最新的索引作为leader index。

&emsp;&emsp;使用[create auto-follow pattern API](#Create auto-follow pattern API)来添加一个新的auto-follow pattern配置。

##### Retrieve auto-follow patterns

&emsp;&emsp;若要查看现有的auto-follow patterns并修改backing patterns，那就访问你的remote cluster上的Kibana。

&emsp;&emsp;选择你想要查看的auto-follow pattern，然后就可以对其做变更，你可以查看auto-follow pattern下的follower index。

&emsp;&emsp;你可以通过[get auto-follow pattern API](#Get auto-follow pattern API)查看所有配置的auto-follow pattern集合。

##### Pause and resume auto-follow patterns

&emsp;&emsp;若要停止或者恢复auto-follow pattern 集合，访问Kibana，然后选择auto-follow并且停止复制（replication）。

&emsp;&emsp;若要恢复复制，选择pattern并选择**Manage pattern > Resume replication**。

&emsp;&emsp;可以使用[pause auto-follow pattern API](#Pause auto-follow pattern API)来停止auto-follow pattern，并使用[pause auto-follow pattern API](#Resume auto-follow pattern API)来恢复auto-follow pattern。

##### Delete auto-follow patterns

&emsp;&emsp;若要删除auto-follow pattern集合，访问Kibana，选择auto-follow pattern，然后停止复制。

&emsp;&emsp;当pattern状态变成Paused，选择**Manage pattern > Delete pattern**。

&emsp;&emsp;也可以使用[delete auto-follow pattern API](#Delete auto-follow pattern API)来删除一个auto-follow pattern集合。

#### Upgrading clusters using cross-cluster replication
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ccr-upgrading.html)

&emsp;&emsp;对正在使用CCR的集群升级要谨慎对待，下面的情况可能会导致在rolling upgrades时发生索引follow出现失败：

- 尚未升级的集群将拒绝从升级后的集群复制的新的index setting或mapping
- 当follower index尝试回退到file-based的恢复时，未升级的集群中的节点将拒绝来自已升级集群中的节点的索引文件。这个限制是由于Lucene不向前兼容

&emsp;&emsp;基于uni-directional和bi-directional index following，在开启CCR的集群上运行rolling upgrade有所不同。

##### Uni-directional index following

&emsp;&emsp;在uni-directional配置中，只有一个集群包含leader index，并且其他集群只包含复制leader index的follower index。

&emsp;&emsp;这种策略中，follower index所在的集群应该先升级并且leader index所在的集群最后升级。按这种顺序处理能保证在升级过程中能继续index following并且不需要下线。

&emsp;&emsp;你也可以使用这个策略升级一个[replication chain](#Cross-cluster replication)，先升级链中最后的集群再升级包含leader index的集群。

&emsp;&emsp;例如，ClusterA包含了所有的索引，ClusterB follow ClusterA中的索引，ClusterC follow ClusterB中的索引。

```text
Cluster A
        ^--Cluster B
                   ^--Cluster C
```

&emsp;&emsp;在这种配置中，按照下面的顺序升级：

1. Cluster C
2. Cluster B
3. Cluster A

##### Bi-directional index following

&emsp;&emsp;在bi-directional的配置中，每一个集群包含leader和follower index。

&emsp;&emsp;在这种配置中升级时，在升级集群前先[pause all index following](#Pause and resume replication)和[pause auto-follow patterns ](#Pause and resume auto-follow patterns)。

&emsp;&emsp;在升级结束后，恢复index following以及恢复auto-follow pattern的复制。

##### Recreate a follower index

##### Pause and resume replication

## Snapshot and restore
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/snapshot-restore.html)

&emsp;&emsp;快照（snapshot）是运行中的Elasticsearch集群的备份。你可以使用快照用于：

- 定期备份集群，不停机
- 删除或者硬件故障后恢复数据
- 集群间传递数据
- 在cold和frozen data tiers中使用[searchable snapshots](#Searchable snapshots)降低存储开销

#### The snapshot workflow

&emsp;&emsp;Elasticsearch将快照存储在集群外的称为snapshot repository的存储位置中。在创建或恢复快照之前，你必须在集群中[register a snapshot repository](#Register a snapshot repository)。Elasticsearch支持一些带有云仓库选项的仓库类型，包括：

- AWS S3
- Google Cloud Storage（GCS）
- Microsoft Azure

&emsp;&emsp;注册一个快照仓库（snapshot repository）后，你可以使用[snapshot lifecycle management (SLM)](#Automate snapshots with SLM)自动的创建和管理快照。你可以[restore a snapshot](#Restore a snapshot)来恢复或传输数据。

#### Snapshot contents

&emsp;&emsp;默认情况下，集群的快照包含了集群状态（cluster state），所有的常规的（regular）data streams，所有常规的索引。集群状态包括：

- [Persistent cluster settings](#Cluster and node setting types)
- [Index templates](#Index templates)
- [Legacy index templates](#Create or update index template API（legacy）)
- [Ingest pipelines](#Ingest pipelines)
- [ILM policies](#ILM: Manage the index lifecycle)
- For snapshots taken after 7.12.0, [feature states](#Feature states)

&emsp;&emsp;你也可以在创建快照时只指定集群中的data stream或者索引。包含data stream或者索引的快照自动会包含它们的alias。当你恢复一个快照，你可以选择是否恢复这些alias。

&emsp;&emsp;快照中不包括或者说不备份：

- 临时的集群设置
- 注册的快照仓库
- 节点配置文件
- [Security configuration files](#Security files)

##### Feature states

&emsp;&emsp;Feature states包含索引和data streams，用于为一个Elastic feature存储配置、历史和其他数据，例如Elasticsearch security或者Kibana。

&emsp;&emsp;一个feature state通常包括一个或者多个[system indices or system data streams](#System indices)。它也可能包括feature使用的常规索引和data stream。例如，某个feature state可能包括了一个常规索引，这个索引包含了feature的执行历史。在一个常规索引中存储这个历史让你更容易查询到。

&emsp;&emsp;在Elasticsearch 8.0及以后的版本中，feature states备份和恢复系统索引和系统data stream的唯一方法。

#### How snapshots work

&emsp;&emsp;快照会自动的删除重复数据（deduplicate）来节省存储空间并且能减少网络传输开销。快照通过拷贝索引的[segment](#Near real-time search)来备份一个索引并且将其存储在快照仓库中。由于segment是不可变的，快照只需要拷贝仓库中上一个快照后新的段。

&emsp;&emsp;每一个快照在逻辑上是独立的。当你删除一个快照，Elasticsearch仅仅（exclusively）删除那个快照使用的段。如果段被仓库中的其他快照使用，则不会删除。

##### Snapshots and shard allocation

&emsp;&emsp;快照对索引的主分片进行拷贝。当你开始一个快照，Elasticsearch会马上开始拷贝任意可见的（available）主分片的段。如果某个分片正在启动或者relocating，Elasticsearch会在拷贝分片的段（shard's segment）之前等待它们完成。如果一个或者多个分片不可见，the snapshot attempt fails。

&emsp;&emsp;一旦快照开始拷贝分片的段，Elasticsearch不会将这个分片移动到其他节点，即使rebalancing或者shard allocation settings会触发reallocation。Elasticsearch只会在快照完成复制分片数据后才移动分片。

##### Snapshot start and stop times

&emsp;&emsp;快照不代表某个精确时间点（a precise point in time）的集群。而是每个快照包含一个开始跟结束时间。快照代表的是两个时间之间每一个分片数据的视图。

#### Snapshot compatibility

&emsp;&emsp;若要往某个集群中恢复一个快照，快照、集群以及每一个恢复的索引版本必须兼容。

##### Snapshot version compatibility

| **Snapshot version** | 6.8  | 7.0–7.1 | 7.2–7.17 | 8.0–8.2 |
| :------------------: | :--: | :-----: | :------: | :-----: |
|       5.0–5.6        |  √   |    ×    |    ×     |    ×    |
|       6.0–6.7        |  √   |    √    |    √     |    ×    |
|         6.8          |  √   |    ×    |    √     |    ×    |
|       7.0–7.1        |  ×   |    √    |    √     |    √    |
|       7.2–7.17       |  ×   |    ×    |    √     |    √    |
|       8.0–8.2        |  ×   |    ×    |    ×     |    √    |

&emsp;&emsp;你不能将快照恢复到旧版本中。例如，你不能把在7.6.0中创建的快照恢复到运行7.5.0的集群中。

##### Index compatibility

&emsp;&emsp;恢复的每一个索引必须和当前集群的版本兼容。如果你尝试在一个不兼容的集群中恢复一个索引，尝试恢复会失败。

| **Index creation version** | 6.8  | 7.0–7.1 | 7.2–7.17 | 8.0–8.2 |
| :------------------: | :--: | :-----: | :------: | :-----: |
|       5.0–5.6        |  √   |    ×    |    ×     |    ×    |
|       6.0–6.7        |  √   |    √    |    √     |    ×    |
|         6.8          |  √   |    ×    |    √     |    ×    |
|       7.0–7.1        |  ×   |    √    |    √     |    √    |
|       7.2–7.17       |  ×   |    ×    |    √     |    √    |
|       8.0–8.2        |  ×   |    ×    |    ×     |    √    |

&emsp;&emsp;你不可以在旧版本中恢复索引。例如，你不能把在7.6.0中创建的索引恢复到运行7.5.0的集群中。

&emsp;&emsp;一个兼容的快照中可能会包含在不兼容的版本中创建的索引。例如，7.17集群的快照可以包含6.8中创建的索引。但你如果要在8.2的集群中恢复6.8的索引，尝试会失败。如果在升级之前创建快照请注意这个情况。

&emsp;&emsp;存在这么一种替代方法（workaround），你可以先将索引恢复到同时兼容你当前集群和索引的最新版本的集群上。你随后可以使用[reindex-from-remote](#Reindex select fields with a source filter)在你当前集群上重新构建索引。只有索引开启[\_source](#\_source field)才可以从remote上reindex。

#### Warnings

##### Other backup methods

&emsp;&emsp;**Taking a snapshot is the only reliable and supported way to back up a cluster** 。你不能通过拷贝data目录中的节点数据进行备份。没有一种支持方式（supported way）从文件系统层面的备份来恢复数据。如果你想尝试从这种备份中恢复集群，可能会失败并且Report出corruption或者文件丢失或者其他数据一致性的问题，或者出现成功的悄无声息的丢失一些你的数据。

&emsp;&emsp;拷贝data目录中的节点数据不能用于备份是因为它不是在单个时间点上对其内容的一致表示（consistent representation）。你不能通过将节点下线，然后执行那些拷贝或者是采用文件系统层面的快照来修复刚才的问题。因为Elasticsearch有跨整个集群的一致性要求。你必须使用内置的快照功能来备份集群。

##### Repository contents

&emsp;&emsp;**Don’t modify anything within the repository or run processes that might interfere with its contents**。如果某个除了Elasticsearch以外的东西修改了仓库的内容，那以后快照或者恢复操作可能会失败，可能会出现轻微的数据丢失。

&emsp;&emsp;你可以安全的[ restore a repository from a backup ](#Back up a repository)，只要你：

- 在你恢复仓库的内容时，它没有注册在Elasticsearch
- 当你恢复完仓库，它的内容跟你创建它时的内容完全一样

&emsp;&emsp;另外，快照可能包含security-sensitive的信息，那你可能需要[store in a dedicated repository](#Dedicated cluster state snapshots)。

### Register a snapshot repository
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/snapshots-register-repository.html)

#### Azure repository
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/repository-azure.html)

#### Google Cloud Storage repository
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/repository-gcs.html)

#### S3 repository
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/repository-s3.html)

#### Shared file system repository
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/snapshots-filesystem-repository.html)

#### Read-only URL repository
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/snapshots-read-only-repository.html)

#### Source-only repository
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/snapshots-source-only-repository.html)


### Create a snapshot
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/snapshots-take-snapshot.html#automate-snapshots-slm)

&emsp;&emsp;这一章节介绍了如何对运行中的集群创建快照。随后你可以通过[restore a snapshot](#Restore a snapshot)来恢复或者传输数据。

&emsp;&emsp;你将学习到：

- 自动创建快照以及使用snapshot lifecycle management（SLM）保留快照
- 手动创建一个快照
- 监控快照的处理过程（snapshot's progress）
- 删除或取消一个快照
- 备份集群配置文件

&emsp;&emsp;这一章同样介绍了创建专用的集群状态快照的技巧以及在不同的时间间隔创建快照。

#### Prerequisites

- 若要使用Kibana的**Snapshot and Restore**功能，你必须要有以下的权限（permission）：
  - [Cluster  privilege](#Cluster privileges)：`monitor`, `manage_slm`, `cluster:admin/snapshot` 以及 `cluster:admin/repository`
  - [Index privilege](#Indices privileges)：`all` on the `monitor` 索引
- 你只可以在选举为[master](#Master-eligible node)的节点上对运行中的集群创建快照
- 快照仓库必须[registered](#Register a snapshot repository) 并且对集群可见（available）
- 集群的global metadata必须可读。要在快照中包含索引，所以索引和它的metadata必须也是可读的。确保没有任何的[cluster blocks](#Metadata)或者[index blocks](#Index blocks)阻碍读取访问

#### Considerations

- 仓库中快照名称必须是唯一的。尝试创建一个仓库中同名的快照会失败
- 快照自动删除重复的数据（deduplicated）。你可以频繁创建快照，对存储开销只有很小的影响
- 每个快照逻辑上是独立的。你可以删除一个快照，不会影响其他的快照
- 创建一个快照会临时暂停分片的分配。见[Snapshots and shard allocation](#Snapshots and shard allocation)
- 创建快照不会阻塞索引操作获取其他的请求。然而快照不会包含开始备份后的变更
- 你可以在同一时间创建多个快照。集群设置[snapshot.max_concurrent_operations](#snapshot.max_concurrent_operations)限制的快照操作的数量最大值并发量
- 如果快照中包含了一个data stream，那快照中同样包含了这个流的backing Index和metadata
  - 你可以在快照中包含指定的backing Index。然而就不会包含data stream的metadata或者其他的backing Index
- 快照可以包含一个data stream但是排除指定的backing Index。当你恢复这样的data stream，就只包含快照中的backing Index。如果流中原先的write Index不在快照中，那快照中最近（most recent）的backing index成为流的write index

#### Automate snapshots with SLM

&emsp;&emsp;Snapshot lifecycle management（SLM）是定期备份集群的最简单的方法。SLM策略根据设定好的定时计划自动的创建快照。该策略同样可以根据你定义的保留规则删除快照。

> TIP：Elasticsearch Service deployments automatically include the cloud-snapshot-policy SLM policy. Elasticsearch Service uses this policy to take periodic snapshots of your cluster. For more information, see the [Elasticsearch Service snapshot documentation](https://www.elastic.co/guide/en/cloud/current/ec-snapshot-restore.html).

##### SLM security

&emsp;&emsp;在开启Elasticsearch security feature后，下面的[cluster privilege](#Cluster privileges)控制SLM动作的访问：

###### manage_slm

&emsp;&emsp;允许用户执行所有的SLM动作。包括创建/更新策略以及开始/停止SLM。

###### read_slm

&emsp;&emsp;允许用户执行所有的只读的SLM动作，例如获取策略以及检查SLM状态。

###### cluster:admin/snapshot/*

&emsp;&emsp;允许用户创建/删除任意的索引，无论是否有访问那个索引的权限

&emsp;&emsp;你可以通过Kibana创建/管理角色来分配这些privilege。

&emsp;&emsp;为了能保证必要的privilege来创建/管理SLM策略和快照，你可以设置一个拥有`manage_slm`和`cluster:admin/snapshot/*`privilege的角色以及访问SLM history index的全部访问权限。

&emsp;&emsp;下面的例子中，下面的请求创建了名为`slm-admin`的角色：

```text
POST _security/role/slm-admin
{
  "cluster": [ "manage_slm", "cluster:admin/snapshot/*" ],
  "indices": [
    {
      "names": [ ".slm-history-*" ],
      "privileges": [ "all" ]
    }
  ]
}
```

&emsp;&emsp;为了保证有SLM 策略和快照历史的只读访问权限，你可以设置一个拥有`read_slm`的集群privilege的角色以及SLM history index的读取访问权限。

&emsp;&emsp;下面的请求中创建了一个名为`slm-read-only`的角色：

```text
POST _security/role/slm-read-only
{
  "cluster": [ "read_slm" ],
  "indices": [
    {
      "names": [ ".slm-history-*" ],
      "privileges": [ "read" ]
    }
  ]
}
```

##### Create an SLM policy

&emsp;&emsp;若要在Kibana中管理SLM，进入主菜单并且点击**tack Management > Snapshot and Restore > Policies**，若要创建一个策略，则点击**Create Policy**。

&emsp;&emsp;你也可以使用[SLM APIs](#Snapshot lifecycle management APIs)管理SLM。使用[create SLM policy API](#Create or update snapshot lifecycle policy API)创建一个策略。

&emsp;&emsp;下面的请求创建了一个策略，该策略在**1:30 a.m.UTC**备份了集群状态、所有的data streams、以及所有的索引。

```text
PUT _slm/policy/nightly-snapshots
{
  "schedule": "0 30 1 * * ?",       
  "name": "<nightly-snap-{now/d}>", 
  "repository": "my_repository",    
  "config": {
    "indices": "*",                 
    "include_global_state": true    
  },
  "retention": {                    
    "expire_after": "30d",
    "min_count": 5,
    "max_count": 50
  }
}
```

&emsp;&emsp;第3行，使用[Cron syntax](#Watcher cron schedule)定义创建快照的时间
&emsp;&emsp;第4行，快照的名称，支持[date math](#Date math support in index and index alias names)。为了防止命名冲突，这个策略在每个名称后面追加了一个UUID
&emsp;&emsp;第5行，[Registered snapshot repository](#Register a snapshot repository)用于存储这个策略的快照
&emsp;&emsp;第7行，这个策略的快照中包含了data stream和 index
&emsp;&emsp;第8行，如果为`true`，该策略的快照中包含集群状态（cluster state）。同时也默认包含所有的 feature state。若要只包含指定的feature states，见[Back up a specific feature state](#Back up a specific feature state)
&emsp;&emsp;第10行，保留规则（可选）。该配置将为快照保留30天，无论快照的寿命（age）是多少，至少保留5个以及最多50个快照。见[SLM retention](#SLM retention)和[Snapshot retention limits](#Snapshot retention limits)


##### Manually run an SLM policy

&emsp;&emsp;你可以手动运行一个SLM策略来立即创建一个快照。在升级前测试一个策略或者快照时很有用。手动运行一个策略不会影响它的快照定时计划（snapshot schedule）。

&emsp;&emsp;若要在Kibana中运行一个策略，计入**Policies**页面，在**Action**列下并且点击运行图标。你也可以使用[execute SLM policy API](#Execute snapshot lifecycle policy API)。

```text
POST _slm/policy/nightly-snapshots/_execute
```

&emsp;&emsp;快照程序在后台运行。若要监控其运行过程，见[Monitor snapshot](#Monitor a snapshot)。

##### SLM retention

&emsp;&emsp;SLM snapshot retention属于cluster-level的任务，独立于策略中的快照定时计划。若要控制SLM retention任务的运行时间，配置集群设置[slm.retention_schedule](#slm.retention_schedule)。

```text
PUT _cluster/settings
{
  "persistent" : {
    "slm.retention_schedule" : "0 30 1 * * ?"
  }
}
```

&emsp;&emsp;若要马上运行retention任务，使用[execute SLM retention policy API](#Execute snapshot retention policy API)。

```text
POST _slm/_execute_retention
```

&emsp;&emsp;SLM策略中的保留规则（retention rule）只会应用到使用这个策略创建的快照。其他快照不会受到（not count toward）这个策略的保留限制。

##### Snapshot retention limits

&emsp;&emsp;我们建议你在SLM策略中包含保留规则来删除你不再需要的快照。

&emsp;&emsp;快照仓库可以安全的扩展到上千个快照。然而，若要管理它的metadata，规模大的（large）仓库要求在master node上有更多的内存。保留策略能保证仓库的metadata一直增长而造成master node的不稳定。

#### Manually create a snapshot

&emsp;&emsp;若不使用SLM策略创建一个快照，则使用[create snapshot API](#Create snapshot API)，快照的名称支持[date math](#Date math support in index and index alias names)。

```text
# PUT _snapshot/my_repository/<my_snapshot_{now/d}>
PUT _snapshot/my_repository/%3Cmy_snapshot_%7Bnow%2Fd%7D%3E
```

&emsp;&emsp;快照需要花点时间时间完成，取决于它的大小。默认情况下，快照创建API用于初始化快照，快照的处理在后台运行。若要阻塞客户端直到快照完成，则将请求参数`wait_for_completion`设置为`true`。

```text
PUT _snapshot/my_repository/my_snapshot?wait_for_completion=true
```

&emsp;&emsp;你可以使用[clone snapshot API](#Clone snapshot API)克隆一个现有的快照。

#### Monitor a snapshot

&emsp;&emsp;若要监控当前正在运行的快照，使用参数`_current`的[get snapshot API](#Get snapshot API)。

```text
GET _snapshot/my_repository/_current
```

&emsp;&emsp;To get a complete breakdown of each shard participating in any currently running snapshots, use the [get snapshot status API](#Get snapshot API).

```text
GET _snapshot/_status
```

##### Check SLM history

&emsp;&emsp;若要获取某个集群SLM的执行历史（execution history），包括每一个SLM策略的统计，则使用[get SLM stats API](#Get snapshot lifecycle stats API)。这个API同样返回集群快照保留任务历史（retention task history）

```text
GET _slm/stats
```
&emsp;&emsp;若要获取指定的SLM策略的执行历史，则使用[get SLM policy API](#Get snapshot lifecycle policy API)。响应包括：

- 下一计划的策略执行
- 上一次成功开始进行快照的时间。开始进行快照不能保证快照的完成
- 上一次策略执行失败的时间，以及有相关的错误（if applicable）

```text
GET _slm/policy/nightly-snapshots
```

#### Delete or cancel a snapshot

&emsp;&emsp;若要在Kibana中删除一个快照，进入**Snapshots**页面并点击**Action**列下面的trash icon。你也可以使用[delete snapshot API](#Delete snapshot API)。

```text
DELETE _snapshot/my_repository/my_snapshot_2099.05.06
```

&emsp;&emsp;如果你删除一个正在处理中的快照，Elasticsearch会取消这个快照。快照的处理会被停止并删除为这个快照生成的文件。如果某些文件被其他快照使用，那么不会删除这些文件。

#### Back up configuration files

&emsp;&emsp;如果在你自己的硬件上运行Elasticsearch，我们建议作额外的备份，选择一个文件备份软件对每一个节点上的[$ES_PATH_CONF directory ](#Config files location)进行常规备份（regular backup）。快照不会备份这些文件。注意的是每一个节点上的这些文件可能会不同，所以节点上的文件需要各自备份。

> IMPORTANT：The elasticsearch.keystore, TLS keys, and [SAML](#Realm settings), [OIDC](#OpenID Connect realm settings), and [Kerberos](#Kerberos realm settings) realms private key files contain sensitive information. Consider encrypting your backups of these files.

#### Back up a specific feature state

&emsp;&emsp;默认情况下，包含集群状态的快照也包含所有的[feature states](#Feature states)。不包含集群状态的快照默认不包含所有的feature states。

&emsp;&emsp;你可以配置一个快照只包含指定的feature states，不用关心集群状态。

&emsp;&emsp;若要获取可用的feature信息，使用[get features API](#Get Features API)。

```text
GET _features
```

&emsp;&emsp;这个API的响应：

```text
{
  "features": [
    {
      "name": "tasks",
      "description": "Manages task results"
    },
    {
      "name": "kibana",
      "description": "Manages Kibana configuration and reports"
    },
    {
      "name": "security",
      "description": "Manages configuration for Security features, such as users and roles"
    },
    ...
  ]
}
```

&emsp;&emsp;若要在快照中包含指定的feature state，则在`feature_states`数组中指定feature的`name`。

&emsp;&emsp;例如，下面的SLM策略中包含Kibana和Elasticsearch security features的feature states。

```text
PUT _slm/policy/nightly-snapshots
{
  "schedule": "0 30 2 * * ?",
  "name": "<nightly-snap-{now/d}>",
  "repository": "my_repository",
  "config": {
    "indices": "*",
    "include_global_state": true,
    "feature_states": [
      "kibana",
      "security"
    ]
  },
  "retention": {
    "expire_after": "30d",
    "min_count": 5,
    "max_count": 50
  }
}
```

&emsp;&emsp;feature state对应的索引或者data stream都会在快照内容中展示出来。例如，如果你备份了`security` feature state，`security-*`这些系统索引会在[get snapshot API](#Get snapshot API)的`indices`和`feature_states`下展示。

#### Dedicated cluster state snapshots

&emsp;&emsp;一些feature states包含敏感数据。例如`security` feature state包含的系统索引包含了用户名称和加密后的密码。因为密码使用[cryptographic hashes](#User cache and password hash algorithms)存储，快照的泄露（disclosure）不会让第三方授权作为其中的一个用户或者使用API keys。然而，如果第三方可以更改快照，它们可以安装一个后门，则会丢失机密信息。

&emsp;&emsp;若要更好的保护数据，考虑为集群状态的快照创建一个专用的仓库SLM策略。这能让你严格限制以及审计仓库的访问。

&emsp;&emsp;例如，下面的SLM策略只备份集群状态。策略中使用了专用的仓库存储这些快照。

```text
PUT _slm/policy/nightly-cluster-state-snapshots
{
  "schedule": "0 30 2 * * ?",
  "name": "<nightly-cluster-state-snap-{now/d}>",
  "repository": "my_secure_repository",
  "config": {
    "include_global_state": true,                 
    "indices": "-*"                               
  },
  "retention": {
    "expire_after": "30d",
    "min_count": 5,
    "max_count": 50
  }
}
```

&emsp;&emsp;第7行，备份集群状态，同时包含所有的feature states
&emsp;&emsp;第8行，不包括常规的data stream和index

#### Create snapshots at different time intervals

&emsp;&emsp;如果你使用单个SLM策略，那就很难同时做到频繁创建快照并且长时间保留快照。

&emsp;&emsp;比如说，某个策略每30分钟创建一个快照同时快照数量最大值为100个，那么每个快照最多只能保留2天（(100 * 30) / 60 / 24 = 2.08天 ）的时间。这种设置对备份最新的改动是不错的，但是没法让你恢复一星期或者一个月之前的数据。

&emsp;&emsp;为了解决这个问题，你可以使用相同的仓库创建多个SLM策略，这些策略的执行计划间隔不一样。由于策略的保留规则只应用到这个策略生成的快照，某个策略不会删除其他策略创建的快照。

&emsp;&emsp;例如，下面的SLM策略每小时创建一次快照并最多保留24个快照。每个快照在这个策略中只保留一天。

```text
PUT _slm/policy/hourly-snapshots
{
  "name": "<hourly-snapshot-{now/d}>",
  "schedule": "0 0 * * * ?",
  "repository": "my_repository",
  "config": {
    "indices": "*",
    "include_global_state": true
  },
  "retention": {
    "expire_after": "1d",
    "min_count": 1,
    "max_count": 24
  }
}
```

&emsp;&emsp;下面的SLM策略每天创建一次快照并最多保留31个快照。每个快照在这个策略中只保留30天。

```text
PUT _slm/policy/daily-snapshots
{
  "name": "<daily-snapshot-{now/d}>",
  "schedule": "0 45 23 * * ?",          
  "repository": "my_repository",
  "config": {
    "indices": "*",
    "include_global_state": true
  },
  "retention": {
    "expire_after": "30d",
    "min_count": 1,
    "max_count": 31
  }
}
```

&emsp;&emsp;每天下午11点45 UTC运行。

&emsp;&emsp;下面的SLM策略每月创建一次快照并最多保留12个快照。每个快照在这个策略中只保留366天。

```text
PUT _slm/policy/monthly-snapshots
{
  "name": "<monthly-snapshot-{now/d}>",
  "schedule": "0 56 23 1 * ?",            
  "repository": "my_repository",
  "config": {
    "indices": "*",
    "include_global_state": true
  },
  "retention": {
    "expire_after": "366d",
    "min_count": 1,
    "max_count": 12
  }
}
```

&emsp;&emsp;每个月第一天的下午11点56分 UTC运行。


### Restore a snapshot
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/snapshots-restore-snapshot.html)

### Searchable snapshots
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/searchable-snapshots.html)

&emsp;&emsp;Searchable snapshots能让你使用[snapshots](#Snapshot and restore)以一种cost-effective的方式查询很少被访问并且是只读的数据。[cold](#Cold tier)和[frozen](#Frozen tier) data tiers使用Searchable snapshots来降低你的存储和操作开销。

&emsp;&emsp;Searchable snapshots消除了[replica shards](#Scalability and resilience: clusters, nodes, and shards)的需要，可能将搜索数据需要的本地空间减半（halve）。searchable snapshots依赖相同的快照机制，这个机制也用于备份并且对你的快照仓库存储开销有极小的影响。

#### Using searchable snapshots

&emsp;&emsp;查询一个searchable snapshot index跟查询其他索引是一样的。

&emsp;&emsp;默认情况下，searchable snapshot index没有副本分片（replica）。底层的快照提供弹性，由于对查询体量（query volume）的期望是很低的，所以单个分片就足够了。然而，如果你需要支持一个高查询体量，你可以通过调整索引设置`index.number_of_replicas`添加副本分片。

&emsp;&emsp;如果节点发生故障，searchable snapshot shards需要在其他地方恢复，这时候就会出一个短暂的（brief）的时间窗口，在这个时间窗口内，Elasticsearch将分片分配到其他节点，集群的颜色不再是`green`。在那个时间内查询这些分片可能会失败或者返回部分结果，直到将分片分配到健康的节点。

&emsp;&emsp;你通常会通过ILM来管理searchable snapshots。当常规（regular）索引到达`cold`或者`frozen`阶段， [searchable snapshots](#Searchable snapshot)动作自动将常规索引转化为searchable snapshot index。你也可以手动的通过使用[mount snapshot](#Mount snapshot API) API让现有的快照中的索引变成可以被搜索。

&emsp;&emsp;若要从快照中mount一个索引，但是快照中拥有很多的索引，我们建议你创建一个快照的[clone](#Clone snapshot API)，克隆后的快照中只包含你想要搜索的那个索引，并mount那个克隆。你不应该删除一个被mount的快照，所以创建一个克隆能你管理这个快照备份的生命周期，独立于其他任何的searchable snapshots。如果你使用ILM来管理你的searchable snapshots，ILM会在快照需要时自动的进行克隆。

&emsp;&emsp;你可以使用跟常规索引相同的机制来控制searchable snapshot index的分片的分配。例如，你可以使用[Index-level shard allocation filtering](#Index-level shard allocation filtering)来限制searchable snapshot shard分配到某些节点子集上。

&emsp;&emsp;searchable snapshot index的恢复速度受制于仓库设置`max_restore_bytes_per_sec`以及跟普通的恢复操作一样的节点设置`indices.recovery.max_bytes_per_sec`。默认情况下，`indices.recovery.max_bytes_per_sec`的是无限制的，但是`indices.recovery.max_bytes_per_sec`的默认值取决于节点的配置。见[Recovery settings](#Recovery settings)。

&emsp;&emsp;我们建议在创建一个快照前，索引的每个分片都[force-merge](#Force merge API)到单个段中，这样快照将会被mount成一个searchable snapshot index。从快照仓库中的每一次读取都耗时耗钱，段的数量越少，恢复快照或者查询的响应时的读取次数越少。

> TIP：Searchable snapshot用来管理大型归档历史数据室是最理想的的办法。历史信息相较于最新的数据通常很少被查询，因此不需要副本分片来提高性能。
> 对于更多复杂或者耗时的查询，你可以在Searchable snapshots中使用[Async Searchable](#Async search)。

&emsp;&emsp;使用下面任意一个仓库类型来进行searchable snapshots：

- [AWS S3](#S3 repository)
- [Google Cloud Storage](#Google Cloud Storage repository)
- [Azure Blob Storage](#Azure repository)
- [Hadoop Distributed File Store (HDFS)](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/repository-hdfs.html)
- [Shared filesystems](#Shared file system repository) such as NFS
- [Read-only HTTP and HTTPS repositories](#Read-only URL repository)

&emsp;&emsp;You can also use alternative implementations of these repository types, for instance [MinIO](#Client settings), as long as they are fully compatible. Use the [Repository analysis ](#Repository analysis API) API to analyze your repository’s suitability for use with searchable snapshots。

#### How searchable snapshots work

&emsp;&emsp;当从某个快照中mount了一个索引，Elasticsearch将它的分片分配到集群中的data node上。data node随后基于指定的[mount options](#Mount options)，自动的从仓库中将相关的分片数据找回（retrieve）到本地储存。如果数据在本地不可见（available），Elasticsearch从快照仓库中下载需要的数据。

&emsp;&emsp;如果包含了分片的节点发生故障，Elasticsearch自动的将受到影响的分片分配到其他的节点，并且节点是通过仓库来恢复相关的分片数据。不需要副本分片（replica），不需要负载的监控或者orchestration用于恢复丢失的分片。尽管searchable snapshot index默认不需要副本分片，你也可以通过调整`index.number_of_replicas`来添加副本分片。searchable snapshot shard的副本分片是通过拷贝快照仓库中的数据获得，像是searchable snapshot primary shard。与此相反，常规索引的副本分片则是从主分片中拷贝而来。

##### Mount options

&emsp;&emsp;若要搜索一个快照，你必须首先mount快照到本地成为一个索引。通常ILM会自动完成。但你也可以自己调用[mount snapshot](#Mount snapshot API)。有两个选项从快照中mount出一个索引，它们都不同的性能特色（performance characteristic）和本地储存占用（local storage footprints）：

###### Fully mounted index

&emsp;&emsp;加载完整的被快照的索引分片（snapshotted index's shards）的拷贝到集群的本地节点存储上。ILM在`hot`和`cold`阶段使用这个选项。

&emsp;&emsp;fully mounted index的查询性能跟常规索引（regular index）相当，因为极小需要访问快照仓库。随着恢复的进行（while recover is ongoing），查询性能可能比常规索引要慢，因为某个查询所需要的数据还没有找回（retrieve）到本地。如果发生这种情况，Elasticsearch会急切的找回需要的数据来完成这次查询，并且是跟恢复过程并行处理。磁盘上的数据在重启后也会被保留，所以节点不需要在重启后重新下载已经存储在节点上的数据。

&emsp;&emsp;fully mount后，ILM管理的索引名都有`restored-`的前缀。

###### Partially mounted index

&emsp;&emsp;使用本地缓存，只包含最新查询的索引数据。该缓存有一个固定大小，其大小由frozen tier的所有节点共享。ILM在`frozen`阶段使用这个选项。

&emsp;&emsp;如果查询需要的数据不在缓存中，Elasticsearch会从快照仓库中获取丢失的数据。需要获取数据的查询会很慢，但是获取到的数据会存储在缓存中，以后类似的查询可以非常快。Elasticsearch会evict缓存中不常被使用的数据来释放空间。节点重启后，缓存会被清空。

&emsp;&emsp;尽管比fully mounted或者常规索引慢，partially mounted index仍然能很快的返回查询结果，即使是很大的数据集，因为仓库的数据层为查询进行了heavily optimization。许多的查询在返回结果前只需要检索分片数据总量的一个小的子集。

&emsp;&emsp;partiallly mounted后，ILM管理的索引名都有`partial-`的前缀。

&emsp;&emsp;若要partially mount an index，你必须要有一个或者多个节点上分片缓存可用 。默认情况下，专用的frozen data tier中的所有节点共享一个配置，使用磁盘总空间的90%和磁盘总空间与headroom（100GB）的差值。

&emsp;&emsp;强力推荐在生产使用中使用一个专用的frozen tier。如果没有，你必须配置`xpack.searchable.snapshot.shared_cache.size`为一个或者多个节点的缓存保留空间。Partially mounted indice只会分配到有分片缓存的节点。

- `xpack.searchable.snapshot.shared_cache.size`

&emsp;&emsp;（[Static](#Static（settings） )）为partially mounted index的分片缓存保留的磁盘空间大小。接收一个磁盘空间的百分比或者一个absolute [byte value](#Byte size units)。对于专用的frozen data tires中的节点，默认值是90%。其他默认是`0b`。

- `xpack.searchable.snapshot.shared_cache.size.max_headroom`

&emsp;&emsp;（[Static](#Static（settings） ) [byte value](#Byte size units)）对于专用frozen tier中的节点，headroom的最大值。如果`xpack.searchable.snapshot.shared_cache.size`没有显示设置， 则headroom设置为100GB。否则默认是`-1`（not set）。只有`xpack.searchable.snapshot.shared_cache.size`用百分比设置才能配置`xpack.searchable.snapshot.shared_cache.size.max_headroom`。

&emsp;&emsp;为了说明这些设置如何协同工作（work in concert）。下面两个例子是在一个专用的frozen node上使用了默认值：

- 4000GB的磁盘将产生3900GB的分片缓存。4000GB的90%是3600G，留下的空间是400G的headroom。由于默认的`max_headroom`是100GB，因此最终的结果是3900GB
- 400GB的磁盘会产生360GB的分片缓存

&emsp;&emsp;你可以在`elasticsearch.yml`中配置：

```text
xpack.searchable.snapshot.shared_cache.size: 4TB
```

> IMPORTANT：你只能在[data_frozen](#Frozen data node)角色的节点上配置这些设置。另外，有分片缓存的节点只能有单个[data path](#Path settings)。

&emsp;&emsp;Elasticsearch同样使用了一个名为`.snapshot-blob-cache`的专用系统索引来加速searchable snapshot的恢复。这个索引是位于partially or fully mounted data之上的缓存层，它包含了启动searchable snapshot shard的最少需要的数据。Elasticsearch自动删除索引中不再使用的文档。可以通过以下的设置来调节这个周期性的清除：

- searchable_snapshots.blob_cache.periodic_cleanup.interval

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）`.snapshot-blob-cache`索引周期性清除计划的间隔时间。默认值是每1小时 。

- searchable_snapshots.blob_cache.periodic_cleanup.retention_period

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）在`.snapshot-blob-cache`索引中保留过时文档的保留期。默认值是`100`。

- searchable_snapshots.blob_cache.periodic_cleanup.batch_size

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）`.snapshot-blob-cach`周期性清除时，每次为bulk-delete查询的文档数量。默认值是`10m`。

- searchable_snapshots.blob_cache.periodic_cleanup.pit_keep_alive

&emsp;&emsp;（[Dynamic](#Dynamic（settings）)）The value used for the <point-in-time-keep-alive,point-in-time keep alive>> requests executed during the periodic cleanup of the .snapshot-blob-cache index. Defaults to 10m.

#### Reduce costs with searchable snapshots

&emsp;&emsp;在大多数的情况下，searchable snapshot降低开销的方式是通过消除副本分片以及节点之间分片数据的拷贝。然而，如果从快照仓库中取回数据的开销是特别大的，那么searchable snapshot可能比常规索引有更多的开销。在使用之前，确保操作系统的cost structure和searchable snapshot是兼容的。

##### Replica costs

&emsp;&emsp;为了弹性（resiliency），常规索引要求在多个节点上有冗余的分片拷贝（shard copy）。如果节点发生故障，Elasticsearch使用冗余分片来重构丢失的分片拷贝。searchable snapshot不要求副本分片。如果包含searchable snapshot index的节点发生故障，Elasticsearch会从快照仓库中重建丢失的分片拷贝 。

&emsp;&emsp;没有副本分片后，很少被访问的searchable snapshot index要求非常少（far fewer）的资源。包含replica-free fully-mounted的searchable snapshot index要求的节点和磁盘空间只有常规索引的一半。只包含partially-mounted searchable snapshot index的frozen tier甚至只要更少的资源。

##### Data transfer costs

&emsp;&emsp;当常规索引的分片在节点间移动时，分片的内容从集群中的另一个节点拷贝。在许多环境中，在节点间移动数据的开销是很大的。特别是在节点位于不同区字段的云环境中运行时。相反的是，当mount一个searchable snapshot index或者移动其中一个分片时，总是从快照仓库中拷贝。通常来说成本是很低的。

> WARNING：Most cloud providers charge significant fees for data transferred between regions and for data transferred out of their platforms. You should only mount snapshots into a cluster that is in the same region as the snapshot repository. If you wish to search data across multiple regions, configure multiple clusters and use [cross-cluster search](#Search across clusters) or [cross-cluster replication](#Cross-cluster replication) instead of searchable snapshots.

#### Back up and restore searchable snapshots

&emsp;&emsp;你可以使用[regular snapshots](#Create a snapshot)备份一个包含searchable snapshot index的集群。当你恢复一个包含searchable snapshots index的快照后，这些索引又再次恢复成searchable snapshot index。

&emsp;&emsp;在你恢复一个包含searchable snapshot index的快照之前，你必须首先[register ](#Register a snapshot repository)，它包含最初的索引快照（original index snapshot）。在恢复时，searchable snapshot index从最初的索引快照中挂载索引快照。你可以使用不同的仓库分别用于常规快照（regular snapshot）和searchable snapshot。

&emsp;&emsp;searchable snapshot index的快照中只包含了少量的metadata用来识别它的original index snapshot，它不包含原始索引中的任何数据。如果original index snapshot不可见，那么在恢复searchable snapshot indices时会失败。

&emsp;&emsp;由于searchable snapshot index不是常规索引，所以她不能使用[source-only repository](#Source-only repository)对searchable snapshot indices生成快照。

##### Reliability of searchable snapshots

&emsp;&emsp;searchable snapshot index的数据拷贝依赖的是仓库中的快照。如果仓库出现故障或者快照损坏会导致数据丢失。尽管Elasticsearch在本地储存中有这些数据的副本（copy），这些副本可能是不兼容的并且在仓库发生故障时不能用于恢复数据。你必须保证你的仓库是可靠的并且保护仓库里面的数据不受到破坏。

&emsp;&emsp;主流公共云提供商的blob storage都能很好的提供对数据丢失或者破坏的保护，如果你自己管理仓库，那你需要负责其可靠性。

## Secure the Elastic Stack
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/secure-cluster.html)

### Manually configure security
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/manually-configure-security.html)

#### Set up basic security for the Elastic Stack
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/security-basic-setup.html#encrypt-internode-communication)

##### Encrypt internode communications with TLS

#### Security files
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/security-files.html)

### Enable audit logging
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/enable-audit-logging.html)

#### Audit events
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/audit-event-types.html)

#### Logfile audit events ignore policies
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/audit-log-ignore-policy.html)

#### Auditing search queries
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/auditing-search-queries.html)

### User authentication
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/setting-up-authentication.html)

#### Built-in roles
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/built-in-roles.html)

##### remote_monitoring_agent

#### Native user authentication
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/native-realm.html)

#### Built-in users
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/built-in-users.html)

### User authorization
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/authorization.html)

#### Defining roles
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/defining-roles.html)

#### Security privileges
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/security-privileges.html#privileges-list-indices)

##### Cluster privileges

##### Indices privileges

#### Submitting requests on behalf of other users
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/run-as-privilege.html)

## Watcher
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/xpack-alerting.html)

### Watcher triggers
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/trigger.html)

#### Watcher schedule trigger
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/trigger-schedule.html#trigger-schedule)

## Command line tools
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/commands.html)

### elasticsearch-node
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/node-tool.html#node-tool-repurpose)

#### Changing the role of a node

### elasticsearch-reset-password
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/reset-password.html)

## How to
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/how-to.html)

&emsp;&emsp;Elasticsearch附带了默认的配置目的是想让用户有一个很好的开箱体验，用户应该在不需要做任何更改的情况下就可以使用全文搜索（full  text search），高亮、聚合和索引这些功能。

&emsp;&emsp;一旦你更好的理解掌握了Elasticsearch后，你可以使用一些优化根据你自己的用例做性能的提升。

&emsp;&emsp;这一章节会指导你哪些变更是应该做以及不应该做。

### General recommendations
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/general-recommendations.html)

#### Don’t return large result sets

&emsp;&emsp;Elasticsearch 被设计为一个搜索引擎，这使得它非常擅长取回与查询匹配的top documents。然而将Elasticsearch作为数据库领字段的那种工作负载是不太好的，比如返回满足某个查询的所有文档。如果你真的需要这么做，确保使用[Scroll](#Scroll search results) API来实现。

#### Avoid large documents

&emsp;&emsp;默认的[http.max_content_length](#http.max_content_length)的值为100M。Elasticsearch会拒绝索引大于该值的文档。你可能想要增加这个特定的设置，但 Lucene 仍然有大约 2GB 的限制。

&emsp;&emsp;即使抛开这些硬性的限制，索引一个large document不是一个很好的实践。large document会对网络、内存使用量和硬盘造成更多的压力。即使查询请求中不要求查询`_source`，Elasticsearch在所有情况下都需要获取文档的`_id`，而获取large document的这个字段的开销由于文件系统缓存的原因会更大。索引这个文档可能使用的内存量是文档原始大小的倍数。Proximity search（phrase queries for instance）和[highlighting](#Highlighting)同样会变得开销昂贵，因为它们的开销直接取决于原始文档的大小。

&emsp;&emsp;It is sometimes useful to reconsider what the unit of information should be。例如，如果你想要让一本书可以被搜索到，并不是意味着文档中包含这一整本书的内容。一种可能比较好的方式是使用章节（chapter）或者甚至是段落（paragraph）作为一篇文档，并且在这个文档中有其他的属性（property）来标识属于哪一本书。这种方式不仅仅会避免索引large document，同时也使得查询体验会更好。比如说用户想要查询两个单词`foo`和`bar`。跨章节的匹配结果可能较差，而段落中同时出现这两个单词的匹配结果则看起来是好的。的

### Recipes
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/recipes.html)

&emsp;&emsp;这一章节包含了一些用于解决常见问题的方法：

- [Mixing exact search with stemming](#Mixing exact search with stemming)
- [Getting consistent scores](#Getting consistent scoring)
- [Incorporating static relevance signals into the score](#Incorporating static relevance signals into the score)

#### Mixing exact search with stemming
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/mixing-exact-search-with-stemming.html)

&emsp;&emsp;在构建一个查询应用时，stemming通常是必要的，因为在查询`skiing`时会渴望匹配到包含`ski`或者`skis`的文档。但如果用户就想专门查询`skiing`呢？通常的做法就是使用[multi-field](#fields)使得对相同的内容用两种不同的方式进行索引：

```text
PUT index
{
  "settings": {
    "analysis": {
      "analyzer": {
        "english_exact": {
          "tokenizer": "standard",
          "filter": [
            "lowercase"
          ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "body": {
        "type": "text",
        "analyzer": "english",
        "fields": {
          "exact": {
            "type": "text",
            "analyzer": "english_exact"
          }
        }
      }
    }
  }
}

PUT index/_doc/1
{
  "body": "Ski resort"
}

PUT index/_doc/2
{
  "body": "A pair of skis"
}

POST index/_refresh
```

&emsp;&emsp;根据这个设置，在`body`上查询`ski`会同时返回这两篇文档。

```text
GET index/_search
{
  "query": {
    "simple_query_string": {
      "fields": [ "body" ],
      "query": "ski"
    }
  }
}
```

```text
{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped" : 0,
    "failed": 0
  },
  "hits": {
    "total" : {
        "value": 2,
        "relation": "eq"
    },
    "max_score": 0.18232156,
    "hits": [
      {
        "_index": "index",
        "_id": "1",
        "_score": 0.18232156,
        "_source": {
          "body": "Ski resort"
        }
      },
      {
        "_index": "index",
        "_id": "2",
        "_score": 0.18232156,
        "_source": {
          "body": "A pair of skis"
        }
      }
    ]
  }
}
```

&emsp;&emsp;另外，在`body.exact`上查询`ski`将只返回文档1，因为`body.exact`的analysis chain不会执行stemming。

```text
GET index/_search
{
  "query": {
    "simple_query_string": {
      "fields": [ "body.exact" ],
      "query": "ski"
    }
  }
}
```

```text
{
  "took": 1,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped" : 0,
    "failed": 0
  },
  "hits": {
    "total" : {
        "value": 1,
        "relation": "eq"
    },
    "max_score": 0.8025915,
    "hits": [
      {
        "_index": "index",
        "_id": "1",
        "_score": 0.8025915,
        "_source": {
          "body": "Ski resort"
        }
      }
    ]
  }
}
```

&emsp;&emsp;This is not something that is easy to expose to end users，我们需要找出一个方法来知道用户是想要查找一个精确匹配（exact match），如果不是的话我们就重定向到一个合适的字段。还需要考虑的是如果查询中的一部分需要精确匹配而其他部分仍然要stemming？

&emsp;&emsp;幸运的是，`query_string`和`simple_query_string`有一个功能来解决这个exact problem: `quote_field_suffix`。这个参数会告诉Elasticsearch被双引号包起来的词需要重定向到另一个不同的字段，如下所示：

```text
GET index/_search
{
  "query": {
    "simple_query_string": {
      "fields": [ "body" ],
      "quote_field_suffix": ".exact",
      "query": "\"ski\""
    }
  }
}
```

```text
{
  "took": 2,
  "timed_out": false,
  "_shards": {
    "total": 1,
    "successful": 1,
    "skipped" : 0,
    "failed": 0
  },
  "hits": {
    "total" : {
        "value": 1,
        "relation": "eq"
    },
    "max_score": 0.8025915,
    "hits": [
      {
        "_index": "index",
        "_id": "1",
        "_score": 0.8025915,
        "_source": {
          "body": "Ski resort"
        }
      }
    ]
  }
}
```

&emsp;&emsp;在上面的例子中，`ski`使用了双引号（in-between quotes），根据`quote_field_suffix`参数将在`body.exact`字段上查询。所以只有文档1返回。这允许用户基于自己喜好混合查询exact search和 stemming search。

> NOTE：如果`quote_field_suffix`中的字段不存在，那么查询就会回滚（fall back）到使用默认的字段进行查询。

#### Getting consistent scoring
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/consistent-scoring.html)

&emsp;&emsp;Elasticsearch在执行跨分片和副本的文档打分操作时会增加一定的挑战。

##### Scores are not reproducible

&emsp;&emsp;比如说同一个用户执行两次相同的列和文档的查询（the same request twice in a row and documents），但是返回的结果是不一样的，这是一种很差的体验对吗？事实上如果你有多个副本时（[index.number_of_replicas](#index.number_of_replicas)大于0）是有可能发生这种情况的。原因是Elasticsearch在选择分片时采用了一种循环（round-robin）的方式，所以两次相同的列和文档的查询会在同一个分片的不同的两个副本上执行。

&emsp;&emsp;那为什么这会产生问题呢？Index statistics是打分的重要的组成部分。由于被删除的文档（deleted documents）的存在使得同一个分片的不同副本上的index statistics可能是不相同的。也许你知道当文档被删除或者更新后，旧的的文档不会从索引中移除，这些旧的文档只是被标记为被删除的（deleted）并且只有在所属的段下次被合并后才会被移除。然而由于生产实践的原因（However for practical reasons），index statistics会包含这些被删除的文档。所以考虑这种情况，当主分片刚刚完成了一次合并并且移除了很多被删除的文档，使得它的index statistic跟分片有很大的不同（分片中依旧有很多被删除的文档）导致打分也是不同的。

&emsp;&emsp;一种推荐的方式是使用一个字符串（用户的id或者session）来确认登陆的用户的身份作为一个[preference](#preference)，使得给定的用户总是去命中相同的分片，最终多个查询的打分始终是一致的。

&emsp;&emsp;这种work around还有别的好处：当两篇文档的打分相同时，它们会默认的根据lucene内部id（跟`_id没有关系`）进行排序。然而这些lucene内部id在同一个分片的不同副本中可能是不同的。所以通过总是命中相同的分片，使得相同打分值的文档总是有一致的排序结果。

##### Relevancy looks wrong

&emsp;&emsp;如果你注意到两篇内容一样的文档会得到不同的分数或者一次exact match不会让文档的排序靠前，这个问题可能跟sharding相关。默认情况下，Elasticsearch会让每一个分片负责生成自己的分数。然而由于index statistic对打分有重要的贡献，所以只有分片间有相同的index statistics才能工作正常。这个假设说的是文档会默认被均匀的（evenly）的路由到不同的分片，这些分片的index statistic应该是非常相似的那么就会得到期望的分数。然而如果有下面的假设：

- 要么在索引期间使用路由（use routing at index time）
- 查询多个索引（query multiple indices）
- 或者你的索引中数据很少

&emsp;&emsp;不然很大的可能会让参与查询请求的所有分片有不相同的index statistics使得相关性会很差。

&emsp;&emsp;如果是一个较小的数据集，那么你可以把所有的东西都索引到只有一个副本的分片（[index.number_of_shards](#index.number_of_shards): 1）中来work around这个问题，这是默认的配置。所有的文档有相同的index statistics并且分数是将保持一致。

&emsp;&emsp;其他推荐的方式就是使用[dfs_query_then_fetch](#search_type)的查询类型来work around这个问题。这使得Elasticsearch会遍历所有涉及的分片，向这些分片索要跟这次请求相关的index statistics，然后coordinating node会合并这些index statistics，伴随着查询将合并后的index statistics发给查询的分片，这样分片就可以使用全局的index statistics而不是自身的index statistics进行打分。

&emsp;&emsp;大多数情况下，这个额外的round trip开销非常小。然而有些查询如果包含了数量很多的字段或者term，由于所有的term都需要从[terms dictionary](https://www.amazingkoala.com.cn/Lucene/suoyinwenjian/2019/0401/索引文件之tim&&tip)中查找并收集统计值使得开销不会很小。

#### Incorporating static relevance signals into the score
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/static-scoring-signals.html)

&emsp;&emsp;有些领域具有static single并且它跟相关性有关。比如说[PageRank](https://en.wikipedia.org/wiki/PageRank)跟url长度都是用于web查询的两个通用的功能来调整对网页的打分，并且独立于查询条件。

&emsp;&emsp;目前有两种查询允许对静态分数跟文本相关性进行组合。比如跟BM25一起打分：

- [script_score](#Script score query) query 
- [rank_feature](#Rank feature query) query

&emsp;&emsp;比如说你有一个`pagerank`字段，你希望跟BM25进行组合，并且最终的打分公式为：`score = bm25_score + pagerank / (10 + pagerank)`。

&emsp;&emsp;使用[script_score](#Script score query) query 的话如下所示：

```text
GET index/_search
{
  "query": {
    "script_score": {
      "query": {
        "match": { "body": "elasticsearch" }
      },
      "script": {
        "source": "_score * saturation(doc['pagerank'].value, 10)" 
      }
    }
  }
}
```

&emsp;&emsp;第9行，`pagerank`必须使用[Numeric](#Numeric field types)类型。

&emsp;&emsp;使用[rank_feature](#Rank feature query) query 的话如下所示：

```text
GET _search
{
  "query": {
    "bool": {
      "must": {
        "match": { "body": "elasticsearch" }
      },
      "should": {
        "rank_feature": {
          "field": "pagerank", 
          "saturation": {
            "pivot": 10
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;第10行，`pagerank`必须是[rank_feature](#Rank feature field type)字段。

&emsp;&emsp;两种方式都可以获得相同的分数，但是各有取舍（trade-off）：[script_score](#Script score query) query 提供了更多的灵活性，使得可以让你用文本相关性分数跟你想要的static singles进行组合。另外[rank_feature](#Rank feature query) query只暴露（expose）了一些方法将static singles纳入到分数中。然而它基于[rank_feature](#Rank feature field type)跟[rank_features](#Rank features field type)字段，这两个字段的字段值通过了特殊的方式进行索引，使得[rank_feature](#Rank feature query) query可以跳过不具竞争力（non-competitive）的文档并快速的返回top match。


### Tune for indexing speed
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/tune-for-indexing-speed.html)

#### Use bulk requests

&emsp;&emsp;Bulk request能获得比每次只索引一篇文档更高的性能。为了能找出bulk request中包含最合适的文档数量，你应该在一个单节点的单个分片上做基准测试（benchmark）。首先一次性索引100篇文档，然后200、400篇。每次的基准测试都将索引的文档数量翻倍。当索引的速度趋于平缓（start to plateau）那么说明针对你的数据类型已经达到了bulk request包含的最合适的文档数量。In case of tie, it is better to err in the direction of too few rather than too many documents。需要注意的是当并发发送large bulk request时，可能会对集群造成内存的压力，所以建议避免每次请求发送几十兆（a couple tens of megabytes）的数据，即使这些请求看起来性能没有问题。

#### Use multiple workers/threads to send data to Elasticsearch

&emsp;&emsp;单线程发送bulk request不大可能最大化Elasticsearch集群的索引能力。为了能使用集群所有的资源，你应该使用多线程或者多进程来发送数据。另外，除了可以更好的利用集群的资源，也应该会有助于降低每个 fsync 的成本。

&emsp;&emsp;确保观察`TOO_MANY_REQUESTS (429)`的响应码（`EsRejectedExecutionException` with the Java client）。Elasticsearch通过这个响应码告诉你它处理的速度赶不上当前的索引速率。当这个情况发生后，你应该暂停下索引再进行重试，ideally with randomized exponential backoff。

&emsp;&emsp;跟量化bulk request中的文档数量一样，只有通过测试才能知道优化后的worker的数量。可以通过持续的增加worker的数量直到集群中I/O or CPU达到饱和（saturate）。

#### Unset or increase the refresh interval

&emsp;&emsp;让变更的内容对搜索可见的操作称为[refresh](#Refresh API)，这是一项开销很大的操作。频繁的refresh会对同时进行中的索引速率造成影响。

&emsp;&emsp;默认情况下，Elasticsearch周期性的每一秒执行refresh。但只有最近30秒收到一个或者请求的索引才会按这种方式进行refresh。

&emsp;&emsp;这是一个优化配置，如果没有或者有很少的搜索流量（search traffic，5分钟中有一次或者没有查询请求）就可以优化索引的速度。这个做法目的是在没有查询请求时能优化bulk Indexing。可以通过显示的设置refresh的间隔来opt out这种方式。

&emsp;&emsp;另外，如果你的索引用于常规的查询请求（if your index experiences regular search requests），默认的行为意味着Elasticsearch每隔一秒种就会执行一次refresh。如果你能够承受（afford to）文档被索引后和对搜索可见之间的时间间隔，提高[index.refresh_interval](#index.refresh_interval)的值，比如说30s，以此来提高索引速度。

#### Disable replicas for initial loads

&emsp;&emsp;如果你想要马上将大量的数据写入到Elasticsearch中，可以将`index.number_of_replicas`设置为0来提高索引速度。没有副本（replica）意味着丢失单个节点后会遇到数据丢失，所以很重要的一点是数据在其他地方是live使得在最初的导入（initial load）在遇到这种情况时可以通过重试来解决这个问题。最初的导入完成后，你再把`index.number_of_replicas`的值设置为原来的值。

&emsp;&emsp;如果在index settings中设置了`index.refresh_interval`，可以更助于在最初导入时先unset然后在完成后重新设置为原来的值。

#### Disable swapping(how to)

&emsp;&emsp;You should make sure that the operating system is not swapping out the java process by [disabling swapping](#Disable swapping).

#### Give memory to the filesystem cache

&emsp;&emsp;文件系统缓存（filesystem cache）用于缓存I/O操作，你应该确保将运行 Elasticsearch 的机器的至少一半内存分配给文件系统缓存（You should make sure to give at least half the memory of the machine running Elasticsearch to the filesystem cache）。

#### Use auto-generated ids

&emsp;&emsp;当索引一个带有显示id（explicit id）的文档时，Elasticsearch会检查分片中是否有跟这个id一样的文档，这是一种开销大的操作并且随着索引量的增加开销会增加。通过自动生成的id，Elasticsearch会跳过检查来提高索引速度。

#### Use faster hardware

&emsp;&emsp;如果索引受 I/O 限制（I/O-bound），考虑增加文件系统缓存的大小（见上文）或使用更快的存储。Elasticsearch通常创建独立的文件并顺序写入（sequential write），然而索引过程中会并发写入多个文件，所以是随机和顺序写入的混合模式。因此SSD驱动器的性能往往比旋转磁盘（spinning disk）好。

#### Indexing buffer size

&emsp;&emsp;如果你的节点执行繁重的索引（heavy Indexing），确保[indices.memory.index_buffer_size](#Indexing buffer settings)设置的足够大，让每一个正在执行繁重索引的分片最多拥有512M Indexing buffer（超过这个值通常不会再有提升）。Elasticsearch会让一个节点上所有活动的分片（active shard）都分享这个设置（a percentage of the java heap or an absolute byte-size）。非常活跃的分片自然的会比执行轻量级索引（lightweight indexing）的分片更多地使用这个缓冲区。

&emsp;&emsp;默认值是10%，这个值通常是够用的：如果你分配给JVM 10G内存，那么1G的内存将会用于index buffer。这个值足够一个节点上的两个分片来执行繁重的索引。

#### Use cross-cluster replication to prevent searching from stealing resources from indexing

&emsp;&emsp;单个集群中，查询和搜索会为了资源进行竞争（compete for resources），通过设置两个集群并配置[cross-cluster replication](#Cross-cluster replication)，将副本数据从一个集群复制到另一个集群中。并将所有的查询请求路由到follower index所在的集群，查询行为（search activity）将不再跟leader index所在的集群的索引（Indexing）抢夺资源。

#### Additional optimizations

&emsp;&emsp;另外在[Tune for disk usage](#Tune for disk usage)中提供了许多的策略也能提高索引的速度。

### Tune for search speed
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/tune-for-search-speed.html)


##### Give memory to the filesystem cache

&emsp;&emsp;Elasticsearch非常依赖（heavily depend on）文件系统缓存来实现快速的查询。一般情况下你要让一半的内存用于文件系统缓存使得Elasticsearch让hot regions of the index保持在物理内存中。

##### Avoid page cache thrashing by using modest readahead values on Linux

&emsp;&emsp;查询会带来很多随机I/O访问。当底层的块设备（underlying block device）有大量的预读值（readhead value）时，会有很多不必要的I/O，特别是文件通过mmap（[memory mapping](#File system storage types) ）方式访问时。

&emsp;&emsp;大多数的linux发行版会中的单个plain device会使用一个合理的`128KiB`的预读值。然而使用了[software raid](https://en.wikipedia.org/wiki/RAID#SOFTWARE)、[LVM](https://en.wikipedia.org/wiki/Logical_volume_management)或者[dm-crypt](https://en.wikipedia.org/wiki/Dm-crypt)后可能会导致块设备（用于支持Elasticsearch的[path.data](#Path settings)）使用一个很大的预读值（在几个 MiB 的范围内）。这通常会导致严重的页面（文件系统）缓存抖动（page cache thrashing adversely），从而对搜索（或[update](#Document APIs)）性能产生不利影响。

&emsp;&emsp;你可以使用`lsblk -o NAME,RA,MOUNTPOINT,TYPE,SIZE`检查当前以字节为单位的预读值。参考Linux发行版的文档来了解如何更改预读值（比如使用`udev`命令设置为一个常量，需要重启，或者通过[blockdev --setra](https://man7.org/linux/man-pages/man8/blockdev.8.html)设置为一个临时的值），我们建议你将该值设置为`128KiB`。

> NOTE：`blockdev`期望的是512个字节为一个扇区（section）的值然而`lsblk`的值是以`KiB`为单位的。如果要为`/dev/nvme0n1`的预读值临时设置为`128KiB`，可以这么指定：`blockdev --setra 256 /dev/nvme0n1`。

##### Use faster hardware

&emsp;&emsp;如果查询受 I/O 限制（I/O-bound），考虑增加文件系统缓存的大小（见上文）或使用更快的存储。每一个查询涉及到在多个文件上进行随机和顺序的读取，并且可能在每一个分片上并发的进行查询。因此SSD驱动器的性能往往比旋转磁盘（spinning disk）好。

&emsp;&emsp;Directly-attached类型的存储（local storage）通常比远程储存（remote storage）有更好的性能因为它配置更方便并且避免了communications overheads。通过精心的调整，远程储存也可以达到可接受的性能要求。使用实际的工作负载进行基准测试来检测你用于调整的参数。如果达不到你期望的性能要求，跟存储的供应商一起找出问题所在。

&emsp;&emsp;如果查询受CPU限制，考虑使用更多更快的cpu。

##### Document modeling

&emsp;&emsp;可以对文档进行建模（Documents should be modeled），使得search-time的操作的开销尽可能的少。

&emsp;&emsp;特别是尽量避免join，[nested](#Nested field type)会以好几倍（several times）的代价降低查询速度并且[parent-child relations](#Join field type)会降低几百倍（hundreds of times）的查询速度。所以如果能通过重新标准化文档（denormalizing document）而不是用join解决同样的问题，那么可以获得非常好的查询速度。

##### Search as few fields as possible

&emsp;&emsp;[query_string](#Query string query)或者[multi_match](#Multi-match query)查询的字段越多，查询越慢。一个常用的用于提高查询速度的技术就是在索引阶段将多个字段的值索引到单个字段，然后只查询这个字段。可以通过[copy_to](#copy_to)实现自动的映射并且不需要更改源文档。下面的例子是一个包含电影的索引，该例子通过将电影的名称跟情节索引到`name_and_plot`字段来优化查询。

```text
PUT movies
{
  "mappings": {
    "properties": {
      "name_and_plot": {
        "type": "text"
      },
      "name": {
        "type": "text",
        "copy_to": "name_and_plot"
      },
      "plot": {
        "type": "text",
        "copy_to": "name_and_plot"
      }
    }
  }
}
```

##### Pre-index data

&emsp;&emsp;你应该利用查询中的模式（pattern）来优化索引的方式。如果你的文档中有`price`字段并且大部分的查询是指定了多个范围的[range](#Range aggregation)聚合，你可以将范围信息写入到索引中，然后使用[term](#Terms aggregation)聚合来提高聚合的速度。

&emsp;&emsp;比如说文档长这个样子：

```text
PUT index/_doc/1
{
  "designation": "spoon",
  "price": 13
}
```

&emsp;&emsp;并且查询请求长这个样子：

```text
GET index/_search
{
  "aggs": {
    "price_ranges": {
      "range": {
        "field": "price",
        "ranges": [
          { "to": 10 },
          { "from": 10, "to": 100 },
          { "from": 100 }
        ]
      }
    }
  }
}
```

&emsp;&emsp;然后这篇文档可以在索引期间丰富一个`price_range`字段，并且应该设为[keyword](#Keyword type family)类型。

```text
PUT index
{
  "mappings": {
    "properties": {
      "price_range": {
        "type": "keyword"
      }
    }
  }
}

PUT index/_doc/1
{
  "designation": "spoon",
  "price": 13,
  "price_range": "10-100"
}
```

&emsp;&emsp;然后查询请求可以在新的字段上进行聚合查询而不是在`price`上执行`range`聚合。

```text
GET index/_search
{
  "aggs": {
    "price_ranges": {
      "terms": {
        "field": "price_range"
      }
    }
  }
}
```

##### Consider mapping identifiers as keyword

&emsp;&emsp;不是所有的数值类型的数据都必须用[numeric](#Numeric field types)类型的。Elasticsearch为`integer`、`long`这些数值类型的字段优化了[range](#Range query
)查询。然而[keyword](#Keyword type family)字段更适合用于[term](#Term query)或者[term-level](#Terms query)的查询。

&emsp;&emsp;类似ISBN或者产品ID的唯一标示（identifier），它们很少用于范围查询，然而它们经常使用term-level的查询。

&emsp;&emsp;可以在下列的场景中考虑对数值类型的唯一标示使用keyword：

- 你不计划对唯一标示的数据使用range查询
- 快速检索是非常重要的。在`keyword`上执行`term`查询通常比在数值类型的字段类型上更快

&emsp;&emsp;如果你不确定怎么使用，可以通过[multi-field](#fields) 将数据同时索引为`keyword`或者数值类型的字段。

##### Avoid scripts

&emsp;&emsp;如果可以的话，避免在聚合中使用脚本、基于[脚本](#Scripting)的排序以及[script_score](#Script score query)查询。见[Scripts, caching, and search speed](#Scripts, caching, and search speed)。

##### Search rounded dates

&emsp;&emsp;在时间类型的字段上使用`now`的查询通常是无法缓存的，因为匹配的时间范围一直在发生变更。然而在一些用户体验中对时间进行四舍五入（rounded）是可接受的，使得可以利用查询缓存的功能带来的好处。

&emsp;&emsp;比如说下面这个查询：

```text
PUT index/_doc/1
{
  "my_date": "2016-05-11T16:30:55.328Z"
}

GET index/_search
{
  "query": {
    "constant_score": {
      "filter": {
        "range": {
          "my_date": {
            "gte": "now-1h",
            "lte": "now"
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;上面这个查询可以替换为下面这个：

```text
GET index/_search
{
  "query": {
    "constant_score": {
      "filter": {
        "range": {
          "my_date": {
            "gte": "now-1h/m",
            "lte": "now/m"
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;在上面的例子中， 我们对`分钟`进行了四舍五入，如果当前时间是`16:31:29`，这个范围查询将会匹配`my_date`字段上所有在`15:31:00` 和 `16:31:59`之间的文档。如果一些用户执行了在同一`分钟`内的查询，查询缓存（query cache）会帮助提升一点查询速度。The longer the interval that is used for rounding, the more the query cache can help，但是注意的是过于激进的（too aggressive）四舍五入会降低用户的体验。

> NOTE：也可以尝试将查询范围切分为一个范围大可以缓存的部分和范围小但是不可以缓存的部分，这样就可以利用好查询缓存：

```text
GET index/_search
{
  "query": {
    "constant_score": {
      "filter": {
        "bool": {
          "should": [
            {
              "range": {
                "my_date": {
                  "gte": "now-1h",
                  "lte": "now-1h/m"
                }
              }
            },
            {
              "range": {
                "my_date": {
                  "gt": "now-1h/m",
                  "lt": "now/m"
                }
              }
            },
            {
              "range": {
                "my_date": {
                  "gte": "now/m",
                  "lte": "now"
                }
              }
            }
          ]
        }
      }
    }
  }
}
```

&emsp;&emsp;然而这种方式在实践中可能会因为引入`bool`带来开销导致查询速度变慢，破坏了更好地利用查询缓存所节省的成本。

##### Force-merge read-only indices

&emsp;&emsp;那些只读的索引可以从[merged down to a single segment](#Force merge API)中受益，这类索引通常是基于时间的索引（time-base indices）：只有当前时间范围的索引会获取新文档，而旧索引是只读的。那些被force-merged到单个段的分片可以使用简单并且高效的数据结构来执行查询操作。

> IMPORTANT：不要force-merge索引到你正在写入的索引中，或者以后你会再次写入的索引中。而是依赖后台合并进程按需合并来保持索引平滑运行。如果你继续往一个合并后的索引中写数据会另性能变差。

##### Warm up global ordinals

&emsp;&emsp;[Global ordinals](#eager_global_ordinals)是一个用来优化聚合的数据结构。它会被延迟计算并且保存在JVM堆中作为[field data cache](#Field data cache settings)的一部分。用于频繁桶聚合（heavily used for bucket aggregation）的字段，可以在查询这些字段之前，让Elasticsearch对这些字段构造并且缓存global ordinals。注意的是这种操作会增加堆内存的使用并且增加[refreshes](#Refresh API) 的时间。可以通过mapping参数eager global ordinals对现有的mapping进行动态的更新。

```text
PUT index
{
  "mappings": {
    "properties": {
      "foo": {
        "type": "keyword",
        "eager_global_ordinals": true
      }
    }
  }
}
```

##### Warm up the filesystem cache

&emsp;&emsp;如果运行Elasticsearch的服务器重启了，那文件系统缓存会被清空。 所以需要一定的时间让操作系统将hot regions of the index 载入到内存中，使得可以查询操作可以变快。你可以使用[ index.store.preload](#Preloading data into the file system cache) 设置明确地告诉操作系统哪些文件应该根据文件扩展名尽快的（eagerly）加载到内存中。

>WARNING：如果把太多的索引跟文件载入到内存中，并且文件系统缓存没有那么大来容下所有的数据，则会导致查询速度变慢。


##### Use index sorting to speed up conjunctions（how to）

&emsp;&emsp;[Index sorting](#Index Sorting)可以提高交集（conjunctions）查询的性能，代价是降低索引（Indexing）的速度。查看[index sorting documentation](#Use index sorting to speed up conjunctions)了解更多内容。

##### Use preference to optimize cache utilization

&emsp;&emsp;有多种类型的缓存可以用来提高查询性能，比如[filesystem cache](https://en.wikipedia.org/wiki/Page_cache)、[request cache](#Shard request cache settings)、以及 [query cache](#Node query cache settings)。目前所有的缓存在节点层进行维护。如果你连续（in a row）执行两次相同的查询并且存在一个副本分片以及使用默认路由算法的[round-robin](https://en.wikipedia.org/wiki/Round-robin_DNS)，那这两次的查询会请求不同的两个分片，就没法使用节点层的缓存优化了。

&emsp;&emsp;由于使用查询应用的用户先后执行相同的查询是很常见的。比如说为了分析索引的较窄子集（a narrower subset of the index），使用preference来标识当前用户或会话可以帮助优化缓存的使用。

##### Replicas might help with throughput, but not always

&emsp;&emsp;除了提高弹性（resiliency），副本分片可以用来提高吞吐量。比如说你有个单分片索引和三个节点，你可以将分片的数量设置为2来拥有一共三份分片的拷贝，这样所有的节点都可以用来查询。

&emsp;&emsp;现在你想象你有一个索引，它有两个分片。在第一个场景中，副本分片的数量为0，那么每个节点都有单个分片。在第二个场景中，副本分片的数量为1，那么每个节点都有两个分片。哪一种方式可以有更好的查询性能呢？通常有较少分片的节点有更好的性能。原因是每一个分片可以有更大的文件系统缓存，这个文件系统缓存的大小是所有分片共享的，并且文件系统缓存可能是Elasticsearch性能指标中最重要的影响因子（the filesystem cache is probably Elasticsearch’s number 1 performance factor）。同时注意的是没有副本的那个场景在单个节点发生故障后就不可用了。所以要考虑好吞吐量跟高可用之间的trade-off。

&emsp;&emsp;所以正确的分片数量是多少呢？如果集群中一共有`num_nodes`个节点，`num_primaries`个主分片。如果你想要最多能够承受`max_failures`个节点同时发生故障，那么正确的分片数量是：`max(max_failures, ceil(num_nodes / num_primaries) - 1)`。

####  Tune your queries with the Search Profiler
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/_tune_your_queries_with_the_search_profiler.html)

&emsp;&emsp;[Profile API](#Profile API)提供了这次查询处理的详细信息，包括每一个子查询和聚合花费的时间。

&emsp;&emsp;Kibana中的[Search Profiler](https://www.elastic.co/guide/en/kibana/8.2/xpack-profiler.html)能更方便的定位和分析profile results，并让你深入了解如何调整查询以提高性能并减少负载。

&emsp;&emsp;由于Profile API自身就会带来额外的开销，所以主要用来观察各个子查询之间相对的开销，不会真正的给出实际的处理花费的时间。

#### Faster phrase queries with index_phrases
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/faster-phrase-queries.html#faster-phrase-queries)

&emsp;&emsp;[text](#Text type family)类型的字段有一个[index_phrases](#index_phrases)选项会索引2-shingles，可以自动的被query parser用来查询查询没有间隔（slop）的短语查询（phrase query）。如果你的用例中有很多短语查询，那么这个选项可以很好的提高查询速度。

#### Faster prefix queries with index_prefixes
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/faster-prefix-queries.html)

&emsp;&emsp;[text](#Text type family)类型的字段有一个[index_prefixes](#index_prefixes)选项会将所有term的前缀索引，可以自动的被query parser用来前缀查询（prefix query）。如果你的用例中有很多的前缀查询，那么这个选项可以很好的提高查询速度。

#### Use constant_keyword to speed up filtering
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/faster-filtering-with-constant-keyword.html)

&emsp;&emsp;通常来说，filter的开销取决于匹配到的文档数量。比如你有一个包含车子（cycles）的索引，并且有大量的bicycles，然后很多的查询都是过滤出`cycle_type: bicycle`。这是一个很普通的过滤操作但是开销很大，因为匹配到了大量的文档。有一个简单的办法来避免执行这个过滤操作：将包含bicycles的文档索引到自己的索引中，并且通过查询这个索引来过滤bicycles，而不是作为一个[filter query](#Example of query and filter contexts)。

&emsp;&emsp;但是这样做会使得客户端的逻辑变得复杂（logic tricky），不过可以通过`constant_keyword`来帮忙。通过在包含bicycles的索引上将`cycle_type`的类型映射为`constant_keyword`并且赋值为`bicycles`，客户端可以使用相同的query查询整体的索引（monolithic index），Elasticsearch会在bicycles的索引上做出正确的方式，即忽略`cycle_type`为`bicycles`的过滤，否则不返回任何命中。

&emsp;&emsp;mapping看起来是这样的：

```text
PUT bicycles
{
  "mappings": {
    "properties": {
      "cycle_type": {
        "type": "constant_keyword",
        "value": "bicycle"
      },
      "name": {
        "type": "text"
      }
    }
  }
}

PUT other_cycles
{
  "mappings": {
    "properties": {
      "cycle_type": {
        "type": "keyword"
      },
      "name": {
        "type": "text"
      }
    }
  }
}
```

&emsp;&emsp;我们把索引切分成了两个：一个索引只包含bicycle，另一个索引包含其他类型的车子：独轮车（unicycle）、三轮车（tricycle）。然后在查询期间，我们需要对两个索引进行查询，并且不需要更改query。

```text
GET bicycles,other_cycles/_search
{
  "query": {
    "bool": {
      "must": {
        "match": {
          "description": "dutch"
        }
      },
      "filter": {
        "term": {
          "cycle_type": "bicycle"
        }
      }
    }
  }
}
```

&emsp;&emsp;在`bicycles`索引上，Elasticsearch会简单的忽略`cycle_type`的过滤并且将query重写为下面的样子：

```text
GET bicycles,other_cycles/_search
{
  "query": {
    "match": {
      "description": "dutch"
    }
  }
}
```

&emsp;&emsp;在`other_cycles`索引上，Elasticsearch会快速的判断出在`cycle_type`字段中不存在bicycles然后不返回任何的命中。

&emsp;&emsp;将常用的值放在专用的索引上是一种非常有力的使得查询开销很小的一种方式。这个想法可以跨多个字段进行组合：比如说，如果你要跟踪每一种车子的颜色，并且`bicycles`索引中有大量的黑色的bicycles，那么你可以继续将索引划分成`bicycles-black`和`bicycles-other-colors`的两个索引。

&emsp;&emsp;这种优化不是一定严格需要`constant_keyword`的：也可以在客户端的逻辑中基于不同的filter将查询路由到对应的索引上。然而`constant_keyword`能够让这个过程透明，并允许将搜索请求与索引拓扑分离，以换取非常少的开销。

### Tune for disk usage
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/tune-for-disk-usage.html)

#### Disable the features you do not need

&emsp;&emsp;默认情况下，Elasticsearch对大部分的字段同时进行索引（倒排、bkd）和添加doc value（正排），使得这些字段默认（out of box）可以用于查询和聚合。比如说你有一个名为`foo`的数值类型的字段，并且你需要在这个字段上进行histogram并且从来不会应用于filter，那你可以在[mappings](#Mappings（create index api）)中安全的关闭索引（只使用bkd存储，不用倒排）：

```text
PUT index
{
  "mappings": {
    "properties": {
      "foo": {
        "type": "integer",
        "index": false
      }
    }
  }
}
```

&emsp;&emsp;[text](#Text type family)类型的字段会存储标准化因子（normalization factor）用于（facilitate）文档打分。如果你只关心是否匹配`text`字段而不用生成分数，那么你可以使用[match_only_text](#Match-only text field type)。这种类型的字段会扔掉分数和位置信息来节省很大的空间。

#### Don’t use default dynamic string mappings

&emsp;&emsp;[dynamic string mappings](#Dynamic mapping)会默认将字符串的值用[text](#Text type family)和[keyword](#Keyword type family)进行索引，如果你只需要其中一种，那么显然这种默认的方式会有一些浪费。比如说`id`类型的值只需要用`keyword`字段类型进行索引而`body`类型的值只要用`text`字段类型进行索引。

&emsp;&emsp;可以通过显示的（explicit）在字符串字段上或者在dynamic templates上进行配置，使得将字符串字段使用`keyword`或者`text`索引。

&emsp;&emsp;比如，下面的模板将字符串字段使用`keyword`索引：

```text
PUT index
{
  "mappings": {
    "dynamic_templates": [
      {
        "strings": {
          "match_mapping_type": "string",
          "mapping": {
            "type": "keyword"
          }
        }
      }
    ]
  }
}
```

#### Watch your shard size

&emsp;&emsp;更大的分片（larger shard）更能有效的存储数据。你可以在[creating indices](#Create index API)时设置较少的分片，生成较少的索引（比如说利用[Rollover API](#Rollover API)）或者使用[Shrink API](#Shrink index API)更改现有的索引来降低主分片的数量。

&emsp;&emsp;注意的（keep in mind）是大分片（large shard）也会带来缺点，比如说分片恢复的时间会很长。

#### Disable \_source

&emsp;&emsp;[\_source](#\_source field)会存储文档对应的原始的json内容。如果你不需要的话可以关闭这个配置，然而访问`_source`的API比如说更新或者reindex将无法正常工作。

#### Use best_compression

&emsp;&emsp;`_source`和stored filed很容易占用大量的磁盘空间（can easily take a non negligible amount of disk space）。可以使用`best_compression` [codec](#index.codec)对这些数据进行更加激进的压缩。

#### Force merge

&emsp;&emsp;Elasticsearch中的索引会被存储在一个或者多个分片上。每一个分片是一个Lucene索引，这个索引由一个或者多个[段](https://www.amazingkoala.com.cn/Lucene/suoyinwenjian/2019/0610/索引文件之segments_N
)组成。每一个段中的文件都是磁盘上对应的文件。更大的段（larger segment）更能有效的存储数据。

&emsp;&emsp;[force merge API](#Force merge API)可以用来降低每一个分片中段的数量。大部分情况下，可以通过设置`max_num_segments=1`将每一个分片中段的数量降至一个。

> WARNING：**Force merge should only be called against an index after you have finished writing to it**。Force merge会产生非常大（> 5G）的段，并且如果你持续往这个索引里面写数据，自动执行的合并策略以后也不会将新的段进行合并直到这些段中的文档都是标记为被删除的。这会导致索引中有很多的大段并且降低查询性能

#### Shrink index

&emsp;&emsp;[Shrink API](#Shrink index API)可以让你降低索引的分片数量。结合上文中Force merge可以很好的降低分片数量以及分片中段的数量。

#### Use the smallest numeric type that is sufficient

&emsp;&emsp;为[numeric data](#Numeric field types)选择不同的字段会对磁盘使用有着很大的影响。特别是integer应该使用integer类型（比如说`byte`、`short`、`integer`、`long`）存储并且浮点型数据应该要么使用`scaled_float`（如果合适的话），要么使用最小的类型（比特位）：使用`float`而不是用`double`，使用`half_float`而不是用`float`也能帮助节省磁盘空间。

#### Use index sorting to colocate similar documents

&emsp;&emsp;当Elasticsearch存储`_source`时，会一次性的处理多个文档来提高压缩率。比如文档间包含相同的字段是很常见的，并且这些字段包含相同的值也是很常见的。especially on fields that have a low cardinality or a zipfian distribution。

&emsp;&emsp;默认情况下会按照添加到索引的顺序对文档进行压缩。如果你开启了[index sorting](#Index Sorting)后则会根据排序后的文档进行压缩。排序后的文档有着类似的结构、字段、字段值，使得可以提高压缩率。

#### Put fields in the same order in documents

&emsp;&emsp;由于是多篇文档被压缩到block中，所以如果每篇文档中`_source`中的字段是有序的，那么更容易找到更长的相同的值（duplicate strings）。

#### Roll up historical data

&emsp;&emsp;保留旧的数据对以后进行数据分析是有帮助的。但由于磁盘开销经常就不会保留。你可以使用data rollups以原始数据存储成本的一小部分来汇总和存储历史数据。见[Rolling up historical data](#Rolling up historical data)了解更多内容。

### Fix common cluster issues
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/fix-common-cluster-issues.html#circuit-breaker-errors)

&emsp;&emsp;这一章节介绍如何处理Elasticsearch集群的一些常见错误和问题。

#### Error: disk usage exceeded flood-stage watermark, index has read-only-allow-delete block

&emsp;&emsp;这个错误指出数据节点的磁盘空间大小已经严重不足（critical low）了并且达到了[flood-stage disk usage watermark](#cluster.routing.allocation.disk.watermark.flood_stage)。为了防止出现full disk，当一个节点达到watermark，Elasticsearch会block在这个节点上写入索引。如果这个block影响到了系统的索引，Kibana和其他Elastic Stack feature可能会不可用。

&emsp;&emsp;当受到影响的节点的磁盘使用量降到[high disk watermark](#cluster.routing.allocation.disk.watermark.high)以下，Elasticsearch会自动移除write block。Elasticsearch会自动的将受到影响的节点的分配移动到有相同data tier的其他节点上。

&emsp;&emsp;若要验证分片已从受到影响的节点上移走，使用[cat shards API](#cat shards API)。

```text
GET _cat/shards?v=true
```

&emsp;&emsp;如果分片仍然在节点上，使用[cluster allocation explanation API ](#Cluster allocation explain API)获取分配状态的介绍。


```text
GET _cluster/allocation/explain
{
  "index": "my-index",
  "shard": 0,
  "primary": false,
  "current_node": "my-node"
}
```

&emsp;&emsp;如果要马上恢复写操作，你可以临时的提高disk watermark然后移除write block。

```text
PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.disk.watermark.low": "90%",
    "cluster.routing.allocation.disk.watermark.high": "95%",
    "cluster.routing.allocation.disk.watermark.flood_stage": "97%"
  }
}

PUT */_settings?expand_wildcards=all
{
  "index.blocks.read_only_allow_delete": null
}
```

&emsp;&emsp;作为长期解决方案（long-term solution）。你应该在受到影响的data tier中添加节点或者更新现有的节点的磁盘空间。若要释放额外的磁盘空间，你可以使用[delete index API](#Delete index API)来删除不需要的索引。

```text
DELETE my-index
```

&emsp;&emsp;当一个长期解决方案落实后，重置或者重新配置disk watermark。

```text
PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.disk.watermark.low": null,
    "cluster.routing.allocation.disk.watermark.high": null,
    "cluster.routing.allocation.disk.watermark.flood_stage": null
  }
}
```

#### Circuit breaker errors

&emsp;&emsp;Elasticsearch使用[circuit breakers](#Circuit breaker settings)防止节点发生OOM。如果Elasticsearch估算出一个操作会超出circuit breakers，它将停止这个操作并返回一个错误。

&emsp;&emsp;默认情况下，[parent circuit breaker](#Circuit breaker settings)在达到95%的JVM内存量时触发。为了防止错误的发生，我们建议使用量一旦超过85%时就采取行动来降低内存压力。

##### Diagnose circuit breaker errors

###### Error messages

&emsp;&emsp;如果一个请求触发了circuit breaker，Elasticsearch会返回一个`429`的错误码。

```text
{
  'error': {
    'type': 'circuit_breaking_exception',
    'reason': '[parent] Data too large, data for [<http_request>] would be [123848638/118.1mb], which is larger than the limit of [123273216/117.5mb], real usage: [120182112/114.6mb], new bytes reserved: [3666526/3.4mb]',
    'bytes_wanted': 123848638,
    'bytes_limit': 123273216,
    'durability': 'TRANSIENT'
  },
  'status': 429
}
```

&emsp;&emsp;Elasticsearch同样会将circuit breaker的错误写入到[elasticsearch.log](#Logging)中，这有助于记录一些自动处理，例如分配这种操作触发circuit breaker对应的错误。

```text
Caused by: org.elasticsearch.common.breaker.CircuitBreakingException: [parent] Data too large, data for [<transport_request>] would be [num/numGB], which is larger than the limit of [num/numGB], usages [request=0/0b, fielddata=num/numKB, in_flight_requests=num/numGB, accounting=num/numGB]
```

###### Check JVM memory usage

&emsp;&emsp;如果你开启了Stack Monitoring，你可以在Kibana中查看JVM的内存使用。在主菜单中，点击**Stack Monitoring**。在Stack Monitoring的**Overview**页面，点击**Nodes**，**JVM Heap**会列出每一个节点的当前内存使用情况。

&emsp;&emsp;你同样可以使用[cat nodes API ](#cat nodes API)来获取每一个节点当前的`heap.percent`。

```text
GET _cat/nodes?v=true&h=name,node*,heap*
```

&emsp;&emsp;若要获取每一个circuit breaker的JVM 内存使用量。使用[node stats API](#Nodes stats API)：

```text
GET _nodes/stats/breaker
```

##### Prevent circuit breaker errors

###### Reduce JVM memory pressure

&emsp;&emsp;高的JVM内存压力通常会导致circuit breaker error，见[ High JVM memory pressure](#High JVM memory pressure)。

- Avoid using fielddata on text fields
  - 对于high-cardinality的`text`字段，fileddata会使用大量的内存。若要避免这个问题，Elasticsearch默认关闭`text`字段的[fileddata](#fielddata mapping parameter)。如果你开启了fielddata，并且触发了[fielddata circuit breaker](#Field data circuit breaker)，考虑使用关闭它或者使用`keyword`。见[fielddata mapping parameter](#fielddata mapping parameter)。
- Clear the fieldata cache
  - 如果你触发了fielddata circuit breaker并且不能关闭fielddata，使用[clear cache API](#Clear cache API)来清除fileddata cache。这可能会中断（disrupt）正在使用fielddata的查询。
  
```text
POST _cache/clear?fielddata=true
```

#### High CPU usage

&emsp;&emsp;Elasticsearch使用[thread pools](#Thread pools)来管理并发操作所需的CPU资源，High CPU usage通常说的是一个或者多个线程池运行不足（running low）。

&emsp;&emsp;如果一个线程池耗尽（depleted）了，Elasticsearch会[reject](#Rejected requests)跟线程池相关的[request](#Rejected requests)。例如如果`search` 线程池耗尽了，Elasticsearch会reject查询请求直到有可用线程。

##### Diagnose high CPU usage

###### Check CPU usage

- Elasticsearch Service
  - 从部署菜单中，点击**Performance**。页面中的**CPU Usage**章节显示了你部署的CPU使用百分比。
  - High CPU usage can also deplete your CPU credits. CPU credits let Elasticsearch Service provide smaller clusters with a performance boost when needed. The CPU credits chart shows your remaining CPU credits, measured in seconds of CPU time.
  - 你可以使用[cat nodes API](#cat nodes API)获取每一个节点当前的CPU使用量

```text
GET _cat/nodes?v=true&s=cpu:desc
```

&emsp;&emsp;响应中`cpu`这一列包含了当前CPU使用量的百分比，`node`这一列包含了节点的名称。

- Self-managed
  - 你可以使用[cat nodes API](#cat nodes API)获取每一个节点当前的CPU使用量

```text
GET _cat/nodes?v=true&s=cpu:desc
```

&emsp;&emsp;响应中`cpu`这一列包含了当前CPU使用量的百分比，`node`这一列包含了节点的名称。

###### Check hot threads

&emsp;&emsp;如果一个节点有很高的CPU使用量，使用[nodes hot threads API ](#Nodes hot threads API)检查正在节点上运行的资源密集（resource-intensive）的线程。

```text
GET _nodes/my-node,my-other-node/hot_threads
```

&emsp;&emsp;这个API会返回hot thread的plain text形式的breakdown。

##### Reduce CPU usage

&emsp;&emsp;下面的内容介绍常见的导致high cup usage以及解决方案。

###### Scale your cluster

&emsp;&emsp;繁重的Index和search负载会耗尽（deplete）smaller thread pool。若要能更好的处理繁重的负载，在你的集群中添更多的节点或者更新现有的节点来提高承载能力（capacity）。

###### Spread out bulk requests(1)

&emsp;&emsp;尽管[bulk indexing ](#Bulk API)和[multi-search](#Multi search API)的请求比单独的请求有更高的效率，但是它们要求更高的CPU usage。如果可以的话，提交数量较小的请求然后运行提交多次。

###### Cancel long-running searches

&emsp;&emsp;运行时间长的查询会阻塞住[search](#search) 线程池中的线程。使用[task management API](#Task management API)来检查这些运行时间长的查询。

```text
GET _tasks?actions=*search&detailed
```

&emsp;&emsp;response中的`description`描述了请求索引以及query信息。`running_time_in_nanos`描述了这个查询已经运行了的时间。

```text
{
  "nodes" : {
    "oTUltX4IQMOUUVeiohTt8A" : {
      "name" : "my-node",
      "transport_address" : "127.0.0.1:9300",
      "host" : "127.0.0.1",
      "ip" : "127.0.0.1:9300",
      "tasks" : {
        "oTUltX4IQMOUUVeiohTt8A:464" : {
          "node" : "oTUltX4IQMOUUVeiohTt8A",
          "id" : 464,
          "type" : "transport",
          "action" : "indices:data/read/search",
          "description" : "indices[my-index], search_type[QUERY_THEN_FETCH], source[{\"query\":...}]",
          "start_time_in_millis" : 4081771730000,
          "running_time_in_nanos" : 13991383,
          "cancellable" : true
        }
      }
    }
  }
}
```

&emsp;&emsp;若要取消一个查询并且释放资源，使用endpoint `_cancel`。

```text
POST _tasks/oTUltX4IQMOUUVeiohTt8A:464/_cancel
```

&emsp;&emsp;见[Avoid expensive searches](#Avoid expensive searches)了解更多关于跟踪以及避免资源密集型的查询的内容。

#### High JVM memory pressure

&emsp;&emsp;High JVM memory usage会降低集群的性能以及触发[circuit breaker errors](#Circuit breaker errors)。为了防止发生这个情况，我们建议当节点的JVM内存使用量一旦超过85%时就要采取行动来降低内存压力。

##### Diagnose high JVM memory pressure

###### Check JVM memory pressure

- Elasticsearch Service
  - 在你的部署菜单中，点击**Elasticsearch**，在**Instances**下，每一个实例都显示了**JVM memory pressure**指示器（indicator）。当内存压力达到75%后，指示器就会变成红色。

&emsp;&emsp;你也可以使用[nodes stats API](#Nodes stats API)来计算每一个节点上的内存压力。

```text
GET _nodes/stats?filter_path=nodes.*.jvm.mem.pools.old
```

&emsp;&emsp;使用response中的内容按照下面的方式计算内存压力：

&emsp;&emsp;JVM Memory Pressure = `used_in_bytes / max_in_bytes`

- Self-managed

&emsp;&emsp;你也可以使用[nodes stats API](#Nodes stats API)来计算每一个节点上的内存压力。

```text
GET _nodes/stats?filter_path=nodes.*.jvm.mem.pools.old
```

&emsp;&emsp;使用response中的内容按照下面的方式计算内存压力：

&emsp;&emsp;JVM Memory Pressure = `used_in_bytes / max_in_bytes`

###### Check garbage collection logs

&emsp;&emsp;随着内存使用量的增加，垃圾回收器会变的更加频繁并消耗更长的时间。你可以在[elasticsearch.log](#Logging)中垃圾回收器事件的频率跟长度（the frequency and length of garbage collection events）。例如下面的例子中，Elasticsearch花费了超过一般的时间来执行垃圾回收。

```text
[timestamp_short_interval_from_last][INFO ][o.e.m.j.JvmGcMonitorService] [node_id] [gc][number] overhead, spent [21s] collecting in the last [40s]
```

##### Reduce JVM memory pressure

###### Reduce your shard count

&emsp;&emsp;每一个分片都会占用内存，大多数情况下，体积很大但是数量较小的分片比体积很小但是数量较大占用更少的资源。见[Size your shards](#Size your shards)来降低分片的数量。

###### Avoid expensive searches

&emsp;&emsp;Expensive searches会使用大量的内存。可以开启[show logs](#Slow Log)来更好的追踪集群上的expensive searches。

&emsp;&emsp;Expensive searches可能是因为使用了非常大的[size argument](#Paginate search results)，使用了大量分桶的聚合，或者包含了[expensive queries](#Allow expensive queries（Query DSL）)。若要防止expensive searches，考虑进行更改下面的设置：

- 使用索引设置[index.max_result_window](#index.max_result_window)来降低`size`的上限
- 使用集群设置[search.max_buckets](#search.max_buckets)降低允许分桶的数量
- 使用集群设置[search.allow_expensive_queries](#Allow expensive queries（Query DSL）)关闭expensive query

```text
PUT _settings
{
  "index.max_result_window": 5000
}

PUT _cluster/settings
{
  "persistent": {
    "search.max_buckets": 20000,
    "search.allow_expensive_queries": false
  }
}
```

###### Prevent mapping explosions

&emsp;&emsp;定义太多的字段，或者嵌套的字段太深会导致[mapping explosions](#Settings to prevent mapping explosion)，使得占用大量的内存。若要防止mapping explosions，使用[mapping limit settings ](#Mapping limit settings)来限制字段的数量。

###### Spread out bulk requests

&emsp;&emsp;尽管[bulk indexing ](#Bulk API)和[multi-search](#Multi search API)的请求比单独的请求有更高的效率，但是它们造成更高的JVM memory pressure。如果可以的话，提交数量较小的请求然后运行提交多次。

###### Upgrade node memory

&emsp;&emsp;繁重的索引跟查询会带来较高的JVM memory pressure。为了能更好的处理高负载，升级你的节点，提高内存容量。

#### Red or yellow cluster status

&emsp;&emsp;`red`或者`yellow`的集群状态说明一个或者多个分片missing或者未分配（unallocated）。这些为unassign的分片会带来丢失数据的风险以及降低集群性能。

##### Diagnose your cluster status

###### Check your cluster status

&emsp;&emsp;使用[cluster health API](#Cluster health API)。

```text
GET _cluster/health?filter_path=status,*_shards
```

&emsp;&emsp;一个健康的集群的颜色是`green`，并且没有`unassigned_shards`。`yellow`意味着只有副本分片没有被assign。`red`意味着一个或者多个主分片没有被assign。

###### View unassigned shards

&emsp;&emsp;若要查看unassigned分片，使用[cat shards API](#cat shards API)。

```text
GET _cat/shards?v=true&h=index,shard,prirep,state,node,unassigned.reason&s=state
```

&emsp;&emsp;unassigned的分片有`UNASSIGNED`的状态。`prirep`的值为`p`时指的是主分片，值为`r`时指的是副本分片。

&emsp;&emsp;若要明白为什么unassigned 分片没有被assign以及你应该采取行动运行Elasticsearch去assign这个分片，使用[cluster allocation explanation API](#Cluster allocation explain API)。

```text
GET _cluster/allocation/explain?filter_path=index,node_allocation_decisions.node_name,node_allocation_decisions.deciders.*
{
  "index": "my-index",
  "shard": 0,
  "primary": false,
  "current_node": "my-node"
}
```

##### Fix a red or yellow cluster status

&emsp;&emsp;一个分片不能被assign有很多的原因。下面列出了最常见的原因以及解决方案。

###### Re-enable shard allocation

&emsp;&emsp;你通常会在[restart](#Full-cluster restart and rolling restart)节点或者维护节点时关闭分配（disable allocation）。如果你忘记了重新开启它，Elasticsearch将无法assign分片。若要开启，reset集群设置`cluster.routing.allocation.enable`。

```text
PUT _cluster/settings
{
  "persistent" : {
    "cluster.routing.allocation.enable" : null
  }
}
```

###### Recover lost nodes

&emsp;&emsp;当data node离开集群后分片会无法被assign。导致这个的原因很多，从连接问题到硬件问题。当你处理完这些问题并恢复节点后，它会重新加入到集群中。Elasticsearch将随后自动的分配（allocate）那些未assign的分片。

&emsp;&emsp;为了避免资源浪费以及temporary issue，Elasticsearch会默认[delays allocation](#Delaying allocation when a node leaves)一分钟。如果你想要恢复一个节点并且不想等待延迟周期（delay period）。你可以调用不携带参数[cluster reroute API](#Cluster reroute API)来启动分配处理（allocation progress）。这个处理在后台异步处理。

```text
POST _cluster/reroute
```

###### Fix allocation settings

&emsp;&emsp;错误的配置可能会导致一个unassgin的主分片。这些设置包括：

- [Shard allocation](#Index-level shard allocation filtering) index settings
- [Allocation filtering](#Cluster-level shard allocation filtering) cluster settings
- [Allocation awareness](#Shard allocation awareness) cluster settings


&emsp;&emsp;若要查看分配设置，可以使用[get index settings](#Get index settings API)和[cluster get settings](#Cluster get settings API)。

```text
GET my-index/_settings?flat_settings=true&include_defaults=true

GET _cluster/settings?flat_settings=true&include_defaults=true
```

&emsp;&emsp;你可以使用[update index settings](#Update index settings API)和[cluster update settings](#Cluster update settings API)来更改这些设置。


###### Allocate or reduce replicas

&emsp;&emsp;为了应对硬件故障，Elasticsearch不会将副本分片和主分片分配到相同的节点上。如果没有其他data node可以存放这个副本分片，它仍然是unassign的。若要修复这个问题，你可以：

- 在相同的节点层（data tier）上添加data node
- 修改索引设置`index.number_of_replicas`来降低每一个分片的副本数量。我们建议每一个主分片至少要有一个副本。

```text
PUT _settings
{
  "index.number_of_replicas": 1
}
```

###### Free up or increase disk space

&emsp;&emsp;Elasticsearch会使用[low disk watermark](#Disk-based shard allocation settings)来保证data node 有足够的磁盘空间来处理即将生成的分片。默认情况下，Elasticsearch不会将分片分配到磁盘使用超过85%的节点上。

&emsp;&emsp;若要检查当前节点的磁盘使用情况，使用[cat allocation API](#cat allocation API)。

```text
GET _cat/allocation?v=true&h=node,shards,disk.*
```

&emsp;&emsp;如果节点的磁盘空间不足，你有下面的Optional：

- 增加节点上的磁盘空间
- 删除不需要的索引来释放磁盘空间。如果你使用了ILM，你可以更新策略，使用[searchable snapshots](#Searchable snapshot)或者添加一个delete phase。如果你不再需要搜索这些数据，你可以使用[snapshot](#Snapshot and restore)存储并脱离于集群。
- 如果你不再需要往索引中写入数据，使用[force merge API](#Force merge API)或者ILM的[force merge action](#Force merge)将段合并为大段。

```text
POST my-index/_forcemerge
```

- 如果索引是只读的，使用[shrink index API ](#Shrink index API)或者[shrink action ](#Shrink)来降低主分片的数量

```text
POST my-index/_shrink/my-shrunken-index
```

- 如果节点的磁盘空间很大，你可以提高low disk watermark或者设置一个显示的字节为单位的值。

```text
PUT _cluster/settings
{
  "persistent": {
    "cluster.routing.allocation.disk.watermark.low": "30gb"
  }
}
```

###### Reduce JVM memory pressure

&emsp;&emsp;分片的分配需要JVM heap memory。高的内存压力会导致[circuit breakers](#Circuit breaker settings)，它会停止分配使得分片无法被assign。见[High JVM memory pressure](#High JVM memory pressure)。

###### Recover data for a lost primary shard

&emsp;&emsp;如果某个节点上的主分片丢失了，Elasticsearch通常会使用另一个节点的副本分片来替代它。如果你不能恢复节点并且不存在副本分片或者它无法用于恢复。那你需要从[snapshot](#Snapshot and restore)中或者源数据（original data source）重新添加。

> WARNING：只有再也无法恢复节点的情况下才考虑这个选项。这个操作会分配一个空的主分片。如果节点随后重新加入到集群。Elasticsearch会用新的空的分片中的数据进行覆盖，导致数据丢失。

&emsp;&emsp;使用[cluster reroute API](#Cluster reroute API)手动的分配一个unassign的主分片到另一个相同node tier的data node中。设置`accept_data_loss`为`true`。

```text
POST _cluster/reroute
{
  "commands": [
    {
      "allocate_empty_primary": {
        "index": "my-index",
        "shard": 0,
        "node": "my-node",
        "accept_data_loss": "true"
      }
    }
  ]
}
```

&emsp;&emsp;如果你使用snapshot备份了丢失的索引数据，使用[ restore snapshot API](#Restore snapshot API)来恢复。或者你可以从源数据中重新索引一遍。

#### Rejected requests

&emsp;&emsp;当Elasticsearch reject一个请求，它会停止这个请求的操作并且返回`429`的响应码。请求被reject通常由下面的情况导致：

- [depleted thread pool](#High CPU usage)，已经耗尽的`search`或者`write` 线程池会返回`TOO_MANY_REQUESTS`的错误信息
- [circuit breaker error](#Circuit breaker errors)
- 高的[indexing pressure](#Indexing pressure)，超过了[indexing_pressure.memory.limit](#Memory limits)。

##### Check rejected tasks

&emsp;&emsp;可以使用[cat thread pool API](#cat thread pool API)检查每一个thread pool中reject的任务数量。特别是在`search`和`write` thread pool中`rejected` to `completed`的任务的比例很高，意味着Elasticsearch经常（regular）reject请求。

```text
GET /_cat/thread_pool?v=true&h=id,name,active,rejected,completed
```

##### Prevent rejected requests

###### Fix high CPU and memory usage

&emsp;&emsp;如果Elasticsearch经常reject请求或者其他任务，说明你的集群可能有很高的CPU或者JVM内存的使用压力。见[High CPU usage](#High CPU usage)和[High JVM memory pressure](#High JVM memory pressure)。

###### Prevent circuit breaker errors

&emsp;&emsp;如果你经常触发circuit breaker errors，见[Circuit breaker errors](#Circuit breaker errors)进行诊断并防止再次出现。

#### Task queue backlog

&emsp;&emsp;用于积压任务的队列会导致任务无法完成并让集群陷入不健康的状态。资源受限（resource constraint），触发大量的任务以及长时间运行的任务都会导致任务积压。

#####  Diagnose a task queue backlog

###### Check the thread pool status

&emsp;&emsp;[depleted thread pool](#High CPU usage)会导致[rejected requests](#Rejected requests)。

&emsp;&emsp;你可以使用[cat thread pool API](#cat thread pool API)查看每一个线程池中活跃的线程数量以及有多少个任务在排队，有多少被reject，以及有多少已经完成。

```text
GET /_cat/thread_pool?v&s=t,n&h=type,name,node_name,active,queue,rejected,completed
```

###### Inspect the hot threads on each node

&emsp;&emsp;如果某个线程发生了积压，你可以周期性的调用[Nodes hot threads](#Nodes hot threads API)来查看是否线程有足够的资源处理以及观察它处理的速度。

```text
GET /_nodes/hot_threads
```

###### Look for long running tasks

&emsp;&emsp;长时间运行的任务也会导致积压，你可以使用[task management](#Task management API) API 获取运行中的任务的信息。检查`running_time_in_nanos`来确认任务是否花费了特别长的时间。

```text
GET /_tasks?filter_path=nodes.*.tasks
```

##### Resolve a task queue backlog

###### Increase available resources

&emsp;&emsp;如果任务允许缓慢并且队列发生堆积，你可能需要采取行动来[Reduce CPU usage](#Reduce CPU usage)。

&emsp;&emsp;在一些用例中，提高thread pool size可能会有帮助。例如`force_merge` thread pool默认只有一个线程，提高两个线程有助于降低合并线程的堆积。

###### Cancel stuck tasks

&emsp;&emsp;如果你发现活跃的线程没有在处理并且发生了堆积，可以考虑取消这个任务。

### Size your shards
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/size-your-shards.html)

&emsp;&emsp;Elasticsearch中的每一个索引会被划分成一个或多个分片，每一个分片有对应的副本分片（replica），这些副本分布在多个节点上，用于应对硬件故障。如果你正在使用[Data streams](#Data streams)，每一个data stream会通过a sequence of indices实现备份。单个节点上能存储的数据量是有限制的，所以你可以通过添加节点和提高索引以及分片的数量 的方式来提高集群的承载能力。然而每一个索引和分片都是有开销的，如果你将数据划分出太多的分片，那么其开销会很大（overwhelming）。有太多索引或者分片的集群会有`oversharding`的问题。一个oversharding 的集群在响应查询方面会变得低效，极端情况下（in extreme case）会导致集群不稳定。

#### Create a sharding strategy

&emsp;&emsp;防止oversharding以及其他分片相关（shard-related）问题最好的办法 就是创建一个分片策略（ sharding strategy）。sharding strategy通过限制分片的数量来帮助你明确以及维护集群中进行优化后的分片数量。

&emsp;&emsp;不幸的是，不存在 one-size-fits-all的sharding strategy。在某个环境中工作不错的策略可能不适用于其他环境。一个好的sharding strategy必须考虑 你的基础设施，用例以及性能预期（performance expectation）。

&emsp;&emsp;创建sharding strategy最好的方式就是使用与生产环境相同的硬件，对生产数据使用跟生产中相同的查询和索引负载进行基准测试。可以查看[quantitative cluster sizing video](https://www.elastic.co/cn/elasticon/conf/2016/sf/quantitative-cluster-sizing)了解我们的推荐方式。若要测试不同的分片配置，使用Kibana的[Elasticsearch monitoring tools](https://www.elastic.co/guide/en/kibana/8.2/elasticsearch-metrics.html)来跟踪集群的稳定性和性能。

&emsp;&emsp;下面的内容提供了一些提醒（reminder）以及指导你在创建你自己的sharding strategy时需要考虑的点。如果你的集群已经有oversharding的问题，见[Reduce a cluster’s shard count](#Reduce a cluster’s shard count)。

#### Sizing considerations

&emsp;&emsp;在构建你自己的sharding strategy时，要牢记下面的内容：

##### Searches run on a single thread per shard

&emsp;&emsp;大多数的查询会命中多个分片。每一个分片上运行查询时需要一个CPU线程。一个分片上运行多个并发的查询，在大量的分片上查询会消耗（deplete） [search thread pool](#Thread pools)。这会导致吞吐量和查询速度的下降。

##### Each index, shard, segment and field has overhead

&emsp;&emsp;每一个索引和每一个分片需要内存和CPU资源。大多数情况下，数量少但是体积大的分片比数量多体积小的分片使用更少的资源。

&emsp;&emsp;段（segment）在分片的资源使用中起着重要的作用。大多数的分片包含多个段，段中存放了索引数据。Elasticsearch会在内存中保留一些segment metadata用于快速的查询。随着分片的增长，分片中的段会[merged](#Merge)成数量更少，体积更大的段。合并可以降低段的数量，意味着在堆内存中保留更少的metadata。

&emsp;&emsp;mapping中的每一个字段都会带有一些内存使用和磁盘空间的开销。默认情况下，Elasticsearch会自动创建被索引的文档中每一个字段的mapping，但是你可以通过[take control of your mappings](#Explicit mapping)来关闭这个行为。

&emsp;&emsp;并且segment会为每一个mapped field分配少量的堆内存。这个 per-segment-per-field的内存开销包括copy of the field name，使用 ISO-8859-1或者UTF-16编码。尽管它们占用的内存量不是很显著（not noticeable），但如果你的分片有很多的段，相关的mapping中包含了很多的字段或者非常长的字段名，这时候你是需要考虑它们的开销的。

##### Elasticsearch automatically balances shards within a data tier

&emsp;&emsp;集群中的所有节点会被分组到[data tiers](#Data tiers)，在每一层，Elasticsearch会尝试将索引的分片尽可能的分布到许多节点上。当你添加了 一个新节点或者一个节点发生了故障，Elasticsearch会自动的在同一层中剩余节点上进行rebalance。

#### Best practices

&emsp;&emsp;如果适用的话，使用下面的最佳实践开始创建你的sharding strategy。

##### Delete indices, not documents

&emsp;&emsp;被删除的文档不会马上从Elasticsearch的文件系统中移除。而是在每一个相关的分片上将这些文档标记为被删除的。这些被标记的文档仍然会占用资源，直到在周期性的[segment merge](#Merge)中移除。

&emsp;&emsp;如果可以的话，删除整个索引。Elasticsearch会马上从文件系统中移除被删除的索引并释放资源。

##### Use data streams and ILM for time series data

&emsp;&emsp;[Data streams](#Data streams)可以让你在多个基于时间的backing索引中存储时序数据。你可以使用[index lifecycle management](#ILM: Manage the index lifecycle)来自动的管理这些backing 索引。

&emsp;&emsp;这个设置的其中一个好处就是[automatic rollover](#Tutorial: Automate rollover with ILM)，它会在满足定义的`max_primary_shard_size`、`max_age`、`max_docs`、`max_size`阈值后创建一个新的write index。当不再需要某个索引时，你可以使用ILM自动的删除它并释放资源。

&emsp;&emsp;ILM可以让你随时轻松更改你的sharding strategy：

- **想要为新的索引降低分片数量？**
  - 在data stream的[matching index template](#Change mappings and settings for a data stream)中修改[index.number_of_shards ](#index.number_of_shards)
- **想要更大的分片或者更少的backing索引?**
  - 提高你的ILM策略中的[rollover threshold](#Rollover)
- **需要让索引之间有更短的时间跨度**
  -  通过删除旧的的索引来抵消（offset）增加的索引。你可以在策略中的[delete phase](#Index lifecycle)中降低`min_age`的阈值

&emsp;&emsp;每一个新backing index都是进一步调整策略的机会。

##### Aim for shard sizes between 10GB and 50GB

&emsp;&emsp;体积很大的分片会在发生故障后进行恢复时花费更多的时间。当一个节点发生故障，Elasticsearch会在同一层中剩余节点上进行rebalance。恢复的过程通常包括通过网络拷贝分片的内容，所以一个100G的分片会比50GB的分片花费两倍的时间。相比之下，体积小的分片会有更多的开销并且更低的查询效率。查询50个1GB的分片会比查询单个50GB包含相同内容的分片花费更多的资源。

&emsp;&emsp;尽管在分片大小上没有严格的限制，但是从经验上来说，大小在10GB和50GB之间能很好的在日志以及时序数据上工作。你可能会基于你的网络以及用例使用体积更大的分片。较小的分片可能适用于[Enterprise Search](https://www.elastic.co/guide/en/enterprise-search/8.2/index.html)和类似用例。

&emsp;&emsp;如果你使用ILM，设置[rollover action](#Rollover)的`max_primary_shard_size`阈值到`50gb`来避免分片的大小超过50GB。

&emsp;&emsp;使用[cat shards API](#cat shards API)来查看分片当前的大小。

```text
GET _cat/shards?v=true&h=index,prirep,shard,store&s=prirep,store&bytes=gb
```

&emsp;&emsp;`pri.store.size`的值显示了这个索引的所有主分片的大小总和。

```text
index                                 prirep shard store
.ds-my-data-stream-2099.05.06-000001  p      0      50gb
...
```

##### Master-eligible nodes should have at least 1GB of heap per 3000 indices

&emsp;&emsp;master node能管理的索引数量跟它的堆大小成比例（proportional ）。每一个索引对应的准确的堆内存大小取决于多个因素，例如mapping的大小和每一个索引的分片数量。

&emsp;&emsp;作为一般的经验法则（As a general rule of thumb），master node上有1GB的内存就应该拥有不大于3000个索引。例如，如果你的集群中有一个拥有4GB的dedicated master node，那你应该拥有不大于12000个索引。如果你的master node不是dedicated master node：你应该让集群中每一个master-eligible node有1GB的内存就应该拥有不大于3000个索引。

&emsp;&emsp;注意的是这个规则定义的是一个master node能管理的绝对最大数量的索引，但是不能保证在这么多数量的索引下的查询或者索引的性能。你必须保证你的data node有足够的资源满足你的工作负载，并且你整体的sharding strategy能满足你所有的性能要求。见[Searches run on a single thread per shard](#Searches run on a single thread per shard)和[Each index, shard, segment and field has overhead](#Each index, shard, segment and field has overhead)。

&emsp;&emsp;使用[cat nodes API](#cat nodes API)检查每一个节点的内存配置。

```text
GET _cat/nodes?v=true&h=heap.max
```

&emsp;&emsp;你可以使用[cat shards API](#cat shards API)检查每一个节点的分片数量。

```text
GET _cat/shards?v=true
```

##### Data nodes should have at least 1kB of heap per field per index, plus overheads

&emsp;&emsp;每一个mapped field需要的准确的资源使用量取决于字段的类型，但是一般的经验法则是允许每一个索引的每一个字段要有接近1KB的内存。你必须还要允许有足够的堆内存用于Elasticsearch最基本的用于索引，查询或者聚合所需要的内存。0.5GB的内存对于大多数的合理的工作负载都是够用的。如果你的工作负载是轻量级的那么可以需要更少的内存，反之需要更多的内存。

&emsp;&emsp;例如，如果节点中有1000个索引，每一个索引包含了4000个mapped field，那么你应该允许为这些字段分配接近1000 \* 4000 \* 1KB =  4GB的内存，以及额外的0.5GB用于工作负载和其他开销。因此这个节点需要至少4.5GB。

&emsp;&emsp;注意的是这个规则定义的是一个data node能管理的索引的绝对最大数量，但是不能保证在这么多数量的索引下的查询或者索引的性能。你必须保证你的data node有足够的资源满足你的工作负载，并且你整体的sharding strategy能满足你所有的性能要求。见[Searches run on a single thread per shard](#Searches run on a single thread per shard)和[Each index, shard, segment and field has overhead](#Each index, shard, segment and field has overhead)。

##### Avoid node hotspots

&emsp;&emsp;如果太多的分片分配到了一个指定的节点，这个节点会成为一个热点（Hotspot）节点。例如，如果单个节点包含一个索引并带有大规模的索引操作，这个索引会有太多的分片，这个节点则很有可能出现问题。

&emsp;&emsp;为了防止出现Hotspot。使用索引设置[index.routing.allocation.total_shards_per_node](#index.routing.allocation.total_shards_per_node)来显示的（explicit）限制单个节点上分片的数量。你可以使用[update index settings API](#Update index settings API)配置`index.routing.allocation.total_shards_per_node`。

```text
PUT my-index-000001/_settings
{
  "index" : {
    "routing.allocation.total_shards_per_node" : 5
  }
}
```

##### Avoid unnecessary mapped fields

&emsp;&emsp;默认情况下，Elasticsearch会为文档中的每一个字段[automatically creates a mapping](#Dynamic mapping)。每一个mapped field对应磁盘上的一些数据结构，使得用于高效的查询，检索以及聚合。Details about each mapped field are also held in memory。在很多用例中这种开销是不需要的因为有些字段没有用于查询或者聚合。使用[Explicit mapping](#Explicit mapping)替换dynamic mapping来防止创建从不会被使用的字段。如果有些字段通常一起使用，考虑在索引阶段使用[copy_to](#copy_to)。如果一个字段很少被使用，使用[Runtime field](#Runtime fields)可能更合适。

&emsp;&emsp;你可以通过[Field usage stats API](#Field usage stats API)获取哪些字段正在被使用的信息，并且你可以通过[Analyze index disk usage API](#Analyze index disk usage API)分析mapped field的磁盘使用。注意的是，不必要的mapped field也会带来一些内存开销以及它们的磁盘使用。

#### Reduce a cluster’s shard count

&emsp;&emsp;如果集群中已经oversharded了，你可以使用下面的一个或者多个方法来减少分片数量。

##### Create indices that cover longer time periods

&emsp;&emsp;如果你使用了ILM并且retention policy允许的话，不使用`max_age`阈值用于rollover action，而是使用`max_primary_shard_size`来避免创建空索引或者许多小的分片。

&emsp;&emsp;如果你的retention policy要求`max_age`阈值，提高阈值来处理longer time interval。例如你可以按周或者按月创建索引而不是按天创建。

##### Delete empty or unneeded indices

&emsp;&emsp;如果你正在使用ILM并且基于`max_age`阈值对索引roll over，你可能会在不经意间（inadvertently）创建没有文档的索引。这些空的索引没有什么用但仍然会消耗资源。

&emsp;&emsp;你可以使用[ cat count API](#cat count API)找出空的索引。

```text
GET _cat/count/my-index-000001?v=true
```

&emsp;&emsp;一旦有了空的索引列表，你可以使用[delete index API](#Delete index API)删除它们。你也可以删除不再与需要的索引。

```text
DELETE my-index-000001
```

##### Force merge during off-peak hours

&emsp;&emsp;如果你不再往一个索引中写入数据，你可以使用[force merge API](#Force merge API)将小段[merge](#Merge)到大段中。 这样能降低分片开销并能提高查询速度。然而force merge是一种资源密集型（resource-intensive）的操作，如果可以的话，尽量在非高峰期（off-peak）运行。

```text
POST my-index-000001/_forcemerge
```

##### Shrink an existing index to fewer shards

&emsp;&emsp;如果你不再往一个索引中写入数据，你可以使用[shrink index API](#Shrink index API) 来降低分片数量。

&emsp;&emsp;ILM同样在warm阶段有[shrink action](#Shrink)。

##### Combine smaller indices

&emsp;&emsp;你也可以使用[reindex API ](#Reindex API)将mapping类似的索引合并到一个更大的索引中。对于时序数据，你可以将短时间周期（short time period）的索引重新索引到长时间周期的索引中。类，例如你可以基于索引名模板重新索引按天索引的数据，例如`my-index-2099.10.11`重新索引到按月索引的`my-index-2099.10`中。reindex之后，删除按月索引的索引。

```text
POST _reindex
{
  "source": {
    "index": "my-index-2099.10.*"
  },
  "dest": {
    "index": "my-index-2099.10"
  }
}
```

#### Troubleshoot shard-related errors

&emsp;&emsp;下面介绍的是常见的分片相关的错误的解决办法。

##### this action would add [x] total shards, but this cluster currently has [y]/[z] maximum shards open;

&emsp;&emsp; 集群的设置[cluster.max_shards_per_node](#cluster.max_shards_per_node)限制了集群中可以打开的分片的最大数量。这个错误说的是某个行为会超过这个限制。

&emsp;&emsp;如果你有信心保证你的更改不会降低集群的稳定性，你可以通过[cluster update settings API](#Cluster update settings API)临时的提高限制然后重新这个行为。.

```text
PUT _cluster/settings
{
  "persistent" : {
    "cluster.max_shards_per_node": 1200
  }
}
```

&emsp;&emsp;这种解决方式只能是临时解决方案。作为一个长期解决方案，我们建议在发生oversharded的数据层添加节点或者[reduce your cluster’s shard count](#Reduce a cluster’s shard count)。在做出变更后可以通过[ cluster stats API](#Cluster stats API)查看当前的分片数量。

```text
GET _cluster/stats?filter_path=indices.shards.total
```

&emsp;&emsp;当部署了一个长期解决方案后，我们建议你reset `cluster.max_shards_per_nod`。

```text
PUT _cluster/settings
{
  "persistent" : {
    "cluster.max_shards_per_node": null
  }
}
```

### Use Elasticsearch for time series data
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/use-elasticsearch-for-time-series-data.html#search-visualize-your-data)

&emsp;&emsp;Elasticsearch提供的功能能帮助你存储，管理，以及搜索时序数据，例如日志和指标。使用了Elasticsearch后，你可以使用Kibana和其他Elastic Stack来分析以及可视化你的数据。

#### Set up data tiers

&emsp;&emsp;Elasticsearch的[ILM](#ILM: Manage the index lifecycle)使用[data tiers](#Data tiers)根据索引的寿命（age）自动的将旧的数据移动到成本较低的硬件中。这有助于提高性能以及降低存储开销。

&emsp;&emsp;hot和content层是必须要有的，而warm，cold以及frozen 层是可选的。

&emsp;&emsp;在hot和warm层使用高性能的节点用于快速的索引和查询最近的数据。在cold和frozen层使用较慢，成本较低的节点来降低开销。

&emsp;&emsp;设置datatiers的步骤基于你的部署类型：

- Elasticsearch Service
  - 登陆到[Elasticsearch Service Console](https://www.elastic.co/cn/cloud/elasticsearch-service/signup?baymax=docs-body&elektra=docs)
  - 在Elasticsearch Service 主页或者部署页面添加或者选择你的部署
  - 从你的部署菜单中，选择**Edit deployment**
  - 若要开启一个data tier，点击**Add capacity**
  - Enable autoscaling
    - [Autoscaling](https://www.elastic.co/guide/en/cloud/current/ec-autoscaling.html)能自动的调整你的部署的capacity来满足你的存储需要。若要开启autoscaling，在**Edit deployment**页面选择**Autoscale this deployment**。Autoscaling只会Elasticsearch Service可见
  
- Self-managed
  - 在节点的`elasticsearch.ym`文件中添加对应的[node role](#Node roles)将这个节点分配到某个data tier。修改节点现有的节点角色要求[rolling restart](#Rolling restart)

```text
# Content tier
node.roles: [ data_content ]

# Hot tier
node.roles: [ data_hot ]

# Warm tier
node.roles: [ data_warm ]

# Cold tier
node.roles: [ data_cold ]

# Frozen tier
node.roles: [ data_frozen ]
```

&emsp;&emsp;我们建议你在frozen tier使用专门的节点。如果有需要的话，你可以给一个节点赋予多个data tier。

```text
node.roles: [ data_content, data_hot, data_warm ]
```

&emsp;&emsp;为你集群中的节点分配其他的角色。例如，一个小规模的集群中一个节点可以有多个角色。

```text
node.roles: [ master, ingest, ml, data_hot, transform ]
```

#### Register a snapshot repository ()

&emsp;&emsp;cold和frozen层可以使用[searchable snapshots](#Searchable snapshots)来降低本地储存开销。

&emsp;&emsp;若要使用searchable snapshots，你必须要注册一个支持的snapshots repository。注册的步骤取决于你的部署和提供的存储而有所不同：

##### Elasticsearch Service
  - 当你创建一个集群，Elasticsearch Service自动的注册一个默认的[found-snapshots repository](https://www.elastic.co/guide/en/cloud/current/ec-snapshot-restore.html)。这个仓库支持searchable snapshots。
  - `found-snapshots`仓库特定于你的集群。见[Share a repository across clusters](https://www.elastic.co/guide/en/cloud/current/ec_share_a_repository_across_clusters.html)使用其他集群默认的仓库。
  - 你也可以下列自定义的仓库类型用于searchable repository：
    - [Google Cloud Storage (GCS)](https://www.elastic.co/guide/en/cloud/current/ec-gcs-snapshotting.html)
    - [Azure Blob Storage](https://www.elastic.co/guide/en/cloud/current/ec-azure-snapshotting.html)
    - [Amazon Web Services (AWS)](https://www.elastic.co/guide/en/cloud/current/ec-aws-custom-repository.html)

##### Self-managed
  - 使用下列任意的仓库用于searchable repository：
    - [AWS S3](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/repository-s3.html)
    - [Google Cloud Storage](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/repository-gcs.html)
    - [Azure Blob Storage](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/repository-azure.html)
    - [Hadoop Distributed File Store (HDFS)](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/repository-hdfs.html)
    - [Shared filesystems](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/snapshots-filesystem-repository.html) such as NFS
    - [Read-only HTTP and HTTPS repositories](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/snapshots-read-only-repository.html)

  - You can also use alternative implementations of these repository types, for instance MinIO, as long as they are fully compatible. Use the Repository analysis API to analyze your repository’s suitability for use with searchable snapshots

#### Create or edit an index lifecycle policy

&emsp;&emsp;[data stream](#Data streams)在多个backing索引中存储你的数据。ILM使用一个[index lifecycle policy](#Index lifecycle)自动的将这些索引移动到你的data tier。

&emsp;&emsp;如果你使用Fleet或者Elastic Agent，可以编辑Elasticsearch内置的lifecycle policy。如果你使用自定义的应用，则需要创建你自己的策略：

- 包括你配置的每一个data tier的阶段内容
- 从rollover中为阶段转换（phase transition）计算阈值，或`min_age`
- 如果需要的话，在cold和frozen阶段使用searchable snapshots
- 如果需要的话，包含一个delete 阶段

##### Fleet or Elastic Agent

&emsp;&emsp;Fleet和Elastic Agent使用下列内置的生命周期策略“

- logs
- metrics
- synthetics

&emsp;&emsp;你可以基于你自己的性能，弹性（resilience）以及保留需求（retention requirement）自定义策略。

&emsp;&emsp;若要在Kibana中编辑一个策略，打开主菜单然后跳转到**Stack Management > Index Lifecycle Policies**，点击你想要编辑的策略。

&emsp;&emsp;你可以使用[update lifecycle policy API](#Create or update lifecycle policy API)。

```text
PUT _ilm/policy/logs
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_primary_shard_size": "50gb"
          }
        }
      },
      "warm": {
        "min_age": "30d",
        "actions": {
          "shrink": {
            "number_of_shards": 1
          },
          "forcemerge": {
            "max_num_segments": 1
          }
        }
      },
      "cold": {
        "min_age": "60d",
        "actions": {
          "searchable_snapshot": {
            "snapshot_repository": "found-snapshots"
          }
        }
      },
      "frozen": {
        "min_age": "90d",
        "actions": {
          "searchable_snapshot": {
            "snapshot_repository": "found-snapshots"
          }
        }
      },
      "delete": {
        "min_age": "735d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

##### Custom application

&emsp;&emsp;若要在Kibana中创建一个策略，打开主菜单然后跳转到**StackManagement > Index Lifecycle policies**。点击 **Create policy**。

&emsp;&emsp;你可以使用[update lifecycle policy API](#Create or update lifecycle policy API)。

```text
PUT _ilm/policy/my-lifecycle-policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_primary_shard_size": "50gb"
          }
        }
      },
      "warm": {
        "min_age": "30d",
        "actions": {
          "shrink": {
            "number_of_shards": 1
          },
          "forcemerge": {
            "max_num_segments": 1
          }
        }
      },
      "cold": {
        "min_age": "60d",
        "actions": {
          "searchable_snapshot": {
            "snapshot_repository": "found-snapshots"
          }
        }
      },
      "frozen": {
        "min_age": "90d",
        "actions": {
          "searchable_snapshot": {
            "snapshot_repository": "found-snapshots"
          }
        }
      },
      "delete": {
        "min_age": "735d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

#### Create component templates

> TIP：如果你使用Fleet或者Elastic Agent，跳过[Search and visualize your data](#Search and visualize your data)。Fleet和Elastic Agent使用内置的模板为你创建data streams

&emsp;&emsp;如果你使用自定义的应用，你需要设置你自己的data stream。data stream要求用于匹配的index template。大多数情况下，你会使用一个或多个component template组合成index template。你通常为mapping和index setting使用不同的component template。这可以让你在多个index template中复用component template。

&emsp;&emsp;当你创建component template，包括：

- 为`@timestamp`字段使用[date](#Date field type)或者[date_nanos](#Date nanoseconds field type)。如果你不指定mapping，Elasticsearch默认为`@timestamp`使用`date`字段。
- 索引设置中`index.lifecycle.name` 设置生命周期策略

> TIP：当对字段进行映射时，使用[Elastic Common Schema (ECS)](https://www.elastic.co/guide/en/ecs/8.2/index.html)。ECS fields integrate with several Elastic Stack features by default。
> 
> 如果你不确定如何映射字段，使用[runtime fields](#Define runtime fields in a search request)在查询期间从[unstructured content ](#Wildcard field type
)中提取字段。例如你可以索引一条日志消息为`wildcard`，随后在查询中从这个字段中提取出IP address和其他数据。

&emsp;&emsp;若要在Kibana中创建一个component template，那么打开主菜单然后跳转到**Stack Management > Index Management**。在**Index Templates**视图中，点击**Create component template**。

&emsp;&emsp;你也可以使用[create component template API](#Create or update component template API)。

```text
# Creates a component template for mappings
PUT _component_template/my-mappings
{
  "template": {
    "mappings": {
      "properties": {
        "@timestamp": {
          "type": "date",
          "format": "date_optional_time||epoch_millis"
        },
        "message": {
          "type": "wildcard"
        }
      }
    }
  },
  "_meta": {
    "description": "Mappings for @timestamp and message fields",
    "my-custom-meta-field": "More arbitrary metadata"
  }
}

# Creates a component template for index settings
PUT _component_template/my-settings
{
  "template": {
    "settings": {
      "index.lifecycle.name": "my-lifecycle-policy"
    }
  },
  "_meta": {
    "description": "Settings for ILM",
    "my-custom-meta-field": "More arbitrary metadata"
  }
}
```

#### Create an index template

&emsp;&emsp;使用你的component templates来创建一个index template：

- 一个或多个index pattern满足data stream的名称，我们建议你使用我们的[data stream naming scheme](https://www.elastic.co/guide/en/fleet/8.2/data-streams.html#data-streams-naming-scheme)
- 这个模版是data stream可以使用的（用`data_stream`字段表示该模版可以用于数据流）
- 包含你mapping跟索引设置Index settings的组件模版
- 优先级大于`200`，以避免与内置模板发生冲突。见[Avoid index pattern collisions](#Avoid index pattern collisions))

&emsp;&emsp;若要在Kibana中创建一个index template，那么打开主菜单然后跳转到**Stack Management > Index Management**。在**Index Templates**视图中，点击**Create template**。

&emsp;&emsp;你也可以使用[create index template API](#Create or update index template API)。包含`data_stream` object可以模版用于data streams。

```text
PUT _index_template/my-index-template
{
  "index_patterns": ["my-data-stream*"],
  "data_stream": { },
  "composed_of": [ "my-mappings", "my-settings" ],
  "priority": 500,
  "_meta": {
    "description": "Template for my time series data",
    "my-custom-meta-field": "More arbitrary metadata"
  }
}
```

####  Add data to a data stream

&emsp;&emsp;[Indexing requests](#Add documents to a data stream)将添加文档到data stream中。这些请求必须是使用`create`的`op_type`。文档中必须包含一个`@timestamp`字段。

&emsp;&emsp;若要自动的创建你的data stream，指定stream的名称并提交一个索引请求。这个名称必须匹配你的index template的index patterns中的一个。

```text
PUT my-data-stream/_bulk
{ "create":{ } }
{ "@timestamp": "2099-05-06T16:21:15.000Z", "message": "192.0.2.42 - - [06/May/2099:16:21:15 +0000] \"GET /images/bg.jpg HTTP/1.0\" 200 24736" }
{ "create":{ } }
{ "@timestamp": "2099-05-06T16:25:42.000Z", "message": "192.0.2.255 - - [06/May/2099:16:25:42 +0000] \"GET /favicon.ico HTTP/1.0\" 200 3638" }

POST my-data-stream/_doc
{
  "@timestamp": "2099-05-06T16:21:15.000Z",
  "message": "192.0.2.42 - - [06/May/2099:16:21:15 +0000] \"GET /images/bg.jpg HTTP/1.0\" 200 24736"
}
```

#### Search and visualize your data

&emsp;&emsp;若要在Kibana中展示并且查询你的数据。打开主菜单然后选择**Discover**。见Kibana的[Discover documentation](https://www.elastic.co/guide/en/kibana/8.2/discover.html)。

&emsp;&emsp;使用Kibana的**Dashboard**功能，在图表、表格、地图中可视化你的数据。见Kibana的[Discover documentation](https://www.elastic.co/guide/en/kibana/8.2/discover.html)。

&emsp;&emsp;你也可以使用[search API](#Search API)查询或者聚合你的数据。使用[runtime fields ](#Define runtime fields in a search request)和[grok patterns ](#Grok basics)自动的在查询阶段从日志消息和其他非结构化数据中提取数据。

```text
GET my-data-stream/_search
{
  "runtime_mappings": {
    "source.ip": {
      "type": "ip",
      "script": """
        String sourceip=grok('%{IPORHOST:sourceip} .*').extract(doc[ "message" ].value)?.sourceip;
        if (sourceip != null) emit(sourceip);
      """
    }
  },
  "query": {
    "bool": {
      "filter": [
        {
          "range": {
            "@timestamp": {
              "gte": "now-1d/d",
              "lt": "now/d"
            }
          }
        },
        {
          "range": {
            "source.ip": {
              "gte": "192.0.2.0",
              "lte": "192.0.2.255"
            }
          }
        }
      ]
    }
  },
  "fields": [
    "*"
  ],
  "_source": false,
  "sort": [
    {
      "@timestamp": "desc"
    },
    {
      "source.ip": "desc"
    }
  ]
}
```

&emsp;&emsp;Elasticsearch的查询默认是同步操作。跨frozen data，long time ranges，large datasets这些查询可能需要较长时间。可以使用[async search API ](#Submit async search API)在后台允许这些查询。更多的查询选项见[Search your data](#Search your data)。

```text
POST my-data-stream/_async_search
{
  "runtime_mappings": {
    "source.ip": {
      "type": "ip",
      "script": """
        String sourceip=grok('%{IPORHOST:sourceip} .*').extract(doc[ "message" ].value)?.sourceip;
        if (sourceip != null) emit(sourceip);
      """
    }
  },
  "query": {
    "bool": {
      "filter": [
        {
          "range": {
            "@timestamp": {
              "gte": "now-2y/d",
              "lt": "now/d"
            }
          }
        },
        {
          "range": {
            "source.ip": {
              "gte": "192.0.2.0",
              "lte": "192.0.2.255"
            }
          }
        }
      ]
    }
  },
  "fields": [
    "*"
  ],
  "_source": false,
  "sort": [
    {
      "@timestamp": "desc"
    },
    {
      "source.ip": "desc"
    }
  ]
}
```

## REST APIs
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rest-apis.html)

### API conventions
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/api-conventions.html#time-units)

#### Date math support in index and index alias names

#### Multi-target syntax

##### ignore_unavailable

##### Hidden data streams and indices

##### System indices

##### allow_no_indices

#### Byte size units

#### Time units

### Common options
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/common-options.html#date-math)

#### Date Math

#### Response Filtering

#### Fuzziness（Common options）

### Compact and aligned text (CAT) APIs
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rest-apis.html)

#### cat fielddata API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cat-fielddata.html)

#### cat indices API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cat-indices.html)

#### cat nodes API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cat-nodes.html)


> IMPORTANT：cat APIs只能在命令行或者Kibana控制台中人为使用。不应该在应用中使用。若要在应用中使用，则使用[nodes info API](#Nodes info API)

&emsp;&emsp;返回集群中节点的信息。

##### Request

`GET /_cat/nodes`

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`monitor`或者`manage`的[cluster privilege](#Cluster privileges)来使用这个API。

##### Query parameter

- bytes：（Optional,[byte size units](#Byte size units)）展示字节值的单位。
- format：（Optional, string）[HTTP accept header](https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html)的short version。可选值包括JSON、YAML等等。
- full_id：（Optional, Boolean）如果为`true`，返回完整的node ID。如果为`false`，返回较短的node ID，默认值为`false`。
- h：（Optional, string）需要展示的列名，使用逗号分隔。
  - 如果你不指定列名，API会列出默认列。如果你显示的（explicit）指定一个或者更多的列，那只返回指定的列。
  - 可选的列包括：
    - ip、i：（default）IP地址，比如`127.0.1.1`
    - heap.percent、hp、heapPercent：（default）配置的最大堆内存，比如`7`
    - heap.max、hm、heapMax：（default）堆内存总量，比如`4gb`
    - ram.percent、rp、ramPercent：（default）已使用的内存百分比，比如`47`
    - file_desc.percent、fdp、fileDescriptorPercent：（default）已使用的文件描述符的百分比，比如``
    - node.role、r、role、nodeRole：（default）节点的角色，返回的值包括:
      - `c`（cold node）
      - `d`（data node）
      - `f`（frozen node）
      - `h`（hot node）
      - `i`（ingest node）
      - `l`（machine learning node）
      - `m`（master-eligiable node）
      - `r`（remote cluster client node）
      - `s`（content node）
      - `t`（transform node）
      - `v`（voting-only node）
      - `w`（warm node）
      - `-` （coordinating node only）
      - 比如说，`dim`说的是master-eligiable、data、ingest node。见[Node](#Node)。
    - master、m：（default）节点是否为elected master node。返回的值有
      - `*`（elected master）
      - `-`（not elected master）
    - name、n：（default）节点名称，比如`I8hydUG`
    - id、nodeId：节点的ID，比如`k0zy`
    - pid、p：进程ID、比如`13061`
    - port、po：绑定的transport port、比如`9300`
    - http_address、http：绑定的http地址，比如`127.0.0.0:9200`
    - version、v：Elasticsearch的版本，比如`8.2.3`
    - build，d：构建Elasticsearch的哈希值，比如`5c03844`
    - jdk、j：Java版本，比如`1.8.0`
    - disk.total、dt、diskTotal：磁盘总空间，比如`458.3gb`
    - disk.used、du、diskUsed：已使用的磁盘空间、比如`259.8gb`
    - disk.avail、d、disk、diskAvail：可用的磁盘空间，比如`198.4gb`
    - disk.used_percent、dup、diskUsedPercent：已使用的磁盘空间百分比，比如`47`
    - heap.current、hc、heapCurrent：Used heap, such as `311.2mb`
    - ram.current、rc、ramCurrent：Used total memory, such as `513.4mb`
    - ram.max、rm、ramMax：内存总量、比如`2.9gb`
    - file_desc.current、fdc、fileDescriptorCurrent：已使用的文件描述符数量，比如`123`
    - file_desc.max、fdm、fileDescriptorMax：文件描述符数量最大值，比如`2014`
    - cpu：最近CPU使用量百分比，比如`12`
    - load_1m、l：最近的负载均值，比如`0.22`
    - load_5m、l：最近5分钟的负载均值，比如`0.78`
    - load_15m、l：最近15分钟的负载均值，比如`1.24`
    - uptime、u：节点在线时长，比如`17.3m`
    - completion.size、cs、completionSize：Size of completion, such as 0b.
    - fielddata.memory_size、fm、fielddataMemory：[fielddata cache](#Field data cache settings)的内存占用量，比如`0b`
    - fielddata.evictions、fe、fielddataEvictions：清除掉的fielddata cache的内存量，比如`0`
    - query_cache.memory_size、qcm、queryCacheMemory：查询缓存（query cache）占用的内存量，比如`0b`
    - query_cache.evictions、qce、queryCacheEvictions：清除掉的query cache的内存量，比如`0`
    - query_cache.hit_count、qchc、queryCacheHitCount：命中query cache的次数，比如`0`
    - query_cache.miss_count、qcmc、queryCacheMissCount：未命中query cache的次数，比如`0`
    - request_cache.memory_size、rcm、requestCacheMemory：request cache已使用的内存量，比如`0b`
    - request_cache.evictions、rce、requestCacheEvictions：清除掉的request cache缓存量，比如`0`
    - request_cache.hit_count、rchc、requestCacheHitCount：命中request cache的次数，比如`0`
    - request_cache.miss_count、rcmc、requestCacheMissCount：request cache未命中request cache的次数，比如`0：
    - flush.total、 ft、flushTotal：[flush](#Flush API)的次数，比如`1`
    - flush.total_time、ftt、flushTotalTime：flush操作花费的时间，比如`1`
    - get.current、gc、getCurrent：当前GET操作的数量，比如`0`
    - get.time、gti、getTime：GET操作花费的时间，比如`14ms`
    - get.total、gto、getTotal：GET操作的数量，比如`2`
    - get.exists_time、 geti、getExistsTime：GET操作成功花费的时间，比如`14ms`
    - get.exists_total、geto、getExistsTotal：GET操作成功的数量，比如`2`
    - get.missing_time、gmti、getMissingTime：GET操作失败花费的时间，比如`0s`
    - get.missing_total、gmto、getMissingTotal：GET操作失败的数量，比如`1`
    - indexing.delete_current、idc、indexingDeleteCurrent：当前DELETE操作的数量，比如`0`
    - indexing.delete_time、idti、indexingDeleteTime：DELETE操作花费的时间，比如`2ms`
    - indexing.delete_total、idto、indexingDeleteTotal：DELETE操作的数量，比如`2`
    - indexing.index_current、iic、indexingIndexCurrent：当前索引操作的数量，比如`0`
    - indexing.index_time、iiti、indexingIndexTime：索引操作花费的时间。比如`134ms`
    - indexing.index_total、iito、indexingIndexTotal：索引操作的数量，比如`1`
    - indexing.index_failed、iif、indexingIndexFailed：索引操作失败的数量，比如`0`
    - merges.current、mc、mergesCurrent：当前merge操作的数量，比如`0`
    - merges.current_docs、mcd、mergesCurrentDocs：当前merge操作中文档的数量，比如`0`
    - merges.current_size、mcs、mergesCurrentSize：当前merge的大小，比如`0b`
    - merges.total、mt、mergesTotal：完成merge操作的数量，比如`0`
    - merges.total_docs、mtd、mergesTotalDocs：完成合并的文档数量，比如`0`
    - merges.total_size、mts、mergesTotalSize：Size of current merges, such as `0b`.（怎么跟merges.current_size、mcs、mergesCurrentSize是一样的？）
    - merges.total_time、mtt、mergesTotalTime：合并文档花费的时间，比如`0s`
    - refresh.total、rto、refreshTotal：[refresh](#Refresh API)的数量，比如`16`
    - refresh.time、rti、refreshTime：refresh花费的时间，比如`91ms`
    - script.compilations、scrcc、scriptCompilations：script compilations的次数、比如`17`
    - script.cache_evictions、scrce、scriptCacheEvictions：通过cache进行script compilations的次数、比如`6`
    - search.fetch_current、sfc、searchFetchCurrent：当前fetch阶段操作，比如`0`
    - search.fetch_time、sfti、searchFetchTime：fetch阶段花费的时间，比如`37ms`
    - search.fetch_total、sfto、searchFetchTotal：fetch操作的数量，比如`7`
    - search.open_contexts、so、searchOpenContexts：已打开的search context的数量，比如`0`
    - search.query_current、sqc、searchQueryCurrent：当前query阶段的操作，比如`0`
    - search.query_time、sqti、searchQueryTime：query阶段花费的时间，比如`43ms`
    - search.query_total、sqto、searchQueryTotal：query操作的数量，比如`9`
    - search.scroll_current、scc、searchScrollCurrent：已打开的scroll context的数量，比如`2`
    - search.scroll_time、scti、searchScrollTime：已打开的scroll context的保留时间，比如`2m`
    - search.scroll_total、scto、searchScrollTotal：已完成的scroll context的数量，比如`1`
    - segments.count、sc、segmentsCount：段的数量，例如`4`
    - segments.memory、sm、segmentsMemory：段的内存使用量，例如`4`
    - segments.index_writer_memory、siwm、segmentsIndexWriterMemory：index writer的内存使用量，例如`1.4kb`
    - segments.version_map_memory、svmm、segmentsVersionMapMemory：版本映射（version map）的内存使用量，例如`1.0kb`
    - segments.fixed_bitset_memory、sfbm、fixedBitsetMemory：[nested object](#Nested field type)类型的字段和[join](#Join field type)字段中类型应用的类型过滤使用的固定比特位的内存使用量，例如`1.0kb`
    - suggest.current、suc、suggestCurrent：当前suggest操作的数量，比如`0`
    - suggest.time、suti、suggestTime：suggest花费的时间，例如`0`
    - suggest.total、suto、suggestTotal：suggest操作的数量，比如`0`

- help：（Optional, Boolean）如果为`true`。响应中包含帮助信息。默认值为`false`。
- master_timeout：（Optional, [time units](#Time units)）周期性的等待连接master node。如果在超时前未收到响应，则请求失败并且返回一个错误。默认是`30s`。
- s：（Optional, string）用逗号隔开的列名或者列的别名对响应进行排序。
- time：（Optional, [time units](#Time units)）展示时间使用的时间单位。
- v：（Optional, Boolean）如果为`true`，响应中会包含列名。默认为`false`。
- include_unloaded_segments：（Optional, Boolean）如果为`true`，响应中会包含未载入到内存的段的信息。默认为`false`。

##### Examples

###### Example with default columns

```text
GET /_cat/nodes?v=true
```

&emsp;&emsp;API返回下面的响应：

```text
ip        heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
127.0.0.1           65          99  42    3.07                  dim       *      mJw06l1
```

&emsp;&emsp;`ip`、`heap.percent`、`ram.percent`、`cpu`和`load_*`这个几列提供了每一个节点的IP地址跟性能信息。

&emsp;&emsp;`node.role`、`master`、`name`这几列提供了整个集群的有用信息，特别是在规模较大的集群中。


###### Example with explicit columns

&emsp;&emsp;下面的API请求会返回`id`、`ip`、`port`、`v`（version）以及`m`（master）这几列。

```text
GET /_cat/nodes?v=true&h=id,ip,port,v,m
```

&emsp;&emsp;API返回下面的响应：

```text
id   ip        port  v         m
veJR 127.0.0.1 59938 8.2.3 *
```

#### cat recovery API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cat-recovery.html)

#### cat shards API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cat-shards.html)

> IMPORTANT：cat APIs只能在命令行或者Kibana控制台中人为使用。不应该在应用中使用。

&emsp;&emsp;`shard`命令详细的展示了哪些节点包含了哪些分片，它能告诉你是主分片还是副本分片，文档的数量，磁盘占用的字节数以及分片在哪个节点上。

&emsp;&emsp;对于data stream，API会返回stream中[backing Index](#Backing indices)的信息。

##### Request

`GET /_cat/shards/<target>`
`GET /_cat/shards`

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`monitor`或者`manage`的[cluster privilege](#Cluster privileges)来使用这个API。你也必须要有`monitor`或者`manage` [index privilege](#Indices privileges)用于你需要获取的data stream、Index、或者alias。

##### Path parameters

- `<target>`：（Optional, string）用逗号隔开的一个或多个data stream、Index或者alias。支持通配符(\*)。若要查询所有的data stream、Index，则不指定这个参数或者使用`*`、`_all`。

##### Query parameter

- bytes：（Optional,[byte size units](#Byte size units)）展示字节值的单位。

- format：（Optional, string）[HTTP accept header](https://www.w3.org/Protocols/rfc2616/rfc2616-sec14.html)的short version。可选值包括JSON、YAML等等。

- h：（Optional, string）需要展示的列名，使用逗号分隔。
  - 如果你不指定列名，API会列出默认列。如果你显示的（explicit）指定一个或者更多的列，那只返回指定的列。
  - 可选的列包括：
    - index、i、idx：（default）索引的名称
    - shard、s、sh：（default）分片的名称
    - prirep、p、pr、primaryOrReplica：（default）分片类型，返回值为`primary`或者`replica`
    - state、st：（default）分片状态。返回的值有：
      - `INITIALIZING`：分片正在从对等节点（peer shard）或者gateway中恢复
      - `RELOCATING`：分片正在分配中（relocating）
      - `STARTED`：分片已启用（started）
      - `UNASSIGNED`：分片还未分配给任何一个节点
    - docs、d、dc：（default）分片中文档的数量，比如`25`
    - store、sto：（default）分片的磁盘使用量，比如`5kb`
    - ip：（default）节点的IP地址，比如`127.0.1.1`
    - id：（default）节点的ID，比如`k0zy`
    - node、n：（default）节点的名称，比如`I8hydUG`
    - completion.size、cs、completionSize：Size of completion, such as 0b.
    - fielddata.memory_size、fm、fielddataMemory：[fielddata cache](#Field data cache settings)的内存占用量，比如`0b`
    - fielddata.evictions、fe、fielddataEvictions：清除掉的fielddata cache的内存量，比如`0`
    - flush.total、 ft、flushTotal：[flush](#Flush API)的次数，比如`1`
    - flush.total_time、ftt、flushTotalTimeflush操作花费的时间，比如`1`
    - get.current、gc、getCurrent当前GET操作的数量，比如`0`
    - get.time、gti、getTimeGET操作花费的时间，比如`14ms`
    - get.total、gto、getTotal：GET操作的数量，比如`2`
    - get.exists_time、 geti、getExistsTime：GET操作成功花费的时间，比如`14ms`   
    - get.exists_total、geto、getExistsTotal：GET操作成功的数量，比如`2`
    - get.missing_time、gmti、getMissingTime：GET操作失败花费的时间，比如`0s`
    - get.missing_total、gmto、getMissingTotal：GET操作失败的数量，比如`1`
    - indexing.delete_current、idc、indexingDeleteCurrent：当前DELETE操作的数量，比如`0`
    - indexing.delete_time、idti、indexingDeleteTime：DELETE操作花费的时间，比如`2ms`
    - indexing.delete_total、idto、indexingDeleteTotal：DELETE操作的数量，比如`2`
    - indexing.index_current、iic、indexingIndexCurrent：当前索引（indexing）操作的数量，比如`0`
    - indexing.index_time、iiti、indexingIndexTime：索引操作花费的时间，比如`134ms`
    - indexing.index_total、iito、indexingIndexTotal：索引操作的数量，比如`1`
    - indexing.index_failed、iif、indexingIndexFailed：索引操作失败的数量，比如`0`
    - merges.current、mc、mergesCurrent：当前merge操作的数量，比如`0`
    - merges.current_docs、mcd、mergesCurrentDocs：当前merge操作中文档的数量，比如`0`
    - merges.current_size、mcs、mergesCurrentSize：当前merge的大小，比如`0b`
    - merges.total、mt、mergesTotal：完成merge操作的数量，比如`0`
    - merges.total_docs、mtd、mergesTotalDocs：完成合并的文档数量，比如`0`
    - merges.total_size、mts、mergesTotalSize：Size of current merges, such as `0b`.（怎么跟merges.current_size、mcs、mergesCurrentSize是一样的？）
    - merges.total_time、mtt、mergesTotalTime：合并文档花费的时间，比如`0s`
    - query_cache.memory_size、qcm、queryCacheMemory：查询缓存（query cache）占用的内存量，比如`0b`
    - query_cache.evictions、qce、queryCacheEvictions：清除掉的查询缓存的内存量，比如`0`
    - recoverysource.type、rs：Type of recovery source
    - refresh.total、rto、refreshTotal：[refresh](#Refresh API)的数量，比如`16`
    - refresh.time、rti、refreshTime：refresh花费的时间，比如`91ms`
    - search.fetch_current、sfc、searchFetchCurrent：当前fetch阶段操作，比如`0`
    - search.fetch_time、sfti、searchFetchTime：fetch阶段花费的时间，比如`37ms`
    - search.fetch_total、sfto、searchFetchTotal：fetch操作的数量，比如`7`
    - search.open_contexts、so、searchOpenContexts：已打开的search context的数量，比如`0`
    - search.query_current、sqc、searchQueryCurrent：当前query阶段的操作，比如`0`
    - search.query_time、sqti、searchQueryTime：query阶段花费的时间，比如`43ms`
    - search.query_total、sqto、searchQueryTotal：query操作的数量，比如`9`
    - search.scroll_current、scc、searchScrollCurrent：已打开的scroll context的数量，比如`2`
    - search.scroll_time、scti、searchScrollTime：已打开的scroll context的保留时间，比如`2m`
    - search.scroll_total、scto、searchScrollTotal：已完成的scroll context的数量，比如`1`
    - segments.count、sc、segmentsCount：段的数量，例如`4`
    - segments.memory、sm、segmentsMemory：段的内存使用量，例如`4`
    - segments.index_writer_memory、siwm、segmentsIndexWriterMemory：index writer的内存使用量，例如`1.4kb`
    - segments.version_map_memory、svmm、segmentsVersionMapMemory：版本映射（version map）的内存使用量，例如`1.0kb`
    - segments.fixed_bitset_memory、sfbm、fixedBitsetMemory：[nested object](#Nested field type)类型的字段和[join](#Join field type)字段中类型应用的类型过滤使用的固定比特位的内存使用量，例如`1.0kb`
    - seq_no.global_checkpoint、sqg、globalCheckpoint：全局检查点（global checkpoint）
    - seq_no.local_checkpoint、sql、localCheckpoint：本地检查点（local checkpoint）
    - seq_no.max、sqm、maxSeqNo：sequence数值最大值
    - suggest.current、suc、suggestCurrent：当前suggest操作的数量，比如`0`
    - suggest.time、suti、suggestTime：suggest花费的时间，例如`0`
    - suggest.total、suto、suggestTotal：suggest操作的数量，比如`0`
    - sync_id：分片的Sync ID 
    - unassigned.at, ua：成为未分配分片的时间（[Coordinated Universal Time (UTC)](https://en.wikipedia.org/wiki/List_of_UTC_offsets)）
    - unassigned.details, ud：某个分片成为未分配分片的详细原因。This does not explain why the shard is currently unassigned。若要理解为什么某个分片没有被分片，使用[Cluster allocation explain ](#Cluster allocation explain API) 
    - unassigned.for, uf：分片被要求成为未分配的时间（[Coordinated Universal Time (UTC)](https://en.wikipedia.org/wiki/List_of_UTC_offsets)）
    - unassigned.reason, ur：未分配分片的状态上一次发生变更的原因。This does not explain why the shard is currently unassigned。若要理解为什么某个分片没有被分片，使用[Cluster allocation explain ](#Cluster allocation explain API) ，返回的值包括：
      - ALLOCATION_FAILED：分片分配操作发生故障导致未分配
      - CLUSTER_RECOVERED：[full cluster recovery](#Full cluster restart upgrade)导致未分配
      - DANGLING_INDEX_IMPORTED：正在导入[dangling index](#Dangling indices)导致未分配
      - EXISTING_INDEX_RESTORED：正在恢复成一个closed index导致未分配
      - FORCED_EMPTY_PRIMARY：The shard’s allocation was last modified by forcing an empty primary using the [Cluster reroute API](#Cluster reroute API)
      - INDEX_CLOSED：index被关闭了导致未分配
      - INDEX_CREATED：Unassigned as a result of an API creation of an index
      - INDEX_REOPENED：正在打开一个关闭的索引导致未分配
      - MANUAL_ALLOCATION：分片的分配因为上一次通过[Cluster reroute API](#Cluster reroute API)进行了变更
      - NEW_INDEX_RESTORED：正在恢复成一个新的索引导致未分配
      - NODE_LEFT：节点正在离开集群导致未分配
      - NODE_RESTARTING：类似NODE_LEFT，只是节点通过[Node shutdown API](#Node lifecycle APIs)进行重启以及注册到集群
      - PRIMARY_FAILED：分片正在初始化为一个副本分片（replica），但是在初始化完成之前主分片发生了故障
      - REALLOCATED_REPLICA：识别到了一个更好的副本分片位置使得现有的副本分片被取消了
      - REINITIALIZED：When a shard moves from started back to initializing
      - REPLICA_ADDED：显示的（explicit）增加了一个副本分片导致未分配
      - REROUTE_CANCELLED：显示的取消reroute命令导致未分配

- help：（Optional, Boolean）如果为`true`。响应中包含帮助信息。默认值为`false`。
- master_timeout：（Optional, [time units](#Time units)）周期性的等待连接master node。如果在超时前未收到响应，则请求失败并且返回一个错误。默认是`30s`。
- s：（Optional, string）用逗号隔开的列名或者列的别名对响应进行排序
- time：（Optional, [time units](#Time units)）展示时间使用的时间单位。
- v：（Optional, Boolean）如果为`true`，响应中会包含列名。默认为`false`。

##### Examples

###### Example with a single data stream or index

```text
GET _cat/shards
```

&emsp;&emsp;API返回下面的响应：

```text
my-index-000001 0 p STARTED 3014 31.1mb 192.168.56.10 H5dfFeA
```

###### Example with a wildcard pattern

&emsp;&emsp;如果你的集群中有很多的分片，你可以在`<target>`中使用wildcard pattern来限制API的请求。

&emsp;&emsp;下面的请求返回索引名以`my-index-`开头的所有的data stream或者index。

```text
GET _cat/shards/my-index-*
```

&emsp;&emsp;API返回下面的响应：

```text
my-index-000001 0 p STARTED 3014 31.1mb 192.168.56.10 H5dfFeA
```

###### Example with a relocating shard

```text
GET _cat/shards
```

&emsp;&emsp;API返回下面的响应：

```text
my-index-000001 0 p RELOCATING 3014 31.1mb 192.168.56.10 H5dfFeA -> -> 192.168.56.30 bGG90GE
```

&emsp;&emsp;在`state`列中的值`RELOCATING`说明索引分片正在重新分配中。

###### Example with a shard states

&emsp;&emsp;某个分片在可以使用之前，它会经历`INITIALIZING`状态。你可以使用cat shards API来查看哪些分片正在初始化。

```text
GET _cat/shards
```

&emsp;&emsp;API返回下面的响应：

```text
my-index-000001 0 p STARTED      3014 31.1mb 192.168.56.10 H5dfFeA
my-index-000001 0 r INITIALIZING    0 14.3mb 192.168.56.30 bGG90GE
```

###### Example with reasons for unassigned shards

&emsp;&emsp;下面的请求会返回`unassigned.reason`这一列，来查看某个分片为什么未分配。

```text
GET _cat/shards?h=index,shard,prirep,state,unassigned.reason
```

&emsp;&emsp;API返回下面的响应：

```text
my-index-000001 0 p STARTED    3014 31.1mb 192.168.56.10 H5dfFeA
my-index-000001 0 r STARTED    3014 31.1mb 192.168.56.30 bGG90GE
my-index-000001 0 r STARTED    3014 31.1mb 192.168.56.20 I8hydUG
my-index-000001 0 r UNASSIGNED ALLOCATION_FAILED
```


#### cat thread pool API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cat-thread-pool.html)

### Cluster APIs
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-update-settings.html)

##### Node specification

&emsp;&emsp;可以通过指定`node filters`，让一些cluster-level API只在一个节点子集合上操作。例如[Task Management](#Task management API)、[Nodes Stats](#Nodes stats API)、[Nodes Info](#Nodes info API)这些API只报告（report）筛选出的一个节点集合，而不是所有节点的结果。

&emsp;&emsp;`node filters`是一组独立的过滤项（individual filter）并且用逗号隔开。每一个过滤项会从已选择的节点结合中增加或者移除节点。每一个过滤项可以是下列之一：

- `_all`，所有的节点添加到子集中（sbuset）
- `_local`，本地节点（local node）添加到子集中
- `_master`，当前被选为master的节点添加到子集中
- node id或者节点名称添加到子集中
- IP地址或者hostname，匹配到的所有节点添加到子集中
- 使用`*`通配符，满足pattern的节点名称，hostname或者地址的节点添加到子集中
- `master:true`，`data:true`，`ingest:true`，`voting_only:true`，`ml:true`，`coordinating_only:true`这些过滤项分别将 master-eligible、所有的数据节点（data node），所有的ingest node，所有的voting-only node、所有的machine learning node和所有的coordinating-only node 添加到子集中
- `master:false`，`data:false`，`ingest:false`，`voting_only:false`，`ml:false`，或者`coordinating_only:false`这些过滤项分别从节点集合中移除master-eligible、所有的数据节点（data node），所有的ingest node，所有的voting-only node、所有的machine learning node和所有的coordinating-only node 
- a pair of patterns，使用`*`通配符或者`attrname:attrvalue`这种格式。将自定义的节点属性满足这种pattern的节点添加到子集中。自定义的节点属性是在配置文件中按照`node.attr.attrname: attrvalue`这种格式设置

> NOTE：`node filter`按照给定的顺序有序执行，这个规则对于从节点集合中移除节点是很重要的。比如说过滤项为`_all, master:false`时的意思是 除了master-eligible之外的所有节点，而`master:false, _all`则是跟`_all`是同一个意思，因为`_all`在`master:false`之后运行，也就是`_all`会把所有的节点都添加到集合中。

> NOTE：如果没有给定过滤项，默认就是选择所有的节点。然而如果给定了任意的过滤项，那么则是从一个空的节点子集开始处理。意味着比如`master:false`这种过滤项，只有放在其他过滤项之后使用才能发挥作用。如果只指定了`master:false`，那么不会添加任何的节点到子集中。

&emsp;&emsp;下列是一些使用`node filter`的[Nodes Info](#Nodes info API) API的例子：

```text
# If no filters are given, the default is to select all nodes
GET /_nodes
# Explicitly select all nodes
GET /_nodes/_all
# Select just the local node
GET /_nodes/_local
# Select the elected master node
GET /_nodes/_master
# Select nodes by name, which can include wildcards
GET /_nodes/node_name_goes_here
GET /_nodes/node_name_goes_*
# Select nodes by address, which can include wildcards
GET /_nodes/10.0.0.3,10.0.0.4
GET /_nodes/10.0.0.*
# Select nodes by role
GET /_nodes/_all,master:false
GET /_nodes/data:true,ingest:true
GET /_nodes/coordinating_only:true
GET /_nodes/master:true,voting_only:false
# Select nodes by custom attribute (e.g. with something like `node.attr.rack: 2` in the configuration file)
GET /_nodes/rack:2
GET /_nodes/ra*:2
GET /_nodes/ra*:2*
```

#### Cluster allocation explain API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cluster-allocation-explain.html)

&emsp;&emsp;提供某个分片当前的分配信息

```text
GET _cluster/allocation/explain
{
  "index": "my-index-000001",
  "shard": 0,
  "primary": false,
  "current_node": "my-node"
}
```

##### Request

`GET _cluster/allocation/explain`
`POST _cluster/allocation/explain`

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有`monitor`或者`manage`的[cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;该API 的目的是提供集群中分片分配的信息。对于未分配的分片，这个API会提供未分配的原因。对于已分配的分片，这个API会提供为什么这个分片仍然在当前节点并且还未移动或者Rebalance到其他的节点的原因。当尝试诊断某个分片为什么还未分配或者分片不像你期望那样仍然在当前节点时使用这个API就非常的有用。

##### Query parameters

- include_disk_info：（Optional, Boolean）如果为`true`，则返回磁盘使用量和分片大小的信息。默认为`false`。
- include_yes_decisions：（Optional, Boolean）如果为`true`，在返回的信息中返回YES decisions，默认值为`false`。

##### Request body

- current_node：（Optional, string）指定节点的ID或者名称，只返回在指定节点上的分片的分配信息。
- index：（Optional, string）指定你想要了解的索引的名称。
- primary：（Optional, Boolean）如果为`true`，返回给定分片ID对应的主分片的分配信息。
- shard：（Optional,integer）指定你要了解的分片ID。

##### Example

###### Unassigned primary shard

&emsp;&emsp;下面的请求为一个未分配的主分片返回分配信息。

```text
GET _cluster/allocation/explain
{
  "index": "my-index-000001",
  "shard": 0,
n  "primaryn "primary"'g: '  "primaryn "primary"'g: 'n   "primaryn "primary"'g: '  "primaryn "primary"'g: 'nztrhuoe
}
```

&emsp;&emsp;API响应中指出这个分片只能被分配到一个不存在的（nonexistent）节点上。

```text
{
  "index" : "my-index-000001",
  "shard" : 0,
  "primary" : true,
  "current_state" : "unassigned",                 
  "unassigned_info" : {
    "reason" : "INDEX_CREATED",                   
    "at" : "2017-01-04T18:08:16.600Z",
    "last_allocation_status" : "no"
  },
  "can_allocate" : "no",                          
  "allocate_explanation" : "Elasticsearch isn't allowed to allocate this shard to any of the nodes in the cluster. Choose a node to which you expect this shard to be allocated, find this node in the node-by-node explanation, and address the reasons which prevent Elasticsearch from allocating this shard there.",
  "node_allocation_decisions" : [
    {
      "node_id" : "8qt2rY-pT6KNZB3-hGfLnw",
      "node_name" : "node-0",
      "transport_address" : "127.0.0.1:9401",
      "node_attributes" : {},
      "node_decision" : "no",                     
      "weight_ranking" : 1,
      "deciders" : [
        {
          "decider" : "filter",                   
          "decision" : "NO",
          "explanation" : "node does not match index setting [index.routing.allocation.include] filters [_name:\"nonexistent_node\"]"  
        }
      ]
    }
  ]
}
```

&emsp;&emsp;第5行，当前分片的状态
&emsp;&emsp;第7行，分片最开始未分配的原因
&emsp;&emsp;第11行，能否能分配这个分片
&emsp;&emsp;第19行，是否能将这个分片分配到指定的节点
&emsp;&emsp;第23行，desider为这个节点指向了`no` decision
&emsp;&emsp;第25行，解释了为什么desider返回了一个`no` decision，并且给出了有用的信息：配置导致指向这个`no` decision

&emsp;&emsp;下面的响应中为一个未分配的主分片提供了一个分配信息，这个分片之前已经被分配了。

```text
{
  "index" : "my-index-000001",
  "shard" : 0,
  "primary" : true,
  "current_state" : "unassigned",
  "unassigned_info" : {
    "reason" : "NODE_LEFT",
    "at" : "2017-01-04T18:03:28.464Z",
    "details" : "node_left[OIWe8UhhThCK0V5XfmdrmQ]",
    "last_allocation_status" : "no_valid_shard_copy"
  },
  "can_allocate" : "no_valid_shard_copy",
  "allocate_explanation" : "Elasticsearch can't allocate this shard because there are no copies of its data in the cluster. Elasticsearch will allocate this shard when a node holding a good copy of its data joins the cluster. If no such node is available, restore this index from a recent snapshot."
}
```

###### Unassigned replica shard

&emsp;&emsp;下面的响应中为副本分片提供了一个分配信息，是因为[delayed allocation](#Delaying allocation when a node leaves)导致的。

```text
{
  "index" : "my-index-000001",
  "shard" : 0,
  "primary" : false,
  "current_state" : "unassigned",
  "unassigned_info" : {
    "reason" : "NODE_LEFT",
    "at" : "2017-01-04T18:53:59.498Z",
    "details" : "node_left[G92ZwuuaRY-9n8_tc-IzEg]",
    "last_allocation_status" : "no_attempt"
  },
  "can_allocate" : "allocation_delayed",
  "allocate_explanation" : "The node containing this shard copy recently left the cluster. Elasticsearch is waiting for it to return. If the node does not return within [%s] then Elasticsearch will allocate this shard to another node. Please wait.",
  "configured_delay" : "1m",                      
  "configured_delay_in_millis" : 60000,
  "remaining_delay" : "59.8s",                    
  "remaining_delay_in_millis" : 59824,
  "node_allocation_decisions" : [
    {
      "node_id" : "pmnHu_ooQWCPEFobZGbpWw",
      "node_name" : "node_t2",
      "transport_address" : "127.0.0.1:9402",
      "node_decision" : "yes"
    },
    {
      "node_id" : "3sULLVJrRneSg0EfBB-2Ew",
      "node_name" : "node_t0",
      "transport_address" : "127.0.0.1:9400",
      "node_decision" : "no",
      "store" : {                                 
        "matching_size" : "4.2kb",
        "matching_size_in_bytes" : 4325
      },
      "deciders" : [
        {
          "decider" : "same_shard",
          "decision" : "NO",
          "explanation" : "a copy of this shard is already allocated to this node [[my-index-000001][0], node[3sULLVJrRneSg0EfBB-2Ew], [P], s[STARTED], a[id=eV9P8BN1QPqRc3B4PLx6cg]]"
        }
      ]
    }
  ]
}
```

&emsp;&emsp;第14行，由于刚刚离开集群的节点拥有这个分片，并且配置了`1m`的延迟（如果在超时前那个节点重新回到集群，就不用生成新的副本分片）
&emsp;&emsp;第16行，即将分配副本分片的剩余时间
&emsp;&emsp;第30行，节点上分片数据的信息

###### Assigned shard

&emsp;&emsp;下面的响应中为一个已分配的分片提供了分配信息。这个响应指出该分片不允许继续呆在当前节点上，必须重新分配。

```text
{
  "index" : "my-index-000001",
  "shard" : 0,
  "primary" : true,
  "current_state" : "started",
  "current_node" : {
    "id" : "8lWJeJ7tSoui0bxrwuNhTA",
    "name" : "node_t1",
    "transport_address" : "127.0.0.1:9401"
  },
  "can_remain_on_current_node" : "no",            
  "can_remain_decisions" : [                      
    {
      "decider" : "filter",
      "decision" : "NO",
      "explanation" : "node does not match index setting [index.routing.allocation.include] filters [_name:\"nonexistent_node\"]"
    }
  ],
  "can_move_to_other_node" : "no",                
  "move_explanation" : "This shard may not remain on its current node, but Elasticsearch isn't allowed to move it to another node. Choose a node to which you expect this shard to be allocated, find this node in the node-by-node explanation, and address the reasons which prevent Elasticsearch from allocating this shard there.",
  "node_allocation_decisions" : [
    {
      "node_id" : "_P8olZS8Twax9u6ioN-GGA",
      "node_name" : "node_t0",
      "transport_address" : "127.0.0.1:9400",
      "node_decision" : "no",
      "weight_ranking" : 1,
      "deciders" : [
        {
          "decider" : "filter",
          "decision" : "NO",
          "explanation" : "node does not match index setting [index.routing.allocation.include] filters [_name:\"nonexistent_node\"]"
        }
      ]
    }
  ]
}
```

&emsp;&emsp;第11行，分片是否允许一直待在当前节点上
&emsp;&emsp;第12行，decider指出为什么分片不允许待在当前节点上
&emsp;&emsp;第19行，分片是否允许移动到其他节点上

&emsp;&emsp;下面的响应中为一个必须仍然待在当前节点的分片提供了分配信息。移动到其他节点不会提高集群的平衡。

```text
{
  "index" : "my-index-000001",
  "shard" : 0,
  "primary" : true,
  "current_state" : "started",
  "current_node" : {
    "id" : "wLzJm4N4RymDkBYxwWoJsg",
    "name" : "node_t0",
    "transport_address" : "127.0.0.1:9400",
    "weight_ranking" : 1
  },
  "can_remain_on_current_node" : "yes",
  "can_rebalance_cluster" : "yes",                
  "can_rebalance_to_other_node" : "no",           
  "rebalance_explanation" : "Elasticsearch cannot rebalance this shard to another node since there is no node to which allocation is permitted which would improve the cluster balance. If you expect this shard to be rebalanced to another node, find this node in the node-by-node explanation and address the reasons which prevent Elasticsearch from rebalancing this shard there.",
  "node_allocation_decisions" : [
    {
      "node_id" : "oE3EGFc8QN-Tdi5FFEprIA",
      "node_name" : "node_t1",
      "transport_address" : "127.0.0.1:9401",
      "node_decision" : "worse_balance",          
      "weight_ranking" : 1
    }
  ]
}
```

&emsp;&emsp;第13行，集群是否允许Rebalance
&emsp;&emsp;第14行，分片是否能Rebalance到其他节点
&emsp;&emsp;第21行，分片不能Rebalance到其他节点的原因，指出相较于待在当前节点并不能通过移动到其他节点来获得更好的平衡

###### No arguments

&emsp;&emsp;如果你不带参数调用这个API，Elasticsearch会随意的找出一个未分配的主分片或者副本分片并提供分配信息。

```text
GET _cluster/allocation/explain
```

&emsp;&emsp;如果集群中没有未分配的分片，API则返回`400`的错误。

#### Cluster get settings API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cluster-get-settings.html)

&emsp;&emsp;返回集群层面的设置：

```text
GET /_cluster/settings
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有`monitor`或者`manage`的[cluster privilege](#Cluster privileges)来使用这个API。

##### Request

```text
GET /_cluster/settings
```

##### Description

&emsp;&emsp;默认情况下，这个API只返回显示定义的设置，但也可以通过使用`include_defaults`参数返回一些默认设置。

##### Query parameters

- flat_settings：（Optional, Boolean）如果为`true`，以铺开的格式返回。默认值为`false`。
- include_defaults：（Optional, Boolean）如果为`true`，返回所有默认的集群设置。默认值为`false`
- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

#### Cluster health API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cluster-health.html)

&emsp;&emsp;返回集群的健康状态。

##### Request

&emsp;&emsp;`GET /_cluster/health/<target>`

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有`monitor`或者`manage`的[cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;cluster health API返回一个集群较简明的状态。你可以通过这个api只获取指定的data stream和index。对于data stream，该API会检索（retrieve）流中backing index的健康状态。

&emsp;&emsp;集群健康状态是`green`，`yellow`，或者`red`。在分片层，`red`说明分片没有被分配到集群中，`yellow`说明主分片已经被分配了，但是副本分片（replica）没有分配。`green`说明所有的分片（主分片和副本分片）都已经被分配。index level的状态由最差的分片状态决定。cluster level的状态由最差的index level的状态决定。

&emsp;&emsp;这个API最重要的好处能够等待集群达到某个特定的（a certain high water-mark）健康状态。例如，下面的请求会等等待集群变成`yellow`状态，最多等待50秒（如果在50秒之前就达到了`green`或者`yellow`，那么就马上返回）。


##### Path parameters

- `<target>`：（Optional, string）用逗号隔开的一个或多个data stream、Index或者alias。支持通配符(\*)。若要查询所有的data stream、Index，则不指定这个参数或者使用`*`、`_all`。

##### Query parameters
- level：（Optional, string）可以是`cluster`、`indices`或者`shards`。用来控制健康信息的详细层级。默认值：`cluster`
- local：（Optional, Boolean）如果为`true`，则只从local node获取信息。默认是`false`，意味着从master node获取信息
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- wait_for_active_shards：（Optional, string）用来控制等待多个active shard（正在初始化、恢复、关闭属于不活跃的分片），如果是`all`则是等待集群中所有的分片为active，或者是`0`则不等待。默认是`0`
- wait_for_events：（Optional, string）可以是`immediate`、`urgent`、`high`、`normal`、`low`、`languid`。该参数选择事件优先级，等待集群中这些优先级的事件完成后才返回（能让返回的健康状态信息更加准确和实时）
- wait_for_no_initializing_shards：（Optional, Boolean）是否等待集群中没有正在初始化的分片（比如正在启动的分片）后再放回。默认值为`false`，意味着不会等分配初始化结束就返回。
- wait_for_no_relocating_shards：（Optional, Boolean）是否等待集群中没有正在迁移（比如集群再平衡、硬件故障修复、手动迁移）的分片再返回。默认值为`false`，意味着不会等relocation结束就返回。
- wait_for_nodes：（Optional, string）等待直到可用的节点数量为指定的参数值。参数值可以是`>=N`、`<=N`、`>N`以及`<N`。或者可以使用`ge(N)`、`le(N)`、`gt(N)`以及`lt(N)`
- wait_for_status：（Optional, string）可以是`green`、`red`或者`yellow`。等待直到集群状态达到或者高于指定的状态类型再返回。默认情况下不会等待。

##### Response body

- cluster_name：(string) 集群名称
- status：(string) 集群状态。基于主分片跟副本分片的状态，有以下的几个状态：
  - green：所有的分片都已经分配
  - yellow：所有的主分片已经分配，但是一个或者多个副本分片没有分配。如果集群中的某个节点发生故障，一些数据可能不可见直到节点恢复
  - 一个或多个主分片没有分配，因此一些数据是不可见的。经常发生在集群启动时主分片正在被分配
- timed_out：（Boolean）如果为`false`，说明请求在超时前就返回了，超时时间通过上文中的`timeout`指定（默认`30s`）
- number_of_nodes：（integer）集群中节点的数量
- number_of_data_nodes：（integer）专用的数据节点（data node）的数量
- active_primary_shards：（integer）活跃的主分片数量
- active_shards：（integer）活跃的主分片跟副本分片的数量
- relocating_shards：（integer）正在重新分配的分片数量
- initializing_shards：（integer）正在初始化的分片数量
- unassigned_shards：（integer）未分配的分片数量
- delayed_unassigned_shards：（integer）延迟重分配（防止在节点短暂离线时引起不必要的分片重分配）的分片数量
- number_of_pending_tasks：（integer）未完成的cluster-level的变更的数量
- number_of_in_flight_fetch：（integer）未完成的分片fetch（通常发生在集群重新分配分片时，比如在一个节点失败后，或者在进行集群平衡时，从集群的其他节点获取分片的副本的操作就是shard fetch）的数量
- task_max_waiting_in_queue_millis：（integer）队列中等待时间最长的任务已经消耗的时间（单位：毫秒）
- active_shards_percent_as_number（float）集群中活跃的分片的比例

##### Example

```text
GET _cluster/health
```

&emsp;&emsp;API的响应中是一个单机群的、单索引、单个主分片/副本的例子：

```text
{
  "cluster_name" : "testcluster",
  "status" : "yellow",
  "timed_out" : false,
  "number_of_nodes" : 1,
  "number_of_data_nodes" : 1,
  "active_primary_shards" : 1,
  "active_shards" : 1,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 1,
  "delayed_unassigned_shards": 0,
  "number_of_pending_tasks" : 0,
  "number_of_in_flight_fetch": 0,
  "task_max_waiting_in_queue_millis": 0,
  "active_shards_percent_as_number": 50.0
}
```

&emsp;&emsp;下面是获取分片层（shard-level）集群状态的请求：

```text
GET /_cluster/health/my-index-000001?level=shards
```

#### Cluster reroute API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cluster-reroute.html)

&emsp;&emsp;更改集群中分片的分配。

##### Request

&emsp;&emsp;`POST /_cluster/reroute`

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有`manage`的[cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;reroute命令允许你手动改变集群中的单个分片的分配。例如你可以显示的将一个分片从一个节点移动到另一个节点，可以取消分片的分配并且没有被分配的分片可以显示的分配到指定的节点上。

&emsp;&emsp;很重要的一个注意点是在reroute命令执行后，Elasticsearch会执行rebalancing（遵循`cluster.routing.rebalance.enable`这个配置）来维持集群平衡的状态。例如，如果将一个分片从`node1`移动到`node2`，那随后可能会导致一个分片从`node2`移动到`node1`来平衡进出（even things out）。

&emsp;&emsp;可以设置`cluster.routing.allocation.enable`关闭集群中的allocation。一旦关闭后，那么分配动作只会在使用`reroute`命令或者realancing时发生。

&emsp;&emsp;通过在URL的请求参数中添加`?dry_run`或者在请求体中传递`dry_run:true`可以让`reroute`命令在"dry run"模式中运行。它会计算对当前集群状态应用这次reroute命令后会产生的结果，并返回这个结果，但不会真正的执行这些请求变更。

&emsp;&emsp;如果在URI请求参数中添加`?explain`，那么在响应中会有一个详细的介绍来说明为什么可以或者不可以执行这个命令。

&emsp;&emsp;集群会最多`index.allocation.max_retries`次来尝试（默认值为5）分配一个节点，否则就不再分配并且这个分片成为未分配的分片。这种情况可能由一些结构性的问题（structural problem）导致，比如分词器需要引用停用词文件，但不是所有的节点上都有这个文件。

&emsp;&emsp;一旦问题解决了，就可以手动的通过`reroute`API并且使用请求参数`?retry_failed`进行重试。这将对这些分片尝试一轮单独的重试。

##### Query parameters

- dry_run：（Optional, Boolean）如果为`true`，请求只是模拟操作并返回产生的状态
- explain：（Optional, Boolean）如果为`true`，响应中会包含某个命令为什么可以或者不可以执行的详细说明
- metric：（Optional, string）限制响应中返回的指标信息。默认返回所有的指标，如下所示：
  - \_all：显示所有的指标
  - blocks：显示响应中`blocks`部分的内容
  - master_node：显示响应中`metadata`部分的内容。如果你提供了用逗号隔开的索引列表，那返回的结果中只包含这些索引的元数据
  - nodes：显示响应中`nodes`部分的内容
  - routing_table：显示响应中`routing_tabel`部分的内容
  - version：显示集群状态版本
- retry_failed：（Optional, Boolean）如果为`true`，会重试那些因为之前多次分配失败而被阻塞的分片的分配。（这是一种有用的手段来快速恢复集群正常状态，尤其是在紧急修复了阻止分片分配的问题之后）
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Request body

- commands：（Required, array of objects） 定义了执行的命令，支持以下的命令：
  - move：将一个Started Shard（对应还有Initializing Shards、Unassigned Shards、Relocating Shards）从一个节点移动到另一个节点。使用`index`跟`shard`描述索引名称跟分配编号，以及`from_node`跟`to_node`描述节点移动前后的两个节点。
  - cancel：取消某个分片的分配（或者恢复）。使用`index`跟`shard`描述索引名称跟分配编号。以及`node`描述被取消的分片所在的节点。通过取消副本分片的分配来强制从主分片的重新同步并通过标准的恢复处理来重新初始化副本分片。默认情况下只能取消副本分片的分配。如果需要取消主分片的分配，那么需要在请求中通过`allow_primary`指定
  - allocate_replica：将未分配的副本分片分配到一个节点。使用`index`跟`shard`描述索引名称跟分配编号。以及`node`描述待分配的目标节点。遵循[allocation deciders](#Cluster-level shard allocation and routing settings)。

另外两个命令可以允许将一个主分片分配给节点。然而，这些命令应该非常小心地使用，因为主分片的分配通常完全由Elasticsearch自动处理。主分片不能自动分配的原因包括以下几点：
- 新创建了一个索引，但没有节点满足allocation deciders的要求。
- 无法在集群当前的数据节点上找到数据的最新副本。为了防止数据丢失，系统不会自动将陈旧的副本提升为主分片。

下面两个命令是危险的，可能导致数据丢失。原始数据无法恢复并且集群管理员允许数据丢失的前提下可以使用。如果你遇到了一个可以修复的临时问题，见上文中的`retry_failed`。这里需要强调的是：如果执行了这些命令，然后一个持有受影响分片副本的节点加入集群，那么新加入节点上的副本将会被删除或覆盖。

  - allocate_stale_primary：将一个主分片分配给持有陈旧副本的节点。使用`index`跟`shard`描述索引名称跟分配编号，以及要分配分片的`node`。使用这个命令可能会导致所提供的分片ID的数据丢失。如果一个拥有良好数据副本的节点后来重新加入集群，那么该数据将被删除或用这个命令强制分配的陈旧副本的数据覆盖。为确保充分理解这些后果，这个命令要求显式地将标志 `accept_data_loss` 设置为 true。
  - allocate_empty_primary：将一个空的主分片分配给节点。使用`index`跟`shard`描述索引名称跟分配编号，以及要分配分片的`node`。使用这个命令会导致此分片中索引的所有数据的完全丢失，如果它之前已经启动。如果一个拥有数据副本的节点后来重新加入集群，那么该数据将被删除。为确保充分理解这些后果，这个命令要求显式地将标志 `accept_data_loss` 设置为 true。

##### Examples

&emsp;&emsp;以下是一个简短的reroute API调用的例子：

```text
POST /_cluster/reroute
{
  "commands": [
    {
      "move": {
        "index": "test", "shard": 0,
        "from_node": "node1", "to_node": "node2"
      }
    },
    {
      "allocate_replica": {
        "index": "test", "shard": 1,
        "node": "node3"
      }
    }
  ]
}
```

#### Cluster state API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cluster-state.html)

&emsp;&emsp;返回集群状态（cluster state）的内部表示（internal representation）用于调试或者诊断目的。

##### Request

`GET /_cluster/state/<metrics>/<target>`

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`monitor`或者`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;`cluster state`是内部的数据结构，它记录了每个节点所需的各种信息，包括：

- 集群中其他节点的身份和属性。
- 集群范围（cluster-wide）的设置。
- 索引元数据（Index metadata），包括每个索引的mapping和设置。
- 集群中每个分片副本的位置和状态。

&emsp;&emsp;被选为master的节点能保证集群中每个节点有相同的集群状态的副本。cluster state API能让你检索集群状态的内部表示用于调试或者诊断目的。你可能需要查看Elasticsearch的源码来明确响应中确切含义（precise meaning）。

&emsp;&emsp;默认情况下，cluster state API将请求路由到被选为master的节点，因为它是集群状态的权威源头。你也可以在API请求添加请求参数`?local=true`获取本地节点上的集群状态。

&emsp;&emsp;在较大的集群中，Elasticsearch可能需要花费大量的努力来计算对这个API的响应，而且响应可能包含非常大量的数据。如果你重复使用这个API，集群可能会变得不稳定。

> WARNING：响应是对内部数据结构的表示。它的格式不受与其他更稳定API相同的兼容性保证，可能会随版本而变化。**不要使用外部监控工具查询此API**。相反，请使用其他更稳定的[cluster APIs](#Cluster APIs)来获取您需要的信息。

##### Path parameters

&emsp;&emsp;集群状态有时候会很大，Elasticsearch计算这个API响应时可能会花费大量资源。若要降低响应的大小，你可以只请求你感兴趣的部分集群状态：

- `<metric>`：（Optional, string）有以下的指标可选，多个指标之间用逗号隔开
  - `_all`：显示所有的指标
  - `blocks`：显示响应的`blocks`部分的内容
  - `master_node`：显示响应的`master_node`部分的内容
  - `metadata`：显示响应的`metadata`部分的内容。如果你提供了用逗号隔开的索引列表，返回的结果中只包含这些索引的元数据
  - `nodes`：显示响应的`nodes`部分的内容
  - `routing_nodes`：显示响应的`routing_nodes`部分的内容
  - `routing_table`：显示响应的`routing_table`部分的内容。果你提供了用逗号隔开的索引列表，返回的结果中只包含这些索引的routing_table（索引的分片路由信息。它包含了该索引所有分片的分配和位置信息，诸如哪些分片是主分片、哪些是副本分片，以及这些分片分布在集群中的哪些节点上）
  - `version`：显示集群状态版本
- `<target>`：（Optional, string）用逗号隔开的一个或多个data stream、Index或者alias。支持通配符(\*)。若要查询所有的data stream、Index，则不指定这个参数或者使用`*`、`_all`。

##### Query parameters
- allow_no_indices：（Optional, Boolean）如果为`true`。如果通配符索引表达式没有解析为任何具体的索引，这种情况将被忽略（不会报错）。这包括使用`\_all` 字符串或没有指定任何索引的情况。默认值为`true`
- expand_wildcards：（Optional, string）通配符索引表达式解析为具体的索引可以是open、closed或者全部。Optional为：`open`、`closed`、`none`、`all`
- flat_settings：（Optional, Boolean）如果为`true`，以铺开的格式返回。默认值为`false`。
- ignore_unavailable：（Optional, Boolean）如果为`true`，不可用的索引（missing或者closed）会忽略
- local：（Optional, Boolean）如果为`true`，则只从local node获取信息。默认是`false`，意味着从master node获取信息
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- wait_for_metadata_version：（Optional, integer）等待元数据的版本号大于等于指定的版本号
- wait_for_timeout：（Optional,[time units](#Time units)）指定wait_for_metadata_version中等待超时时间。

##### Examples

&emsp;&emsp;下面的例子中只返回`metadata`以及名为`foo`跟`bar`的索引或者Data streams的`routing_talbe`：

```text
GET /_cluster/state/metadata,routing_table/foo,bar
```

&emsp;&emsp;下一个例子返回`foo`跟`bar`所有可用的元数据：

```text
GET /_cluster/state/_all/foo,bar
```

&emsp;&emsp;下面的例子只返回`blocks`：

```text
GET /_cluster/state/blocks
```

#### Cluster stats API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cluster-stats.html)

&emsp;&emsp;

#### Cluster update settings API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cluster-update-settings.html)

&emsp;&emsp;用来配置[dynamic cluster settings](#Dynamic（settings）)。

##### Request

`PUT /_cluster/settings`

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;你可以使用cluster update settings API对允许中的集群配置以及更新动态设置（dynamic settings）。你对未启动的或者关闭的节点上使用`elasticsearch.yml`配置本地的动态设置。

&emsp;&emsp;使用cluster update settings API的更新方法是可持久的（对于persistent settings），即使集群重启也继续生效；也可以是临时的（transient settings），即有些配置在集群重启后重置。你也可以使用这个API通过赋予`null`值重置持久/临时的设置。

&emsp;&emsp;如果你用多种方式配置了一样的设置，那么Elasticsearch会根据下面的优先顺序（order of precedence）来应用设置（apply settings）。

1. Transient Setting
2. Persistent Setting
3. `elasticsearch.yml`  setting
4. Default setting value

&emsp;&emsp;例如，你可以使用一个transient setting覆盖persistent setting或者`elasticsearch.yml`。然而，在`elasticsearch.yml`上的变更不会覆盖定义好的（defined）transient 或者 persistent setting。

>TIP：如果你使用Elasticsearch Service，使用[user settings](https://www.elastic.co/guide/en/cloud/current/ec-add-user-settings.html)功能来配置所有的设置。这个方法能让Elasticsearch自动的拒绝（reject）掉任何会破坏你集群的设置。
如果你在自己的设备（hardware）上运行Elasticsearch，可以使用cluster update settings API来配置集群动态设置。对于集群或者节点的静态设置只使用elasticsearch.yml来配置。使用API不会要求重启并且保证所有节点都被配置成相同的值。

>WARNING: 我们不再推荐使用临时集群设置。请改用持久集群设置。如果集群变得不稳定，临时设置可能会意外清除，导致可能不希望的集群配置。请参阅 [Transient settings migration guide](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/transient-settings-migration-guide.html)。

##### Query parameters

- flat_settings：（Optional, Boolean）如果为`true`，以铺开的格式返回。默认值为`false`。
- include_defaults：（Optional, Boolean）如果为`true`，返回所有默认的集群设置。默认值为`false`
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Examples

&emsp;&emsp;以下是更新持久类型（persistent）设置的例子：

```text
PUT /_cluster/settings
{
  "persistent" : {
    "indices.recovery.max_bytes_per_sec" : "50mb"
  }
}
```

&emsp;&emsp;以下是更新临时类型（transient）设置的例子：

>WARNING: 我们不再推荐使用临时集群设置。请改用持久集群设置。如果集群变得不稳定，临时设置可能会意外清除，导致可能不希望的集群配置。请参阅 [Transient settings migration guide](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/transient-settings-migration-guide.html)。

```text
PUT /_cluster/settings?flat_settings=true
{
  "transient" : {
    "indices.recovery.max_bytes_per_sec" : "20mb"
  }
}
```

&emsp;&emsp;响应中返回更改后的设置，对于临时类型设置的更新也是如此：

```text
{
  ...
  "persistent" : { },
  "transient" : {
    "indices.recovery.max_bytes_per_sec" : "20mb"
  }
}
```

&emsp;&emsp;下面的例子重置了一个设置：

```text
PUT /_cluster/settings
{
  "transient" : {
    "indices.recovery.max_bytes_per_sec" : null
  }
}
```

&emsp;&emsp;响应中不包含被重置的设置：

```text
{
  ...
  "persistent" : {},
  "transient" : {}
}
```

&emsp;&emsp;你也可以使用通配符重置设置。例如，重置所有`indices.recovery`设置：

```text
PUT /_cluster/settings
{
  "transient" : {
    "indices.recovery.*" : null
  }
}
```

#### Nodes feature usage API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cluster-nodes-usage.html)

&emsp;&emsp;返回功能的使用情况（usage of feature）的信息。

##### Request

`GET /_nodes/usage`
`GET /_nodes/<node_id>/usage`
`GET /_nodes/usage/<metric>`
`GET /_nodes/<node_id>/usage/<metric>`

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`monitor`或者`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;这个API允许你查看每个节点上的功能使用情况的信息。所有节点上的Optional见[Node specification](#Node specification)。

##### Path parameters

- `<metric>`：（Optional, string）限制指定的指标返回的信息
  - `_all`：返回所有的统计
  - `rest_actions`：返回REST actions（接口操作）的类名以及一个统计值，即这个节点上的调用次数
- `<node_id>`：（Optional, string）逗号分隔的node id或者名称来限制返回的信息

##### Query parameters

- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Examples

&emsp;&emsp;Rest action example：

```text
GET _nodes/usage
```

&emsp;&emsp;返回的响应：

```text
{
  "_nodes": {
    "total": 1,
    "successful": 1,
    "failed": 0
  },
  "cluster_name": "my_cluster",
  "nodes": {
    "pQHNt5rXTTWNvUgOrdynKg": {
      "timestamp": 1492553961812, 
      "since": 1492553906606, 
      "rest_actions": {
        "nodes_usage_action": 1,
        "create_index_action": 1,
        "document_get_action": 1,
        "search_action": 19, 
        "nodes_info_action": 36
      },
      "aggregations": {
        ...
      }
    }
  }
}
```

&emsp;&emsp;第10行，这个节点执行nodes usage请求的时间
&emsp;&emsp;第11行，开始记录节点使用信息的时间，这个值跟节点启动时间是一致的
&emsp;&emsp;第16行，这个节点执行了19次的查询操作

#### Nodes hot threads API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cluster-nodes-hot-threads.html)

&emsp;&emsp;返回集群中选定节点上的hot thread。

##### Request

`GET /_nodes/hot_threads`
`GET /_nodes/<node_id>/hot_threads`

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`monitor`或者`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;这个 API 提供了集群中每个选定节点的hot thread的详细分解（breakdown）。输出是纯文本格式，包含了每个节点顶级hot thread的详细分解。

##### Path parameters

- `<node_id>`：（Optional, string）逗号分隔的node id或者名称来限制返回的信息

##### Query parameters

- ignore_idle_threads：（Optional, Boolean）如果为 true，则过滤掉已知的（known idle）空闲线程（例如，在 socket select 中等待或从空队列获取任务）。默认为 `true`。
- interval：（Optional, [time units](#API conventions)）采样线程的时间间隔，默认为 `500ms`。
- snapshots：（Optional, integer）线程堆栈跟踪（thread stacktrace）的样本数量，默认为 10。
- threads：（Optional, integer）提供信息的热线程数量，默认为 `3`。用于故障排除时，可以将此参数设置为较大数值（如 9999）以获取系统中所有线程的信息。
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- type：（Optional, string）采样的类型，Optional包括 `block`、`cpu` 和 `wait`。默认为 cpu

##### Examples

```text
GET /_nodes/hot_threads
GET /_nodes/nodeId1,nodeId2/hot_threads
```

#### Nodes info API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cluster-nodes-info.html)

&emsp;&emsp;返回集群节点信息。

##### Request

`GET /_nodes`
`GET /_nodes/<node_id>`
`GET /_nodes/<metric>`
`GET /_nodes/<node_id>/<metric>`

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`monitor`或者`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;该接口能让你获取集群中一个或者多个（全部）的节点信息。所有节点Optional见[这里](#Node specification)。

&emsp;&emsp;默认情况下对于每一个节点，会返回其所有的属性和核心设置。

##### Path Parameters

- `<metric>`：（Optional, string）指定指标来限制返回的信息。通过逗号分隔多个指标，例如`http`、`ingest`
  - aggregation：
  - http：这个节点的HTTP接口信息
  - indices：节点级别（node level）的跟indexing相关的配置
    - total_indexing_buffer：该节点上indexing buffer的最大值
  - ingest：ingest pipeline和其processor的信息
  - jvm：JVM信息，包括名称、版本、以及它的配置
  - os：操作系统信息，包括名称和版本
  - plugins：每一个节点中已安装的插件和模块的详细信息，如下所示：
    - name：插件名称
    - version：插件适用的Elasticsearch版本
    - description：插件的使用目的的简要描述
    - classname：插件入口点的完全限定类名（fully-qualified）
    - has_native_controller：表示该插件是否包含原生控制器进程（原生控制器进程通常是指运行于Elasticsearch之外的独立进程，这种进程可以与Elasticsearch节点进行交互，为特定插件提供额外的功能或服务）
  - process：进程信息。包括进程ID
  - settings：列出在`Elasticsearch.yml`文件中节点的设置
  - thread_pool：每一个线程池的配置
  - transport：节点的transport接口信息

&emsp;&emsp;如果你使用`GET /_nodes/<node_id>/<metric>`并且指定了所有的指标，那么你也可以使用指标名`_all`来获取所有的指标，或者你可以使用指标名`_none`来抑制（suppress）所有的指标并且只获取节点的身份信息。

- `<node_id>`：（Optional, string）用逗号隔开的node id或者node name来限制返回的信息

##### Response body

- build_hash：这次发布的git commit的short hash值
- host：节点的host name
- ip：节点的IP地址
- name：节点的名称
- total_indexing_buffer：最近索引的文档被写入到磁盘前，允许的内存使用量。这个值是节点上所有分片共享的，通过[Indexing Buffer settings](#Indexing buffer settings)控制
- total_indexing_buffer_in_bytes：同`total_indexing_buffer`，只是用字节表示
- transport_address：transport HTTP连接要求的host跟port
- version：节点上运行的Elasticsearch的版本

&emsp;&emsp;`os`系列的配置用来获取操作系统的信息：

- os.refresh_interval_in_millis：刷新统计OS信息的间隔时间
- os.name：操作系统的名称（ex：Linux，WIndows，Mac OS X）
- os.arch：JVM架构的名称（ex：adm64，x86）
- os.version：操作系统的版本
- os.available_processors：JVM中可用处理器的数量
- os.allocated_processors：处理器的数量，可以用来计算线程池大小。该值可以通过节点的设置[node.processors](#Allocated processors setting)配置，默认值操作系统提供的处理器数量。

&emsp;&emsp;`process`系列的配置用来获取当前运行中的进程信息：

- process.refresh_interval_in_millis：刷新统计进程信息的间隔时间
- process.id：进程标识（PID）
- process.mlockall：是否已经成功地将进程的地址空间锁定在内存中（Elasticsearch会尝试锁定其运行时的内存，以防止其被操作系统交换到磁盘（swap）。这样做的主要目的是为了防止因内存交换导致的性能问题）。

##### Query Parameters

- flat_settings：（Optional, Boolean）如果为`true`，以铺开的格式返回。默认值为`false`。
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Examples

```text
# return just process
GET /_nodes/process

# same as above
GET /_nodes/_all/process

# return just jvm and process of only nodeId1 and nodeId2
GET /_nodes/nodeId1,nodeId2/jvm,process

# same as above
GET /_nodes/nodeId1,nodeId2/info/jvm,process

# return all the information of only nodeId1 and nodeId2
GET /_nodes/nodeId1,nodeId2/_all
```

&emsp;&emsp;设置`_all`用来返回所有的信息。

###### Example for plugins metric

&emsp;&emsp;如果指定了`plugins`，接口返回已安装的插件跟模块的详细信息：

```text
GET /_nodes/plugins
```

&emsp;&emsp;返回以下的内容：

```text
{
  "_nodes": ...
  "cluster_name": "elasticsearch",
  "nodes": {
    "USpTGYaBSIKbgSUJR2Z9lg": {
      "name": "node-0",
      "transport_address": "192.168.17:9300",
      "host": "node-0.elastic.co",
      "ip": "192.168.17",
      "version": "{version}",
      "build_flavor": "{build_flavor}",
      "build_type": "{build_type}",
      "build_hash": "587409e",
      "roles": [
        "master",
        "data",
        "ingest"
      ],
      "attributes": {},
      "plugins": [
        {
          "name": "analysis-icu",
          "version": "{version}",
          "description": "The ICU Analysis plugin integrates Lucene ICU module into elasticsearch, adding ICU relates analysis components.",
          "classname": "org.elasticsearch.plugin.analysis.icu.AnalysisICUPlugin",
          "has_native_controller": false
        }
      ],
      "modules": [
        {
          "name": "lang-painless",
          "version": "{version}",
          "description": "An easy, safe and fast scripting language for Elasticsearch",
          "classname": "org.elasticsearch.painless.PainlessPlugin",
          "has_native_controller": false
        }
      ]
    }
  }
}
```

###### Example for ingest metric

&emsp;&emsp;如果指定了`ingest`，接口返回每一个节点的processors信息。

```text
GET /_nodes/ingest
```

&emsp;&emsp;API返回以下的内容：

```text
{
  "_nodes": ...
  "cluster_name": "elasticsearch",
  "nodes": {
    "USpTGYaBSIKbgSUJR2Z9lg": {
      "name": "node-0",
      "transport_address": "192.168.17:9300",
      "host": "node-0.elastic.co",
      "ip": "192.168.17",
      "version": "{version}",
      "build_flavor": "{build_flavor}",
      "build_type": "{build_type}",
      "build_hash": "587409e",
      "roles": [],
      "attributes": {},
      "ingest": {
        "processors": [
          {
            "type": "date"
          },
          {
            "type": "uppercase"
          },
          {
            "type": "set"
          },
          {
            "type": "lowercase"
          },
          {
            "type": "gsub"
          },
          {
            "type": "convert"
          },
          {
            "type": "remove"
          },
          {
            "type": "fail"
          },
          {
            "type": "foreach"
          },
          {
            "type": "split"
          },
          {
            "type": "trim"
          },
          {
            "type": "rename"
          },
          {
            "type": "join"
          },
          {
            "type": "append"
          }
        ]
      }
    }
  }
}
```

#### Nodes reload secure settings API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cluster-nodes-reload-secure-settings.html)

&emsp;&emsp;重新加载集群中节点上的keystore。

##### Request

`POST /_nodes/reload_secure_settings`
`POST /_nodes/<node_id>/reload_secure_settings`

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;[Secure settings](#Secure settings)存储在on-disk keystore中。这些设置都是[reloadable](#Reloadable secure settings)。也就是说你可以在磁盘上对齐更改并且不需要重启集群中的任何节点就可以重新加载。当你更新了keystore中reloadable的安全设置后，你就可以使用这个API在每一个节点上重新加载这些设置。

&emsp;&emsp;如果Elasticsearch keystore使用密码保护而不仅仅是被混淆时（simply obfuscated） ，那当你重载安全设置时需要提供密码。为整个集群重载安全设置的前提是所有节点的keystores使用相同的密码保护，这个方法仅在[inter-node communications are encrypted](#Encrypt internode communication-1)时可用。或者你可以通过本地访问每一个节点，使用节点对应的Elasticsearch keystore密码来重载安全设置。

##### Path Parameters

- `<node_id>`：（Optional, string）集群中特定的节点目标。例如`nodeId1，nodeId2`。对于节点的Optional,见[Node specification](#Node-specification)。

> NOTE：Elasticsearch要求集群节点的安全设置一致性，但是这个一致性不是强制的。因此，重新加载指定节点不是标准做法。通常用于重载操作失败后使用这种方法比较合理

##### Request body

- `secure_settings_password`：（Optional, string）Elasticsearch keystore的密码。

##### Examples

&emsp;&emsp;下面的例子假设集群的每一个节点有相同的Elasticsearch keystore的密码：

```text
POST _nodes/reload_secure_settings
{
  "secure_settings_password":"keystore-password"
}
POST _nodes/nodeId1,nodeId2/reload_secure_settings
{
  "secure_settings_password":"keystore-password"
}
```

&emsp;&emsp;响应中包含了`nodes`对象，key就是node id。每一个value中有`name`以及一个可选的`reload_exception`字段。在重新加载的过程中抛出异常时就会序列化这个`reload_exception`字段：

```text
{
  "_nodes": {
    "total": 1,
    "successful": 1,
    "failed": 0
  },
  "cluster_name": "my_cluster",
  "nodes": {
    "pQHNt5rXTTWNvUgOrdynKg": {
      "name": "node-0"
    }
  }
}
```

#### Nodes stats API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cluster-nodes-stats.html)

&emsp;&emsp;返回集群结点的统计信息。

##### Request

`GET /_nodes/stats`
`GET /_nodes/<node_id>/stats`
`GET /_nodes/stats/<metric>`
`GET /_nodes/<node_id>/stats/<metric>`
`GET /_nodes/stats/<metric>/<index_metric>`
`GET /_nodes/<node_id>/stats/<metric>/<index_metric>`

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`monitor`或者`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;你可以使用这个API获取集群中节点的统计信息。

&emsp;&emsp;节点的Optional见[Node specification](#Node specification)。

&emsp;&emsp;默认返回所有的统计信息。你可以通过使用指标`<metric>`限制返回的内容。

##### Path Parameters

- `<metric>`：（Optional, string）指定指标限制返回的信息。可以指定下了的选项并用逗号隔开：
  - `adaptive_selection`：[Adaptive replica selection](#Adaptive replica selection)相关的统计信息
  - `breaker`：field data circuit breaker（防止因过度内存消耗而导致的内存溢出错误）相关的统计信息
  - `discovery`：`发现`（集群中节点的发现和通信相关）相关的统计
  - `fs`：文件系统信息，数据路径，空闲磁盘空间爱你。读写统计
  - `http`：HTTP连接信息
  - `indexing_pressure`：节点索引负载和相关的rejection的统计
  - `indices`：索引大小，文档数量，索引（indexing）和删除时间，查询时间，field cache大小，合并和flush的索引相关统计信息
  - `ingest`：[ingest preprocessing](#Ingest pipelines)相关的统计 
  - `jvm`：JVM统计数据，包括内存池信息、垃圾回收、缓冲池、加载/卸载类的数量。
  - `os`：操作系统统计数据，包括平均负载、内存和交换区。
  - `process`：进程统计数据，如内存消耗、CPU使用量和打开的文件描述符。
  - `thread_pool`：每个线程池的统计数据，包括当前大小、队列和被拒绝（reject）的任务。
  - `transport`：集群通信中发送和接收字节的传输统计数据。
- `index_metric`：（Optional, string）指定索引指标（index metric）限制返回的`indices`指标信息。只有指定部分指标或者全部指标（`_all`）时才能使用。支持下列的指标：
  - `completion`
  - `docs`
  - `fielddata`
  - `flush`
  - `get`
  - `indexing`
  - `merge`
  - `query_cache`
  - `recovery`
  - `refresh`
  - `request_cache`
  - `search`
  - `segments`
  - `store`
  - `translog`
  - `warmer`
- `<node_id>`：（Optional, string）用逗号隔开的节点列表或者节点名称来限制返回的信息

##### Query parameters

- completion_fields：（Optional, string）用逗号隔开或者通配符表达式指定字段，获取包含`fielddata`和`suggest`的统计信息
- fielddata_fields：（Optional, string）用逗号隔开或者通配符表达式指定字段，获取包含`fielddata`的统计信息
- fields：（Optional, string）逗号隔开或者通配符表达式来获取包含所有字段的统计信息。除非在`completion_fields`或`fielddata_fields`参数中提供了特定的字段列表，否则将作为默认列表使用。
- groups：（Optional, string）用逗号隔开指定搜索组（search group），包含于`search`的统计信息中
- level：（Optional, string）是否在cluster、index、shard层进行聚合。可选值为：
  - cluster
  - indices
  - shards
- types：（Optional, string）用逗号隔开的文档类型用于`indexing`的索引指标
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- include_segment_file_sizes：（Optional, Boolean）如果为`true`，该调用生成每一个Lucene索引文件的聚合后的磁盘使用量（只有请求Segment stats时才生效）。默认值为`false`
- include_unloaded_segments：（Optional, Boolean）如果为`true`，响应中包含未加载到内存的段的信息。默认值为`false`

##### Response body

- \_nodes：（object）包含请求中指定的节点数量的统计信息
  - total：（integer）请求中选择的节点数量总数
  - successful：（integer）成功响应的节点数量
  - failed：（integer）拒绝请求或者请求失败的节点数量。如果这个值不是`0`，拒绝或者失败的原因会包含在响应中
- cluster_name：（string）集群的名称。基于这里的设置[Cluster name setting](#Cluster name setting)
- nodes：（object）请求中指定的节点的统计信息
  - timestamp：（integer）对于这次响应收集节点统计信息的时间。毫秒级别，since [Unix Epoch](https://en.wikipedia.org/wiki/Unix_time)
  - name：（string）可读的节点标识、基于[Node name setting](#Advanced transport settings)这个设置
  - transport_address：（string）[transport layer](#Advanced transport settings)的主机和端口号，用于集群中节点之间的内部通信
  - host：（string）节点的网络主机地址，基于[Network host setting](#Network host setting)这个设置
  - ip：（string）节点的IP跟Port信息
  - roles：（array of string）节点被赋予的角色。见[Node](#Node)
  - attributes：（object）用列表描述的节点的一些属性
  - indices：（object）分配到节点的索引分片的统计信息
    - docs：（object）节点上所有主分片的文档统计信息
      - count：（integer）Lucene提供的文档数量的报告。不包含被删除的文档以及[nested documents](#Nested field type)中从它们的parent分离的文档。同样不包含已经索引到内存但还未属于某个段的文档
      - deleted：（integer）Lucene提供的被删除的文档数量。这个数字可能高于或低于你执行的删除操作次数。这个数字不包括最近执行的且尚未属于任何段的删除。被删除的文档可以通过[][automatic merge process](#Merge)进行清理，如果这样做是有意义的话。此外，Elasticsearch会内部创建额外的已删除文档，用以跟踪分片上最近的操作历史。
    - store：（object）分配到该节点上的分片大小的统计信息
      - size：（[byte value](#Byte size units)）分配到该结点上所有分片的大小（所有索引数据的物理磁盘占用大小，存储优化后的数据）总量
      - size_in_bytes：（integer）同`size`，单位为字节
      - total_data_set_size：（[byte value](#Byte size units)）分配到该结点上所有分片中的数据集大小（原始数据在未压缩或未进行Elasticsearch索引处理之前的大小）总数。包括没有完全存储在该结点上的分片大小，比如[partially mounted indices](# Partially mounted index)的缓存
      - total_data_set_size_in_bytes：（integer）同`total_data_set_size`，单位为字节
      - reserved：（[byte value](#Byte size units)）预计由于正在进行的对等恢复、快照恢复和类似活动，该节点上的分片存储将最终增长多少。如果这个值是-1b，表示这个信息不可用。这个预测有助于管理节点存储空间，确保有足够空间处理这些正在进行的操作
      - reserved_in_bytes：（Integer）同`reserved`，单位为字节
    - indexing：（object）节点上索引操作（indexing operation）相关的统计信息
      - index_total：（integer）索引操作的数量
      - index_time：（[time value](#API conventions)）索引操作花费的时间总量
      - index_time_in_millis：（integer）同`inde_time`，单位为毫秒
      - index_current：（integer）正在执行索引操作的数量
      - index_failed：（integer）索引操作失败的数量
      - delete_total：（integer）删除操作的数量
      - delete_time：（[time value](#API conventions)）删除操作花费的时间总量
      - delete_time_in_millis：（integer）同`delete_time`，单位为毫秒
      - delete_current：（integer）正在执行删除操作的数量
      - noop_update_total：（integer）noop_update（一个更新请求，该请求由于数据已经是最新的而不执行任何实际的更新操作）操作的数量
      - is_throttled：（Boolean）被限流的操作的数量
      - throttle_time：（[time value](#API conventions)）被限流的操作花费的时间总量
      - throttle_time_in_millis：（integer）同`throttle_time`，单位为毫秒
    - get：（object）节点上GET操作（用于检索特定的单个文档。它根据文档的ID直接从索引中获取文档）的统计信息
      - total：（integer）get操作的数量
      - getTime：（[time value](#API conventions)）执行get操作花费的时间总量
      - time_in_millis：（integer）同getTime，单位为毫秒
      - exists_total：（integer）成功的get操作的数量
      - exists_time：（[time value](#API conventions)）成功的get操作花费的时间总量
      - exists_time_in_millis：（integer）同`exists_time`，单位为毫秒
      - missing_total：（integer）失败的get操作的数量
      - missing_time：（[time value](#API conventions)）失败的get操作花费的时间总量
      - missing_time_in_millis：（integer）同`missing_time`，单位为毫秒
      - current：（integer）：正在运行get操作的数量
    - search：（object）节点上search 操作（在一个或多个索引中进行广泛的搜索。它可以基于各种搜索条件（如关键词、过滤器、范围查询等）来查找文档）相关的统计信息
      - open_contexts：（integer）打开的search contexts的数量
      - query_total：（integer）query操作的数量
      - query_time：（[time value](#API conventions)）执行query操作花费的时间总量
      - query_time_in_millis：（integer）同`query_time`，单位为毫秒
      - query_current：（integer）正在运行query操作的数量
      - fetch_total：（integer）fetch操作的数量
      - fetch_time：（[time value](#API conventions)）执行fetch操作花费的时间总量
      - fetch_time_in_millis：（integer）同`fetch_time`，单位为毫秒
      - fetch_current：（integer）正在运行fetch操作的数量
      - scroll_total（integer）scroll操作的数量
      - scroll_time：（[time value](#API conventions)）执行scroll操作花费的时间总量
      - scroll_time_in_mills（integer）同`scroll_time`，单位为毫秒
      - scroll_current：（integer）正在运行scroll操作的数量
      - suggest_total：（integer）suggest操作的数量
      - suggest_time：（[time value](#API conventions)）执行suggest操作花费的时间总量
      - suggest_time_in_millis：（integer）同`suggest_time`，单位为毫秒
      - suggest_current：（integer）正在运行suggest操作的数量
    - merges：（object）节点上merge操作相关的统计信息
      - current：（integer）正在运行的merge操作的数量
      - current_docs：（integer）正在运行的merge操作中，合并的文档数量
      - current_size：（[byte value](#Byte size units)）当前执行文档合并的内存使用量
      - current_size_in_bytes：（integer）同`current_size`，单位为毫秒
      - total：（integer）merge操作的数量
      - total_time：（[time value](#API conventions)）执行merge操作花费的时间总量
      - total_time_in_millis：（integer）同`total_time`，单位为毫秒
      - total_docs：（integer）被合并的文档数量
      - total_size：（[byte value](#Byte size units)）被合并的文档大小
      - total_size_in_bytes：（integer）同`total_size`，单位为毫秒
      - total_stopped_time：（[time value](#API conventions)）停止合并操作上花费的时间（在某些情况下，出于性能或资源管理的考虑，可能需要暂时停止这些合并操作）
      - total_stopped_time_in_millis：（integer）同`total_stopped_time`，单位为毫秒
      - total_throttled_time：（[time value](#API conventions)）限制合并操作上花费时间
      - total_throttled_time_in_millis：（integer）同`total_throttled_time`，单位为毫秒
      - total_auto_throttle：（[byte value](#Byte size units)）自动限制合并操作的大小
      - total_auto_throttle_in_bytes：（integer）同`total_throttled_time`，单位为字节
    - refresh：（object）节点上refresh相关的统计信息
      - total：（Integer）refresh操作的数量
      - total_time：（[time value](#API conventions)）执行refresh操作花费的时间
      - total_time_in_millis：（integer）同`total_time`，单位为毫秒
      - external_total：（integer）
      - external_total_time：（[time value](#API conventions)）
      - external_total_time_in_millis：（integer）
      - listeners：（integer）
    - flush：（object）节点上flush相关的统计信息
      - total：（Integer）flush操作的数量
      - periodic：（integer）周期性flush操作的数量
      - total_time：（[time value](#API conventions)）执行flush操作花费的时间
      - total_time_in_millis：（integer）同`total_throttled_time`，单位为毫秒
    - warmer：（object）节点上index warming operation（index warmer（索引预热器）是一个功能，用于在搜索请求发生之前预加载特定的索引数据到内存中，从Elasticsearch 5.x版本开始，官方已经移除了这个功能）相关的统计信息
      - current：（integer）活跃的index warmer的数量
      - total：（integer）index warmer的数量总数
      - total_time：（[time value](#API conventions)）执行index warming operation花费的时间
      - total_time_in_millis（integer）同`total_time`，单位为毫秒
    - [query_cache](#Node query cache settings)：（object）节点上所有查询缓存的统计信息
      - memory_size：（[byte value](#Byte size units)）节点中所有分片上query cache占用的内存量
      - memory_size_in_bytes：（integer）同`memory_size`，，单位为字节
      - total_count：（integer）命中、未命中query cache和被移除的查询的总数
      - hit_count：（integer）命中query cache的次数
      - miss_count：（integer）未命中query cache的次数
      - cache_size：（integer）当前被缓存的query的数量
      - cache_count：（integer）被缓存的query的数量（包括已经被移除的query）
      - evictions：（integer）被移除的query的数量（基于query cache的缓存策略，一般是LRU策略）
    - fielddata：（object）节点上所有分片的field data cache的统计信息
      - memory_size：（[byte value](#Byte size units)）节点中所有分片用于缓存field data所需的内存量
      - memory_size_in_bytes：（integer）同`memory_size`，单位为字节
      - evictions：（integer）被移除的fielddata的数量
    - completion：（object）节点上所有completions（completion suggester功能）的统计信息
      - size：（[byte value](#Byte size units)）completion suggester功能在所有分配给该节点的分片上使用的总内存量
      - size：（integer）同`size`，单位为字节
    - segments：（object）节点上所有分片的段的统计信息
      - count：（integer）段的数量
      - memory：（[byte value](#Byte size units)）分配给该节点的所有分片中的段所使用的总内存量
      - memory_in_bytes：（integer）同`memory`，单位为字节
      - terms_memory：（[byte value](#Byte size units)）该节点的所有分片中的terms所使用的内存总量
      - terms_memory_in_bytes：（integer）同`terms_memory`，单位为字节
      - stored_fields_memory：（[byte value](#Byte size units)）该节点的所有分片中的存储字段所使用的内存总量
      - stored_fields_memory_in_bytes：（integer）同`stored_fields_memory`，单位为字节
      - term_vectors_memory：（[byte value](#Byte size units)）该节点的所有分片中的term vector所使用的内存总量
      - term_vectors_memory_in_bytes：（integer）同`term_vectors_memory`，单位为字节
      - norms_memory：（[byte value](#Byte size units)）该节点的所有分片中的准化因子所使用的内存总量
      - norms_memory_in_bytes：（integer）同`norms_memory`，单位为字节
      - points_memory：（[byte value](#Byte size units)）该节点的所有分片中的点数据结构所使用的内存总量
      - points_memory_in_bytes：（integer）同`points_memory`，单位为字节
      - doc_values_memory：（[byte value](#Byte size units)）该节点的所有分片中的doc value所使用的内存总量
      - doc_values_memory_in_bytes：（integer）同`doc_values_memory`，单位为字节
      - index_writer_memory：（[byte value](#Byte size units)）该节点的所有分片中的index_writer（当文档被添加或更新时，index_writer负责处理这些更改，并将它们写入磁盘）所使用的内存总量
      - index_writer_memory_in_bytes：（integer）同`index_writer_memory`，单位为字节
      - version_map_memory：（[byte value](#Byte size units)）该节点的所有分片中的version_map（version map是一种内部数据结构，用于追踪和管理索引文档的版本信息，助于在执行读写操作时快速检查版本冲突）所使用的内存总量
      - version_map_memory_in_bytes：（integer）同`version_map_memory`，单位为字节
      - fixed_bit_set：（[byte value](#Byte size units)）该节点的所有分片中的fixed_bit_set（fixed_bit_set是一个数据结构，用于追踪哪些文档被标记为已删除）所使用的内存总量
      - fixed_bit_set_memory_in_bytes：（integer）同`fixed_bit_set`，单位为字节
      - max_unsafe_auto_id_timestamp：（integer）最近重试索引请求的时间，该时间以自Unix纪元（1970年1月1日）起的毫秒数计（这个属性主要用于处理自动生成的文档ID时的内部机制，以确保即使在高并发的情况下，文档ID的唯一性和一致性仍然得到维护）
      - file_sizes（object）：段文件大小相关的统计信息
        - size：（[byte value](#Byte size units)）段文件的大小
        - size_in_bytes：（integer）同`size`，单位为字节
        - description：（string）段文件的描述信息
    - translog：（object）节点上transaction log operations的统计信息
      - operations：（integer）事务日志（transaction log）操作的数量
      - size：（[byte value](#Byte size units)）事务日志文件的大小
      - size_in_bytes：（integer）同`size`，单位为字节
      - uncommitted_operations（integer）未提交的事务日志操作的数量
      - uncommitted_size：（[byte value](#Byte size units)）未提交事务日志的大小
      - uncommitted_size_in_bytes：（integer）同`uncommitted_size`，单位为字节
      - earliest_last_modified_age：（integer）事务日志中最早的未提交操作距离现在的时间
    - [request_cache](#Shard request cache settings)：（object）节点上所有分片的请求缓存的统计信息
      - memory_size：（[byte value](#Byte size units)）节点中所有分片上request cache占用的内存量
      - memory_size_in_bytes：（integer）同`memory_sizes`，单位为字节
      - evictions：（integer）request cache操作的数量
      - hit_count：（integer）命中request cache的次数
      - miss_count：（integer）未命中request cache的次数
    - recovery：（object）节点上恢复操作的统计信息
      - current_as_source：（integer）恢复操作中，作为源数据的索引分片的数量
      - current_as_target：（integer）恢复操作中，作为目标索引分片的数量
      - throttle_time：（[time value](#API conventions)）因限流导致延迟花费的时间
      - throttle_time_in_millis：（integer）同`throttle_time`，单位为毫秒
    - shard_stats：（object）节点上所有分片的统计信息
      - total_count：（integer）分片到节点上的分片数量
  - os：（object）节点所在操作系统的统计信息
    - timestamp：（integer）操作系统统计信息上次刷新时间。since the [Unix Epoch](https://en.wikipedia.org/wiki/Unix_time)
    - cpu：（object）节点上cpu的统计信息
      - percent：（integer）整个系统最近的CPU使用量。不支持的话则为`-1`
      - load_average：（object）系统平均负载的统计信息
        - 1m：（float）系统一分钟内的平均负载（如果不支持则不提供这个字段）
        - 5m：（float）系统五分钟内的平均负载（如果不支持则不提供这个字段）
        - 15m：（float）系统十五分钟内的平均负载（如果不支持则不提供这个字段）
    - mem：（object）节点上内存使用的统计信息
      - total：（[byte value](#Byte size units)）物理内存总量
      - total_in_bytes：（integer）同`total`，单位为字节
      - adjusted_total：（[byte value](#Byte size units)）如果使用`es.total_memory_bytes`覆盖了系统的物理内存总量，那么就报告这个值，否则该值跟`total`是一样的
      - adjusted_total_in_bytes：（integer）同`adjusted_total`，单位为字节
      - free：（[byte value](#Byte size units)）可用的物理内存量
      - free_in_bytes：（integer）同`free`，单位为字节
      - used：（[byte value](#Byte size units)）已使用的物理内存量
      - used_in_bytes：（integer）同`used`，单位为字节
      - free_percent：（integer）可用的物理内存量的占比
      - used_percent：（integer）已使用的物理内存量的占比
    - swap（object）：节点上交换空间的统计信息
      - total：（[byte value](#Byte size units)）交换空间的总量
      - total_in_bytes：（integer）同`total`，单位为字节
      - free：（[byte value](#Byte size units)）可用的交换空间的总量
      - free_in_bytes：（integer）同`free`，单位为字节
      - used：（[byte value](#Byte size units)）已经使用的交换空间的总量
      - used_in_bytes：（integer）同`used`，单位为字节
    - cgroup（Linux only）（object）节点上cgroup的统计信息
      - 未完成
  - process：（object）节点上进程的统计信息
    - 未完成
  - jvm：（object）节点上JVM的统计信息
    - 未完成
  - thread_pool：（object）节点上线程池的统计信息
    - 未完成
  - fs：（object）节点上文件储存的统计信息
    - 未完成
  - transport：（object）节点上transport的统计信息
    - 未完成
  - http：（object）节点上http的统计信息
    - 未完成
  - breakers：（object）节点上断路器（circuit breaker）的统计信息
  - script：（object）节点上脚本的统计信息
    - 未完成
  - discovery：（object）节点上服务发现的统计信息
    - 未完成
  - ingest：（object）节点上ingest的统计信息
    - 未完成
  - indexing_pressure：（object）节点上[indexing pressure](#Indexing pressure)的统计信息
    - 未完成
  - adaptive_selection：（object）节点上自适应选择（adaptive selection，用于基于各个节点的当前性能和负载状况来优化查询和操作的路由决策）的统计信息
    - 未完成

##### Examples

```text
# return just indices
GET /_nodes/stats/indices

# return just os and process
GET /_nodes/stats/os,process

# return just process for node with IP address 10.0.0.1
GET /_nodes/10.0.0.1/stats/process
```

&emsp;&emsp;可以通过`/_nodes/stats/_all`或者`/_nodes/stats?metric=_all`请求所有的统计信息。

&emsp;&emsp;你可以在`node`、`indices`、或者`shards`层获取索引的统计信息。

```text
# Fielddata summarized by node
GET /_nodes/stats/indices/fielddata?fields=field1,field2

# Fielddata summarized by node and index
GET /_nodes/stats/indices/fielddata?level=indices&fields=field1,field2

# Fielddata summarized by node, index, and shard
GET /_nodes/stats/indices/fielddata?level=shards&fields=field1,field2

# You can use wildcards for field names
GET /_nodes/stats/indices/fielddata?fields=field*
```

&emsp;&emsp;你可以获取节点上执行查询的search group的统计信息。

```text
# All groups with all stats
GET /_nodes/stats?groups=_all

# Some groups from just the indices stats
GET /_nodes/stats/indices?groups=foo,bar
```

###### Retrieve ingest statistics only

&emsp;&emsp;若只要返回ingest相关的节点统计，将`<metric>`设置为`ingest`并且使用[filter_path](#Response Filtering)查询参数。

```text
GET /_nodes/stats/ingest?filter_path=nodes.*.ingest
```

&emsp;&emsp;你可以使用`metric`以及`filter_path`查询参数获取相同响应。

```text
GET /_nodes/stats?metric=ingest&filter_path=nodes.*.ingest
```

&emsp;&emsp;若要进一步精确响应，修改`filter_path`的值。例如的下面的请求只返回ingest pipeline的统计信息。

```text
GET /_nodes/stats?metric=ingest&filter_path=nodes.*.ingest.pipelines
```

#### Pending cluster tasks API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cluster-pending.html)

&emsp;&emsp;返回集群层（cluster-level）还未执行结束的变更。

##### Request

`GET /_cluster/pending_tasks`

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`monitor`或者`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;该接口返回所有的集群层未完成的变更（比如创建索引，更新mapping，分片分配或者分配失败）。

> NOTE：这个API返回一个集群状态待处理更新的列表。这些更新与[Task Management API](#Task management API)报告的任务不同，后者包括周期性任务和用户发起的任务，如节点统计、搜索查询或创建索引请求。然而，如果用户发起的任务（例如创建索引命令）导致了集群状态的更新，这个任务的活动可能会同时被Task Management API和Pending cluster tasks报告。

##### Path Parameters

- `local`：（Optional, Boolean）如果为`true`，该请求只从本地节点获取信息。默认是`false`。即从master node获取
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。

##### Response body

- task：（object）pending中的任务列表
- insert_order：（integer）一个数值代表该任务插入到了任务队列
- priority：（string）pending任务的优先级。合法的优先级从大到小依次为：`IMMEDIATE` > `URGENT` > `HIGH` > `NORMAL` > `LOW` > `LANGUID`
- source：（string）提供了集群任务的一般描述，可能包括原因和来源（如果一个任务是由于创建新的索引而启动的，source字段可能会包含这方面的信息，如“创建索引[索引名]”）。
- executing：（boolean）布尔值，表示pending任务当前是否在执行
- time_in_queue_millis：（integer）表示自任务等待执行以来的时间（以毫秒为单位）
- time_in_queue：（string）表示自任务等待执行以来的时间

##### Example

&emsp;&emsp;由于cluster level的变更都非常快，因此通常这个请求会返回一个空的list。然而，如果任务队列开始堆积，响应中就有类似下面的内容：

```text
{
   "tasks": [
      {
         "insert_order": 101,
         "priority": "URGENT",
         "source": "create-index [foo_9], cause [api]",
         "executing" : true,
         "time_in_queue_millis": 86,
         "time_in_queue": "86ms"
      },
      {
         "insert_order": 46,
         "priority": "HIGH",
         "source": "shard-started ([foo_2][1], node[tMTocMvQQgGCkj7QDHl3OA], [P], s[INITIALIZING]), reason [after recovery from shard_store]",
         "executing" : false,
         "time_in_queue_millis": 842,
         "time_in_queue": "842ms"
      },
      {
         "insert_order": 45,
         "priority": "HIGH",
         "source": "shard-started ([foo_2][0], node[tMTocMvQQgGCkj7QDHl3OA], [P], s[INITIALIZING]), reason [after recovery from shard_store]",
         "executing" : false,
         "time_in_queue_millis": 858,
         "time_in_queue": "858ms"
      }
  ]
}
```

#### Remote cluster info API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cluster-remote-info.html)

&emsp;&emsp;返回远程集群（remote cluster）的配置信息

##### Request

`GET /_remote/info`

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`monitor`或者`manage`的[cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;该接口能让你获取远程集群的配置信息。返回的信息中用远端集群的别名作为key,包含连接和endpoint信息。

##### Response body

- mode：远程集群的连接模式。返回`sniff`或`proxy`
- connected：如果至少连接了一个远程集群则该值为`true`
- initial_connect_timeout：连接远程集群的初始超时时间
- skip_unavailable：跨集群搜索（CCS，cross-cluster search）在查询时是否跳过不可用的节点。如果为`true`，CCS会忽略这些集群返回的相关错误。参考[Optional remote clusters](#Optional-remote-clusters)
- seeds：当配置sniff后，远程集群中初始化的seed transport addresses
- num_nodes_connected：当配置sniff后，连接的远程集群的节点数量
- max_connections_per_cluster：当配置sniff模式时，为远程集群维护的最大连接数
- proxy_address：当配置代理模式时，远程连接的地址
- num_proxy_sockets_connected：当配置代理模式时，打开的远程集群套接字连接数
- max_proxy_socket_connections：当配置代理模式时，远程集群的最大套接字连接数

#### Task management API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/tasks.html)

> WARNING：task management API是一个新功能，但你应该将其认为是一个测试版功能。这个API可能会发生调整使得不能向后兼容。对于这个功能的更多信息见[#51628](https://github.com/elastic/elasticsearch/issues/51628)

&emsp;&emsp;返回集群中正在执行的任务信息。

##### Request

`GET /_tasks/<task_id>`
`GET /_tasks`

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;该接口返回集群中一个或多个节点上正在运行的任务信息。

##### Path parameters

- `<task_id>`：（Optional, string）要求查询的任务的ID（node_Id:task_number）

##### Query parameters

- actions：（Optional, string）用逗号隔开，或者通配符表达式，用来限制请求
- detailed：（Optional, Boolean）如果为`true`，响应中包含关于分片恢复的详细信息。默认为`false`
- group_by：（Optional, string）响应中对任务进行分组的字段（key）
  - nodes：（Default）节点ID
  - parents：父级任务ID
  - none：不对任务进行分组
- node_id：（Optional, string）用逗号隔开的节点ID或节点名称列表，用来限制返回的信息
- parent_task_id：（Optional, string）父级任务ID，用来限制返回的信息。若要返回所有的任务，不使用这个参数或者另该值为`-1`
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- wait_for_completion：（Optional, Boolean）如果为`true`。请求会阻塞直到操作完成。默认值为`false`

##### Response codes

- `404（missing resources）`
  - 如果没有找到指定的`<taks_id>`，这个响应码说明没有资源匹配到该请求

##### Examples

```text
GET _tasks 
GET _tasks?nodes=nodeId1,nodeId2 
GET _tasks?nodes=nodeId1,nodeId2&actions=cluster:* 
```

&emsp;&emsp;第1行，获取集群中所有节点上的所有任务的信息
&emsp;&emsp;第2行，获取`nodeId1`和`nodeId2`上的所有任务的信息。关于如何选择单个节点的更多信息见[Node specification](#Node specification)。
&emsp;&emsp;第3行，获取节点`nodeId1`和`nodeId2`上跟集群相关的所有任务的信息

&emsp;&emsp;这个接口返回下面的结果：

```text
{
  "nodes" : {
    "oTUltX4IQMOUUVeiohTt8A" : {
      "name" : "H5dfFeA",
      "transport_address" : "127.0.0.1:9300",
      "host" : "127.0.0.1",
      "ip" : "127.0.0.1:9300",
      "tasks" : {
        "oTUltX4IQMOUUVeiohTt8A:124" : {
          "node" : "oTUltX4IQMOUUVeiohTt8A",
          "id" : 124,
          "type" : "direct",
          "action" : "cluster:monitor/tasks/lists[n]",
          "start_time_in_millis" : 1458585884904,
          "running_time_in_nanos" : 47402,
          "cancellable" : false,
          "parent_task_id" : "oTUltX4IQMOUUVeiohTt8A:123"
        },
        "oTUltX4IQMOUUVeiohTt8A:123" : {
          "node" : "oTUltX4IQMOUUVeiohTt8A",
          "id" : 123,
          "type" : "transport",
          "action" : "cluster:monitor/tasks/lists",
          "start_time_in_millis" : 1458585884904,
          "running_time_in_nanos" : 236042,
          "cancellable" : false
        }
      }
    }
  }
}
```

###### Retrieve information from a particular task

&emsp;&emsp;同样可以获取特定任务的信息。下面的例子中获取了任务`oTUltX4IQMOUUVeiohTt8A:124`的信息：

```text
GET _tasks/oTUltX4IQMOUUVeiohTt8A:124
```

&emsp;&emsp;如果没有找到该任务，API返回一个404。

&emsp;&emsp;获取指定任务的所有子任务：

```text
GET _tasks?parent_task_id=oTUltX4IQMOUUVeiohTt8A:123
```

&emsp;&emsp;如果父级任务不存在，API返回一个404。

###### Get more information about tasks

&emsp;&emsp;你也可以使用`detailed`这个请求参数来获取更多关于运行中的任务信息。对用来区分任务很有用，不过需要一定的开销。例如，下面是使用`detailed`请求参数获取到的所有的查询：

```text
GET _tasks?actions=*search&detailed
```

&emsp;&emsp;API返回下面的结果：

```text
{
  "nodes" : {
    "oTUltX4IQMOUUVeiohTt8A" : {
      "name" : "H5dfFeA",
      "transport_address" : "127.0.0.1:9300",
      "host" : "127.0.0.1",
      "ip" : "127.0.0.1:9300",
      "tasks" : {
        "oTUltX4IQMOUUVeiohTt8A:464" : {
          "node" : "oTUltX4IQMOUUVeiohTt8A",
          "id" : 464,
          "type" : "transport",
          "action" : "indices:data/read/search",
          "description" : "indices[test], types[test], search_type[QUERY_THEN_FETCH], source[{\"query\":...}]",
          "start_time_in_millis" : 1483478610008,
          "running_time_in_nanos" : 13991383,
          "cancellable" : true,
          "cancelled" : false
        }
      }
    }
  }
}
```

&emsp;&emsp;`description`字段包含了某任务正在执行的特定请求相关的可读的内容，例如上面示例中搜索任务正在执行的搜索请求。有些其他类型的任务有不同的描述，比如[\_reindex](#Reindex API)中同时有`search`以及`destination`，或者[\_bulk](#Bulk API)中有请求的数量和目标索引。许多请求的描述都是空的，是因为这些请求更加详细的信息不能轻易获取或者对于明确这个请求不是很有帮助。

> IMPORTANT：带有`detailed`的`_task`请求可能也会返回一个`status`。它是任务内部状态的一个报告。因此不同的任务有不同的格式。我们尝试对于某一个任务的状态在不同的版本间保持一致性，但这通常是不可能的，因为我们有时会更改实现方式。因此对于某些特定的请求，我们可能会移除`status`这个字段，那在minor release后，对这个字段进行解析可能会出错

###### Wait for completion

&emsp;&emsp;task API可以等待特定任务结束后再返回。下面的请求会阻塞10秒或者id为`oTUltX4IQMOUUVeiohTt8A:12345`的任务完成后返回。

```text
GET _tasks/oTUltX4IQMOUUVeiohTt8A:12345?wait_for_completion=true&timeout=10s
```

&emsp;&emsp;你也可以等待某些动作对应的任务的完成。下面的命令会等待所有`reindex`任务的完成：

```text
GET _tasks?actions=*reindex&wait_for_completion=true&timeout=10s
```

###### Task Cancellation

&emsp;&emsp;如果某个运行时间很长的任务支持取消，那可以通过这个API取消。下面的例子取消了`oTUltX4IQMOUUVeiohTt8A:12345`这个任务：

```text
POST _tasks/oTUltX4IQMOUUVeiohTt8A:12345/_cancel
```

&emsp;&emsp;取消任务命令支持跟上文中说到的一些参数，因此可以在同一时间取消多个任务。例如，下面的命令会取消在节点`nodeId1`和`nodeId2`上运行的所有reindex任务。

```text
POST _tasks/_cancel?nodes=nodeId1,nodeId2&actions=*reindex
```

&emsp;&emsp;有些任务被取消后，可能还会继续运行一段时间，因为它可能无法立即安全地停止当前活动。罗列任务的API（list tasks API）会继续列出这些已取消的任务，直到它们完成。list tasks API的响应中的`cancelled`标志表明取消命令已被处理，任务将尽快停止。

###### Task Grouping

&emsp;&emsp;可以通过task API的`group_by`参数对任务根据节点（默认）或者根据父级任务进行分组。下面的命令根据父级任务分组：

```text
GET _tasks?group_by=parents
```

&emsp;&emsp;可以指定`group_by`的参数为`none`来关闭分组：

```text
GET _tasks?group_by=none
```

###### Identifying running tasks

&emsp;&emsp;当在HTTP请求头中提供X-Opaque-Id头部时，它将在响应头以及任务信息的`headers`字段中返回。这允许跟踪特定调用，或将某些任务与启动它们的客户端关联起来。这个功能有助于更好地管理和识别正在运行的任务：

```text
curl -i -H "X-Opaque-Id: 123456" "http://localhost:9200/_tasks?group_by=parents"
```

&emsp;&emsp;返回下面的结果：

```text
HTTP/1.1 200 OK
X-Opaque-Id: 123456 
content-type: application/json; charset=UTF-8
content-length: 831

{
  "tasks" : {
    "u5lcZHqcQhu-rUoFaqDphA:45" : {
      "node" : "u5lcZHqcQhu-rUoFaqDphA",
      "id" : 45,
      "type" : "transport",
      "action" : "cluster:monitor/tasks/lists",
      "start_time_in_millis" : 1513823752749,
      "running_time_in_nanos" : 293139,
      "cancellable" : false,
      "headers" : {
        "X-Opaque-Id" : "123456" 
      },
      "children" : [
        {
          "node" : "u5lcZHqcQhu-rUoFaqDphA",
          "id" : 46,
          "type" : "direct",
          "action" : "cluster:monitor/tasks/lists[n]",
          "start_time_in_millis" : 1513823752750,
          "running_time_in_nanos" : 92133,
          "cancellable" : false,
          "parent_task_id" : "u5lcZHqcQhu-rUoFaqDphA:45",
          "headers" : {
            "X-Opaque-Id" : "123456" 
          }
        }
      ]
    }
  }
}
```

&emsp;&emsp;第1行，id作为响应中header的一部分
&emsp;&emsp;第17行，由REST请求初始化的这个任务的id，
&emsp;&emsp;第30行，子任务中由REST请求初始化的这个任务的id



#### Voting configuration exclusions API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/voting-config-exclusions.html)

&emsp;&emsp;添加或者移除[voting configuration exclusion list](#Voting configurations)中的master-eligible 节点。

##### Request

`POST /_cluster/voting_config_exclusions?node_names=<node_names>`

`POST /_cluster/voting_config_exclusions?node_ids=<node_ids>`

`DELETE /_cluster/voting_config_exclusions`

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage` [cluster privilege](#Cluster privileges)来使用这个API。
- 如果开启了[operator privileges feature](#Operator privileges)，只有具有操作权限的用户才能使用这个API

##### Description

&emsp;&emsp;默认情况下，如果集群中有超过三个具备主节点资格的节点，并且一次性移除的这些节点少于一半，那么[voting configuration](#Voting configurations)会自动缩减。

&emsp;&emsp;如果需要将voting configuration缩减到少于三个节点，或一次性移除一半或更多的master-eligible的节点，必须使用此 API 手动从voting configuration中移除已离开的节点。这包括在voting configuration的排除列表中为该节点添加条目，促使集群尝试重新配置voting configuration以移除该节点并防止其返回。

&emsp;&emsp;如果接口调用失败，你可以安全的进行重试。一旦有成功的响应，那么就能保证节点从voting configuration总移除并且不会重新加入（reinstated）。

> NOTE：投票排除（voting exclusions）仅在短时间内从集群中移除至少一半的master-eligible时才需要。在移除master-ineligible或少于一半的具备主节点资格的节点时，不需要投票排除。

&emsp;&emsp;更多信息见: [Removing master-eligible nodes](#Removing master-eligible nodes)。

##### Query parameters

- node_names：从voting configuration中移除的节点，用逗号隔开。如果指定了节点名称，你可能无法指定`?node_ids`
- node_ids：从voting configuration中移除的节点，用逗号隔开。如果指定了节点名称，你可能无法指定`?node_names`
- timeout：（Optional, [time units](#API conventions)）在添加一个voting configuration exclusion时，请求会等待指定的节点从voting configuration中移除后再返回。通过`?timeout`指定时超时时间。如果在满足条件之前超时，请求将失败并返回错误。默认值为 30 秒。
- wait_for_removal：（Optional, Boolean）它指定在清除voting configuration 排除列表之前是否等待所有被排除的节点从集群中移除。默认值为 true，这意味着在 API 采取任何行动之前，所有被排除的节点必须从集群中移除。如果设置为 false，则即使一些被排除的节点仍在集群中，voting configuration 排除列表也会被清除。简而言之，这个参数控制了在清除排除列表之前是否需要等待所有标记为排除的节点实际离开集群

##### Examples

&emsp;&emsp;将名为`nodeName1`和`nodeName2`的节点添加到voting configuration排除列表中：

```text
POST /_cluster/voting_config_exclusions?node_names=nodeName1,nodeName2
```

&emsp;&emsp;Remove all exclusions from the list:

```text
DELETE /_cluster/voting_config_exclusions
```

#### Create or update desired nodes API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/update-desired-nodes.html)

> NOTE：这个功能不能直接使用，而是设计为只被[Elasticsearch Service](https://www.elastic.co/cn/cloud/elasticsearch-service/signup?page=docs&placement=docs-body)、[Elastic Cloud Enterprise](https://www.elastic.co/guide/en/cloud-enterprise/current/index.html)和[Elastic Cloud on Kubernetes](https://www.elastic.co/guide/en/cloud-on-k8s/current/index.html)使用。不支持直接使用。

&emsp;&emsp;创建或者更新所需节点。

##### Request

```text
PUT /_internal/desired_nodes/<history_id>/<version>
{
    "nodes" : [
        {
            "settings" : {
                 "node.name" : "instance-000187",
                 "node.external_id": "instance-000187",
                 "node.roles" : ["data_hot", "master"],
                 "node.attr.data" : "hot",
                 "node.attr.logical_availability_zone" : "zone-0"
            },
            "processors" : 8,
            "memory" : "58gb",
            "storage" : "1700gb",
            "node_version" : "{version}"
        }
    ]
}
```

##### Query parameters

- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。

##### Description

&emsp;&emsp;这个接口创建或者更新所需节点。外部编排器（external orchestrator）可以使用此API来让Elasticsearch了解集群拓扑，包括未来的变化，如添加或移除节点。利用这些信息，系统能够做出更好的决策。

##### Examples

&emsp;&emsp;In this example, a new version for the desired nodes with history Ywkh3INLQcuPT49f6kcppA is created. This API only accepts monotonically increasing versions.

```text
PUT /_internal/desired_nodes/Ywkh3INLQcuPT49f6kcppA/100
{
    "nodes" : [
        {
            "settings" : {
                 "node.name" : "instance-000187",
                 "node.external_id": "instance-000187",
                 "node.roles" : ["data_hot", "master"],
                 "node.attr.data" : "hot",
                 "node.attr.logical_availability_zone" : "zone-0"
            },
            "processors" : 8,
            "memory" : "58gb",
            "storage" : "1700gb",
            "node_version" : "{version}"
        }
    ]
}
```

&emsp;&emsp;这个API返回下面的结果：

```text
{
  "replaced_existing_history_id": false
}
```

#### Get desired nodes API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/get-desired-nodes.html)

> NOTE：这个功能不能直接使用，而是设计为只被[Elasticsearch Service](https://www.elastic.co/cn/cloud/elasticsearch-service/signup?page=docs&placement=docs-body)、[Elastic Cloud Enterprise](https://www.elastic.co/guide/en/cloud-enterprise/current/index.html)和[Elastic Cloud on Kubernetes](https://www.elastic.co/guide/en/cloud-on-k8s/current/index.html)使用。不支持直接使用。

&emsp;&emsp;获取所需节点

##### Request

```text
GET /_internal/desired_nodes/_latest
```

##### Description

&emsp;&emsp;这个接口获取最新的所需的节点。

##### Examples

&emsp;&emsp;这个例子获取最新的所需的节点。

```text
GET /_internal/desired_nodes/_latest
```

&emsp;&emsp;返回以下结果：

```text
{
    "history_id": <history_id>,
    "version": <version>,
    "nodes": [
        {
            "settings": <node_settings>,
            "processors": <node_processors>,
            "memory": "<node_memory>",
            "storage": "<node_storage>",
            "node_version": "<node_version>"
        }
    ]
}
```

#### Delete desired nodes API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/delete-desired-nodes.html)

> NOTE：这个功能不能直接使用，而是设计为只被[Elasticsearch Service](https://www.elastic.co/cn/cloud/elasticsearch-service/signup?page=docs&placement=docs-body)、[Elastic Cloud Enterprise](https://www.elastic.co/guide/en/cloud-enterprise/current/index.html)和[Elastic Cloud on Kubernetes](https://www.elastic.co/guide/en/cloud-on-k8s/current/index.html)使用。不支持直接使用。

&emsp;&emsp;删除所需节点

##### Request

```text
DELETE /_internal/desired_nodes
```

##### Description

&emsp;&emsp;这个接口删除所需节点。

##### Examples

&emsp;&emsp;下面的例子删除当前所需节点

```text
DELETE /_internal/desired_nodes
```

### Cross-cluster replication APIs
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cluster-nodes-stats.html)

&emsp;&emsp;你可以使用这些接口来执行CCR操作。

##### Top-Level

- [Get cross-cluster replication stats](#Get cross-cluster replication stats API)

##### Follow

- [Create follower index](#Create follower API)
- [Pause follower](#Pause follower API)
- [Resume follower](#Resume follower API)
- [Convert follower index to a regular index](#Unfollow API)
- [Remove follower retention leases from the leader](#Forget follower API)
- [Get stats about follower indices](#Get follower stats API)
- [Get info about follower indices](#Get follower info API)

##### Auto-follow

- [Create auto-follow pattern](#Create auto-follow pattern API)
- [Delete auto-follow pattern](#Delete auto-follow pattern API)
- [Get auto-follow patterns](#Get auto-follow pattern API)
- [Pause auto-follow pattern](#Pause auto-follow pattern API)
- [Resume auto-follow pattern](#Resume auto-follow pattern API)

#### Get cross-cluster replication stats API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cluster-nodes-stats.html)

&emsp;&emsp;获取cross-cluster replication的统计信息。

##### Request

```text
GET /_ccr/stats
```
##### Prerequisites

- 如果开启了Elasticsearch security features，你必须在包含follower index的集群上有`monitor`的[cluster privilege](#Cluster privileges)。见[Security privileges](####Security privileges)。

##### Description

&emsp;&emsp;这个API获取CCR的统计信息。它返回跟CCR相关的所有统计信息。该接口返回auto-following信息、以及跟[get  follower stats API](#Get follower stats API)一样的分片层（shard-level）的信息。

##### Response body

- auto_follow_stats：（object）auto-follow coordinator的统计信息
  - number_of_failed_follow_indices：（long）auto-follow coordinator自动跟随失败的索引数量。最近的失败日志记录在master node以及`auto_follow_stats.recent_auto_follow_errors`中
  - number_of_failed_remote_cluster_state_requests：（long）auto-follow coordinator无法从auto-follow pattern中定义的远端集群获取集群状态的次数
  - number_of_successful_follow_indices：（long）auto-follow coordinator成功跟随的索引数量
  - recent_auto_follow_errors：（array）auto-follow coordinator中错误相关信息
- follow_stats：（object）描述了follower index的分片层级的统计信息。详情见[get follower stats API](#Get follower stats API)

##### Example

&emsp;&emsp;这个例子获取CCR的统计信息：

```text
GET /_ccr/stats
```

&emsp;&emsp;该接口返回以下结果：

```text
{
  "auto_follow_stats" : {
    "number_of_failed_follow_indices" : 0,
    "number_of_failed_remote_cluster_state_requests" : 0,
    "number_of_successful_follow_indices" : 1,
    "recent_auto_follow_errors" : [],
    "auto_followed_clusters" : []
  },
  "follow_stats" : {
    "indices" : [
      {
        "index" : "follower_index",
        "shards" : [
          {
            "remote_cluster" : "remote_cluster",
            "leader_index" : "leader_index",
            "follower_index" : "follower_index",
            "shard_id" : 0,
            "leader_global_checkpoint" : 1024,
            "leader_max_seq_no" : 1536,
            "follower_global_checkpoint" : 768,
            "follower_max_seq_no" : 896,
            "last_requested_seq_no" : 897,
            "outstanding_read_requests" : 8,
            "outstanding_write_requests" : 2,
            "write_buffer_operation_count" : 64,
            "follower_mapping_version" : 4,
            "follower_settings_version" : 2,
            "follower_aliases_version" : 8,
            "total_read_time_millis" : 32768,
            "total_read_remote_exec_time_millis" : 16384,
            "successful_read_requests" : 32,
            "failed_read_requests" : 0,
            "operations_read" : 896,
            "bytes_read" : 32768,
            "total_write_time_millis" : 16384,
            "write_buffer_size_in_bytes" : 1536,
            "successful_write_requests" : 16,
            "failed_write_requests" : 0,
            "operations_written" : 832,
            "read_exceptions" : [ ],
            "time_since_last_read_millis" : 8
          }
        ]
      }
    ]
  }
}
```

#### Create follower API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ccr-put-follow.html)

&emsp;&emsp;创建一个follower index。

##### Request

```text
PUT /<follower_index>/_ccr/follow?wait_for_active_shards=1
{
  "remote_cluster" : "<remote_cluster>",
  "leader_index" : "<leader_index>"
}
```

##### Prerequisites

&emsp;&emsp;如果开启了Elasticsearch security features，你必须在follower index上有`write`、`monitor`、`manage_follow_index`的index privilege。在leader index上必须要有`read`和`monitor`的index privilege。在包含follower index的集群上要有`manage_ccr`的cluster privilege。更多信息见[Security privileges](####Security privileges)

##### Description

&emsp;&emsp;该接口用来创建一个新的follower index配置为跟随（follow）一个被引用的leader  index。。该接口返回后，生成了follower index，然后CCR开始从leader index执行复制操作到follower index中

##### Path parameters

- `<follower_index>`：（Required, string）follower index的名称

##### Query parameters

- wait_for_active_shards：（Optional,integer）在收到响应前指定的分片数量要处于活跃（active）状态。默认不等待。分片必须在活跃之前需先从leader index 中恢复。恢复一个follower分片要求传输远端的Lucene段文件到follower index中。

##### Request body

- leader_index：（string）在leader cluster 中跟随（follow）的索引名称
- remote_cluster：（string）leader Index所在的[remote cluster](###Remote clusters)
- settings：（object）覆盖来自leader index 的settings。有些settings是不能被覆盖的（比如`index.number_of_shards`）
- max_read_request_operation_count：（integer）从远端集群中执行的读请求中，每一个请求中包含的操作数量最大值（设置`max_read_request_operation_count`为1000，那么在执行一次从leader到follower的数据同步时，每一次读取请求将最多包含1000个操作）
- max_outstanding_read_requests：（long）正在从远端集群中执行读取请求的数量最
- max_read_request_size：（[byte value](#Byte size units) ）从远端集群中的读请求中，批量操作的字节数最大值
- max_write_request_operation_count：（integer）在follower上每一个批量写入的最大操作数
- max_write_request_size：（[byte value](#Byte size units) ）在follower上每一个批量写入的最大字节数
- max_outstanding_write_requests：（integer）正在follower上执行写入请求的数量最大值
- max_write_buffer_count：（integer）排队等待写入的最大操作数。一旦达到该限制，将暂停从leader中拉去更多操作，直到在队里中的操作已被写入
- max_write_buffer_size：（[byte value](#Byte size units) ）排队等待写入的最大字节数。一旦达到该限制，将暂停从leader中拉去更多操作，直到在队里中的操作已被写入
- max_retry_delay：（[time value](#API conventions)）某个操作发生异常后，在重试之前的等待时间。基于exponential Backoff策略
- read_poll_timeout：（[time value](#API conventions)）follower index同步leader index时，等待远端集群中出现新的操作的时间。超时后，拉去操作将返回到follower，然后更新一些统计信息，随后再次尝试从leader中读取

###### Default values

&emsp;&emsp;下面的输出来自follower info API，描述了这个接口中请求参数的所有默认值：

```text
{
  "follower_indices" : [
    {
      "parameters" : {
        "max_read_request_operation_count" : 5120,
        "max_read_request_size" : "32mb",
        "max_outstanding_read_requests" : 12,
        "max_write_request_operation_count" : 5120,
        "max_write_request_size" : "9223372036854775807b",
        "max_outstanding_write_requests" : 9,
        "max_write_buffer_count" : 2147483647,
        "max_write_buffer_size" : "512mb",
        "max_retry_delay" : "500ms",
        "read_poll_timeout" : "1m"
      }
    }
  ]
}
```

##### Example

&emsp;&emsp;下面的例子创建了一个名为`follower_index`的follower index：

```text
PUT /follower_index/_ccr/follow?wait_for_active_shards=1
{
  "remote_cluster" : "remote_cluster",
  "leader_index" : "leader_index",
  "settings": {
    "index.number_of_replicas": 0
  },
  "max_read_request_operation_count" : 1024,
  "max_outstanding_read_requests" : 16,
  "max_read_request_size" : "1024k",
  "max_write_request_operation_count" : 32768,
  "max_write_request_size" : "16k",
  "max_outstanding_write_requests" : 8,
  "max_write_buffer_count" : 512,
  "max_write_buffer_size" : "512k",
  "max_retry_delay" : "10s",
  "read_poll_timeout" : "30s"
}
```

&emsp;&emsp;该接口返回下面的结果：

```text
{
  "follow_index_created" : true,
  "follow_index_shards_acked" : true,
  "index_following_started" : true
}
```

#### Pause follower API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ccr-post-pause-follow.html)

&emsp;&emsp;暂停一个follower index。

##### Request

```text
POST /<follower_index>/_ccr/pause_follow
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须在包含follower index的集群上有`manage_ccr`的[cluster privilege](#Cluster privileges)。见[Security privileges](####Security privileges)。

##### Description

&emsp;&emsp;该接口用来暂停一个follower index。当接口返回后，follower index不会从leader index中获取任何额外的操作。你可以通过[resume follwer API](#Resume follower API)恢复。暂停然后恢复一个follower index可以用来修改following task的配置。

##### Path parameters

- `<follower_index>`：（Required, string）follower index的名称

##### Example

&emsp;&emsp;下面的例子暂停了一个名为`follower_index`的follower index。

```text
POST /follower_index/_ccr/pause_follow
```

&emsp;&emsp;该接口返回以下结果：

```text
{
  "acknowledged" : true
}
```

#### Get auto-follow pattern API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ccr-get-auto-follow-pattern.html)

&emsp;&emsp;获取auto-follow patterns。

##### Request

```text
GET /_ccr/auto_follow/
```

```text
GET /_ccr/auto_follow/<auto_follow_pattern_name>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须在包含follower index的集群上有`manage_ccr`的[cluster privilege](#Cluster privileges)。见[Security privileges](####Security privileges)。

##### Description

&emsp;&emsp;该接口获取已配置的[auto-follow patterns](#Manage auto-follow patterns)。返回指定的auto-follow patterns集合。

##### Path parameters

- `<auto_follow_pattern_name>`：（Optional, string）指定你想要获取的auto-follow pattern。如果你不指定，则返回所有的集合

##### Example

&emsp;&emsp;这个例子获取名为`my_auto_follow_pattern`的auto-follow pattern 集合。

```text
GET /_ccr/auto_follow/my_auto_follow_pattern
```

&emsp;&emsp;这个接口返回以下结果：

```text
{
  "patterns": [
    {
      "name": "my_auto_follow_pattern",
      "pattern": {
        "active": true,
        "remote_cluster" : "remote_cluster",
        "leader_index_patterns" :
        [
          "leader_index*"
        ],
        "leader_index_exclusion_patterns":
        [
          "leader_index_001"
        ],
        "follow_index_pattern" : "{{leader_index}}-follower"
      }
    }
  ]
}
```

#### Resume follower API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ccr-post-resume-follow.html)

&emsp;&emsp;恢复一个follower index。

##### Request

```text
POST /<follower_index>/_ccr/resume_follow
{
}
```

##### Prerequisites

&emsp;&emsp;如果开启了Elasticsearch security features，你必须在follower index上有`write`、`monitor`的index privilege。在leader index上必须要有`read`和`monitor`的index privilege。在包含follower index的集群上要有`manage_ccr`的cluster privilege。更多信息见[Security privileges](####Security privileges)

##### Description

&emsp;&emsp;显示的通过[pause follower API]()暂停或者在跟随（follow）过程中某些操作由于无法通过重试来执行导致失败而暂停的follower index，可以使用该接口恢复。在接口返回后，follower index将恢复从leader index中获取操作。

##### Path parameters

- `<follower_index>`：（Required, string）follower index的名称

##### Request body

- settings：（object）覆盖来自leader index 的settings。有些settings是不能被覆盖的（比如`index.number_of_shards`）
- max_read_request_operation_count：（integer）从远端集群中执行的读请求中，每一个请求中包含的操作数量最大值（设置`max_read_request_operation_count`为1000，那么在执行一次从leader到follower的数据同步时，每一次读取请求将最多包含1000个操作）
- max_outstanding_read_requests：（long）正在从远端集群中执行读取请求的数量最
- max_read_request_size：（[byte value](#Byte size units) ）从远端集群中的读请求中，批量操作的字节数最大值
- max_write_request_operation_count：（integer）在follower上每一个批量写入的最大操作数
- max_write_request_size：（[byte value](#Byte size units) ）在follower上每一个批量写入的最大字节数
- max_outstanding_write_requests：（integer）正在follower上执行写入请求的数量最大值
- max_write_buffer_count：（integer）排队等待写入的最大操作数。一旦达到该限制，将暂停从leader中拉去更多操作，直到在队里中的操作已被写入
- max_write_buffer_size：（[byte value](#Byte size units) ）排队等待写入的最大字节数。一旦达到该限制，将暂停从leader中拉去更多操作，直到在队里中的操作已被写入
- max_retry_delay：（[time value](#API conventions)）某个操作发生异常后，在重试之前的等待时间。基于exponential Backoff策略
- read_poll_timeout：（[time value](#API conventions)）follower index同步leader index时，等待远端集群中出现新的操作的时间。超时后，拉去操作将返回到follower，然后更新一些统计信息，随后再次尝试从leader中读取

###### Default values

&emsp;&emsp;下面的输出来自follower info API，描述了这个接口中请求参数的所有默认值：

```text
{
  "follower_indices" : [
    {
      "parameters" : {
        "max_read_request_operation_count" : 5120,
        "max_read_request_size" : "32mb",
        "max_outstanding_read_requests" : 12,
        "max_write_request_operation_count" : 5120,
        "max_write_request_size" : "9223372036854775807b",
        "max_outstanding_write_requests" : 9,
        "max_write_buffer_count" : 2147483647,
        "max_write_buffer_size" : "512mb",
        "max_retry_delay" : "500ms",
        "read_poll_timeout" : "1m"
      }
    }
  ]
}
```

##### Example

&emsp;&emsp;下面的例子恢复了一个名为`follwer_index`的follower index：

```text
POST /follower_index/_ccr/resume_follow
{
  "max_read_request_operation_count" : 1024,
  "max_outstanding_read_requests" : 16,
  "max_read_request_size" : "1024k",
  "max_write_request_operation_count" : 32768,
  "max_write_request_size" : "16k",
  "max_outstanding_write_requests" : 8,
  "max_write_buffer_count" : 512,
  "max_write_buffer_size" : "512k",
  "max_retry_delay" : "10s",
  "read_poll_timeout" : "30s"
}
```

&emsp;&emsp;该接口返回以下结果：

```text
{
  "acknowledged" : true
}
```

#### Unfollow API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ccr-post-unfollow.html)


&emsp;&emsp;将一个follower index转化为一个常规索引（regular index）。

##### Request

```text
POST /<follower_index>/_ccr/unfollow
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`manage_follow_index`的[cluster privilege](#Cluster privileges)来使用这个API

##### Description

&emsp;&emsp;该接口会停止相关follower index的following task并且从CCR中移除相关的index metadata以及settings。将follower index视为一个常规索引。follower index必须先暂停并且关闭才能调用该接口

> NOTE：当前CCR不支持将一个现有的常规索引转化为一个follower index。因此将一个follower index转化为一个常规索引是一个不可逆的操作

##### Path parameters

- `<follower_index>`：（Required, string）follower index的名称

##### Example

&emsp;&emsp;这个例子将名为`follower_index`的follower Index转化为一个常规索引：

```text
POST /follower_index/_ccr/unfollow
```

&emsp;&emsp;该接口返回以下结果：

```text
{
  "acknowledged" : true
}
```

#### Forget follower API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ccr-post-forget-follower.html)

&emsp;&emsp;从leader中移除follower retention leases（保留租约）。

##### Request

```text
POST /<leader_index>/_ccr/forget_follower
{
  "follower_cluster" : "<follower_cluster>",
  "follower_index" : "<follower_index>",
  "follower_index_uuid" : "<follower_index_uuid>",
  "leader_remote_cluster" : "<leader_remote_cluster>"
}
```

```text
{
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0,
    "failures" : [ ]
  }
}
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须在leader index上有`manage_leader_index`的[index privilege](#Indices privileges)，更多信息见[Security privileges](####Security privileges)

##### Description

&emsp;&emsp;一个following index会在其leader index上创建保留租约（retention leases）。这些保留租约用于提高leader index的分片保留following index分片需要执行复制的操作历史的可能性。当一个following index通过unfollow API转换为一个常规索引时（无论是通过显式执行此API还是通过索引生命周期管理隐式执行），这些保留租约会被移除。然而，移除这些保留租约有可能失败（例如，如果包含leader index的远程集群不可用）。虽然这些保留租约最终会自动过期，但它们延长存在的时间可能导致leader index保留比必要更多的历史记录，并阻止索引生命周期管理在leader index上执行某些操作。此API的存在是为了在unfollow API无法做到这一点时，手动移除这些保留租约。

> NOTE：该接口不会停止following index的复制。如果你对一个仍然处于跟随（following）的follower index使用该接口，following index将会重新添加保留租约。这个接口只处理在[unfollow API](#Unfollow API)调用后无能移除保留租约的情况。

##### Path parameters

- `<leader_index>`：（Required, string）leader index的名称

##### Request body

- follower_cluster：（Required, string）包含follower index的汲取名称
- follower_index：（Required, string）follower index的名称
- follower_index_uuid：（Required, string）follower index的UUID
- leader_remote_cluster：（Required, string）包含leader index的[remote cluster]()的别名（包含follower index的集群的视角）

##### Example

&emsp;&emsp;下面的例子从`leader_index`中移除`follower_index`的保留租约。

```text
POST /leader_index/_ccr/forget_follower
{
  "follower_cluster" : "follower_cluster",
  "follower_index" : "follower_index",
  "follower_index_uuid" : "vYpnaWPRQB6mNspmoCeYyA",
  "leader_remote_cluster" : "leader_cluster"
}
```

&emsp;&emsp;该接口返回以下结果：

```text
{
  "_shards" : {
    "total" : 1,
    "successful" : 1,
    "failed" : 0,
    "failures" : [ ]
  }
}
```

#### Get follower stats API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ccr-get-follow-stats.html)

&emsp;&emsp;获取follower的统计信息。

##### Request

```text
GET /<index>/_ccr/stats
```
##### Prerequisites

- 如果开启了Elasticsearch security features，你必须在包含follower index的集群上有`monitor`的[cluster privilege](#Cluster privileges)。见[Security privileges](####Security privileges)。

##### Description

&emsp;&emsp;这个接口获取follower的统计信息。这个接口将返回指定索引中与每一个分片相关的follower task

##### Path parameters

- `<index>`：（Required, string）用逗号隔开的index pattern列表

##### Response body

- indices：（array）follower index统计数据的数组
  - fatal_exception：（object）描述了被取消的following task的fetal异常信息。在这种情况下，只能通过[resume follower API](#Resume follower API)手动恢复following task
  - index：（String）follower index 的名称
  - shards：（array）分片层（shard-level）的following task的统计信息
    - bytes_read：（long）从leader传输的字节数总量

       >NOTE：
       >这只是一个估值并且没有考虑压缩的情况（如果开启了压缩的话）

    - failed_read_requests：（long）读取失败的数量
    - failed_write_requests：（long）在follower上执行的批量写请求的失败数量
    - follower_aliases_version：（long）follower同步的索引别名的版本号
    - follower_global_checkpoint：（long）follower上全局检查点（可以理解为它至少描述了follower index中已成功复制并确认的最高文档操作的序列号（sequence number））。`leader_global_checkpoint`与`follower_global_checkpoint`之间的差异指示了follower落后于leader的程度
    - follower_index：（string）follower index的名称
    - follower_mapping_version：（long）follower同步的mapping版本号
    - follower_max_seq_no：（long）follower上当前最大的序号（这个序号是文档的序号`_seq_no`，最新被处理（添加/更新）的文档有最大的序号）
    - follower_settings_version：（long）follower同步的索引设置（Index settings）的版本号
    - last_requested_seq_no：（long）从leader执行批量请求时，这批文档中的第一篇文档的序列号
    - leader_global_checkpoint：（long）follower task所知道的在leader上当前的全局检查点
    - leader_index：（string）正在follow的leader集群中索引的名称
    - leader_max_seq_no：（long）follower task所知道的在leader上最大的序列号
    - operations_read：（long）从leader读取操作的数量
    - operations_written：（long）在follower的写操作的数量
    - outstanding_read_requests：（integer）follower上正在读取的操作数量
    - outstanding_write_requests：（integer）从follower上正在批量写入的操作数量
    - read_exceptions：（array）描述读取失败的对象数组
      - exception：（object）导致读取失败的异常内容
      - from_seq_no：（long）从leader批量请求中第一个序列号
      - retries：（integer）批量操作重试次数
    - remote_cluster：（string）包含leader index的[remote cluster](#Remote clusters)
    - shard_id：（integer）分片ID，从0开始的值，最大值就是分片的总数-1
    - successful_read_requests：（long）成功获取的数量
    - successful_write_requests：（long）follower上成功批量写入请求的数量
    - time_since_last_read_millis：（long）The number of milliseconds since a read request was sent to the leader.

      > NOTE：
      >When the follower is caught up to the leader, this number will increase up to the configured read_poll_timeout at which point another read request will be sent to the leader.

    - total_read_remote_exec_time_millis：（long）在远程集群上执行读取花费总时间
    - total_read_time_millis：（long）读取请求从发送给leader到follower收到回复的总时间
    - total_write_time_millis：（long）在follower上写入操作花费的总时间
    - write_buffer_operation_count：（integer）在follower上排队等待写入的操作数量
    - write_buffer_size_in_bytes：（long）当前排队等待写入的操作的总字节
    
##### Example

&emsp;&emsp;下面的例子来获取follower的统计信息：

```text
GET /follower_index/_ccr/stats
```

&emsp;&emsp;该接口返回以下结果：

```text
{
  "indices" : [
    {
      "index" : "follower_index",
      "shards" : [
        {
          "remote_cluster" : "remote_cluster",
          "leader_index" : "leader_index",
          "follower_index" : "follower_index",
          "shard_id" : 0,
          "leader_global_checkpoint" : 1024,
          "leader_max_seq_no" : 1536,
          "follower_global_checkpoint" : 768,
          "follower_max_seq_no" : 896,
          "last_requested_seq_no" : 897,
          "outstanding_read_requests" : 8,
          "outstanding_write_requests" : 2,
          "write_buffer_operation_count" : 64,
          "follower_mapping_version" : 4,
          "follower_settings_version" : 2,
          "follower_aliases_version" : 8,
          "total_read_time_millis" : 32768,
          "total_read_remote_exec_time_millis" : 16384,
          "successful_read_requests" : 32,
          "failed_read_requests" : 0,
          "operations_read" : 896,
          "bytes_read" : 32768,
          "total_write_time_millis" : 16384,
          "write_buffer_size_in_bytes" : 1536,
          "successful_write_requests" : 16,
          "failed_write_requests" : 0,
          "operations_written" : 832,
          "read_exceptions" : [ ],
          "time_since_last_read_millis" : 8
        }
      ]
    }
  ]
}
```

#### Get follower info API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ccr-get-follow-info.html)

&emsp;&emsp;获取所有follower index的信息。

##### Request

```text
GET /<index>/_ccr/info
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须在包含follower index的集群上有`monitor`的[cluster privilege](#Cluster privileges)。见[Security privileges](####Security privileges)。

##### Description

&emsp;&emsp;该接口列出了每一个follower index的参数以及状态。例如，响应中会包括follower的索引名称，leader 索引名称。复制选项（replica options）以及follower index处于活跃还是暂停中。

##### Path parameters

- `<index>`：（Required, string）用逗号隔开的index pattern列表

##### Response body

- follower_indices：（array）follower index的统计信息
  - follower_index：（string）follower index的名称
  - leader_index：（string）在leader cluster 中跟随（follow）的索引名称
  - parameters：（object）该字段概述了CCR的参数。如果follower index的`status`为`paused`，则忽略该字段
    - max_outstanding_read_requests：（long）正在从远端集群中执行读取请求的数量最大值
    - max_outstanding_write_requests：（integer）正在follower上执行写入请求的数量最大值
    - max_read_request_operation_count：（integer）从远端集群中执行的读请求中，每一个请求中包含的操作数量最大值（设置`max_read_request_operation_count`为1000，那么在执行一次从leader到follower的数据同步时，每一次读取请求将最多包含1000个操作）
    - max_read_request_size：（[byte value](#Byte size units) ）从远端集群中的读请求中，批量操作的字节数最大值
    - max_retry_delay：（[time value](#API conventions)）某个操作发生异常后，在重试之前的等待时间。基于exponential Backoff策略
    - max_write_buffer_count：（integer）排队等待写入的最大操作数。一旦达到该限制，将暂停从leader中拉去更多操作，直到在队里中的操作已被写入
    - max_write_buffer_size：（[byte value](#Byte size units) ）排队等待写入的最大字节数。一旦达到该限制，将暂停从leader中拉去更多操作，直到在队里中的操作已被写入
    - max_write_request_operation_count：（integer）在follower上每一个批量写入的最大操作数
    - max_write_request_size：（[byte value](#Byte size units) ）在follower上每一个批量写入的最大字节数
    - read_poll_timeout：（[time value](#API conventions)）follower index同步leader index时，等待远端集群中出现新的操作的时间。超时后，拉去操作将返回到follower，然后更新一些统计信息，随后再次尝试从leader中读取
  - remote_cluster：（string）leader Index所在的[remote cluster](###Remote clusters)
  - status：（string）follower index的`status`为`active`还是`paused`

##### Example

&emsp;&emsp;下面的例子获取follower信息：

```text
GET /follower_index/_ccr/info
```

&emsp;&emsp;如果follower index的`status`为`active`，接口返回下面的结果：

```text
{
  "follower_indices": [
    {
      "follower_index": "follower_index",
      "remote_cluster": "remote_cluster",
      "leader_index": "leader_index",
      "status": "active",
      "parameters": {
        "max_read_request_operation_count": 5120,
        "max_read_request_size": "32mb",
        "max_outstanding_read_requests": 12,
        "max_write_request_operation_count": 5120,
        "max_write_request_size": "9223372036854775807b",
        "max_outstanding_write_requests": 9,
        "max_write_buffer_count": 2147483647,
        "max_write_buffer_size": "512mb",
        "max_retry_delay": "500ms",
        "read_poll_timeout": "1m"
      }
    }
  ]
}
```

&emsp;&emsp;如果follower index的`status`为`paused`，接口返回下面的结果：

```text
{
  "follower_indices": [
    {
      "follower_index": "follower_index",
      "remote_cluster": "remote_cluster",
      "leader_index": "leader_index",
      "status": "paused"
    }
  ]
}
```

#### Delete auto-follow pattern API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ccr-delete-auto-follow-pattern.html)

&emsp;&emsp;删除auto-follow patterns。

##### Request

```text
DELETE /_ccr/auto_follow/<auto_follow_pattern_name>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须在包含follower index的集群上有`manage_ccr`的[cluster privilege](#Cluster privileges)。见[Security privileges](####Security privileges)。

##### Description

&emsp;&emsp;删除一个已配置的[auto-follow patterns](#Manage auto-follow patterns)集合。

##### Path parameters

- `<auto_follow_pattern_name>`：（Required, string）待删除的auto-follow pattern集合。

##### Example

&emsp;&emsp;这个例子删除了一个名为`my_auto_follow_pattern`的auto-follow pattern集合。

```text
DELETE /_ccr/auto_follow/my_auto_follow_pattern
```

&emsp;&emsp;该接口返回以下结果：

```text
{
  "acknowledged" : true
}
```

#### Create auto-follow pattern API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ccr-put-auto-follow-pattern.html)

&emsp;&emsp;创建一个auto-folllow pattern。

##### Request

```text
PUT /_ccr/auto_follow/<auto_follow_pattern_name>
{
  "remote_cluster" : "<remote_cluster>",
  "leader_index_patterns" :
  [
    "<leader_index_pattern>"
  ],
  "leader_index_exclusion_patterns":
  [
    "<leader_index_exclusion_pattern>"
  ],
  "follow_index_pattern" : "<follow_index_pattern>"
}
```

##### Prerequisites

&emsp;&emsp;如果开启了Elasticsearch security features，你必须在leader index patterns上有`read`、`monitor`的index privilege。在包含follower index的集群上要有`manage_ccr`的cluster privilege。更多信息见[Security privileges](####Security privileges)

##### Description

&emsp;&emsp;该接口对请求体中指定的集群创建了[auto-follow pattern](#Manage auto-follow patterns)集合。在远端集群中新创建的索引会自动在本地配置follower index。另外，这个接口可以用来更新现有的[auto-follow patterns](#Manage auto-follow patterns)。注意的是即使在更新auto-follow pattern后它们不再匹配新的pattern，之前自动配置的follower index将保持不变。

##### Path parameters

- `<auto_follow_pattern_name>`：（Required, string）auto-follow patterns集合的名称

##### Request body
- remote_cluster：（Required, string）包含用来匹配的leader index的[remote cluster](#Remote clusters)
- leader_index_patterns：（Optional,array）用来在远端集群（`remote_cluster`中定义）中匹配leader index的index pattern集合
- leader_index_exclusion_patterns：（Optional,array）用来在远端集群（`remote_cluster`中定义）中排除匹配到leader index的index pattern集合。远端集群中匹配到一个或者多个`leader_index_patterns`并且匹配到一个或多个`leader_index_exclusion_patterns`的索引不会被跟随
- follow_index_pattern：（Optional, string）follower index的名称。模版`{{leader_index}}`可以将从leader index的名称衍生到follower Index中。当跟随一个data stream，使用`{{leader_index}}`，CCR不支持修改一个follower data stream中的backing Index
- settings：（object）覆盖来自leader index 的settings。有些settings是不能被覆盖的（比如`index.number_of_shards`）
- max_read_request_operation_count：（integer）从远端集群中执行的读请求中，每一个请求中包含的操作数量最大值（设置`max_read_request_operation_count`为1000，那么在执行一次从leader到follower的数据同步时，每一次读取请求将最多包含1000个操作）
- max_outstanding_read_requests：（long）正在从远端集群中执行读取请求的数量最
- max_read_request_size：（[byte value](#Byte size units) ）从远端集群中的读请求中，批量操作的字节数最大值
- max_write_request_operation_count：（integer）在follower上每一个批量写入的最大操作数
- max_write_request_size：（[byte value](#Byte size units) ）在follower上每一个批量写入的最大字节数
- max_outstanding_write_requests：（integer）正在follower上执行写入请求的数量最大值
- max_write_buffer_count：（integer）排队等待写入的最大操作数。一旦达到该限制，将暂停从leader中拉去更多操作，直到在队里中的操作已被写入
- max_write_buffer_size：（[byte value](#Byte size units) ）排队等待写入的最大字节数。一旦达到该限制，将暂停从leader中拉去更多操作，直到在队里中的操作已被写入
- max_retry_delay：（[time value](#API conventions)）某个操作发生异常后，在重试之前的等待时间。基于exponential Backoff策略
- read_poll_timeout：（[time value](#API conventions)）follower index同步leader index时，等待远端集群中出现新的操作的时间。超时后，拉去操作将返回到follower，然后更新一些统计信息，随后再次尝试从leader中读取

##### Default values

&emsp;&emsp;下面的输出来自follower info API，描述了这个接口中请求参数的所有默认值：

```text
{
  "follower_indices" : [
    {
      "parameters" : {
        "max_read_request_operation_count" : 5120,
        "max_read_request_size" : "32mb",
        "max_outstanding_read_requests" : 12,
        "max_write_request_operation_count" : 5120,
        "max_write_request_size" : "9223372036854775807b",
        "max_outstanding_write_requests" : 9,
        "max_write_buffer_count" : 2147483647,
        "max_write_buffer_size" : "512mb",
        "max_retry_delay" : "500ms",
        "read_poll_timeout" : "1m"
      }
    }
  ]
}
```

##### Example

&emsp;&emsp;这个例子创建了一个名为`my_auto_folowe_pattern`的auto-follow pattern：

```text
PUT /_ccr/auto_follow/my_auto_follow_pattern
{
  "remote_cluster" : "remote_cluster",
  "leader_index_patterns" :
  [
    "leader_index*"
  ],
  "follow_index_pattern" : "{{leader_index}}-follower",
  "settings": {
    "index.number_of_replicas": 0
  },
  "max_read_request_operation_count" : 1024,
  "max_outstanding_read_requests" : 16,
  "max_read_request_size" : "1024k",
  "max_write_request_operation_count" : 32768,
  "max_write_request_size" : "16k",
  "max_outstanding_write_requests" : 8,
  "max_write_buffer_count" : 512,
  "max_write_buffer_size" : "512k",
  "max_retry_delay" : "10s",
  "read_poll_timeout" : "30s"
}
```

&emsp;&emsp;这个接口返回以下结果：

```text
{
  "acknowledged" : true
}
```

#### Pause auto-follow pattern API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ccr-pause-auto-follow-pattern.html)

&emsp;&emsp;暂停一个auto-follow pattern。

##### Request

```text
POST /_ccr/auto_follow/<auto_follow_pattern_name>/pause
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须在包含follower index的集群上有`manage_ccr`的[cluster privilege](#Cluster privileges)。见[Security privileges](####Security privileges)。

##### Description

&emsp;&emsp;该接口暂停一个[auto-follow pattern](#Manage auto-follow patterns)。接口返回后，这个auto-follow pattern则为inactive并且忽略所有在远端集群上创建的并且匹配的新索引。暂停auto-follow pattern会将将[GET auto-follow patterns API](#Get auto-follow pattern API)中的`active`的值设置为`false`。

&emsp;&emsp;你可以使用[resume auto-follow pattern API](#Resume auto-follow pattern API)恢复。一旦恢复，auto-follow pattern再次为active并且匹配到远端集群上创建的新索引后在本地创建follower index。创建的索引即使在pattern被暂定后也可以被跟随（follow），除非同时被关闭或者删除。

##### Path parameters

- `<auto_follow_pattern_name>`：（Required, string）待暂停的auto-follow pattern集合

##### Example

&emsp;&emsp;下面的例子暂停名为`my_auto_follw`的auto-follow pattern：

```text
POST /_ccr/auto_follow/my_auto_follow_pattern/pause
```

&emsp;&emsp;该接口返回以下结果：

```text
{
  "acknowledged" : true
}
```

#### Resume auto-follow pattern API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ccr-resume-auto-follow-pattern.html)

&emsp;&emsp;恢复一个auto-follow pattern。

##### Request

```text
POST /_ccr/auto_follow/<auto_follow_pattern_name>/resume
```
##### Prerequisites

- 如果开启了Elasticsearch security features，你必须在包含follower index的集群上有`manage_ccr`的[cluster privilege](#Cluster privileges)。见[Security privileges](####Security privileges)。

##### Description

&emsp;&emsp;该接口返回一个通过[pause auto-follow pattern API](#Manage auto-follow patterns)暂停的[auto-follow pattern](#Pause auto-follow pattern API)。接口返回后，将会恢复并继续为远端集群中新创建的并且匹配的索引在本地配置follower index。创建的索引即使在pattern被暂定后也可以被跟随（follow），除非同时被关闭或者删除。

##### Path parameters

- `<auto_follow_pattern_name>`：（Required, string）待恢复的auto-follow pattern的名称

##### Example

&emsp;&emsp;这个例子恢复了一个名为`my_auto_follow_pattern`的被暂停的auto-follow pattern：

```text
POST /_ccr/auto_follow/my_auto_follow_pattern/resume
```

&emsp;&emsp;该接口返回以下结果：

```text
{
  "acknowledged" : true
}
```

### Data stream APIs
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/data-stream-apis.html)

&emsp;&emsp;下面的API用来管理[data stream](#Data streams)：

- [Create data stream](#Create data stream API)
- [Delete data stream](#Delete data stream API)
- [Get data stream](#Get data stream API)
- [Migrate to data stream](#Migrate to data stream API)
- [Data stream stats](#Data stream stats API)
- [Promote data stream](#Promote data stream API)
- [Modify data streams](#Modify data streams API)

&emsp;&emsp;更多概念跟教程见[Data streams](#Data streams)。

#### Create data stream API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-create-data-stream.html)

&emsp;&emsp;创建一个新的[data stream](#Data streams)。

```text
PUT /_data_stream/my-data-stream
```

##### Request

```text
PUT /_data_stream/<data-stream>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有这个data stream的`create_index`或者`manage`的[index privilege](#Indices privileges)才能使用这个接口
- 要有data stream能匹配使用的[index template](#Index templates)。见[Set up a data stream](#Set up a data stream)

##### Path parameters

- `<data-stream>`：（Required, string）待创建的data stream的名称。名称必须满足下面的标准：
  - 只能是小写
  - 不能包含`\, /, *, ?, ", <, >, |, ,, #, :`以及空格
  - 不能以` -, _, +, 或者 .ds-`开头
  - 不能是`.`或`..`
  - 不能超过255个字节。多字节的字符会更快达到这个上限

#### Delete data stream API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-delete-data-stream.htm)

&emsp;&emsp;删除一个或多个[data streams](#Data streams)以及它们的backing indices。见[Delete a data stream](#Delete a data stream)。

```text
DELETE /_data_stream/my-data-stream
```

##### Request

```text
DELETE /_data_stream/<data-stream>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有这个data stream的`delete_index`或者`manage`的[index privilege](#Indices privileges)才能使用这个接口

##### Path parameters

- `<data-stream>`：（Required, string）用逗号隔开的待删除的data stream列表。可以使用通配符（`*`）

##### Query parameters

- expand_wildcards：（Optional, string）通配符能匹配的data stream类型。支持多值，例如`open`, `hidden`。合法值有：
  - all、hidden：匹配任意的data stream，包括[hidden](#Hidden data streams and indices-1)类型
  - open、closed：匹配除了`hidden`的data stream。不能是已关闭的data stream
  - none：不展开通配符模式

&emsp;&emsp;默认是`open`。

#### Get data stream API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-get-data-stream.html)

&emsp;&emsp;获取一个或多个[data stream](#Data-streams)的信息。见[Get information about a data stream](#Get-information-about-a-data-stream)。

##### Request

```text
GET /_data_stream/my-data-stream
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有这个data stream的`view_index_metadata`或者`manage`的[index privilege](#Indices privileges)才能使用这个接口

##### Path parameters

- `<data-stream>`：（Optional, string）用逗号隔开的data stream的名称列表来限制请求量。支持通配符（`*`）。如果忽略这个参数，则返回所有的data stream。

##### Query parameters

- expand_wildcards：（Optional, string）通配符能匹配的data stream类型。支持多值，例如`open`, `hidden`。合法值有：
  - all、hidden：匹配任意的data stream，包括[hidden](#Hidden data streams and indices-1)类型
  - open、closed：匹配除了`hidden`的data stream。不能是已关闭的data stream
  - none：不展开通配符模式

&emsp;&emsp;默认是`open`。

##### Response body

- data-streams：（array of objects）获取到的data stream信息
  - name：（string）data stream的名称
  - timestamp_field：（object）data stream中`@timestamp`字段的信息
    - name：（string）data stream中时间戳字段的名称，必须是`@timestamp`。添加到data stream中的文档中必须包含`@timestamp`字段
  - indices：（array of objects）data stream中的backing indices的信息，用数组表示。数组中最后一个backing index是当前data stream的[write index](#Write index)
    - index_name：（string）backing index的名称，对于名称的一些约定，见[Generation](#Generation)
    - index_uuid：（string）索引的唯一标识符
  - generation：（integer）data stream当前的[generation](#Generation)。这个数值表示rollover的统计值，从1开始计数
  - `_meta`：（object）自定义的元数据，从匹配到的[index template](#Create an index template)中的`_meta`拷贝而来。如果为空，响应中就不展示这个属性
  - status：（string）data stream的[Health status](#Cluster health API)，健康状态基于stream中backing indices的主分片跟副本分片的状态
    - green：所有分片都被分配
    - yellow：所有的主分片已经分配，但是一个或多个副本分片没有被分配
    - red：一个或多个主分片没有被分配，意味着有些数据是不可见的
  - template：（string）用来创建data stream的backing indices的索引模板名称
    - 模版中的`index_patterns`必须能匹配data stream的名称。见[create an index template](#Create an index template)
  - ilm_policy：（string）data stream匹配的索引模板中的ILM生命周期策略名称。这个生命周期策略设置于`index.lifecycle.name`中。
    - 如果模版中没有生命周期策略，响应中就不会显示这个属性
    > NOTE：data stream中的backing indices可能被分配了不同的生命周期策略，若要查看某个单独backing indices的生命周期策略，见[get index settings API](#Get index settings API)
  
  - hidden：（Boolean）如果为`true`，data stream是[hidden](#Hidden data streams and indices-1)
  - system：（Boolean）如果为`true`，data stream是由Elastic stack组件创建管理的并且不能被普通用户修改
  - allow_custom_routing：（Boolean）如果为`true`，流入到data stream的数据允许自定义写请求
  - replicated：（Boolean）如果为`true`。data stream由CCR创建管理，local cluster不能写入到这个data stream或者更改它的mapping

##### Example

```text
GET _data_stream/my-data-stream*
```

&emsp;&emsp;这个请求返回以下响应：

```text
{
  "data_streams": [
    {
      "name": "my-data-stream",
      "timestamp_field": {
        "name": "@timestamp"
      },
      "indices": [
        {
          "index_name": ".ds-my-data-stream-2099.03.07-000001",
          "index_uuid": "xCEhwsp8Tey0-FLNFYVwSg"
        },
        {
          "index_name": ".ds-my-data-stream-2099.03.08-000002",
          "index_uuid": "PA_JquKGSiKcAKBA8DJ5gw"
        }
      ],
      "generation": 2,
      "_meta": {
        "my-meta-field": "foo"
      },
      "status": "GREEN",
      "template": "my-index-template",
      "ilm_policy": "my-lifecycle-policy",
      "hidden": false,
      "system": false,
      "allow_custom_routing": false,
      "replicated": false
    },
    {
      "name": "my-data-stream-two",
      "timestamp_field": {
        "name": "@timestamp"
      },
      "indices": [
        {
          "index_name": ".ds-my-data-stream-two-2099.03.08-000001",
          "index_uuid": "3liBu2SYS5axasRt6fUIpA"
        }
      ],
      "generation": 1,
      "_meta": {
        "my-meta-field": "foo"
      },
      "status": "YELLOW",
      "template": "my-index-template",
      "ilm_policy": "my-lifecycle-policy",
      "hidden": false,
      "system": false,
      "allow_custom_routing": false,
      "replicated": false
    }
  ]
}
```

#### Migrate to data stream API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-migrate-to-data-stream.html#indices-migrate-to-data-stream)

&emsp;&emsp;将[index alias](#Aliases)转化为一个[data stream](#Data streams)。

```text
POST /_data_stream/_migrate/my-logs
```

##### Request

```text
POST /_data_stream/_migrate/<alias>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有这个索引别名（Index alias）的`manage`的[index privilege](#Indices privileges)才能使用这个接口
- 比如要有一个能用于data stream的[index template](#Index templates)。见[Set up a data stream](#Set up a data stream)

##### Path parameters

- `<alias>`：（Required, string）转化为data stream的索引别名名称。这个别名必须满足下面的要求：
  - 别名必须要有一个[write index](#Write_index（Alias）)
  - 别名对应的所有索引必须要有一个`date`或者`date_nanos`类型的`@timestamp`
  - 必须不能有任何的[filter](#Filter an alias)
  - 必须没有使用[custom routing](#Routing)

&emsp;&emsp;成功后，该请求会移除别名并且创建一个名称相同的data stream。别名的索引变成这个流的backing indices。别名的writer index变成流的writer index。

#### Data stream stats API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/data-stream-stats-api.html)

&emsp;&emsp;获取[data streams](#Data streams)的统计信息。

```text
GET /_data_stream/my-data-stream/_stats
```

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有data stream的`monitor`或者`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Request

```text
GET /_data_stream/<data-stream>/_stats
```

##### Path parameters

- `<data-stream>`：（Optional, string）用逗号隔开的data stream的名称来限制请求。支持通配符（`*`）
  - 若要获取集群中所有data stream，使用`*`或者忽略这个参数

##### Query parameters

- expand_wildcards：（Optional, string）通配符能匹配的data stream类型。支持多值，例如`open`, `hidden`。合法值有：
  - all、hidden：匹配任意的data stream，包括[hidden](#Hidden data streams and indices-1)类型
  - open、closed：匹配除了`hidden`的data stream。不能是已关闭的data stream
  - none：不展开通配符模式

&emsp;&emsp;默认是`open`。

- human：（Optional, Boolean）如果为`true`，响应中包含具有可读性的[byte values](#Byte size units)统计信息。默认是`false`

##### Response body

- \_shards：（object）尝试请求分片的信息
  - total：（integer）尝试执行的分片数量
  - successful：（integer）成功执行的分片数量
  - failed：（integer）执行失败的分片数量
- data_stream_count：（integer）选择的data streams的数量
- backing_indices：（integer）选择的data streams中的backing_indices的数量
- total_store_sizes：[byte value](#Byte size units) 选择的data stream中分片大小总量。只有设置请求参数`human`后才展示这个字段
- total_store_size_bytes：（integer）选择的data stream中分片大小总量，单位是字节
- data_streams：（array of objects）选择的data stream的统计信息
  - data_stream：（string）data stream的名称
  - backing_indices：（integer）data stream中的backing indices
  - store_size：[byte value](#Byte size units) data stream中所有分片的大小总量。只有请求参数`human`为`true`才会返回
  - store_size_bytes：（integer） data stream中所有分片的大小总量，单位是字节
  - maximum_timestamp：（integer）data stream中`@timestamp`最大值，该值会转化为字符串，见[Unix epoch](https://en.wikipedia.org/wiki/Unix_time)

> NOTE：这个时间戳可能不是最大的。如果满足以下一个或多个条件，那么可能data stream中存在比`@timestamp`数值更大的值：
> - stream中包含[closed](#Open index API) backing indices
> - 有[lower generation](#Generation)的backing indices中包含更大的`@timestamp`的值

##### Example

```text
GET /_data_stream/my-data-stream*/_stats?human=true
```

&emsp;&emsp;API返回下面的响应：

```text
{
  "_shards": {
    "total": 10,
    "successful": 5,
    "failed": 0
  },
  "data_stream_count": 2,
  "backing_indices": 5,
  "total_store_size": "7kb",
  "total_store_size_bytes": 7268,
  "data_streams": [
    {
      "data_stream": "my-data-stream",
      "backing_indices": 3,
      "store_size": "3.7kb",
      "store_size_bytes": 3772,
      "maximum_timestamp": 1607512028000
    },
    {
      "data_stream": "my-data-stream-two",
      "backing_indices": 2,
      "store_size": "3.4kb",
      "store_size_bytes": 3496,
      "maximum_timestamp": 1607425567000
    }
  ]
}
```

#### Promote data stream API
（8.1）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/promote-data-stream-api.html)

&emsp;&emsp;这个接口的目的是将通过CCR复制的data stream转换成一个常规的data stream。

&emsp;&emsp;通过CCR的Auto Following，远端的data stream可以被复制到本地集群中。这些data stream不能再本地集群中rollover 。只有当上游的data stream执行了rollover后，这些被复制的data stream才能执行rollover。当远端集群不可用时，本地集群的data stream可以提升为一个常规的data stream，运行在本地集群中执行rollover。

```text
POST /_data_stream/_promote/my-data-stream
```

##### Request

```text
POST /_data_stream/_promote/<data-stream>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`manage_follow_index`的[cluster privilege](#Cluster privileges)来使用这个API

##### Path parameters

- `<data-stream>`：（Required, string）待提升的data stream的名称

#### Modify data streams API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modify-data-streams-api.html)

&emsp;&emsp;在一个原子操作内执行一个或者多个data stream的修改动作。

```text
POST _data_stream/_modify
{
  "actions": [
    {
      "remove_backing_index": {
        "data_stream": "my-logs",
        "index": ".ds-my-logs-2099.01.01-000001"
      }
    },
    {
      "add_backing_index": {
        "data_stream": "my-logs",
        "index": "index-to-add"
      }
    }
  ]
}
```

##### Request

```text
POST /_data_stream/_modify
```

##### Request body

- actions：（Required, array of objects）执行的动作
  - `<action>`：（Required, object）动作类型的key。至少需要一个动作
    - add_backing_index：添加一个现有的索引到data stream中。索引被隐藏将作为这个操作的一部分
    > WARNING：使用`add_backing_index`来添加索引可能会导致data stream的不当行为。这个接口要视为专家接口
    - remove_backing_index：从data stream中移除一个backing index。索引被隐藏作为这个操作的一部分。writer index不能被移除
      - data_stream：（Required\*, string）执行动作的data stream名
      - index：（Required\*, string）执行动作的索引名



### Document APIs
[link](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs.html)

&emsp;&emsp;

##### Request
##### Prerequisites
##### Path parameters
##### Query parameters
##### Response body
##### Example

#### Reading and Writing documents
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/docs-replication.html#basic-write-model)

##### Introduction

&emsp;&emsp;Elasticsearch中的每一个索引会[divided into shards](#Scalability and resilience: clusters, nodes, and shards)并且每一个分片都有多个拷贝。这些拷贝就是所谓的replication group并且当文档添加或者删除后，每一个replica都要保持同步。如果无法做到同步，那么可能就会导致从不同的拷贝中读取的结果各不相同。保持副本分片间的同步以及读取副本分片的处理过程称为`data replication model`。

&emsp;&emsp;Elasticsearch的data replication model基于主备模式（primary-backup model），微软的一篇论文[PacificA paper](https://www.microsoft.com/en-us/research/publication/pacifica-replication-in-log-based-distributed-storage-systems/)很好的描述了这种模式。这个模型就是从replication group中拿出一个拷贝作为主分片（primary shard），其他的拷贝作为副本分片。主分片用于服务所有索引操作的入口点（entry point）。主分片负责非法检查并保证索引操作的正确性。一旦主分片接受了某个索引操作，主分片负责对其他的副本分片复制这次索引操作。

&emsp;&emsp;本章的内容从high level的视角来了解Elasticsearch的replication model并讨论读操作与写操作之间不同的交互（interaction）带来的影响（implication）。

##### Basic write model

&emsp;&emsp;Elasticsearch中的每一个索引操作首先基于文档编号（document ID）通过[routing](#Routing(Index API))解析（resolve）replication group。一旦检测到replication group，这个操作会在内部转发到组中当前的主分片中。这个索引阶段称为`coordinating stage`。

&emsp;&emsp;索引阶段的下一个阶段就是`primary stage`，在主分片上执行。主分片负责对索引操作进行验证并转发到其他副本分片。由于副本分片可能处于离线状态，主分片不要求一定要转发到所有的副本分片。Elasticsearch维护了副本分片的列表，这些副本会收到主分片的转发。这个列表称为`in-sync copies` 并且master node负责维护。正如in-sync这个名称一样，列表中的都是"good" 副本分片并且它们已经处理好了所有的索引和删除操作并且响应给了用户。主分片负责维护这个列表因此会复制操作给列表中的每一个副本分片。

&emsp;&emsp;主分片遵循下面基本的步骤：

- 检查索引操作，如果结构非法则reject它（例如期望值是数值类型的字段值，但是期望值是数值）
- 本地执行操作。例如索引或者删除相关的文档。同样会检查字段的内容，如果必要的话会reject。（例如Keyword类型的字段值太长，Lucene中无法处理它）
- 将操作转发到`in-sync`列表中的副本分片。如果列表中有多个副本分片，则会并发处理
- 一旦`in-sync`列表中所有的副本分片都成功执行了操作并且响应了主分片，主分片会将成功完成请求的消息响应给客户端

&emsp;&emsp;`in-sync`中的副本分片在本地执行索引操作。这个索引阶段称为`replica stage`。

&emsp;&emsp;这些索引阶段（coordination，primary，replica）是有序执行的。为了能实现内部重试（to enable internal retries），每一个阶段的时间（lifetime）包括（encompass）了 它的下一个阶段的时间。例如，coordination stage直到每一个primary stage完成才算是完成，primary stage可能会分散（spread out）到不同的主分片，等待它们完成。每一个primary stage直到每一个replica stage完成才算是完成，replica stage需要等到每一个副本分配完成本地的文档索引操作并且能成功的响应。

###### Failure handling

&emsp;&emsp;在索引期间会有很多事情会导致发生错误。磁盘损坏（corruption）节点失去连接或者一些错误的配置导致某个操作能在主分片上成功执行但是在副本分片上发生错误。尽管这些问题不常见，但是主分片必须负责处理这些问题。

&emsp;&emsp;如果主分片自身发生了错误，主分片所在的节点会像master node发送消息。索引操作会等待（[默认](#Dynamic index settings)一分钟），直到master晋升（promote）一个副本分片成为主分片。这个索引操作会被转发到新的主分片进行处理。注意的是master node会监控节点的健康度，可能会主动（proactive）降级（demote）一个主分片。发生这种情况一般是拥有主分片的节点因为网络问题从集群中分离（isolate）出来了。

&emsp;&emsp;一旦在主分片上成功的完成操作，主分片必须要处理在执行复制操作给副本分片后，replica stage出现错误的问题。有可能在副本分片上真的出现了错误或者因为网络问题导致复制操作无法到达副本分片，或者副本分片的响应无法到达主分片。这样的问题最终都有相同的结局：a replica which is part of the in-sync replica set misses an operation that is about to be acknowledged。In order to avoid violating the invariant，主分片往master node发送请求要求这个副本分配从`in-sync`中移除。Only once removal of the shard has been acknowledged by the master does the primary acknowledge the operation. Note that the master will also instruct another node to start building a new shard copy in order to restore the system to a healthy state。

&emsp;&emsp;在转发操作到副本分片时，主分片也可以利用副本分配来检查自己是否仍然是active primary。如果主分片由于network partition（long GC）已经被隔离了。它在认识到自己被降级前仍然继续处理索引操作。副本分配会reject来自失效的（stale）的主分片的操作。当主分片接收到副本分配的reject响应后，由于它不再是主分片，它会联系（reach out ）master node然后了解到自己的主分片地位被替换了。然后这个操作会路由到新的主分片。

> 如果没有replica的话会发生什么
> 由于索引配置或者简单来说所有的副本分配都失效的场景是存在的。在这种场景下，主分片处理操作并不需要任何外部的验证，看起来是有问题的。另一方面，主分片不能自己让其他shard而是需要向master node代表它这样做。这意味着master node知道这个主分片唯一"good"的分片。我们因此保证了master node 不会晋升其他的shard成为新的主分片，因此在这个主分片上的任意的索引操作都不会丢失。当然了，由于我们只在一个数据副本上允许，物理的硬件问题会导致数据丢失。见[Active shards](#Active shards)了解一些mitigation选项。

##### Basic read model

&emsp;&emsp;通过查找ID在Elasticsearch中读取是非常轻量的（weightlight），而带有复杂聚合操作的heavy 查询请求则会要求non-trivial的CPU power。主备模式（primary-backup）模式的优点之一是它让所有的副本分片的数据保持一致（with the exception of in-flight operations）。因此单个`in-sync`的分片足够用于服务读取请求。

&emsp;&emsp;当一个节点接收到一个读请求，这个节点负责转发这个请求到拥有相关分片的节点上，整合（collate）结果并响应给客户端。我们称那个节点为`coordinating node`。基本的流程如下所示：

1. 解析（resolve）这个请求对应的分片。注意的是由于大部分的请求都会被发送一个或者多个索引，所以通常来说都需要从多个分片中读取，每一个分片返回的内容都是一个数据子集
2. 从replication group中选择一个相关的分片的active copy。可以是主分片或者是副本分片。默认情况下，Elasticsearch使用[adaptive replica selection](#Adaptive replica selection)来选择分片
3. 发送分片级别（shard level）的读请求到选择的分片上
4. 组合结果和响应。注意的是在通过ID查询的场景中，只有一个分片是相关的，因此这个步骤可以跳过

###### Shard failures

&emsp;&emsp;当一个分片错误的响应读请求后，coordinating node会将请求发送到同属replication group中的其他分片。重复的失败会导致没有可用的分片。

&emsp;&emsp;为了保证快速的响应，下面的APIs在出现一个或者多个失败后返回部分结果：

- [Search](#Search API)
- [Multi Search](#Multi search API)
- [Multi Get](#Multi get \(mget\) API)


&emsp;&emsp;包含部分结果的响应仍旧提供一个`200 OK`的HTTP 状态码。Shard failure会在响应头的`timed_out`和`_shards` 中指出。

##### A few simple implications

&emsp;&emsp;这些基本流程中的每一个都决定了Elasticsearch作为一个读写系统的行为。进一步的说，由于读写请求可以并发执行，它们相互影响。

###### Efficient reads 

&emsp;&emsp;正常情况下一次读操作只会读取相关的replication group中的分片一次。只有在出错的情况下才会对多个相同的分片执行相同的查询。

###### Read unacknowledged

&emsp;&emsp;由于主分片先在自己本地进行索引，然后再复制这个请求到其他分片。有可能出现一个并发的读操作会看到这个未确认（等待其他所有副本分片更改完成的确认）的更改。

###### Two copies by default

&emsp;&emsp;主备模型可以在在仅仅维护两个副本的情况下实现容错，而相较于quorum-based的系统中用于容错的最小的副本数量需要3个。

##### Failures

&emsp;&emsp;发生错误后，下面的情况仍然可能发生。

###### A single shard can slow down indexing

&emsp;&emsp;每一个操作中，由于主分片会等待`in-sync`中所有的副本分片完成。单个slow分片会导致整个replication group变慢。这个是上文Efficient reads带来的代价。当然一个slow分片也会降低路由到这个分片的查询。

###### Dirty reads

&emsp;&emsp;被隔离的（isolate）的主分片仍然会暴露未被确认的数据。这是由于这个分片不会意识到自己已经被隔离了直到它发送请求到副本分片或者通过master node得知。在这个时间点，索引操作已经索引到主分片本地并且可以被并发的读操作读取变更。Elasticsearch通过每一秒对master node进行ping来缓解（mitigation）这个风险，并且在不知道主节点时reject索引操作。

##### The Tip of the Iceberg

&emsp;&emsp;当前这篇文档提供了一个high level的角度介绍了Elasticsearch如何处理数据，当然在底层有着更多的内容。例如primary terms，cluster state publishing以及master election，所有这一切都是为了让这个系统有正确的行为。这篇文档同样没有覆盖已知的并且重要的bug( closed以及open)。我们认识到[GitHub is hard to keep up with](https://github.com/elastic/elasticsearch/issues?q=label%3Aresiliency)，所以我们在页面维护了一个专门的[resiliency page](https://www.elastic.co/guide/en/elasticsearch/resiliency/current/index.html)，强烈建议阅读这些内容。

#### Index API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-index_.html)

###### document IDs automatically

###### Routing(Index API)

&emsp;&emsp;

###### Active shards


#### Get API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/docs-get.html)

#### Delete API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/docs-delete.html)

#### Update API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/docs-update.html)

#### Update By Query API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/docs-update-by-query.html)

&emsp;&emsp;更新满足满足指定Query的文档。如果没有指定，执行data stream或索引中每一篇文档的更新，但是不修改源数据，用来应用mapping的更改是非常有用的。

```text
POST my-index-000001/_update_by_query?conflicts=proceed
```

##### Request

```text
POST /<target>/_update_by_query
```

##### Prerequisites

&emsp;&emsp;如果开启了Elasticsearch security feature，你必须要有目标data stream，index或者alias的这些 [index privilege](#####Indices privileges)：

- `raed`
- `index`或`write`

##### Description

&emsp;&emsp;你在请求URI指定查询规则或者在请求体中使用跟[Search API]()相同的语法。

&emsp;&emsp;当你提交了该接口的请求，Elasticsearch在开始处理请求时会获取数据流或索引的快照，并使用内部版本控制（`internal` versioning）来更新匹配的文档。当版本匹配时，文档会被更新并且提高版本号。如果在获取快照和处理更新操作之间，文档发生了变化，这会导致版本冲突，从而使操作失败。你可以通过将`conflicts`设置为`proceed`，使得接口统计发生版本冲突的数量而不是停止并返回。注意的是如果选择这种方式，那么这次接口操作会尝试从源中更新更多的文档直到更新了`max_doc`数量的文档，或者覆盖了查询范围内的所有文档。

> NOTE：如果文档的版本号是0，那么就不能通过这个接口更新，因为内部版本控制不支持该值，它不是一个有效的版本号

&emsp;&emsp;在处理通过查询更新（update by query）请求时，Elasticsearch会顺序执行多个搜索请求来找到所有匹配的文档。每批匹配的文档会执行一次批量更新请求。任何查询或更新失败都会导致通过查询更新请求失败，并且失败信息会显示在响应中。已经成功完成的更新请求会保持不变，它们不会被回滚

###### Refreshing shards

&emsp;&emsp;指定`refresh`参数使得在请求完成后对所有的分片执行refresh。不同于[Update API](#Update API)中的`refresh`参数，它只会让收到请求的分片执行refresh。不同于[Update API](#Update API)，该接口不支持`wait_for`。

###### Running update by query asynchronously

&emsp;&emsp;如果请求中包含`wait_for_completion=false`。Elasticsearch会执行一些预检查（preflight check），启动请求，并返回一个[task](#Task management API)，你可以用它来取消任务或获取任务状态。Elasticsearch会在`.tasks/task/${taskId}`处以文档形式记录这个任务。完成任务后，你应该删除任务文档，以便Elasticsearch回收空间。

###### Waiting for active shards

&emsp;&emsp;`wait_for_active_shards`控制在执行这个请求前应该有多少数量的分片是活跃的（active）。见[Active shards](#Index API)查看详情。`timeout`控制写请求等待那些不见用的分片变得可用的时间。这两个设置在Bulk API](#Bulk API)中的工作方式完全相同。Update by Query在实现内部使用了滚动搜索（scroll search），因此你也可以指定`scroll`参数来控制它保持搜索上下文活跃的时间长短，例如`?scroll=10m`。默认值是5分钟。

###### Throttling update requests

&emsp;&emsp;若要控制Update By Query以多快的速率发出一批批更新操作，你可以将`requests_per_second`设置为任何正小数。这会给每批操作添加/填充等待时间，以此来控制速率。将`requests_per_second`设置为-1可以禁用节流。

&emsp;&emsp;节流通过在批次之间填充等待时间来实现，使得内部scroll请求在设置超时时间时要考虑到这个等待时间。即批次大小除以`requests_per_second`与写入花费的时间差值。默认批次大小为`1000`，因此如果`requests_per_second`设置为500：

```text
target_time = 1000 / 500 per second = 2 seconds
wait_time = target_time - write_time = 2 seconds - .5 seconds = 1.5 seconds
```

&emsp;&emsp;因为批量请求（"Update By Query"操作中找到的符合条件的文档集合，这些文档将作为一个批次通过Elasticsearch的\_bulk API进行更新）作为单个的\_bulk请求发出，较大的批量大小会导致Elasticsearch创建许多请求并在开始下一组之前等待。这种方式是“bursty突发”的而不是“smooth平滑”的。

###### Slicing

&emsp;&emsp;该接口支持[sliced scroll ]()以并行化更新过程，这可以提高效率并提供一种将请求分解成更小部分的便捷方式。

&emsp;&emsp;将`slices`设置为`auto`会为大多数data stream和index选择一个合理的数量。如果你手动切片或以其他方式调整自动切片，请记住：

- 当切片数量等于index或backing index中分片的数量时，查询性能最高效。如果分片数量很大（例如，500），选择较低的数字，因为太多切片会损害性能。将切片设置得高于分片数量通常不会提高效率，并增加开销。
- 更新性能随着切片数量在可用资源中线性扩展。

&emsp;&emsp;查询或更新性能哪个占据运行时间的主导因素取决于被reindex的文档和集群资源。

##### Path parameters

- `<target>`：（Optional, string）用逗号隔开、待查询的data stream或indices列表。支持通配符（`*`）。若要获取所有的data streams和indices，可以忽略这个参数或者使用`*`、`_all`

##### Query parameters

- allow_no_indices：（Optional, Boolean）如果为`false`，当通配符表达式、[index alias](#Aliases)或者`all`匹配缺失索引或者已关闭的索引则返回一个错误。即使请求找到了打开的索引也可能会返回错误。比如，请求中指定了`foo*, bar*`，但如果找到以`foo`开头的索引，但是没找到以`bar`开头的索引则会返回一个错误。默认为`true`
- analyzer：（Optional, string）用于对query string进行解析
  - 这个参数只有在指定了`q`参数后才会被使用
- analyze_wildcard（Optional, Boolean）如果为`true`，会对Wildcard和prefix query进行解析。默认是`false`
  - 这个参数只有在指定了`q`参数后才会被使用
- conflicts：（Optional, string）在更新时遇到版本冲突时该如何处理室：`abort`或`proceed`。默认是`abort`
- default_operator：（Optional, string）query string query中的默认操作符：AND or OR。默认是`OR`
- df：（Optional, string）如果在query string中为给定字段名则使用这个默认字段名
  - 这个参数只有在指定了`q`参数后才会被使用
- expand_wildcards：（Optional, string）通配符模式可以匹配的索引类型。如果请求目标是data stream，还会检测通配符表达式是否会匹配隐藏的data streams。支持多值，例如`open`, `hidden`。合法值有：
  - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target syntax-1)
  - open：匹配打开的，非隐藏的索引。同样匹配非隐藏的data stream
  - closed：匹配关闭的，非隐藏的索引。同样匹配非隐藏的data stream。Data stream不能关闭
  - hidden：匹配隐藏的data streams和indices。必须和`open`、`closed`中的一个或全部组合使用
  - none：不展开通配符模式
  默认值为`all`。
- from：（Optional,integer）这次请求从哪个文档开始，即该文档的偏移值。非负的值，默认值为`0`
- ignore_unavailable：（Optional, Boolean）如果为`false`，请求中指定的data stream或者index如果缺失的话会返回一个错误。默认是`false`
- lenient：（Optional, Boolean）如果为`true`，例如当在一个[numeric](#Numeric field types)字段中，`query`的内容为文本时会忽略format-based的错误，默认值为`false`。
- max_docs：（Optional, integer）最多可以处理的文档数量。默认是所有的文档。当该值设置了一个不大于`scroll_size`的值时，那么就不会使用scroll机制
- pipeline：（Optional, string）pipeline的ID，用来预先处理待处理的文档
- preference：（Optional, string）指定在哪个节点或分片上执行。默认是随机
- q：（Optional, string）Query in Lucene query string syntax
- request_cache：（Optional, Boolean）如果为`true`，会对这个请求使用请求缓存。默认值是index-level的设置
- refresh：（Optional, string）如果为`true`，Elasticsearch会refresh被更新的分片使得变更可以被搜索到。默认为`false`
- requests_per_second：（Optional,integer）每秒可以执行的子请求数量。默认值为-1，表示不进行节流。
- routing：（Optional, string）自定义的值用来路由到指定分片
- scroll：（Optional,[time value](#Time units)）为scroll操作保留[search context](#Keeping the search context alive)的保留时间。见[scroll search results](#Scroll search results)
- scroll_size：（Optional,integer）定义了驱动操作（power operation）的scroll请求的大小。默认值为1000
- search_type：（Optional, string）搜索操作的类型。可选项有：
  - `query_then_fetch`
  - `dfs_query_then_fetch`
- search_timeout：（Optional, [time units](#Time units)）显示指定每一个查询请求的超时时间。默认不超时
- slices：（Optional, integer）任务中划分出的切片数量。默认为`1`意味着不会切分为多个子任务
- sort：（Optional, string）用逗号隔开的`<field>:<direction>`列表
- stats：（Optional, string）Specific tag of the request for logging and statistical purposes.
- terminate_after：（Optional,integer）每一个分片最多收集的文档数量。如果Query达到了上限，elasticsearch则会提前结束这个Query。Elasticsearch先收集文档再排序

  > IMPORTANG：用户使用这个参数需要特别注意。Elasticsearch将这个参数应用到每一个分片上来处理请求。如果到达上限时，Elasticsearch会自动的执行提前结束查询。对于查询目标是data  stream并且跨数据层的请求不要指定这个参数

- timeout：（Optional, [time units](#Time units)）每一个更新操作中等待下面操作的超时时间：
  - Dynamic mapping updates
  - [Waiting for active shards](#Active shards)
  默认是一分钟。它保证Elasticsearch至少会等待这个超时时间后才会失败。真正的等待时间可能更长，特别发生多次等待时。
  
- version：（Optional, Boolean）如果为`true`，请求返回中会有文档的版本号 
- wait_for_active_shards：（Optional, string） 操作开始前已经启用的shard copy（主分片跟副本分片）的数量。设置成`all`或者一个正整数（不能超过索引的分片总数（`number_of_replicas + 1`）），默认值1，即主分片。见[Active shards](#Index API)。

##### Request body

- query：（Optional, [query object](#Query DSL)）使用[Query DSL](#Query DSL)指定需要更新的文档

##### Response body

- took：整个操作从开始到结束的时间
- timed_out：在更新过程中有任何操作执行超时就会将这个字段设置为`true`
- total： 成功处理的文档数量
- updated：成功更新的文档数量
- deleted：成功删除的文档数量
- batches：执行scroll操作的次数
- version_conflicts：版本发生冲突的次数
- noops：被忽略的文档数量，由于在请求中使用了脚本`ctx.op`返回了`noop`
- retries：Update by query尝试重试的次数。`bulk`是批量操作重试的数量、`search`是查询动作重试的次数
- throttled_millis：因为遵循`requests_per_second`而该请求的休眠时间
- requests_per_second：在update by query中每秒有效执行的请求数量
- throttled_until_millis：这个字段在`_update_by_query`中的应该总是等于0。只有使用[Task API](#Running update by query asynchronously)时它才有意义。此字段表示为了遵守`requests_per_second`，被节流的请求下次执行的时间（从纪元开始的毫秒数）。这有助于了解请求被节流直到何时才能继续执行
- failures：在处理过程中任何无法恢复的错误列表。如果该字段不为空说明因为这些原因导致请求终止。Update by query使用batch实现。任何的失败会导致整个过程终止。但是在当前batch中的失败会被收集到数组中。你可以使用`conflicts`选项来阻止因版本冲突导致终止reindex

##### Example

&emsp;&emsp;`_update_by_query`最简单的使用方式就是对data stream或index中每一个文档执行更新并且不修改原文档。比如说[pick up a new property](#Pick up a new property)或者在线修改mapping。

&emsp;&emsp;若要更新指定文档，可以在请求体中指定一个query：

```text
POST my-index-000001/_update_by_query?conflicts=proceed
{
  "query": { 
    "term": {
      "user.id": "kimchy"
    }
  }
}
```

&emsp;&emsp;第3行，跟[Search API](#Search API)一样的方式，在`query`字段中指定内容。你也可以像Search API一样使用`q`参数。

&emsp;&emsp;在多个data stream或index中更新文档：

```text
POST my-index-000001,my-index-000002/_update_by_query
```

&emsp;&emsp;指定路由值要求在指定分片上执行查询：

```text
POST my-index-000001/_update_by_query?routing=1
```

&emsp;&emsp;默认Update By Query中每一个scroll查询最多获取1000篇文档。你可以通过`scroll_size`参数修改这个值：

```text
POST my-index-000001/_update_by_query?scroll_size=100
```

&emsp;&emsp;使用唯一属性（即`user.id`）来更新一篇文档

```text
POST my-index-000001/_update_by_query
{
  "query": {
    "term": {
      "user.id": "kimchy"
    }
  },
  "max_docs": 1
}
```

###### Update the document source

&emsp;&emsp;可以使用脚本更新文档中的内容。例如下面的请求中对索引`my-index-000001`中字段名为`user.id`字段值为`kimchy`的文档中的`count`字段实现加一：

```text
POST my-index-000001/_update_by_query
{
  "script": {
    "source": "ctx._source.count++",
    "lang": "painless"
  },
  "query": {
    "term": {
      "user.id": "kimchy"
    }
  }
}
```

&emsp;&emsp;注意到的是，这个例子中没有指定`conflicts=proceed`。因此当遇到版本冲突时会终止处理值得你可能处理这个问题。

&emsp;&emsp;跟[Update API](##Update API)一样，你可以设置`ctx.op`的值来修改执行的操作：

- noop：`ctx.op = "noop"`，如果你的脚本决定不需要做任何更改。那么就会跳过不更新这篇文档并且`noop`的值加一
- delete：`ctx.op = "delete"`，如果你的脚本决定文档应该被删除。那么就会删除文档并且`deleted`的值加一

&emsp;&emsp;该接口支持`update`、`noop`、`delete`。将`ctx.op`设置成其他值都会报错。设置`ctx`的其他字段也会报错。这个接口只能让你更改匹配到的文档的内容，你不能移动它们。

###### Update documents using an ingest pipeline

&emsp;&emsp;可以通过指定一个`pipeline`来使用[ingest pipelines](##Ingest pipelines)的功能：

```text
PUT _ingest/pipeline/set-foo
{
  "description" : "sets foo",
  "processors" : [ {
      "set" : {
        "field": "foo",
        "value": "bar"
      }
  } ]
}
POST my-index-000001/_update_by_query?pipeline=set-foo
```

###### Get the status of update by query operations

&emsp;&emsp;可以通过[Task API](#Task management API)获取所有运行中的Update By Query请求的状态：

```text
GET _tasks?detailed=true&actions=*byquery
```

&emsp;&emsp;相应如下：

```text
{
  "nodes" : {
    "r1A2WoRbTwKZ516z6NEs5A" : {
      "name" : "r1A2WoR",
      "transport_address" : "127.0.0.1:9300",
      "host" : "127.0.0.1",
      "ip" : "127.0.0.1:9300",
      "attributes" : {
        "testattr" : "test",
        "portsfile" : "true"
      },
      "tasks" : {
        "r1A2WoRbTwKZ516z6NEs5A:36619" : {
          "node" : "r1A2WoRbTwKZ516z6NEs5A",
          "id" : 36619,
          "type" : "transport",
          "action" : "indices:data/write/update/byquery",
          "status" : {    
            "total" : 6154,
            "updated" : 3500,
            "created" : 0,
            "deleted" : 0,
            "batches" : 4,
            "version_conflicts" : 0,
            "noops" : 0,
            "retries": {
              "bulk": 0,
              "search": 0
            },
            "throttled_millis": 0
          },
          "description" : ""
        }
      }
    }
  }
}
```

&emsp;&emsp;第18行，该对象包含实际的状态。它类似于响应的JSON，但重要的增加了total字段。total是reindex预期执行的操作总数。你可以通过加上updated、created和deleted字段来估计进度。当这些字段的和等于total字段时，请求将完成

&emsp;&emsp;根据任务id你可以直接查看这个任务。下面的例子获取了任务`r1A2WoRbTwKZ516z6NEs5A:36619：`的信息：

```text
GET /_tasks/r1A2WoRbTwKZ516z6NEs5A:36619
```

&emsp;&emsp;这个API的优势在于它与`wait_for_completion=false`集成，以透明方式返回已完成任务的状态。如果任务已完成且在其上设置了`wait_for_completion=false`，则它会返回带有结果或错误字段的响应。这个特性的成本是在`.tasks/task/${taskId}`创建的文档。删除该文档的责任在于用户。

###### Cancel an update by query operation

&emsp;&emsp;通过使用[Task Cancel API](#Task management API)取消Update By Query：

```text
POST _tasks/r1A2WoRbTwKZ516z6NEs5A:36619/_cancel
```

&emsp;&emsp;可以通过[tasks API](#Task management API)获取任务ID。

&emsp;&emsp;取消操作很快就发生但可能需要一点时间。task status API会继续列出Update By Query直到它意识到自己被取消了并终止自己。

###### Change throttling for a request

&emsp;&emsp;可以通过`_rethrottle`接口对运行中的Update By Query修改`requests_per_second`的值：

```text
POST _update_by_query/r1A2WoRbTwKZ516z6NEs5A:36619/_rethrottle?requests_per_second=-1
```

&emsp;&emsp;可以通过[tasks API](#Task management API)获取任务ID。

&emsp;&emsp;在设置`_update_by_query API`时，`requests_per_second`可以设为`-1`禁用节流，或设为任何小数如`1.7`或`12`来限制到该级别的速率。加速查询的重新节流（Rethrottling）立即生效，而减速查询的重新节流将在完成当前批次后生效，以防止滚动超时。

###### Slice manually

&emsp;&emsp;在请求中通过提供slice id和slice的总数来手动对Update By Query进行切分：

```text
POST my-index-000001/_update_by_query
{
  "slice": {
    "id": 0,
    "max": 2
  },
  "script": {
    "source": "ctx._source['extra'] = 'test'"
  }
}
POST my-index-000001/_update_by_query
{
  "slice": {
    "id": 1,
    "max": 2
  },
  "script": {
    "source": "ctx._source['extra'] = 'test'"
  }
}
```

&emsp;&emsp;你可以通过下面的方式进行验证：

```text
GET _refresh
POST my-index-000001/_search?size=0&q=extra:test&filter_path=hits.total
```

&emsp;&emsp;生成类似下面的合理的（sensible）`total`：

```text
{
  "hits": {
    "total": {
        "value": 120,
        "relation": "eq"
    }
  }
}
```

###### Use automatic slicing

&emsp;&emsp;你可以使用[sliced scroll](#Sliced scroll)根据`id`切分来自动实现并行。指定`slices`的数量来使用它：

```text
POST my-index-000001/_update_by_query?refresh&slices=5
{
  "script": {
    "source": "ctx._source['extra'] = 'test'"
  }
}
```

&emsp;&emsp;佴可以通过下面方式验证：

```text
POST my-index-000001/_search?size=0&q=extra:test&filter_path=hits.total
```

&emsp;&emsp;生成类似下面的合理的（sensible）`total`：

```text
{
  "hits": {
    "total": {
        "value": 120,
        "relation": "eq"
    }
  }
}
```

&emsp;&emsp;将`slices`设置为`auto`时，Elasticsearch会自动选择切片数量，通常是每个分片一个切片，直到达到上限。如果存在多个data stream或index，它会根据分片数最少的index或backing index来确定切片数。

&emsp;&emsp;在`_update_by_query`中添加切片，实际上是自动化了手动过程，产生了一些特性（quirk）：

- 这些请求在[Tasks APIs](#Running update by query asynchronously)中可见，对于`slice`对应的任务，这些子请求是这个任务的"child"
- 带`slice`的请求任务状态仅显示已完成切片的状态；
- 这些子请求可以单独被取消或重新节流；
- 重新节流`slice`的请求会按比例重新节流未完成的子请求；
- 取消`slice`的请求将取消所有子请求。
- 由于切片的性质，每个子请求不会获得文档的完全平均分配，但所有文档都会被处理。有些分片可能处理的比其他的大。Expect larger slices to have a more even distribution.
- `requests_per_second`和`max_docs`参数会按比例分配给每个子请求。
- 像requests_per_second和max_docs这样的参数会按比例分配给每个子请求。结合上述分配可能不均的情况，可以得出使用`max_docs`的`slice`可能不会精确更新指定数量的文档。
- 每个子请求获取源data stream或index的略有不同的快照，尽管这些快照几乎同时获取。

###### Pick up a new property

&emsp;&emsp;比如说你没有使用动态mapping创建了一个索引，写入了数据并且然后添加了mapping来从数据获取更多的字段：

```text
PUT test
{
  "mappings": {
    "dynamic": false,   
    "properties": {
      "text": {"type": "text"}
    }
  }
}

POST test/_doc?refresh
{
  "text": "words words",
  "flag": "bar"
}
POST test/_doc?refresh
{
  "text": "words words",
  "flag": "foo"
}
PUT test/_mapping   
{
  "properties": {
    "text": {"type": "text"},
    "flag": {"type": "text", "analyzer": "keyword"}
  }
}
```

&emsp;&emsp;第4行，意味着新的字段不会被索引，只保存在`_source`中
&emsp;&emsp;第21行，添加新的`flag`字段到mapping中。但是你需要reindex才能查询到`flag`的信息

&emsp;&emsp;查询这个`flag`字段不会有任何数据：

```text
POST test/_search?filter_path=hits.total
{
  "query": {
    "match": {
      "flag": "foo"
    }
  }
}
```

```text
{
  "hits" : {
    "total": {
        "value": 0,
        "relation": "eq"
    }
  }
}
```

&emsp;&emsp;但是你可以发起`_update_by_query`来添加新的mapping：

```text
POST test/_update_by_query?refresh&conflicts=proceed
POST test/_search?filter_path=hits.total
{
  "query": {
    "match": {
      "flag": "foo"
    }
  }
}
```

```text
{
  "hits" : {
    "total": {
        "value": 1,
        "relation": "eq"
    }
  }
}
```

&emsp;&emsp;当你向多字段（multifield）添加一个字段时，可以做同样的事情。

#### Bulk API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/docs-bulk.html)

#### Reindex API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/docs-reindex.html)

&emsp;&emsp;从一个源拷贝文档到目的地。

&emsp;&emsp; 数据源可以是现有的索引、别名、或者data stream。目的地跟源必须是不同的。比如你不能将data stream reindex到自身。

> IMPORTANT：Reindex要求源中的文档开启[\_source](#_source field)。
> 目的地应该在调用`_reindex`之前进行配置。Reindex不会从源拷贝settings以及相关的模板。
> Mapping、分片数量、副本分片等等必须提前配置好

```text
POST _reindex
{
  "source": {
    "index": "my-index-000001"
  },
  "dest": {
    "index": "my-new-index-000001"
  }
}
```

##### Request

```text
POST /_reindex
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有以下的安全特权：
  - 源的data stream、Index、alias的`read` [index privilege](#####Indices privileges)
  - 目的地的data stream、Index、alias的`write` [index privilege](#####Indices privileges)
  - 若要自动的通过该接口创建data stream或index，你必须要有目的地的data stream、Index、alias的`auto_configure`, `create_index`或 `manage` [index privilege](#####Indices privileges)
  - 如果从远端集群进行reindex，`source.remote.user`必须要有`monitor` [cluster privilege](#####Cluster privileges)。以及源的data stream、Index、alias的`read` [index privilege](#####Indices privileges)
- 如果从远端集群进行reindex，你必须要显示的在`elasticsearch.yml`配置`reindex.remote.whitelist`来允许远端服务器。见[Reindex from remote](######Reindex from remote)
- 自动创建data stream要求一个能匹配data stream的索引模板。见[Set up a data stream](###Set up a data stream)

##### Description

&emsp;&emsp;从源索引中提取[document Source](#_source field)，然后索引到目标索引中。你可以拷贝所有的文档到目标索引，或者Reindex文档的子集。

&emsp;&emsp;就像[\_update_by_query](#Update By Query API)，`_reindex`会获取源的快照，但不同的是目的地不能是自身（`update_by_query`是对自身更新），因此不会出现version conflict的问题（文档的版本号，响应中`_version`字段）。

##### Path parameters
##### Query parameters
##### Response body
##### Example
##### Reindex from remote

#### Term vectors API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/docs-termvectors.html)

#### ?refresh(api)
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/docs-refresh.html#docs-refresh)

### Enrich APIs
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/enrich-apis.html)

&emsp;&emsp;下面enrich API用来管理 [enrich policies](#enrich policy)。

- [Create enrich policy](#Create enrich policy API) 创建或替换enrich policy
- [Delete enrich policy](#Delete enrich policy API) 删除一个enrich policy
- [Get enrich policy](#Get enrich policy API) 返回enrich policy的信息
- [Execute enrich policy](#Execute enrich policy API) 执行一个 enrich policy
- [Enrich stats](#Enrich stats API) 获取 enrich-related 的统计信息

#### Create enrich policy API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/put-enrich-policy-api.html)

&emsp;&emsp;创建一个enrich policy。

```text
PUT /_enrich/policy/my-policy
{
  "match": {
    "indices": "users",
    "match_field": "email",
    "enrich_fields": ["first_name", "last_name", "city", "zip", "state"]
  }
}
```

##### Request

```text
PUT /_enrich/policy/<enrich-policy>
```
##### Prerequisites

&emsp;&emsp;如果开启了Elasticsearch security features，你必须要有：

- 被使用到的索引的`read` index privilege
- [built-in role](#Built-in roles) `enrich_user`

##### Description

&emsp;&emsp;使用该接口来创建一个[enrich policy](#enrich policy)。

> WARNNING：enrich policy一旦创建完毕，你就不能更新或者修改，不过你可以：
> - 创建并且[execute](#Execute enrich policy API)一个新的enrich policy
> - 在使用中的enrich processor中用新的enrich policy替换之前的
> - 使用[delete enrich policy](#Delete enrich policy API)删除之前的enrich policy

##### Path parameters

- `<enrich-policy>`：（Required, string）待创建/更新enrich policy

##### Request body

- `<policy-type>`：（Required,object）配置enrich policy。key是enrich policy的类型。value可以是以下选项：
  - geo_match：基于[geo_shape query](#Geoshape query)丰富数据到incoming document中。例如，见[Example: Enrich your data based on geolocation](#Example: Enrich your data based on geolocation)
  - match：基于[term query](#Term query)丰富数据到IMcoming document中。例如，见[Example: Enrich your data based on exact values](#Example: Enrich your data based on exact values)
  - range：基于[term query](#Term query)去incoming document中匹配数值、日期或者IP地址类型的字段，然后使用匹配到的字段值去enrich index使用相同的字段执行一个范围查询。例如，见[Example: Enrich your data by matching a value to a range](#Example: Enrich your data by matching a value to a range)
  - indices：（Required, string or array of strings）用来创建enrich index的一个或多个source index
    - 如果指定了多个索引，他们必须有一个共同的`match_field`
  - match_field：（Required, string）source index中的字段，用来跟incoming document匹配
  - enrich_fields：（Required, array of strings）将`enrich_fields`中指定的字段添加到匹配到的incoming document中。source index中必须存在这些字段
  - query：（Optional, [Query DSL query object)](#Query DSL)）用来过滤enrich index中的每一篇文档。enrich policy只使用匹配这个query的文档去丰富incoming document。默认是[match_all](#Match all query) query

#### Delete enrich policy API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/delete-enrich-policy-api.html)

&emsp;&emsp;删除一个现有的[enrich policy](#enrich policy)和它的[enrich index](#enrich index)。

```text
DELETE /_enrich/policy/my-policy
```

##### Request

```text
DELETE /_enrich/policy/<enrich-policy>
```

##### Prerequisites

&emsp;&emsp;如果开启了Elasticsearch security features，你必须要有：

- 被使用到的索引的`read` index privilege
- [built-in role](#Built-in roles) `enrich_user`

##### Description

&emsp;&emsp;使用该接口删除一个现有的enrich policy和enrich index。

> IMPORTANT：删除之前，你必须从ingest Pipeline中移除该策略。你不能移除一个正在被使用的enrich policy

##### Path parameters

- `<enrich-policy>`：（Required, string）待删除的enrich policy


#### Get enrich policy API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/get-enrich-policy-api.html)

&emsp;&emsp;返回[enrich policy](#enrich policy)的信息。

```text
GET /_enrich/policy/my-policy
```

##### Request

```text
GET /_enrich/policy/<name>
GET /_enrich/policy
GET /_enrich/policy/policy1,policy2
```

##### Prerequisites

&emsp;&emsp;如果开启了Elasticsearch security features，你必须要有：

- 被使用到的索引的`read` index privilege
- [built-in role](#Built-in roles) `enrich_user`

##### Path parameters

- `<name>`：（Optional, string）用逗号隔开的enrich policy的名称来限制请求。若要返回所有的enrich policy，则忽略这个参数

##### Example

###### Get a single policy

```text
GET /_enrich/policy/my-policy
```

&emsp;&emsp;该接口返回以下响应：

```text
{
  "policies": [
    {
      "config": {
        "match": {
          "name": "my-policy",
          "indices": [ "users" ],
          "match_field": "email",
          "enrich_fields": [
            "first_name",
            "last_name",
            "city",
            "zip",
            "state"
          ]
        }
      }
    }
  ]
}
```

###### Get multiple policies

```text
GET /_enrich/policy/my-policy,other-policy
```

&emsp;&emsp;该接口返回以下响应：

```text
{
  "policies": [
    {
      "config": {
        "match": {
          "name": "my-policy",
          "indices": [ "users" ],
          "match_field": "email",
          "enrich_fields": [
            "first_name",
            "last_name",
            "city",
            "zip",
            "state"
          ]
        }
      }
    },
    {
      "config": {
        "match": {
          "name": "other-policy",
          "indices": [ "users" ],
          "match_field": "email",
          "enrich_fields": [
            "first_name",
            "last_name",
            "city",
            "zip",
            "state"
          ]
        }
      }
    }
  ]
}
```

###### Get all policies

```text
GET /_enrich/policy
```

&emsp;&emsp;该接口返回以下响应：

```text
{
  "policies": [
    {
      "config": {
        "match": {
          "name": "my-policy",
          "indices": [ "users" ],
          "match_field": "email",
          "enrich_fields": [
            "first_name",
            "last_name",
            "city",
            "zip",
            "state"
          ]
        }
      }
    },
    {
      "config": {
        "match": {
          "name": "other-policy",
          "indices": [ "users" ],
          "match_field": "email",
          "enrich_fields": [
            "first_name",
            "last_name",
            "city",
            "zip",
            "state"
          ]
        }
      }
    }
  ]
}
```

#### Execute enrich policy API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/enrich-stats-api.html)

&emsp;&emsp;执行一个现有的[enrich policy](#enrich policy)。

```text
PUT /_enrich/policy/my-policy/_execute
```

##### Request

```text
PUT /_enrich/policy/<enrich-policy>/_execute
POST /_enrich/policy/<enrich-policy>/_execute
```

##### Prerequisites

&emsp;&emsp;如果开启了Elasticsearch security features，你必须要有：

- 被使用到的索引的`read` index privilege
- [built-in role](#Built-in roles) `enrich_user`

##### Description

&emsp;&emsp;使用该接口为现有的enrich policy创建enrich index。

&emsp;&emsp;enrich index中包含了策略中的source index。enrich index 的名称总是以`.enrich-*`开头，只读索引，并且[force merged](#Force merge API)。

> WARNING：enrich index应该只被[enrich processor](#Enrich processor)使用，避免使用enrich index用于其他目的。

&emsp;&emsp;创建之后，你不能向enrich Index中更新或者添加文档。而是更新你的source index然后再次[execute](#Execute enrich policy API) enrich policy。这样会从更新后的source Index中创建新的enrich index。之前的enrich index会使用一个maintenance job稍后删除。默认是每15分钟。

&emsp;&emsp;由于这个接口执行好多操作，可能需要等一段时间才会返回。

##### Path parameters

- `<enrich-policy>`：（Required, string）待执行的enrich policy

##### Query parameters

- `wait_for_completion`：（Required, Boolean）如果为`true`，该接口会阻塞其他enrich policy的执行，直到它自己完成。默认为`true`

#### Enrich stats API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/execute-enrich-policy-api.html)

&emsp;&emsp;返回当前执行中的[enrich policy](#enrich polic)的信息以及[enrich coordinator](#Node roles)的信息。

```text
GET /_enrich/_stats
```

##### Request

```text
GET /_enrich/_stats
```

##### Response body

- executing_policies：（Array of objects）包含当前正在运行中的每一个enrich policy的信息
  - name：（String）enrich policy的名称
  - task：（[Task object](#Task management API)）策略执行任务的详细信息
- coordinator_stats：（Array of objects）包含每一个配置的enrich processor的[coordinating ingest node](#Node roles)的信息
  - node_id：（String）为配置的enrich processor协调执行搜索请求的节点标识符
  - queue_size：（Integer）队列中查询请求的数量
  - remote_requests_current：（Integer）正在执行的远程请求的数量（其他节点的搜索请求，这些请求是为了获取富集数据）
  - remote_requests_total：（Integer）节点启动以来执行的远程请求的总数
    - 大多数情况下，一个远端请求包含了多次查询请求。取决于当远端请求执行后，队列中查询请求的数量
  - executed_searches_total：（Integer）节点启动以后enrich processor执行的查询请求总数
- cache_stats：（Array of objects）包含每一个ingest node上enrich cache统计信息
  - node_id：（String）有enrich cahe的ingest node的标识符
  - count：（Integer）缓存的entry数量
  - hits：（Integer）从缓存中获取的次数
  - missed：（integer）无法从缓存中获取的次数
  - evictions：（integer）从缓存中移除的entry数量

##### Example

```text
GET /_enrich/_stats
```

&emsp;&emsp;接口返回以下响应：

```text
{
  "executing_policies": [
    {
      "name": "my-policy",
      "task": {
        "id": 124,
        "type": "direct",
        "action": "cluster:admin/xpack/enrich/execute",
        "start_time_in_millis": 1458585884904,
        "running_time_in_nanos": 47402,
        "cancellable": false,
        "parent_task_id": "oTUltX4IQMOUUVeiohTt8A:123",
        "headers": {
          "X-Opaque-Id": "123456"
        }
      }
    }
  ],
  "coordinator_stats": [
    {
      "node_id": "1sFM8cmSROZYhPxVsiWew",
      "queue_size": 0,
      "remote_requests_current": 0,
      "remote_requests_total": 0,
      "executed_searches_total": 0
    }
  ],
  "cache_stats": [
    {
      "node_id": "1sFM8cmSROZYhPxVsiWew",
      "count": 0,
      "hits": 0,
      "misses": 0,
      "evictions": 0
    }
  ]
}
```

### EQL APIs
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/eql-apis.html)

#### EQL search API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/eql-search-api.html#eql-search-api)

### Features APIs
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/features-apis.html)

#### Get Features API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/get-features-api.html)

#### Reset features API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/reset-features-api.html)

### Index APIs
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices.html)

&emsp;&emsp;Index APIs用来管理索引的settings、aliases（别名）、mappings和索引模板（Index Template）。

##### Index management

- [Create index](#Create index API)
- [Delete index](#Delete index API)
- [Get index](#Get index API)
- [Exists](#Exists API)
- [Close index](#Close index API)
- [Open index](#Open index API)
- [Shrink index](#Shrink index API)
- [Split index](#Split index API)
- [Clone index](#Clone index API)
- [Rollover](#Rollover API)
- [Unfreeze index](#Unfreeze index API)
- [Resolve index](#Resolve index API)

##### Mapping management

- [Update mapping](#Update mapping API)
- [Get mapping](#Get mapping API)
- [Get field mapping](#Get field mapping API)
- [Analyze index disk usage](#Analyze index disk usage API)

##### Alias management

- [Aliases](#Aliases API)
- [Create or update alias](#Create or update alias API)
- [Get alias](#Get alias API)
- [Alias exists](#Alias exists API)
- [Delete alias](#Delete alias API)

##### Index settings

- [Update index settings](#Update index settings API)
- [Get index settings](#Get index settings API)
- [Analyze](#Analyze API)

##### Index templates

&emsp;&emsp;索引模板自动将settings、mappings、aliases应用到新的索引上。它们通常用于时序数据中rolling indices时使用，使得新的索引跟之前的有相同的配置。data stream中关联的索引模板会配置backing indices。更信息见[Index Template](#Index templates)。

- [Create or update index template](#Create or update index template API)
- [Get index template](#Get index template API)
- [Delete index template](#Delete index template API)
- [Create or update component template](#Create or update component template API)
- [Get component template](#Get component template API)
- [Delete component template](#Delete component template API)
- [Simulate index](#Simulate index API)
- [Simulate template](#Simulate index template API)

##### Dangling indices API

- [List dangling indices](#List dangling indices API)
- [Import dangling index](#Import dangling index API)
- [Delete dangling index](#Delete dangling index API)

#### Alias exists API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-aliases.html)

&emsp;&emsp;检查某个[alias](#Aliases)是否存在。

```text
HEAD _alias/my-alias
```

##### Request

```text
HEAD _alias/<alias>
HEAD <target>/_alias/<alias>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有这个data stream的`view_index_metadata`或者`manage`的[index privilege](#Indices privileges)才能使用这个接口。如果你指定了目标，你同样需要有该目标上的`view_index_metadata`或者`manage`的index privilege

##### Path parameters

- `<alias>`：（Optional, string）用逗号隔开、待检查的别名名称。支持通配符（`*`）
- `<target>`：（Optional, string）用逗号隔开的data stream或indices的名称来限制请求。支持通配符（`*`）。若要获取所有的data streams和indices，可以忽略这个参数或者使用`*`、`_all`

##### Query parameters

- expand_wildcards：（Optional, string）决定在`<target>`参数中如果有通配符模式时将如何去匹配data streams和indices。支持使用逗号隔开的值，例如`open, hidden`。默认是`all`。合法值有：
  - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target syntax-1)
  - open：匹配打开的data streams和indices
  - closed：匹配关闭的data streams和indices
  - hidden：匹配隐藏的data streams和indices。必须和`open`、`closed`中的一个或全部组合使用
  - none：不展开通配符模式
  默认值为`all`。
- ignore_unavailable：（Optional, Boolean）如果为`false`，请求中指定的data stream或者index如果缺失的话会返回一个错误。默认是`false`
- local：（Optional, Boolean）如果为`true`，则只从local node获取信息。默认是`false`，意味着从master node获取信息

##### Response codes

- 200：指定的别名都存在
- 400：一个或多个指定的别名不存在


#### Aliases API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-aliases.html)

&emsp;&emsp;在一个原子操作内执行一个或多个[alias](#Aliases)动作

```text
POST _aliases
{
  "actions": [
    {
      "add": {
        "index": "my-data-stream",
        "alias": "my-alias"
      }
    }
  ]
}
```

##### Request

```text
POST _aliases
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有以下的[manage index privilege](#Security privileges)。
  - 若要使用`add`或者`remove`动作，你必须要在data streams或者indices上有`manage`的的Index privilege
  - 若要使用`remove_index`动作，你必须要在索引上有`manage`的Index privilege

##### Query parameters

- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Response body

- actions（Required, array of objects）执行的动作
  - `<action>`：（Required, object）动作类型对应的key。只要要求有一个动作
    - add：将一个data stream或index添加到alias中。如果alias不存在，`add`动作会创建它
    
    - remove：从alias中移除一个data stream或index
    
    - remove_index：删除一个index。你不能对data stream执行这个动作
    
      >  object中包含一些alias的可选字段，可以是一个空的object。
    
    - alias：（Required\*, string）动作的别名。索引别名（index alias）支持[date math](#Date math support in system and index alias names-1)，如果`aliases`没有指定，`add`和`remove`动作要求这个参数。对于`remove`动作，这个参数支持通配符（`*`）。`remove_index`动作不支持这个参数
    - aliases：（Required\*, array of strings）动作的别名。索引别名（index alias）支持[date math](#Date math support in system and index alias names-1)。如果`alias`没有指定，`add`和`remove`动作要求这个参数。对于`remove`动作，这个参数支持通配符（`*`）。`remove_index`动作不支持这个参数
    - filter：（Optional, [time units](#API conventions)） 用来限制文档访问的DSL语句。
      - 只有`add`动作支持这个参数
    - index：（Required\*, string）执行动作的data stream或index。支持通配符（`*`）。如果`indices`未指定，这个参数必要指定。对于`add`和`remove_index`动作，通配模式同时匹配到data stream和index会返回一个错误
    - indices：（Required\*, string）执行动作的data stream或index。支持通配符（`*`）。如果`index`未指定，这个参数必要指定。对于`add`和`remove_index`动作，通配模式同时匹配到data stream和index会返回一个错误
    - index_routing（: （Optional, string） 用于索引阶段到指定的分片进行写入索引，这个值会覆盖用于写入索引操作的参数`routing`。data stream不支持这个参数
      - 只有`add`动作支持这个参数
    - is_hidden：（Optional, Boolean） 如果为true，那么别名是 [hidden](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-split-index.html#split-index-api-path-params)，默认为false，所有这个别名的索引都要有相同的`is_hidden`值。
      - 只有`add`动作支持这个参数
    - is_write_index： （Optional, Boolean） 如果为true，则为别名设置writer index或data stream
      - 如果别名指向了多个indices或data stream，并且`is_write_index`未设置，那么会拒绝写请求。如果一个索引别名只向某个索引并且`is_write_index`未指定，那么该索引自动成为writer index。data stream不支持自动设置一个writer data stream。即使别名只指向了一个data stream
      - 只有`add`动作支持这个参数 
    - must_exist：（Optional, Boolean）如果为`true`，别名必须存在才能执行动作。默认是`false`。只有`remove`动作支持这个参数
    - routing：（Optional, string） 用来索引阶段或查询阶段路由到指定分片
      - 只有`add`动作支持这个参数
    - search_routing：（Optional, string） 用于查询阶段到指定的分片进行查询,这个值会覆盖用于查询操作的参数`routing`。data stream不支持这个参数
      - 只有`add`动作支持这个参数

#### Analyze API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-analyze.html)

&emsp;&emsp;在文本上执行[analysis](#Text analysis)并返回生成的token。

```text
GET /_analyze
{
  "analyzer" : "standard",
  "text" : "Quick Brown Foxes!"
}
```

##### Request

```text
GET /_analyze
POST /_analyze
GET /<index>/_analyze
POST /<index>/_analyze
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有索引的`manage`的[index privilege](#Indices privileges)才能使用这个接口。

##### Path parameters

- `<index>`：（Optional, string）用来获取该索引的analyzer
  - 如果指定这个参数、请求参数中的`analyzer`或者`<field>`会覆盖这个值
  - 如果未指定`analyzer`或者`field`，则使用默认的analyzer
  - 如果未指定`<index>`或者索引不存在默认的analyzer，则使用[standard analyzer](#Standard analyzer)

##### Query parameters

- analyzer：（Optional, string）应用到`text`类型的分词器的名称。可以是一个内置的[analyzer](#Built-in analyzer reference)，或者是索引中配置的分词器
  - 如果未指定这个参数，这个接口使用字段在mappings中定义的分词器
  - 如果未指定字段，该接口使用索引默认的分词器
  - 如果未指定索引，或者索引没有一个默认的分词器，该接口则使用[standard analyzer](#Standard analyzer)
- attributes：（Optional, array of strings）token属性列表用来`explain`参数中的输出信息
- char_filter：（Optional, array of strings）character filter列表，用来在分词之前对文本的字符进下处理。见[Character filters reference](#Character filters reference)
- explain：（Optional, Boolean）如果为`true`，响应中包含token attribution和额外的详情。默认为`false`
- field：（Optional, string）用来获取分词器的字段。若使用这个参数，你必须指定一个索引
  - 如果指定该参数，`analyzer`参数会覆盖这个值
  - 如果未指定字段，该接口使用索引默认的分词器
  - 如果未指定索引，或者索引没有一个默认的分词器，该接口则使用[standard analyzer](#Standard analyzer)
- filter：（Optional, Array of strings）token filter的列表，在分词之后对token进行处理。见[Token filter reference](#Token filter reference)
- normalizer：（Optional, string）将文本转化为单个token，见[Normalizers](#Normalizers)
- text：（Required, string or array of strings）待分词的文本。如果提供了字符串数组，它会被分词为多值字段
- tokenizer：（Optional, string）将文本划分为token。见[Tokenizer reference](#Tokenizer reference)

##### Example

###### No index specified

&emsp;&emsp;你可以对文本字符串应用一个内置的分词而不用指定索引。

```text
GET /_analyze
{
  "analyzer" : "standard",
  "text" : "this is a test"
}
```

###### Array of text strings

&emsp;&emsp;如果`text`参数提供了字符串数组，则按照多值字段进下处理

```text
GET /_analyze
{
  "analyzer" : "standard",
  "text" : ["this is a test", "the second text"]
}
```

###### Custom analyzer

&emsp;&emsp;你可以使用这个接口测试自定义的包含tokenizers、token filters以及char filter的分词器。Token filter使用`filter`参数：

```text
GET /_analyze
{
  "tokenizer" : "keyword",
  "filter" : ["lowercase"],
  "text" : "this is a test"
}
```

```text
GET /_analyze
{
  "tokenizer" : "keyword",
  "filter" : ["lowercase"],
  "char_filter" : ["html_strip"],
  "text" : "this is a <b>test</b>"
}
```

&emsp;&emsp;也可以通过下面的方式指定自定义的tokenizers, token filters 以及 character filters：

```text
GET /_analyze
{
  "tokenizer" : "whitespace",
  "filter" : ["lowercase", {"type": "stop", "stopwords": ["a", "is", "this"]}],
  "text" : "this is a test"
}
```

###### Specific index

&emsp;&emsp;你可以对指定的索引调用这个接口：

```text
GET /analyze_sample/_analyze
{
  "text" : "this is a test"
}
```

&emsp;&emsp;上面的例子会对`this is a test`这个文本进行分词，使用名为`analyze_sample`的索引中默认的分词器。可以在`analyzer`参数中指定一个不同的分词器：

```text
GET /analyze_sample/_analyze
{
  "analyzer" : "whitespace",
  "text" : "this is a test"
}
```

###### Derive analyzer from a field mapping

&emsp;&emsp;也可以通过某个字段的mappings获取分词器，例如：

```text
GET /analyze_sample/_analyze
{
  "field" : "obj1.field1",
  "text" : "this is a test"
}
```

&emsp;&emsp;将会使用`obj1.fields`在mappings中定义的分词器进下分词（如果没有指定分词器，则使用索引默认分词器）

###### Normalizer（analyzer API）

&emsp;&emsp;可以为名为`analyze_sample`的索引中keyword类型的字段，使用`normalizer`参数。

```text
GET /analyze_sample/_analyze
{
  "normalizer" : "my_normalizer",
  "text" : "BaR"
}
```

&emsp;&emsp;或者自定义构建一个没有token filter和char filter的normalizer。

```text
GET /_analyze
{
  "filter" : ["lowercase"],
  "text" : "BaR"
}
```

###### Explain analyze

&emsp;&emsp;若要获取更多的信息，可以将`explain`设置为`true`（默认为`false`），它会为每一个token输出其属性。你可以通过`attributes`参数过滤token属性。

> NOTE：额外的详细信息在Lucene中标记为实验性质的，可能在未来发生变更

```text
GET /_analyze
{
  "tokenizer" : "standard",
  "filter" : ["snowball"],
  "text" : "detailed output",
  "explain" : true,
  "attributes" : ["keyword"] 
}
```

&emsp;&emsp;第7行，设置了`keyword`使得只输出`keyword`属性。

&emsp;&emsp;上面的请求返回以下结果：

```text
{
  "detail" : {
    "custom_analyzer" : true,
    "charfilters" : [ ],
    "tokenizer" : {
      "name" : "standard",
      "tokens" : [ {
        "token" : "detailed",
        "start_offset" : 0,
        "end_offset" : 8,
        "type" : "<ALPHANUM>",
        "position" : 0
      }, {
        "token" : "output",
        "start_offset" : 9,
        "end_offset" : 15,
        "type" : "<ALPHANUM>",
        "position" : 1
      } ]
    },
    "tokenfilters" : [ {
      "name" : "snowball",
      "tokens" : [ {
        "token" : "detail",
        "start_offset" : 0,
        "end_offset" : 8,
        "type" : "<ALPHANUM>",
        "position" : 0,
        "keyword" : false 
      }, {
        "token" : "output",
        "start_offset" : 9,
        "end_offset" : 15,
        "type" : "<ALPHANUM>",
        "position" : 1,
        "keyword" : false 
      } ]
    } ]
  }
}
```

&emsp;&emsp;第36行，因为在请求中设置了`attributes`参数，因此只输出了`keyword`属性

###### Setting a token limit

&emsp;&emsp生成过多的token可能导致节点出现OOM。下面的设置可以限制生成的token数量：

- index.analyze.max_token_count：使用该接口可以生成的token数量上限。默认是`10000`。如果超过了这个上限则抛出一个错误。接口的endpoint没有指定索引将总是使用`10000`作为上限值。这个设置允许你控制在某个索引中的生成的token数量：

```text
PUT /analyze_sample
{
  "settings" : {
    "index.analyze.max_token_count" : 20000
  }
}
```

```text
GET /analyze_sample/_analyze
{
  "text" : "this is a test"
}
```

#### Analyze index disk usage API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-disk-usage.html)

>WARNING：这个功能目前处于技术预览阶段，未来的版本中可能会进行更改或移除。Elastic会努力修复任何问题，但是技术预览中的功能不受官方正式发布（GA）功能的支持服务级别协议（SLA）约束。

&emsp;&emsp;分析index或data stream中每一个字段的磁盘使用情况。这个API可能不支持在旧版本中创建的索引。对于较小的索引，由于 API 可能无法分析索引的某些部分，结果可能不准确。

##### Request

```text
POST /<target>/_disk_usage
```
##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须在data stream、indices或alias上有`manage`[index privilege](#Indices privileges)来使用这个API。

##### Path parameters

- `<target>`：（Required, string）用逗号隔开的data streams、indices或aliases用来限制请求。使用该接口时建议只查询单个索引（或者是data stream中的单个backing index），因为该接口的开销是相当可观的

##### Query parameters

- allow_no_indices：（Optional, Boolean）如果为`false`，当通配符表达式、[index alias](#Aliases)或者`all`匹配缺失索引或者已关闭的索引则返回一个错误。即使请求找到了打开的索引也可能会返回错误。比如，请求中指定了`foo*, bar*`，但如果找到以`foo`开头的索引，但是没找到以`bar`开头的索引则会返回一个错误。默认为`true`
- expand_wildcards：（Optional, string）决定在`<target>`参数中如果有通配符模式时将如何去匹配data streams和indices。支持使用逗号隔开的值，例如`open, hidden`。默认是`all`。合法值有：
  - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target syntax-1)
  - open：匹配打开的data streams和indices
  - closed：匹配关闭的data streams和indices
  - hidden：匹配隐藏的data streams和indices。必须和`open`、`closed`中的一个或全部组合使用
  - none：不展开通配符模式
  默认值为`open`
- flush：（Optional, Boolean）如果为`true`，该接口在开始分析之前会执行一次flush。如果为`false`，响应中可能会不包括未提交的数据。默认值为`true`
- ignore_unavailable：（Optional, Boolean）如果为`false`，请求中指定index如果缺失的话或者已关闭会返回一个错误。默认是`false`
- run_expensive_tasks：（Required, Boolean）分析字段的使用情况属于资源密集型。若要使用这个接口，这个参数必须设置为`true`。默认为`false`。
- wait_for_active_shards：（Optional, string） 操作开始前已经启用的shard copy（主分片跟副本分片）的数量。设置成`all`或者一个正整数（不能超过索引的分片总数），默认值1. 见[Active shards](#Index API)。

##### Example

```text
POST /my-index-000001/_disk_usage?run_expensive_tasks=true
```

&emsp;&emsp;这个API返回：

```text
{
    "_shards": {
        "total": 1,
        "successful": 1,
        "failed": 0
    },
    "my-index-000001": {
        "store_size": "929mb", 
        "store_size_in_bytes": 974192723,
        "all_fields": {
            "total": "928.9mb", 
            "total_in_bytes": 973977084,
            "inverted_index": {
                "total": "107.8mb",
                "total_in_bytes": 113128526
            },
            "stored_fields": "623.5mb",
            "stored_fields_in_bytes": 653819143,
            "doc_values": "125.7mb",
            "doc_values_in_bytes": 131885142,
            "points": "59.9mb",
            "points_in_bytes": 62885773,
            "norms": "2.3kb",
            "norms_in_bytes": 2356,
            "term_vectors": "2.2kb",
            "term_vectors_in_bytes": 2310
        },
        "fields": {
            "_id": {
                "total": "49.3mb",
                "total_in_bytes": 51709993,
                "inverted_index": {
                    "total": "29.7mb",
                    "total_in_bytes": 31172745
                },
                "stored_fields": "19.5mb", 
                "stored_fields_in_bytes": 20537248,
                "doc_values": "0b",
                "doc_values_in_bytes": 0,
                "points": "0b",
                "points_in_bytes": 0,
                "norms": "0b",
                "norms_in_bytes": 0,
                "term_vectors": "0b",
                "term_vectors_in_bytes": 0
            },
            "_primary_term": {...},
            "_seq_no": {...},
            "_version": {...},
            "_source": {
                "total": "603.9mb",
                "total_in_bytes": 633281895,
                "inverted_index": {...},
                "stored_fields": "603.9mb", 
                "stored_fields_in_bytes": 633281895,
                "doc_values": "0b",
                "doc_values_in_bytes": 0,
                "points": "0b",
                "points_in_bytes": 0,
                "norms": "0b",
                "norms_in_bytes": 0,
                "term_vectors": "0b",
                "term_vectors_in_bytes": 0
            },
            "context": {
                "total": "28.6mb",
                "total_in_bytes": 30060405,
                "inverted_index": {
                    "total": "22mb",
                    "total_in_bytes": 23090908
                },
                "stored_fields": "0b",
                "stored_fields_in_bytes": 0,
                "doc_values": "0b",
                "doc_values_in_bytes": 0,
                "points": "0b",
                "points_in_bytes": 0,
                "norms": "2.3kb",
                "norms_in_bytes": 2356,
                "term_vectors": "2.2kb",
                "term_vectors_in_bytes": 2310
            },
            "context.keyword": {...},
            "message": {...},
            "message.keyword": {...}
        }
    }
}
```

&emsp;&emsp;第8行，索引中被仅分析的分片的存储大小
&emsp;&emsp;第11行，索引中被分析的分片的存储总量。这个总量通常比第8行中指定的索引大小小，因为一些小型元数据文件被忽略，而 API 可能未扫描数据文件的某些部分。
&emsp;&emsp;第36行，`_id`字段的存储大小
&emsp;&emsp;第54行，`_source`字段的存储大小。由于存储字段使用了压缩格式存储在一起，因此只是尽量估算了存储字段的大小。`_id`字段的存储大小，可能不太准确。`_id` 字段的存储大小可能被低估，而 `_source`字段可能被高估

#### Clear cache API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-clearcache.html)


&emsp;&emsp;

##### Request
##### Prerequisites
##### Path parameters
##### Query parameters
##### Response body
##### Example

#### Clone index API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-clone-index.html)

&emsp;&emsp;克隆一个现有的索引。

```text
POST /my-index-000001/_clone/cloned-my-index-000001
```

##### Request

```text
POST /<index>/_clone/<target-index>
PUT /<index>/_clone/<target-index>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有索引或者索引别名的`manage`的[index privilege](#Indices privileges)才能使用这个接口。
- 若要克隆一个索引，索引必须被标记为只读并且[cluster health](#Cluster health API)的状态是`green`

&emsp;&emsp;比如说，下面的请求禁止了`my_source_index`索引的写操作，因此它可以被克隆。像删除索引这种元数据更改仍然是允许的。

```text
PUT /my_source_index/_settings
{
  "settings": {
    "index.blocks.write": true
  }
}
```

&emsp;&emsp;Data  stream当前的write index不能被克隆。若要克隆当前的write index，data stream必须先执行[roolled over](#Rollover)，使得创建一个新的write index，那么之前的write index就可以被克隆了。

##### Description

&emsp;&emsp;使用该接口将现有的索引克隆到一个新的索引中，每一个源主分片会被克隆到新索引的新的主分片中。

> IMPORTANT：Elasticsearch不会将索引模板应用到生成的索引中。该接口同样不会从源索引中拷贝索引元数据。索引元数据包括aliases，ILM阶段定义以及CCR follower信息。例如，如果你克隆一个CCR follower index，生成的克隆索引不会成为一个follower index。
> 这个接口会从源索引中拷贝索引设置到生成的索引中，除了`index.number_of_replicas`和`index.auto_expand_replicas`。若要在生成的索引中设置副本分片的数量，需要在请求该接口时指定

###### How cloning works

&emsp;&emsp;克隆工作方式如下：

- 首先创建一个跟源索引相同定义的目标索引
- 建立源索引跟目标索引的hard-links segments。（如果系统不支持hard-linking，那么所有的段会拷贝到新的索引，这样花费更多的处理时间。同样的如果使用了多个数据路径，并且它们不在同一个磁盘上，由于hardlink不支持跨磁盘，那么就要求完全拷贝段文件）
- 最后，恢复后的目标索引就像是一个被关闭的索引，并且刚刚被打开

###### Clone an index

&emsp;&emsp;若要克隆`my_srouce_index`到一个名为`my_target_index`的新索引中，可以发起下面的请求：

```text
POST /my_source_index/_clone/my_target_index
```

&emsp;&emsp;目标索引添加到集群状态后，该接口就会马上返回，它不需要等待克隆操作开始。

> IMPORTANT：只有满足以下的要求，索引才可以被克隆：
> - 目标索引必须不存在
> - 源索引必须跟目标索引有相同的主分片
> - 目标索引中的主分片数量必须是源索引的因子。源索引必须比目标索引有更多的主分片
> - 处理克隆操作的节点必须要有足够多空闲磁盘来容纳现有索引的第二个副本

&emsp;&emsp;该接口类似[create index API](#Create index API)，因此目标索引接受`settings`和`aliases`：

```text
POST /my_source_index/_clone/my_target_index
{
  "settings": {
    "index.number_of_shards": 5 
  },
  "aliases": {
    "my_search_indices": {}
  }
}
```

&emsp;&emsp;第4行，目标索引的分片数量。必须跟源索引中的分片数量一致。

> NOTE：Mapping不需要在`_clone`请求中指定，源索引中的mapping将用于目标索引。

###### Monitor the cloning process

&emsp;&emsp;克隆处理进度可以通过[\_cat recovery ](#cat recovery API) API查看，或者可以调用[cluster health API](#Cluster health API)以及参数`wait_for_status`设置为`yellow`来等待所有的主分片分配结束。

&emsp;&emsp;只要目标索引被添加到集群状态中，在分片分配结束之前，该接口就会马上返回。在这个时间点，所有的分片处于`unassigned`状态。如果因为任何原因导致目标索引不能分配到收缩节点上，索引的主分片仍然会处于`unassigned`状态直到可以被分配到那个节点上。

&emsp;&emsp;一旦主分片分配结束，就切换成`initializing`状态，然后克隆处理就开始了。当克隆操作完成，就切换成`active`状态。在这个时间点，Elasticsearch会尽量分配副本分片并且可能决策出将主分片重新分配到其他节点。

###### Wait for active shards

&emsp;&emsp;由于克隆操作创建一个新的索引来克隆分片，因此在索引创建时的[wait for active shards](#Create index API)设置也适用于克隆索引操作。

##### Path parameters

- `<index>`：（Required, string）待克隆的源索引名称
- `<target-index>`：（Required, string）待创建的目标索引名称
  - 索引名称必须满足下面的规则：
    - 只允许小写
    - 不能包含\, /, \*, ?, ", <, >, |, \` \` (space character), `,` , \#
    - 在7.0之前允许包含`:`，7.0之后不被支持
    - 不能以`-`, `_`, `+`开头
    - 不能有 `.` 或者`..`
    - 不能超过255个字节，有些字符用多个字节表示，所以更容易超过255个字节的限制
    - 以`.`开头的索引名被弃用了，除了 [hidden indices](#Index Modules) 以及被插件使用的内部的索引名

##### Query parameters

- wait_for_active_shards：（Optional, string） 操作开始前已经启用的shard copy（主分片跟副本分片）的数量。设置成`all`或者一个正整数（不能超过索引的分片总数（`number_of_replicas + 1`）），默认值1，即主分片。见[Active shards](#Index API)。
- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Response body

- aliases：（Optional, object of objects）索引的别名
  - `<alias>`：（Required, object）别名的key，索引别名支持[date math](#Date math support in system and index alias names-1)，这个对象中包含了别名的选项。支持空对象
    - filter: （Optional, [time units](#API conventions)） 用来限制文档访问的DSL语句。
    - index_routing（: （Optional, string） 用于索引阶段到指定的分片进行写入索引，这个值会覆盖用于写入索引操作的参数`routing`
    - is_hidden: （Optional, Boolean） 如果为true，那么别名是 [hidden](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-split-index.html#split-index-api-path-params)，默认为false，所有这个别名的索引都要有相同的`is_hidden`值。
    - is_write_index: （Optional, Boolean） 如果为true，这个索引是这个别名中的[write index](#Aliases)，默认为false。
    - routing: （Optional, string） 用来索引阶段或查询阶段路由到指定分片
    - search_routing: （Optional, string） 用于查询阶段到指定的分片进行查询,这个值会覆盖用于查询操作的参数`routing`
- settings：（Optional, [index setting object](#Index Settings)）索引的配置，见[Index Settings](#Index modules)


#### Close index API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-close.html)

&emsp;&emsp;关闭一个索引。

```text
POST /my-index-000001/_close
```

##### Request

```text
POST /<index>/_close
```
##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有索引或者索引别名的`manage`的[index privilege](#Indices privileges)才能使用这个接口。

##### Description

&emsp;&emsp;你可以使用这个接口关闭打开中的索引。

&emsp;&emsp;关闭后的索引不能执行打开中的索引允许的读/写操作。不能向关闭后的索引写入/查询文档。这样不需要为写入/查询文档维护内部的数据结构，使得对集群有较小的开销。

&emsp;&emsp;当打开或者关闭一个索引时，对于重启索引分片，master要负责反应索引新的状态。分片才能进行正常的恢复操作。集群会对打开/关闭的索引自动进行replicate，保证总是有足够的shard  copies。

&emsp;&emsp;你可以打开/关闭多个索引。如果请求中显示引用了缺失的索引则会抛出错误。当然可以通过`ignore_unavailable=true`来关闭这个行为。

&emsp;&emsp;你可以显示指定要打开/关闭的索引名称。若要使用`_all`、`*`或者其他通配符表达式打开/关闭索引，需要将`action.destructive_requires_name`修改为`false`。可以通过cluster update settings API更新这个参数。

&emsp;&emsp;关闭的索引会消耗大量的磁盘空间，这可能在受控环境中引发问题。可以通过cluster settings API，将 `cluster.indices.close.enable` 设置为 `false` 来禁用关闭索引的功能。默认值为 `true`。

##### Path parameters

- `<index>`：（Optional, string）用逗号隔开的，或者是通配符表达式的索引名称列表用来限制请求
  - 若要关闭所有的索引，可以使用`_all`或`*`。默认你必须显示指定需要关闭的索引名称。若要使用`_all`或者`*`或者通配符表达式，你必须通过[cluster update settings](#Cluster update settings API)API或者`elasticsearch.yml`将`action.destructive_requires_name`修改为`false`

##### Query parameters

- allow_no_indices：（Optional, Boolean）如果为`false`，当通配符表达式、[index alias](#Aliases)或者`all`匹配缺失索引或者已关闭的索引则返回一个错误。即使请求找到了打开的索引也可能会返回错误。比如，请求中指定了`foo*, bar*`，但如果找到以`foo`开头的索引，但是没找到以`bar`开头的索引则会返回一个错误。默认为`true`
- expand_wildcards：（Optional, string）决定在`<target>`参数中如果有通配符模式时将如何去匹配data streams和indices。支持使用逗号隔开的值，例如`open, hidden`。默认是`all`。合法值有：
  - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target syntax-1)
  - open：匹配打开的data streams和indices
  - closed：匹配关闭的data streams和indices
  - hidden：匹配隐藏的data streams和indices。必须和`open`、`closed`中的一个或全部组合使用
  - none：不展开通配符模式
  默认值为`open`
- ignore_unavailable：（Optional, Boolean）如果为`false`，请求中指定index如果缺失的话或者已关闭会返回一个错误。默认是`false`
- wait_for_active_shards：（Optional, string） 操作开始前已经启用的shard copy（主分片跟副本分片）的数量。设置成`all`或者一个正整数（不能超过索引的分片总数（`number_of_replicas + 1`）），默认值1，即主分片。见[Active shards](#Index API)。
- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Example

&emsp;&emsp;下面的例子中展示了如果关闭一个索引：

```text
POST /my-index-000001/_close
```

&emsp;&emsp;该接口返回以下结果：

```text
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "indices": {
    "my-index-000001": {
      "closed": true
    }
  }
}
```

#### Create index API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-create-index.html)

&emsp;&emsp;创建一个新的索引。

```text
PUT /my-index-000001
```

##### Request

```text
PUT /<index>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有索引的`create_index`或者`manage`的[index privilege](#Indices privileges)才能使用这个接口。若要将这个索引添加到某个别名中，你必须有这个别名的`manage`的index privilege

##### Description

&emsp;&emsp;你可以使用该接口将一个新索引添加到集群中。在创建一个索引时，你可以指定下面的内容：

- 定义索引的settings
- 定义索引中字段的mapping
- 定义索引别名

##### Path parameters

&emsp;&emsp;索引名称必须满足下面的规范：

- 只允许小写
- 不能包含\, /, \*, ?, ", <, >, |, \` \` (space character), `,` , \#
- 在7.0之前允许包含`:`，7.0之后不被支持
- 不能以`-`, `_`, `+`开头
- 不能有 `.` 或者`..`
- 不能超过255个字节，有些字符用多个字节表示，所以更容易超过255个字节的限制
- 以`.`开头的索引名被弃用了，除了 [hidden indices](#Index Modules) 以及被插件使用的内部的索引名

##### Query parameters

- wait_for_active_shards：（Optional, string） 操作开始前已经启用的shard copy（主分片跟副本分片）的数量。设置成`all`或者一个正整数（不能超过索引的分片总数），默认值1. 见[Active shards](#Index API)。
- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Response body

- aliases：（Optional, object of objects）索引的别名
  - `<alias>`：（Required, object）别名的key，索引别名支持[date math](#Date math support in system and index alias names-1)，这个对象中包含了别名的选项。支持空对象
    - filter: （Optional, [time units](#API conventions)） 用来限制文档访问的DSL语句。
    - index_routing（: （Optional, string） 用于索引阶段到指定的分片进行写入索引，这个值会覆盖用于写入索引操作的参数`routing`
    - is_hidden: （Optional, Boolean） 如果为true，那么别名是 [hidden](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-split-index.html#split-index-api-path-params)，默认为false，所有这个别名的索引都要有相同的`is_hidden`值。
    - is_write_index: （Optional, Boolean） 如果为true，这个索引是这个别名中的[write index](#Aliases)，默认为false。
    - routing: （Optional, string） 用来索引阶段或查询阶段路由到指定分片
    - search_routing: （Optional, string） 用于查询阶段到指定的分片进行查询,这个值会覆盖用于查询操作的参数`routing`
- mappings：（Optional, [mapping object](#Mapping)）
  - Field names
  - [Field data types](#Field data types)
  - [Mapping parameters](#Mapping parameters)
- settings：（Optional, [index setting object](#Index Settings)）索引的配置，见[Index Settings](#Index modules)

##### Example

###### Index settings（create index api）

&emsp;&emsp;每一个创建的索引都可以有关联settings，在请求体中定义：

```text
PUT /my-index-000001
{
  "settings": {
    "index": {
      "number_of_shards": 3,  
      "number_of_replicas": 2 
    }
  }
}
```

&emsp;&emsp;第5行，`number_of_shards`的值为1。
&emsp;&emsp;第6行，`number_of_replicas`的值为1（一个主分片有一个副本分片）。

&emsp;&emsp;或者可以更简单一点：

```text
PUT /my-index-000001
{
  "settings": {
    "number_of_shards": 3,
    "number_of_replicas": 2
  }
}
```

> NOTE：你不需要再`settings`字段下显示的指定`index`字段

&emsp;&emsp;更多关于在创建索引时，不需要关心的索引层设置（index level settings）的内容见[index modules](#Index modules)。

###### Mappings（create index api）

&emsp;&emsp;该接口提供了mapping定义：

```text
PUT /test
{
  "settings": {
    "number_of_shards": 1
  },
  "mappings": {
    "properties": {
      "field1": { "type": "text" }
    }
  }
}
```

###### Aliases（create index api）

&emsp;&emsp;该接口同样可以提供[别名](#Aliases)的集合：

```text
PUT /test
{
  "aliases": {
    "alias_1": {},
    "alias_2": {
      "filter": {
        "term": { "user.id": "kimchy" }
      },
      "routing": "shard-1"
    }
  }
}
```

&emsp;&emsp;索引别名同样支持[date math](#Date math support in system and index alias names-1)。

```text
PUT /logs
{
  "aliases": {
    "<logs_{now/M}>": {}
  }
}
```

###### Wait for active shards

&emsp;&emsp;默认情况下，索引操作（Index operation）只有在每一个分片的主分片启用后才会返回一个响应，或者请求超时。索引操作的响应中告知发生了哪些事情：

```text
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "index": "logs"
}
```

&emsp;&emsp;`acknowledged`描述的是否索引在集群中成功创建，`shards_acknowledged`描述的是在超时之前，索引中每一个分片要求的分片数量（shard copies，包含主分片跟副本分片）是否都已经启用（started）。注意的是有可能索引创建成功了，但是`acknowledged`或`shards_acknowledged`的值为`false`。这些值简单的描述了是否在超时之前完成了操作。如果`acknowledged`为`false`，超时之前，集群状态都未包含新创建的索引，但可能稍后就创建成功了。如果`shards_acknowledged`为`false`，超时之前，已启用的分片数量未达到要求（默认只需要主分片都已经启用）。即使集群状态已经成功包含了新创建的索引（比如`acknowledged=true`）。

&emsp;&emsp;我们可以通过索引设置`index.write.wait_for_active_shards`使得只要求主分片都已经启用（注意的是修改这个设置同样会影响接下俩的写操作的`wait_for_active_shards`）：

```text
PUT /test
{
  "settings": {
    "index.write.wait_for_active_shards": "2"
  }
}
```

&emsp;&emsp;或者通过请求参数`wait_for_active_shards`修改：

```text
PUT /test?wait_for_active_shards=2
```

&emsp;&emsp;关于`wait_for_active_shards`的详细介绍以及可选的配置值见Index API中的[Active shards](#Index API)。


#### Create or update alias API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-add-alias.html)

&emsp;&emsp;像某个[alias](#Aliases)中添加一个data stream或index。

```text
PUT my-data-stream/_alias/my-alias
```

##### Request

```text
POST <target>/_alias/<alias>
POST <target>/_aliases/<alias>
PUT <target>/_alias/<alias>
PUT <target>/_aliases/<alias>
```

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须在data stream或indices上有`manage`[index privilege](#Indices privileges)来使用这个API。

##### Path parameters

- `<alias>`：（Required, string）待更新的别名。如果别名不存在，这个请求则会创建。索引别名支持[date math](#Date-math-support-in-system-and-index-alias-names-1)
- `<target>`：（Optional, string）用逗号隔开的、待添加的一个或多个data stream、Index。支持通配符(`*`)。通配符模式如果同时匹配到的data stream和indices会返回一个错误

##### Query parameters

- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Request body

- filter：（Optional, [time units](#API conventions)） 用来限制文档访问的DSL语句。
- index_routing（: （Optional, string） 用于索引阶段到指定的分片进行写入索引，这个值会覆盖用于写入索引操作的参数`routing`。data stream不支持这个参数
- is_hidden：（Optional, Boolean） 如果为true，那么别名是 [hidden](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-split-index.html#split-index-api-path-params)，默认为false，所有这个别名的索引都要有相同的`is_hidden`值。
- is_write_index： （Optional, Boolean） 如果为true，则为别名设置writer index或data stream
  - 如果别名指向了多个indices或data stream，并且`is_write_index`未设置，那么会拒绝写请求。如果一个索引别名只向某个索引并且`is_write_index`未指定，那么该索引自动成为writer index。data stream不支持自动设置一个writer data stream。即使别名只指向了一个data stream
- routing：（Optional, string） 用来索引阶段或查询阶段路由到指定分片
- search_routing：（Optional, string） 用于查询阶段到指定的分片进行查询,这个值会覆盖用于查询操作的参数`routing`。data stream不支持这个参数

##### Example

#### Create or update component template API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-component-template.html)

&emsp;&emsp;创建或个新一个组件模版（component template）。组件模版是构造索引模板（[index template](#Index templates)）的块，指定了[mappings](#Mapping)、[settings](#Index Settings)、[aliases](#Aliases)。

```text
PUT _component_template/template_1
{
  "template": {
    "settings": {
      "number_of_shards": 1
    },
    "mappings": {
      "_source": {
        "enabled": false
      },
      "properties": {
        "host_name": {
          "type": "keyword"
        },
        "created_at": {
          "type": "date",
          "format": "EEE MMM dd HH:mm:ss Z yyyy"
        }
      }
    }
  }
}
```

##### Request

```text
PUT /_component_template/<component-template>
```

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage_index_templates`或者`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;一个索引模板可以由多个组件模版组成。若要使用一个组件模版，只需要在索引模板中的`composed_of`字段中指定即可。组件模板只有作为满足匹配的索引模板的一部分才能用于data stream和indices上。

&emsp;&emsp;在索引模板中或者[create index](#Create-index-API)请求中直接指定的settings和mappings会覆盖组件模版中的定义。

&emsp;&emsp;组件模版只在索引创建时使用，对于data streams，包括data stream创建时以及流中backing indices创建时。对组件模版的修改不会影响已有的索引，包括流中的backing indices。

###### Comments in component templates

&emsp;&emsp;你可以使用C风格的`/**/`在索引模板中添加注释。在请求体的任何位置都可以包含注释，但JSON开头的大括号之前除外。

##### Path parameters

- `<component-template>`：（Required, string）待创建的组件模版名称
  > IMPORTANT：Elasticsearch中包含了下面内置的组件模版：
  > - logs-mappings
  > - logs-settings
  > - metrics-mappings
  > - metrics-settings
  > - synthetics-mapping
  > - synthetics-settings
  > [Elastic Agent](https://www.elastic.co/guide/en/fleet/8.2/fleet-overview.html) 使用这些模板来配置data streams的backing indices。如果你要使用Elastic Agent并且想要重写（overwrite）这些模板，只要将你的模板中的`version`设置为一个更高的值
  > 如果你不想使用Elastic Agent并且禁用内置的组件模板和索引模板，可以使用[cluster update  settings API](#Cluster update settings API)将[stack.templates.enabled](#stack.templates.enabled)设置为`false`

##### Query parameters

- create：（Optional, Boolean）如果为`true`，这个请求不能替换或者更新现有的组件模版。默认值为`false`。
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。

##### Response body

- template：（Required, object）待应用的模板。模板中可能包含`mappings`、`settings`、`aliases`配置
  - aliases：（Optional, object of objects）待添加的别名
    - 如果索引模板中定义了`data_stream`，则它们是data stream别名，否则就是索引别名。Data stream忽略了`index_routing`、`routing`以及`search_routing`选项
      - `<alias>`：（Required, object）别名的名称，索引别名支持[date math](#Date math support in system and index alias names-1)，这个对象中包含了别名的选项。支持空对象
        - filter: （Optional, [time units](#API conventions)） 用来限制文档访问的DSL语句。
        - index_routing（: （Optional, string） 用于索引阶段到指定的分片进行写入索引，这个值会覆盖用于写入索引操作的参数`routing`
        - is_hidden: （Optional, Boolean） 如果为true，那么别名是 [hidden](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-split-index.html#split-index-api-path-params)，默认为false，所有这个别名的索引都要有相同的`is_hidden`值。
        - is_write_index: （Optional, Boolean） 如果为true，这个索引是这个别名中的[write index](#Aliases)，默认为false。
        - routing: （Optional, string） 用来索引阶段或查询阶段路由到指定分片
        - search_routing: （Optional, string） 用于查询阶段到指定的分片进行查询,这个值会覆盖用于查询操作的参数`routing`
  - mappings：（Optional, [mapping object](#Mapping)）
    - Field names
    - [Field data types](#Field data types)
    - [Mapping parameters](#Mapping parameters)
  - settings：（Optional, [index setting object](#Index Settings)）索引的配置，见[Index Settings](#Index modules)
- version：（Optional,  integer）用来管理索引模板版本信息。这个值不会由Elasticsearch自动生成
- allow_auto_create：（Optional, Boolean）这个设置覆盖了集群设置中的[action.auto_create_index](#Automatically create data streams and indices)。如果在模板中设置为`true`，即使`actions.auto_create_index`被禁用了，也会使用这个模板自动创建索引。如果为`false`，匹配模版的indices或data stream必须显示的创建，并且可能永不会自动创建
- \_meta：（Optional, object）跟索引模板相关的，可选的用户自定义的元数据。可能有许多内容。这个字段不是由Elasticsearch自动生成的

##### Example

###### Component template with index aliases

&emsp;&emsp;你可以在组件模版中包含[index aliases](#Aliases)。

```text
PUT _component_template/template_1
{
  "template": {
    "settings" : {
        "number_of_shards" : 1
    },
    "aliases" : {
        "alias1" : {},
        "alias2" : {
            "filter" : {
                "term" : {"user.id" : "kimchy" }
            },
            "routing" : "shard-1"
        },
        "{index}-alias" : {} 
    }
  }
}
```

&emsp;&emsp;第16行，在别名中，`{index}`占位符将在索引创建时替代真正的索引名称。

###### Applying component templates

&emsp;&emsp;你可以使用`version`字段像索引模板中添加版本号。内部系统能通过这个版本号简单的管理模板。

&emsp;&emsp;`version`字段是可选的，Elasticsearch不会自动生成该字段。

&emsp;&emsp;若要unset这个`version`，用未定义该字段的模板替换。

```text
PUT /_component_template/template_1
{
  "template": {
    "settings" : {
        "number_of_shards" : 1
    }
  },
  "version": 123
}
```

&emsp;&emsp;你可以通过[get component template API](#Get component template API)检查这个`verison`字段。

###### Component template metadata

&emsp;&emsp;你可以使用`_meta`字段来添加任意的元数据到索引模板中。这个用户定义的对象存储在集群中，因此最好简短些。

&emsp;&emsp;`_meta`字段是可选的，Elasticsearch不会自动生成该字段。

&emsp;&emsp;若要unset这个`_meta`，用未定义该字段的模板替换。

```text
PUT /_component_template/template_1
{
  "template": {
    "settings" : {
        "number_of_shards" : 1
    }
  },
  "_meta": {
    "description": "set number of shards to one",
    "serialization": {
      "class": "MyComponentTemplate",
      "id": 10
    }
  }
}
```

&emsp;&emsp;你可以使用[get component template](#Get component template API)查看`_meta`字段。

#### Create or update index template API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-put-template.html)

&emsp;&emsp;创建/更新一个索引模板（index template）。索引模板中定义了[settings](#Index Settings)，[mappings](#Mapping)和[aliases ](#Aliases)，使得自动的应用到新的索引上。

```text
PUT /_index_template/template_1
{
  "index_patterns" : ["te*"],
  "priority" : 1,
  "template": {
    "settings" : {
      "number_of_shards" : 2
    }
  }
}
```

##### Request

```text
PUT /_index_template/<index-template>
```

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage_index_templates`或者`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;Elasticsearch基于通配模式匹配索引名称将索引模板作用到新的索引上。

&emsp;&emsp;在创建data stream或者index时应用索引模板。对于data stream，当流中的backing index创建时应用settings和mappings。

&emsp;&emsp;在[create index](#Create index API)请求中定义的settings和mappings会覆盖某个索引模板中的settings和mapping。

&emsp;&emsp;索引模板的变更不会应用到现有的索引，包括data stream中现有的backing indices。

###### Comments in index templates

&emsp;&emsp;你可以使用C风格的`/**/`在索引模板中添加注释。在请求体的任何位置都可以包含注释，但JSON开头的大括号之前除外。

##### Path parameters

- `<index-template>`：（Required, string）待创建的索引模板名称

##### Query parameters

- create：（Optional, Boolean）如果为`true`，这个请求不能替换或者更新现有的index template。默认值为`false`。
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。

##### Response body

- composed_of：（Optional, array of object）有序的组件模版（component template）列表。组件模板按照定义中的顺序进下合并，意味着最后一个组件模版有最高的优先级。见[Composing multiple component templates](#Composing aliases, mappings, and settings)中的例子
- data_stream：（Optional, object）如果请求中有这个字段，说明这个模板用来创建data streams以及它们的backing indices。支持空的对象（即`data_stream: {}`）
  - Data stream要求满足匹配的模板中要有一个`data_stream`字段。见[create an index template](#Create an index template)
    - hidden：（Boolean）如果为`true`，data stream是[hidden](#Hidden data streams and indices-1)
    - allow_custom_routing：（Boolean）如果为`true`，流入到data stream的数据允许自定义写请求([custom routing](#\_routing field))
- index_patterns：（Required, array of strings）通配符（`*`）数组，用于对创建中的data streams和indices进行匹配
  - Elasticsearch中有一些内置的索引模板。若要防止跟这些模板发生冲突（被这些模板匹配），见[Avoid index pattern collisions](#Index templates)
- \_meta：（Optional, object）跟索引模板相关的，可选的用户自定义的元数据。可能有许多内容。这个字段不是由Elasticsearch自动生成的
- priority：（Optional, integer）当data stream或者index创建时，决定了优先匹配哪些索引。总是选择优先级最高的模版。如果索引模板中没有指定优先级，即默认为优先级为`0`（最低的优先级）。Elasticsearch不会自动生成这个字段
- template：（Optional, object）待应用（apply）的模版。它可能包含了`aliases`、`mappings`、`settings`这三个配置
    - aliases：（Optional, object of objects）待添加的别名
      - 如果索引模板中定义了`data_stream`，则它们是data stream别名，否则就是索引别名。Data stream忽略了`index_routing`、`routing`以及`search_routing`选项
        - `<alias>`：（Required, object）别名的名称，索引别名支持[date math](#Date math support in system and index alias names-1)，这个对象中包含了别名的选项。支持空对象
          - filter: （Optional, [time units](#API conventions)） 用来限制文档访问的DSL语句。
          - index_routing（: （Optional, string） 用于索引阶段到指定的分片进行写入索引，这个值会覆盖用于写入索引操作的参数`routing`
          - is_hidden: （Optional, Boolean） 如果为true，那么别名是 [hidden](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-split-index.html#split-index-api-path-params)，默认为false，所有这个别名的索引都要有相同的`is_hidden`值。
          - is_write_index: （Optional, Boolean） 如果为true，这个索引是这个别名中的[write index](#Aliases)，默认为false。
          - routing: （Optional, string） 用来索引阶段或查询阶段路由到指定分片
          - search_routing: （Optional, string） 用于查询阶段到指定的分片进行查询,这个值会覆盖用于查询操作的参数`routing`
    - mappings：（Optional, [mapping object](#Mapping)）
      - Field names
      - [Field data types](#Field data types)
      - [Mapping parameters](#Mapping parameters)
    - settings：（Optional, [index setting object](#Index Settings)）索引的配置，见[Index Settings](#Index modules)
- version：（Optional,  integer）用来管理索引模板版本信息。这个值不会由Elasticsearch自动生成

##### Example

###### Index template with index aliases

&emsp;&emsp;你可以在索引模板中包含[index aliases](#Aliases)。

```text
PUT _index_template/template_1
{
  "index_patterns" : ["te*"],
  "template": {
    "settings" : {
        "number_of_shards" : 1
    },
    "aliases" : {
        "alias1" : {},
        "alias2" : {
            "filter" : {
                "term" : {"user.id" : "kimchy" }
            },
            "routing" : "shard-1"
        },
        "{index}-alias" : {} 
    }
  }
}
```

&emsp;&emsp;第16行，在别名中，`{index}`占位符将在索引创建时替代真正的索引名称。

###### Multiple matching templates

&emsp;&emsp;如果匹配到了多个索引模板，那么使用优先级的高的索引模板。例如：

```text
PUT /_index_template/template_1
{
  "index_patterns" : ["t*"],
  "priority" : 0,
  "template": {
    "settings" : {
      "number_of_shards" : 1,
      "number_of_replicas": 0
    },
    "mappings" : {
      "_source" : { "enabled" : false }
    }
  }
}

PUT /_index_template/template_2
{
  "index_patterns" : ["te*"],
  "priority" : 1,
  "template": {
    "settings" : {
      "number_of_shards" : 2
    },
    "mappings" : {
      "_source" : { "enabled" : true }
    }
  }
}
```

&emsp;&emsp;对于以`te*`开头的索引，它将启动`_source`、2个主分片以及1个副本分片，因为只有`template_2`应用到以`te*`开头的索引。

> NOTE：拥有相同优先级且索引模式重叠的多个模板是不允许的，当尝试创建一个与现有相同优先级的索引模板匹配的模板时，系统会抛出错误。

###### Template versioning

&emsp;&emsp;你可以使用`version`字段像索引模板中添加版本号。内部系统能通过这个版本号简单的管理模板。

&emsp;&emsp;`version`字段是可选的，Elasticsearch不会自动生成该字段。

&emsp;&emsp;若要unset这个`version`，用未定义该字段的模板替换。

```text
PUT /_index_template/template_1
{
  "index_patterns" : ["foo", "bar"],
  "priority" : 0,
  "template": {
    "settings" : {
        "number_of_shards" : 1
    }
  },
  "version": 123
}
```

&emsp;&emsp;你可以通过[get index template](#Get index template API)检查这个`verison`字段。

###### Template metadata

&emsp;&emsp;你可以使用`_meta`字段来添加任意的元数据到索引模板中。这个用户定义的对象存储在集群中，因此最好简短些。

&emsp;&emsp;`_meta`字段是可选的，Elasticsearch不会自动生成该字段。

&emsp;&emsp;若要unset这个`_meta`，用未定义该字段的模板替换。

```text
PUT /_index_template/template_1
{
  "index_patterns": ["foo", "bar"],
  "template": {
    "settings" : {
        "number_of_shards" : 3
    }
  },
  "_meta": {
    "description": "set number of shards to three",
    "serialization": {
      "class": "MyIndexTemplate",
      "id": 17
    }
  }
}
```

###### Data stream definition

&emsp;&emsp;若要为data stream定义一个模板，模板中必须包含`data_stream`字段。见[create an index template](#Create an index template)。

```text
PUT /_index_template/template_1
{
  "index_patterns": ["logs-*"],
  "data_stream": { }
}
```

###### Composing aliases, mappings, and settings

&emsp;&emsp;当在索引模板的`composed_of`字段中指定了多个组件模板时，它们会按照指定的顺序合并，这意味着后面的组件模板会覆盖前面的组件模板。来自父索引模板的任何映射、设置或别名随后会被合并进来。最后，索引请求本身的任何配置也会被合并。

在这个例子中，两个组件模板的顺序改变了索引的分片数量。

```text
PUT /_component_template/template_with_2_shards
{
  "template": {
    "settings": {
      "index.number_of_shards": 2
    }
  }
}

PUT /_component_template/template_with_3_shards
{
  "template": {
    "settings": {
      "index.number_of_shards": 3
    }
  }
}

PUT /_index_template/template_1
{
  "index_patterns": ["t*"],
  "composed_of": ["template_with_2_shards", "template_with_3_shards"]
}
```

&emsp;&emsp;在这种情况下，匹配`t*`的索引将会有三个主分片。如果组成模板的顺序颠倒了，索引将会有两个主分片。

&emsp;&emsp;Mapping定义是递归合并的，这意味着后来的Mapping组件可以引入新的字段Mapping并更新Mapping配置。如果一个字段Mapping已经在前面的组件中存在，它的定义将被后来的组件完全覆盖。

&emsp;&emsp;这种递归合并策略不仅适用于字段mappings，也适用于像`dynamic_templates`和`meta`这样的根选项。如果前面的组件包含一个`dynamic_templates`块，那么默认情况下新的`dynamic_templates`会被添加到末尾。如果已经存在一个具有相同键的key，那么它将被新定义覆盖。

#### Create or update index template API（legacy）
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-templates-v1.html)

>IMPORTANT：这篇文档介绍legacy index templates。在Elasticsearch7.8中legacy index templates已被弃用并且使用可组合的模板（composable  template）代替。更多关于composable templates的信息见[Index templates](#Index templates)。

&emsp;&emsp;创建或者更新一个index template

```json
PUT _template/template_1
{
  "index_patterns": ["te*", "bar*"],
  "settings": {
    "number_of_shards": 1
  },
  "mappings": {
    "_source": {
      "enabled": false
    },
    "properties": {
      "host_name": {
        "type": "keyword"
      },
      "created_at": {
        "type": "date",
        "format": "EEE MMM dd HH:mm:ss Z yyyy"
      }
    }
  }
}
```

##### Request

`PUT /_template/<index-template>`

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage_index_templates`或者`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;index template定义了[settings](#Index Settings)和[mappings](#Mapping)并且自动应用到新建的索引上。Elastic search基于index pattern匹配索引名称来应用到新的索引上。

>NOTE：可组合的索引（composable templates）总是优先于（take precedence）legacy templates。如果composable templates没有匹配到新的索引，则根据其顺序应用匹配的旧模板

&emsp;&emsp;只在索引创建期间应用index template。index template的更改不会影响现有的索引。[create index](#Create index API) API请求中指定的settings和mappings会覆盖模板中settings和mapping。

###### Comments in index templates

&emsp;&emsp;你可以在index template中使用 C 风格的 /* */ 块注释。除了开始的大括号之前，你可以在请求正文中的任何位置包含注释。

######  Getting templates

&emsp;&emsp;见[Get index template (legacy)](#Get index template API)。

##### Path parameters
- `<index-template>`：（Required, string）用于创建index template的名称。

##### Query parameters

- create：（Optional, Boolean）如果为`true`，这个请求不能替换或者更新现有的index template。默认值为`false`。
- order：（Optional,integer）如果索引匹配到多个模板，Elasticsearch根据order的值来应用模板。首先合并order值较低的模板。order值较高的模板稍后合并，覆盖order值较低的模板。
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。

##### Request body

- index_patterns：（array）被替代的模板中定义的`index_pattern`内容
- aliases：（Optional, object of objects）待添加的别名
  - 如果索引模板中定义了`data_stream`，则它们是data stream别名，否则就是索引别名。Data stream忽略了`index_routing`、`routing`以及`search_routing`选项
    - `<alias>`：（Required, object）别名的名称，索引别名支持[date math](#Date math support in system and index alias names-1)，这个对象中包含了别名的选项。支持空对象
      - filter: （Optional, [time units](#API conventions)） 用来限制文档访问的DSL语句。
      - index_routing（: （Optional, string） 用于索引阶段到指定的分片进行写入索引，这个值会覆盖用于写入索引操作的参数`routing`
      - is_hidden: （Optional, Boolean） 如果为true，那么别名是 [hidden](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-split-index.html#split-index-api-path-params)，默认为false，所有这个别名的索引都要有相同的`is_hidden`值。
      - is_write_index: （Optional, Boolean） 如果为true，这个索引是这个别名中的[write index](#Aliases)，默认为false。
      - routing: （Optional, string） 用来索引阶段或查询阶段路由到指定分片
      - search_routing: （Optional, string） 用于查询阶段到指定的分片进行查询,这个值会覆盖用于查询操作的参数`routing`
- mappings：（Optional, [mapping object](#Mapping)）
  - Field names
  - [Field data types](#Field data types)
  - [Mapping parameters](#Mapping parameters)
- settings：（Optional, [index setting object](#Index Settings)）索引的配置，见[Index Settings](#Index modules)
- version：（Optional,  integer）用来管理索引模板版本信息。这个值不会由Elasticsearch自动生成

##### Example

###### Index template with index aliases

&emsp;&emsp;你可以在索引模板中包含[index aliases](#Aliases)。

```text
PUT _template/template_1
{
  "index_patterns" : ["te*"],
  "settings" : {
    "number_of_shards" : 1
  },
  "aliases" : {
    "alias1" : {},
    "alias2" : {
      "filter" : {
        "term" : {"user.id" : "kimchy" }
      },
      "routing" : "shard-1"
    },
    "{index}-alias" : {} 
  }
}
```

&emsp;&emsp;第15行，在别名中，`{index}`占位符将在索引创建时替代真正的索引名称。

###### Indices matching multiple templates

&emsp;&emsp;多个索引模板可能匹配同一个索引，因此，settings和mappings会被合并到最终的索引配置中。通过参数`order`控制合并顺序，优先级低的优先应用，随后优先级高的覆盖它们。例如：

```text
PUT /_template/template_1
{
  "index_patterns" : ["te*"],
  "order" : 0,
  "settings" : {
    "number_of_shards" : 1
  },
  "mappings" : {
    "_source" : { "enabled" : false }
  }
}

PUT /_template/template_2
{
  "index_patterns" : ["tes*"],
  "order" : 1,
  "settings" : {
    "number_of_shards" : 1
  },
  "mappings" : {
    "_source" : { "enabled" : true }
  }
}
```

&emsp;&emsp;上面会关闭对`_source`的存储，但对于以`tes*`开头的索引的`_source`则是开启的。对于mappings，是所谓的深度合并，意味着mappings中的object/property可以在更高优先级的模式进下添加/覆盖，并且低等级的模板提供部分配置

> NOTE：有相同优先级并且匹配到相同的索引会导致无法确定的合并顺序

###### Template versioning

&emsp;&emsp;你可以使用`version`参数像索引模板中添加一个可选的版本号。系统内部可以使用这个版本号对模板进行简单管理

&emsp;&emsp;`version`参数是可选的，不会由Elasticsearch自动生成。

&emsp;&emsp;若要移除`version`，那么就不指定这个参数并且重新替换这个模板：

```text
PUT /_template/template_1
{
  "index_patterns" : ["my-index-*"],
  "order" : 0,
  "settings" : {
    "number_of_shards" : 1
  },
  "version": 123
}
```

&emsp;&emsp;你可以通过[get index template](#Get index template API)查看`version`，并且使用请求参数[filter_path](#Response Filtering)使得只返回版本号。

```text
GET /_template/template_1?filter_path=*.version
```

&emsp;&emsp;返回下面的响应：

```text
{
  "template_1" : {
    "version" : 123
  }
}
```

#### Delete component template API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-delete-component-template.html)

&emsp;&emsp;删除一个已有的的组件模版。

```text
DELETE _component_template/template_1
```

##### Request

```text
DELETE /_component_template/<component-template>
```

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage_index_templates`或者`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;这个接口用来 删除一个或多个组件模版，组件模版用于构造索引模板[index template](#Index templates)，来指定mappings、settings以及aliases。

##### Path parameters

- `<component-template>`：（Required, string）用逗号隔开的，可以是通配符表达式的组件模版名称来限制请求

##### Query parameters

- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

#### Delete dangling index API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/dangling-index-delete.html)

&emsp;&emsp;删除一个dangling Index。

##### Request

```text
DELETE /_dangling/<index-uuid>?accept_data_loss=true
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有`manage`的[cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;如果Elasticsearch遇到的索引数据不在当前集群状态中，那么这些索引会被认为是dangling Index。例如，当Elasticsearch节点下线时你删除了超过`cluster.indices.tombstones.size`数量的索引就有可能发生。

&emsp;&emsp;通过索引的UUID来删除一个dangling Index。可以使用[List dangling indices API](#List dangling indices API)获取索引的UUID。

##### Path parameters

- `<index-uuid>`：（Required, string）待删除的索引的UUID。可以使用[List dangling indices API](#List dangling indices API)获取索引的UUID。

##### Query parameters

- accept_data_loss：（Required, Boolean）This field must be set to true in order to carry out the import, since it will no longer be possible to recover the data from the dangling index。
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

#### Delete alias API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-delete-index.html)

&emsp;&emsp;从一个[alias](#Aliases)中移除一个data stream或index。

```text
DELETE my-data-stream/_alias/my-alias
```

##### Request

```text
DELETE <target>/_alias/<alias>
DELETE <target>/_aliases/<alias>
```
##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有这个data stream或indices的`manage`的[index privilege](#Indices privileges)才能使用这个接口。

##### Path parameters

- `<alias>`：（Required, string）待获取的、用逗号隔开的别名名称。支持通配符（`*`）。若要获取所有的别名，可以忽略这个参数或者使用`*`、`_all`
- `<target>`：（Optional, string）用逗号隔开的data stream、Index来限制请求。支持通配符(`*`)。支持通配符（`*`）

##### Query parameters

- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

#### Delete index API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-delete-index.html)

&emsp;&emsp;删除一个或多个索引。

```text
DELETE /my-index-000001
```

##### Request

```t
DELETE /<index>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有这个索引的`delete_index`或者`manage`的[index privilege](#Indices privileges)才能使用这个接口

##### Description

&emsp;&emsp;删除某个索引会删除它的文档、分片以及元数据。他不会删除相关的Kibana组件，比如Data views、visualizations或者dashboards。

&emsp;&emsp;你不能删除某个data  stream 中当前的write index。若要删除它，你必须[roll over](#Rollover)data stream，使得可以生成一个新的write index。随后你就可以使用这个接口删除它。

##### Path parameters

- `<index>`：（Required, string）待删除的用逗号隔开的索引名称。你不能指定[索引别名](#Aliases)
  - 默认不支持通配符（`*`）或者`_all`。若要使用通配符或者`_all`，需要将集群设置[action.destructive_requires_name](#action.destructive_requires_name)设置为`false`
##### Query parameters

- allow_no_indices：（Optional, Boolean）如果为`false`，当通配符表达式、[index alias](#Aliases)或者`all`匹配缺失索引或者已关闭的索引则返回一个错误。即使请求找到了打开的索引也可能会返回错误。比如，请求中指定了`foo*, bar*`，但如果找到以`foo`开头的索引，但是没找到以`bar`开头的索引则会返回一个错误。默认为`true`
- expand_wildcards：（Optional, string）决定在`<target>`参数中如果有通配符模式时将如何去匹配data streams和indices。支持使用逗号隔开的值，例如`open, hidden`。默认是`all`。合法值有：
  - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target syntax-1)
  - open：匹配打开的data streams和indices
  - closed：匹配关闭的data streams和indices
  - hidden：匹配隐藏的data streams和indices。必须和`open`、`closed`中的一个或全部组合使用
  - none：不展开通配符模式
  默认值为`open, closed`。
- ignore_unavailable：（Optional, Boolean）如果为`false`，请求中指定的index如果缺失的话会返回一个错误。默认是`false`
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。


#### Delete index template API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-delete-template.html)

&emsp;&emsp;删除一个[index template](#Index templates)。

```text
DELETE /_index_template/my-index-template
```

##### Request

```text
DELETE /_index_template/<index-template>
```

&emsp;&emsp;`<index-template>`可以是用逗号隔开的多个模版，如果指定了多个模板名称并且没有使用通配符，那么提供的模版名称必须准确匹配现有的模板。

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage_index_templates`或者`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;使用该接口来删除一个或多个索引模板。索引模板中定义的[settings](#Index Settings)、[mappings](#Mapping)和[aliases](#Aliases)会自动应用到新的索引上。

##### Path parameters

- `<index-template>`：（Required, string）用逗号隔开的索引模板名称用来限制请求。可以使用通配符（`*`）表达式

##### Query parameters

- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

#### Exists API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-exists.html)

&emsp;&emsp;检查某个data stream，index或者alias是否存在。

```text
HEAD my-data-stream
```

##### Request

```text
HEAD <target>
```
##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有这个data stream、index、alias的`view_index_metadata`或者`manage`的[index privilege](#Indices privileges)才能使用这个接口。

##### Path parameters

- `<target>`：（Optional, string）用逗号隔开的、待添加的一个或多个data stream、Index。支持通配符(`*`)。

##### Query parameters

- allow_no_indices：（Optional, Boolean）如果为`false`，当通配符表达式、[index alias](#Aliases)或者`all`匹配缺失索引或者已关闭的索引则返回一个错误。即使请求找到了打开的索引也可能会返回错误。比如，请求中指定了`foo*, bar*`，但如果找到以`foo`开头的索引，但是没找到以`bar`开头的索引则会返回一个错误。默认为`true`
- expand_wildcards：（Optional, string）决定在`<target>`参数中如果有通配符模式时将如何去匹配data streams和indices。支持使用逗号隔开的值，例如`open, hidden`。默认是`all`。合法值有：
  - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target syntax-1)
  - open：匹配打开的data streams和indices
  - closed：匹配关闭的data streams和indices
  - hidden：匹配隐藏的data streams和indices。必须和`open`、`closed`中的一个或全部组合使用
  - none：不展开通配符模式
  默认值为`open`。
- flat_settings：（Optional, Boolean）如果为`true`，以铺开的格式返回。默认值为`false`。
- ignore_unavailable：（Optional, Boolean）如果为`false`，请求中index如果缺失的话会返回一个错误。默认是`false`
- include_defaults：（Optional, Boolean）如果为`true`，返回所有默认的集群设置。默认值为`false`
- local：（Optional, Boolean）如果为`true`，则只从local node获取信息。默认是`false`，意味着从master node获取信息

##### Response codes

- 200：指定目标都存在
- 400：一个或多个指定的目标不存在

#### Field usage stats API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/field-usage-stats.html)

&emsp;&emsp;
##### Request
##### Prerequisites
##### Description
##### Path parameters
##### Query parameters
##### Response body
##### Example

#### Flush API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-flush.html)

#### Force merge API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-forcemerge.html)

#### Get alias API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-get-alias.html)

&emsp;&emsp;获取一个或多个[aliases](#Aliases)。

```text
GET my-data-stream/_alias/my-alias
```

##### Request

```text
GET _alias/<alias>
GET _alias
GET <target>/_alias/<alias>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有这个data stream的`view_index_metadata`或者`manage`的[index privilege](#Indices privileges)才能使用这个接口。如果你指定了目标，你同样需要有该目标上的`view_index_metadata`或者`manage`的index privilege

##### Path parameters

- `<alias>`：（Required, string）待获取的、用逗号隔开的别名名称。支持通配符（`*`）。若要获取所有的别名，可以忽略这个参数或者使用`*`、`_all`。
- `<target>`：（Optional, string）用逗号隔开的、待添加的一个或多个data stream、Index。支持通配符(`*`)。通配符模式如果同时匹配到的data stream和indices会返回一个错误

##### Query parameters
- allow_no_indices：（Optional, Boolean）如果为`false`，当通配符表达式、[index alias](#Aliases)或者`all`匹配缺失索引或者已关闭的索引则返回一个错误。即使请求找到了打开的索引也可能会返回错误。比如，请求中指定了`foo*, bar*`，但如果找到以`foo`开头的索引，但是没找到以`bar`开头的索引则会返回一个错误。默认为`true`
- expand_wildcards：（Optional, string）决定在`<target>`参数中如果有通配符模式时将如何去匹配data streams和indices。支持使用逗号隔开的值，例如`open, hidden`。默认是`all`。合法值有：
  - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target syntax-1)
  - open：匹配打开的data streams和indices
  - closed：匹配关闭的data streams和indices
  - hidden：匹配隐藏的data streams和indices。必须和`open`、`closed`中的一个或全部组合使用
  - none：不展开通配符模式
  默认值为`all`。
- ignore_unavailable：（Optional, Boolean）如果为`false`，请求中指定的data stream或者index如果缺失的话会返回一个错误。默认是`false`
- local：（Optional, Boolean）如果为`true`，则只从local node获取信息。默认是`false`，意味着从master node获取信息

#### Get component template API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/getting-component-templates.html)

&emsp;&emsp;获取一个或多个组件模版（component template）的信息。

```text
GET /_component_template/template_1
```

##### Request

```text
GET /_component_template/<component-template>
```

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage_index_templates`或者`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Path parameters

- `<component-template>`：（Optional, string）用逗号隔开的组件模版的名称用来限制查询。支持使用通配符（`*`）表达式

##### Query parameters

- flat_settings：（Optional, Boolean）如果为`true`，以铺开的格式返回。默认值为`false`。
- local：（Optional, Boolean）如果为`true`，则只从local node获取信息。默认是`false`，意味着从master node获取信息
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。

##### Example

###### Get component templates using a wildcard expression

```text
GET /_component_template/temp*
```

###### Get all component templates

```text
GET /_component_template
```

#### Get field mapping API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-get-field-mapping.html)

&emsp;&emsp;获取一个或多个字段的[mapping definitions](#Mapping)。对于data stream，该接口返回流中backing indices的字段的mapping。

&emsp;&emsp;当你不需要[完整的mapping](#Get mapping API)，或者当某个索引中包含很多数量的字段时，这个接口就非常的有用。

```text
GET /my-index-000001/_mapping/field/user
```

##### Request

```text
GET /_mapping/field/<field>
GET /<target>/_mapping/field/<field>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有这个data stream的`view_index_metadata`或者`manage`的[index privilege](#Indices privileges)才能使用这个接口

##### Path parameters

- `<target>`：（Optional, string）用逗号隔开的data stream、indices以及aliases的名称来限制请求。支持通配符（`*`）。若要获取所有的data streams和indices，可以忽略这个参数或者使用`*`、`_all`
- `<field>`：（Optional, string）用逗号隔开的，或者通配符表达式的字段

##### Query parameters

- allow_no_indices：（Optional, Boolean）如果为`false`，当通配符表达式、[index alias](#Aliases)或者`all`匹配缺失索引或者已关闭的索引则返回一个错误。即使请求找到了打开的索引也可能会返回错误。比如，请求中指定了`foo*, bar*`，但如果找到以`foo`开头的索引，但是没找到以`bar`开头的索引则会返回一个错误。默认为`false`
- expand_wildcards：（Optional, string）决定在`<target>`参数中如果有通配符模式时将如何去匹配data streams和indices。支持使用逗号隔开的值，例如`open, hidden`。默认是`all`。合法值有：
  - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target syntax-1)
  - open：匹配打开的data streams和indices
  - closed：匹配关闭的data streams和indices
  - hidden：匹配隐藏的data streams和indices。必须和`open`、`closed`中的一个或全部组合使用
  - none：不展开通配符模式
  - ignore_unavailable：（Optional, Boolean）如果为`false`，请求中指定的index如果缺失的话会返回一个错误。默认是`false`
  - include_defaults：（Optional, Boolean）如果为`true`，响应中包含默认的mapping值。默认值为`false`

##### Examples

###### Example with index setup

&emsp;&emsp;你可以在创建一个新的索引时提供字段的mappings。下面[create index](#Create index API) API的请求创建了一个名为`publications`的索引，并且定义了一些字段的mappings。

```text
PUT /publications
{
  "mappings": {
    "properties": {
      "id": { "type": "text" },
      "title": { "type": "text" },
      "abstract": { "type": "text" },
      "author": {
        "properties": {
          "id": { "type": "text" },
          "name": { "type": "text" }
        }
      }
    }
  }
}
```

&emsp;&emsp;下面只返回了`title`的mapping：

```text
GET publications/_mapping/field/title
```

&emsp;&emsp;这个API返回以下响应：

```text
{
   "publications": {
      "mappings": {
          "title": {
             "full_name": "title",
             "mapping": {
                "title": {
                   "type": "text"
                }
             }
          }
       }
   }
}
```

###### Specifying fields

&emsp;&emsp;该接口允许你指定用逗号隔开的字段的列表。

&emsp;&emsp;例如选择了`author`字段的子字段`id`，你必须使用全名`author.id`。

```text
GET publications/_mapping/field/author.id,abstract,name
```

&emsp;&emsp;返回如下：

```text
{
   "publications": {
      "mappings": {
        "author.id": {
           "full_name": "author.id",
           "mapping": {
              "id": {
                 "type": "text"
              }
           }
        },
        "abstract": {
           "full_name": "abstract",
           "mapping": {
              "abstract": {
                 "type": "text"
              }
           }
        }
     }
   }
}
```

&emsp;&emsp;该接口也支持通配符表达式：

```text
GET publications/_mapping/field/a*
```

&emsp;&emsp;返回：

```text
{
   "publications": {
      "mappings": {
         "author.name": {
            "full_name": "author.name",
            "mapping": {
               "name": {
                 "type": "text"
               }
            }
         },
         "abstract": {
            "full_name": "abstract",
            "mapping": {
               "abstract": {
                  "type": "text"
               }
            }
         },
         "author.id": {
            "full_name": "author.id",
            "mapping": {
               "id": {
                  "type": "text"
               }
            }
         }
      }
   }
}
```


###### Multiple targets and fields

&emsp;&emsp;该接口可以用来通过单次请求从多个data streams或者indices中获取多个字段的mapping。

&emsp;&emsp;`<target>`和`<field>`参数都支持用逗号隔开的列表以及通配符表达式。

&emsp;&emsp;你可以忽略`<target>`参数或者使用`*`、`_all`来获取集群中所有的data streams和indices。

&emsp;&emsp;同样的，你可以忽略`<field>`参数或者使用`*`来获取目标data stream或者index中所有字段的mapping。然而，`<field>`参数不支持`_all`。

&emsp;&emsp;比如，下面的请求从名为`my-index-000001`或`my-index-000002`的data stream或index中获取名为`message`的字段的mapping。

```text
GET /my-index-000001,my-index-000002/_mapping/field/message
```

&emsp;&emsp;下面的请求获取集群中data  stream 或者index中名为`message`以及`user.id`的mapping。

```text
GET /_all/_mapping/field/message
```

&emsp;&emsp;下面的请求获取集群中data  stream 或者index中有`id`属性的所有字段的mapping。

```text
GET /_all/_mapping/field/*.id
```

#### Get index API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-get-index.html)

&emsp;&emsp;返回一个或多个索引信息。对于data stream，该接口返回流中的backing indices。

```text
GET /my-index-000001
```

##### Request

```text
GET /<target>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有这个data stream、index、alias的`view_index_metadata`或者`manage`的[index privilege](#Indices privileges)才能使用这个接口

##### Path parameters

- `<target>`：（Optional, string）用逗号隔开的data stream、indices的名称来限制请求。支持通配符（`*`）。若要获取所有的data streams和indices，可以忽略这个参数或者使用`*`、`_all`

##### Query parameters

- allow_no_indices：（Optional, Boolean）如果为`false`，当通配符表达式、[index alias](#Aliases)或者`all`匹配缺失索引或者已关闭的索引则返回一个错误。即使请求找到了打开的索引也可能会返回错误。比如，请求中指定了`foo*, bar*`，但如果找到以`foo`开头的索引，但是没找到以`bar`开头的索引则会返回一个错误。默认为`true`
- expand_wildcards：（Optional, string）决定在`<target>`参数中如果有通配符模式时将如何去匹配data streams和indices。支持使用逗号隔开的值，例如`open, hidden`。默认是`all`。合法值有：
  - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target syntax-1)
  - open：匹配打开的data streams和indices
  - closed：匹配关闭的data streams和indices
  - hidden：匹配隐藏的data streams和indices。必须和`open`、`closed`中的一个或全部组合使用
  - none：不展开通配符模式
  默认值为`open`。
- feature：（Optional, string）返回索引的指定特性。支持用逗号隔开。可选值为`aliases`、`mappings`以及`settings`。默认是`aliases`，`mappings`，`settings`
- flat_settings：（Optional, Boolean）如果为`true`，以铺开的格式返回。默认值为`false`。
- ignore_unavailable：（Optional, Boolean）如果为`false`，请求中index如果缺失的话会返回一个错误。默认是`false`
- include_defaults：（Optional, Boolean）如果为`true`，返回所有默认的集群设置。默认值为`false`
- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- local：（Optional, Boolean）如果为`true`，则只从local node获取信息。默认是`false`，意味着从master node获取信息

#### Get index settings API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-get-settings.html)

&emsp;&emsp;从一个或多个索引中返回设置信息（setting information）。对于data stream，返回的时backing indices的设置信息

##### Request

```text
GET /<target>/_settings
GET /<target>/_settings/<setting>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有这个data stream、Index或alias的`view_index_metadata`、`monitor`或者`manage`的[index privilege](#Indices privileges)才能使用这个接口

##### Path parameters

- `<target>`：（Optional, string）用逗号隔开的data stream、indices以及aliases的名称来限制请求。支持通配符（`*`）。若要获取所有的data streams和indices，可以忽略这个参数或者使用`*`、`_all`
- `<setting>`：（Optional, string）用逗号隔开的、也可以是通配符表达式的设置名称来限制请求

##### Query parameters

- allow_no_indices：（Optional, Boolean）如果为`false`，当通配符表达式、[index alias](#Aliases)或者`all`匹配缺失索引或者已关闭的索引则返回一个错误。即使请求找到了打开的索引也可能会返回错误。比如，请求中指定了`foo*, bar*`，但如果找到以`foo`开头的索引，但是没找到以`bar`开头的索引则会返回一个错误。默认为`false`
- expand_wildcards：（Optional, string）决定在`<target>`参数中如果有通配符模式时将如何去匹配data streams和indices。支持使用逗号隔开的值，例如`open, hidden`。默认是`all`。合法值有：
  - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target syntax-1)
  - open：匹配打开的data streams和indices
  - closed：匹配关闭的data streams和indices
  - hidden：匹配隐藏的data streams和indices。必须和`open`、`closed`中的一个或全部组合使用
  - none：不展开通配符模式
  默认值为`all`。
- flat_settings：（Optional, Boolean）如果为`true`，以铺开的格式返回。默认值为`false`。
- ignore_unavailable：（Optional, Boolean）如果为`false`，请求中指定的data stream或者index如果缺失的话会返回一个错误。默认是`false`
- include_defaults：（Optional, Boolean）如果为`true`，返回所有默认的集群设置。默认值为`false`
- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- local：（Optional, Boolean）如果为`true`，则只从local node获取信息。默认是`false`，意味着从master node获取信息

##### Example

###### Multiple data streams and indices

&emsp;&emsp;这个接口使得通过单次调用获取多个data stream或index的设置。若要获取集群中所有索引的设置，你可以使用在`<targer>`中使用`_all`或`*`。支持通配符表达式。例如：

```text
GET /my-index-000001,my-index-000002/_settings
GET /_all/_settings
GET /log_2099_*/_settings
```

###### Filtering settings by name

&emsp;&emsp;通过通配符对设置进下过滤：

```text
GET /log_2099_-*/_settings/index.number_*
```

#### Get index template API（legacy）
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-get-template-v1.html)

>IMPORTANT：这篇文档介绍legacy index templates。在Elasticsearch7.8中legacy index templates已被弃用并且使用可组合的模板（composable  template）代替。更多关于composable templates的信息见[Index templates](#Index templates)。

&emsp;&emsp;获取一个或多个索引模板的信息。

```text
GET /_template/template_1
```

##### Request

```text
GET /_template/<index-template>
```

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage_index_templates`或者`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Path parameters

- `<index-template>`：（Optional, string）待返回的模板名称。接受通配符表达式。
 - 若要返回所有的索引模板，则忽略这个参数或者使用`_all`、`*`

##### Query parameters

- flat_settings：（Optional, Boolean）如果为`true`，以铺开的格式返回。默认值为`false`。
- local：（Optional, Boolean）如果为`true`，则只从local node获取信息。默认是`false`，意味着从master node获取信息
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。

##### Example

###### Get multiple index templates

```text
GET /_template/template_1,template_2
```

###### Get index templates using a wildcard expression

```text
GET /_template/temp*
```

###### Get all index templates

```text
GET /_template
```


#### Get index template API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-get-template.html)

&emsp;&emsp;获取一个或多个索引模板的信息。

```text
GET /_index_template/template_1
```

##### Request

```text
GET /_index_template/<index-template>
```

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage_index_templates`或者`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Path parameters

&emsp;&emsp;（Optional, string）待返回的模板名称。接受通配符表达式。如果忽略这个参数则返回所有的模板。

##### Query parameters

- flat_settings：（Optional, Boolean）如果为`true`，以铺开的格式返回。默认值为`false`。
- local：（Optional, Boolean）如果为`true`，则只从local node获取信息。默认是`false`，意味着从master node获取信息
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。

##### Example

###### Get index templates using a wildcard expression

```text
GET /_index_template/temp*
```

###### Get all index templates

```text
GET /_index_template
```

#### Get mapping API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-get-mapping.html)

&emsp;&emsp;获取一个或多个索引的[mapping definitions](# Mapping)。对于data streams。这个接口获取流中backing indices的mappings。

```text
GET /my-index-000001/_mapping
```

##### Request

```text
GET /_mapping
GET /<target>/_mapping
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有这个data stream、index、alias的`view_index_metadata`或者`manage`的[index privilege](#Indices privileges)才能使用这个接口

##### Path parameters

- `<target>`：（Optional, string）用逗号隔开的data stream、indices和aliases的名称来限制请求。支持通配符（`*`）。若要获取所有的data streams和indices，可以忽略这个参数或者使用`*`、`_all`

##### Query parameters

- allow_no_indices：（Optional, Boolean）如果为`false`，当通配符表达式、[index alias](#Aliases)或者`all`匹配缺失索引或者已关闭的索引则返回一个错误。即使请求找到了打开的索引也可能会返回错误。比如，请求中指定了`foo*, bar*`，但如果找到以`foo`开头的索引，但是没找到以`bar`开头的索引则会返回一个错误。默认为`false`
- expand_wildcards：（Optional, string）决定在`<target>`参数中如果有通配符模式时将如何去匹配data streams和indices。支持使用逗号隔开的值，例如`open, hidden`。合法值有：
  - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target syntax-1)
  - open：匹配打开的data streams和indices
  - closed：匹配关闭的data streams和indices
  - hidden：匹配隐藏的data streams和indices。必须和`open`、`closed`中的一个或全部组合使用
  - none：不展开通配符模式
  默认值为`open`。
  
- ignore_unavailable：（Optional, Boolean）如果为`true`，不可用的索引（missing或者closed）会忽略
- local：（Optional, Boolean）如果为`true`，则只从local node获取信息。默认是`false`，意味着从master node获取信息
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- 

##### Example

###### Multiple data streams and indices

&emsp;&emsp;该接口可以通过单次调用获取一个或多个data stream或index的信息。通常可以按照下面的语法来使用：

- `host:port/<target>/_mapping`：其中`<target>`可以是同逗号隔开的名称
。若要获取集群中所有的data stream和indices，可以使用`_all`或者`*`或者移除`<target>`。以下是一些例子：

```text
GET /my-index-000001,my-index-000002/_mapping
```

&emsp;&emsp;如果你想要获取集群中所有索引的mapping，以下几个请求都是等效的：

```text
GET /*/_mapping
GET /_all/_mapping
GET /_mapping
```

#### Import dangling index API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/dangling-index-import.html)

&emsp;&emsp;导入dangling Index。

##### Request

```text
POST /_dangling/<index-uuid>?accept_data_loss=true
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有`manage`的[cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;如果Elasticsearch遇到的索引数据不在当前集群状态中，那么这些索引会被认为是dangling Index。例如，当Elasticsearch节点下线时你删除了超过`cluster.indices.tombstones.size`数量的索引就有可能发生。

&emsp;&emsp;通过索引的UUID将这个索引导入到集群中。可以使用[List dangling indices API](#List dangling indices API)获取索引的UUID。

##### Path parameters

- `<index-uuid>`：（Required, string）待导入的索引的UUID，你可以使用[List dangling indices API](#List dangling indices API)获取索引的UUID。

##### Query Parameters

- accept_data_loss：（Required, Boolean）若要导入一个dangling Index，该参数必须设置为`true`。因为Elasticsearch不知道这个dangling Index的来源，也无法明确分片的新旧（fresh and  stale），不能保证导入的数据代表上一次在集群中最新的状态。
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Examples

&emsp;&emsp;下面的例子描述了如何导入一个dangling Index：

```text
OST /_dangling/zmM4e0JtBkeUjiHD-MihPQ?accept_data_loss=true
```

&emsp;&emsp;API返回下面的响应：

```text
{
  "acknowledged" : true
}
```

#### Index segments API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-segments.html)

#### Index stats API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-segments.html)

#### Rollover API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-rollover-index.html)

&emsp;&emsp;为[data stream](#Data streams)或者[index aliases](#Aliases)创建一个新的索引。

```text
POST my-data-stream/_rollover
```

##### Request

```text
POST /<rollover-target>/_rollover/
POST /<rollover-target>/_rollover/<target-index>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，对于这个索引，你必须要有管理权限[manage index privilege](#Security privileges)。

##### Description

> TIP：我们建议使用ILM的[rollover](#Rollover)动作来自动执行rollover。见[Index lifecycle](#Index lifecycle)。

&emsp;&emsp;该接口为data stream或index aliases创建一个新的索引。这个接口的行为取决于rollover目标。

###### Roll over a data stream

&emsp;&emsp;如果你要roll over一个 data stream，该接口会为这个流创建一个新的write index。这个流之前的write index会成为一个普通的backing index。rollover同样会提高data  stream的generation。见[Rollover](#Rollover)。

###### Roll over an index alias with a write index

> TIP：在Elasticsearch7.9之前，你通常使用[index alias with a write index ](#Manage time series data without data streams)来管理时序数据。Data stream替代了这个功能，要求更少的维护成本，自动跟[data tiers](#Data tiers)集成。
> 见[Convert an index alias to a data stream](#Convert an index alias to a data stream)。

&emsp;&emsp;如果索引别名指向多个索引，其中一个索引肯定是[write index](#Write index)。该接口会为这个别名创建一个新的write index，把`is_write_index`设置为`true`。API同样会将之前的write index的`is_write_index`设置为`false`。

###### Roll over an index alias with one index

&emsp;&emsp;如果rollover一个索引别名，并且它只指向一个索引，那该接口会为别名创建一个新的索引并且移除别名中原来的索引。

###### Increment index names for an alias

&emsp;&emsp;当你rollover一个索引别名，你可以为新的索引指定一个名称。如果你不指定，那么当前的索引会以`-`以及一个数字结尾，比如`my-index-000001`或`my-index-3`，新的索引名会提高结尾的数字。例如你roll over一个别名，并且当前的索引名称为`my-index-000001`，那么创建出的新索引会命名为`my-index-000002`。无论之前的索引名称是什么，这个数值总是6个字符并且`0`作为填充值。

> **Use date math with index alias rollovers**
> 如果你使用索引别名用于时序数据，你可以在索引名称中使用[date math](#Date math support in system and index alias names-1)来追踪rollover的日期。比如你可以创建一个别名，他指向名为`<my-index-{now/d}-000001>`的索引。如果你在2099年5月6日创建了索引，那索引名称就是`my-index-2099.05.06-000001`。如果你在2099年5月7日rollover了这个别名，那索引名称就是`my-index-2099.05.07-000002`。见[Roll over an index alias with a write index](#Roll over an index alias with a write index)中的例子

###### Wait for active shards

&emsp;&emsp;rollover会创建新的索引并且受限于[wait_for_active_shards](#Create index API)设置。

##### Path parameters

- `<rollover-target>`：（Required, string）待rollover的data stream或index aliases的名称
- `<target-index>`：（Optional, string）待创建的索引名称。支持[date math](#Date math support in system and index alias names-1)，Data  stream不支持这个参数
  - 如果别名当前的write index的名称不是以`-`和数字结尾，比如`my-index-000001`或者`my-index-3`，这个参数必须提供。
  - 索引名称必须满足下面的规则：
  - 只允许小写
    - 不能包含\, /, \*, ?, ", <, >, |, \` \` (space character), `,` , \#
    - 在7.0之前允许包含`:`，7.0之后不被支持
    - 不能以`-`, `_`, `+`开头
    - 不能有 `.` 或者`..`
    - 不能超过255个字节，有些字符用多个字节表示，所以更容易超过255个字节的限制
    - 以`.`开头的索引名被弃用了，除了 [hidden indices](#Index Modules) 以及被插件使用的内部的索引名

##### Query parameters

- dry_run：（Optional, Boolean）如果为`true`，请求只是模拟操作并返回产生的状态
- wait_for_active_shards：（Optional, string） 操作开始前已经启用的shard copy（主分片跟副本分片）的数量。设置成`all`或者一个正整数（不能超过索引的分片总数（`number_of_replicas + 1`）），默认值1，即主分片。见[Active shards](#Index API)。
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Request body

- aliases：（Optional, object of objects）索引的别名
  - `<alias>`：（Required, object）别名的key，索引别名支持[date math](#Date math support in system and index alias names-1)，这个对象中包含了别名的选项。支持空对象
    - filter: （Optional, [time units](#API conventions)） 用来限制文档访问的DSL语句。
    - index_routing（: （Optional, string） 用于索引阶段到指定的分片进行写入索引，这个值会覆盖用于写入索引操作的参数`routing`
    - is_hidden: （Optional, Boolean） 如果为true，那么别名是 [hidden](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-split-index.html#split-index-api-path-params)，默认为false，所有这个别名的索引都要有相同的`is_hidden`值。
    - is_write_index: （Optional, Boolean） 如果为true，这个索引是这个别名中的[write index](#Aliases)，默认为false。
    - routing: （Optional, string） 用来索引阶段或查询阶段路由到指定分片
    - search_routing: （Optional, string） 用于查询阶段到指定的分片进行查询,这个值会覆盖用于查询操作的参数`routing`

- condition：（Optional, object）rollover的条件。如果指定了该参数，Elasticsearch只会在当前索引满足一个或多个条件后才会执行rollover。如果未指定，则会无条件执行。
  > IMPORTANT：若要触发rollover，当前索引必须满足请求发出时当时的条件，Elasticsearch不会再响应后监控索引。若要自动rollover，应该去使用ILM的[rollover](#Rollover（action）)

  - max_age：（Optional,[time units](#Time units)）达到在创建索引后开始流逝的时间（elapsed time）最大值后触发滚动动作。总是从索引的创建时间开始计算流逝的时间，即使索引的原先的数据配置为自定义的数据。比如设置了[index.lifecycle.parse_origination_date](#index.lifecycle.parse_origination_date) 或者 [index.lifecycle.origination_date ](#index.lifecycle.origination_date)。
  - max_docs：（Optional,integer）当达到指定的文档数量最大值时触发滚动。上一次refresh后新增的文档不在文档计数中。副本分片中的文档不在文档计数中。
  - max_size：（Optional,[byte units](#Byte size units)）当索引达到一定的大小时触发滚动。指的是索引中所有主分片的大小总量。副本分片的数量不会参与统计。
  > TIP：可以通过[\_cat indices API](#cat indices API)查看当前索引的大小。`pri.store.size`值显示了所有主分片的大小总量。

  - max_primary_shard_size：（Optional,[byte units](#Byte size units)）当索引中的最大的主分片的大小达到某个值就触发滚动。它是索引中主分片大小的最大值。跟`max_size`一样，副本分片则忽略这个参数。

  > TIP：可以通过[\_cat shard API](#cat shards API)查看当前分片的大小。`store`值显示了每一个分片的大小，`prirep`值指示了一个分片是主分片还是副本分片。

  - max_primary_shard_docs：（Optional,integer）当索引中最大的主分片的文档数量达到某个值就触发滚动。这是索引中主分片中的文档数量最大值。跟`max_doc`一样，副本分片则忽略这个参数。

  > TIP：可以通过[\_cat shard API](#cat shards API)查看当前分片的大小。`doc`值显示了每个分片中的文档数量。

- mappings：（Optional, [mapping object](#Mapping)）
  - Field names
  - [Field data types](#Field data types)
  - [Mapping parameters](#Mapping parameters)
  - Data streams不支持该参数
- settings：（Optional, [index setting object](#Index Settings)）索引的配置，见[Index Settings](#Index modules)
  - Data streams不支持该参数

##### Response body

- acknowledged：（Boolean）如果为`true`，请求在`timeout`时间内从master node收到了响应
- shards_acknowledged：（Boolean）如果为`true`，在`master_timeout`时间内从[active shards](#Index API)收到了响应
- old_index：（string）data stream或alias中之前的write index。对于有write index的data stream或aliase，该值就是之前的write index
- new_index：（string）rollover创建出的索引名称。对于有write index的data stream或aliase，该值就是先在的write index
- rolled_over：（Boolean）如果为`true`，data stream或索引别名执行了rolled over
- dry_run：（Boolean）如果为`true`，Elasticsearch没有执行rollover
- condition：（object）请求中的`conditions`指定的每一个条件。如果没有制定条件，这个字段是一个空对象。
  - `<condition>`：（Boolean）key就是每一个条件。value就是执行结果。如果为`true`，说明索引在rollover中满足了这个条件

##### Example

###### Roll over a data stream

&emsp;&emsp;下面的请求无条件 的rollover一个data stream。

```text
POST my-data-stream/_rollover
```

&emsp;&emsp;下面的请求只有在当前write index满足下面的条件才会rollover：

- 索引创建于7天或更久之前
- 索引中包含1000或更多的文档
- 索引最大的主分片大小达到50GB或者更大

```text
POST my-data-stream/_rollover
{
  "conditions": {
    "max_age": "7d",
    "max_docs": 1000,
    "max_primary_shard_size": "50gb",
    "max_primary_shard_docs": "2000"
  }
}
```

&emsp;&emsp;这个API返回：

```text
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "old_index": ".ds-my-data-stream-2099.05.06-000001",
  "new_index": ".ds-my-data-stream-2099.05.07-000002",
  "rolled_over": true,
  "dry_run": false,
  "conditions": {
    "[max_age: 7d]": false,
    "[max_docs: 1000]": true,
    "[max_primary_shard_size: 50gb]": false,
    "[max_primary_shard_docs: 2000]": false
  }
}
```

###### Roll over an index alias with a write index

&emsp;&emsp;下面的请求创建了`<my-index-{now/d}-000001>`并且让其作为`my-alias`的write index。

```text
# PUT <my-index-{now/d}-000001>
PUT %3Cmy-index-%7Bnow%2Fd%7D-000001%3E
{
  "aliases": {
    "my-alias": {
      "is_write_index": true
    }
  }
}
```

&emsp;&emsp;下面的请求只有在当前write index满足下面的条件才会rollover：

- 索引创建于7天或更久之前
- 索引中包含1000或更多的文档
- 索引最大的主分片大小达到50GB或者更大

```text
POST my-alias/_rollover
{
  "conditions": {
    "max_age": "7d",
    "max_docs": 1000,
    "max_primary_shard_size": "50gb",
    "max_primary_shard_docs": "2000"
  }
}
```

&emsp;&emsp;这个API返回：

```text
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "old_index": "my-index-2099.05.06-000001",
  "new_index": "my-index-2099.05.07-000002",
  "rolled_over": true,
  "dry_run": false,
  "conditions": {
    "[max_age: 7d]": false,
    "[max_docs: 1000]": true,
    "[max_primary_shard_size: 50gb]": false,
    "[max_primary_shard_docs: 2000]": false
  }
}
```

&emsp;&emsp;如果别名的索引名称使用来date math并且按照一定的间隔规律进下rollover，你可以使用data math来缩小查询范围。例如，下面的查询目标是最近三天的数据。

```text
# GET /<my-index-{now/d}-*>,<my-index-{now/d-1d}-*>,<my-index-{now/d-2d}-*>/_search
GET /%3Cmy-index-%7Bnow%2Fd%7D-*%3E%2C%3Cmy-index-%7Bnow%2Fd-1d%7D-*%3E%2C%3Cmy-index-%7Bnow%2Fd-2d%7D-*%3E/_search
```

###### Roll over an index alias with one index

&emsp;&emsp;下面的请求创建了`<my-index-{now/d}-000001>`以及他的别名`my-write-alias`。

```text
# PUT <my-index-{now/d}-000001>
PUT %3Cmy-index-%7Bnow%2Fd%7D-000001%3E
{
  "aliases": {
    "my-write-alias": { }
  }
}
```

&emsp;&emsp;下面的请求只有在当前write index满足下面的条件才会rollover：

- 索引创建于7天或更久之前
- 索引中包含1000或更多的文档
- 索引最大的主分片大小达到50GB或者更大

```text
POST my-write-alias/_rollover
{
  "conditions": {
    "max_age": "7d",
    "max_docs": 1000,
    "max_primary_shard_size": "50gb",
    "max_primary_shard_docs": "2000"
  }
}
```

&emsp;&emsp;这个API返回：

```text
{
  "acknowledged": true,
  "shards_acknowledged": true,
  "old_index": "my-index-2099.05.06-000001",
  "new_index": "my-index-2099.05.07-000002",
  "rolled_over": true,
  "dry_run": false,
  "conditions": {
    "[max_age: 7d]": false,
    "[max_docs: 1000]": true,
    "[max_primary_shard_size: 50gb]": false,
    "[max_primary_shard_docs: 2000]": false
  }
}
```

###### Specify settings during a rollover

&emsp;&emsp;通常你可以使用[index template](#Index templates)在rollover时自动配置索引。如果你rollover一个索引别名，你可以在接口添加额外的index settings或者覆盖模板中的settings。Data stream中不支持`settings`参数。

```text
POST my-alias/_rollover
{
  "settings": {
    "index.number_of_shards": 2
  }
}
```

#### Shrink index API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-shrink-index.html)

&emsp;&emsp;将现有的索引收缩为一个拥有更少主分片的新索引。

```text
POST /my-index-000001/_shrink/shrunk-my-index-000001
```
##### Request

```text
POST /<index>/_shrink/<target-index>
PUT /<index>/_shrink/<target-index>
```
##### Prerequisites

- 如果开启了Elasticsearch security features，对于这个索引，你必须要有管理权限[manage index privilege](#Security privileges)。
- 在你可以收缩索引之前，必须：
  - 索引必须是只读的
  - 索引的每一个分片的拷贝都必须位于同一个节点上
  - 索引的[集群健康](#Cluster health API)必须是`green`

&emsp;&emsp;为了让分片分配更简单，我们建议你移除索引的副本分片。你可以最后重新添加副本分片，并且这个操作作为收缩操作的一部分。

&emsp;&emsp;你可以通过[update index settings API](#Update index settings API)移除索引的副本分片，将剩余的分片重新分配到同一个节点，并且让索引为只读。

```text
PUT /my_source_index/_settings
{
  "settings": {
    "index.number_of_replicas": 0,                                
    "index.routing.allocation.require._name": "shrink_node_name", 
    "index.blocks.write": true                                    
  }
}
```

&emsp;&emsp;第4行，移除索引的副本分片
&emsp;&emsp;第5行，将索引的分片重新分配到名为`shrink_node_name`上，见[Index-level shard allocation filtering](#Index-level shard allocation filtering)
&emsp;&emsp;第6行，阻止索引的写操作。但元数据的变更，比如删除索引，仍然是允许的

&emsp;&emsp;重新分配索引需要花费一定的时间。处理进度可以通过[\_cat recovery ](#cat recovery API) API查看，或者可以调用[cluster health API](#Cluster health API)以及参数`wait_for_no_relocating_shards`等待所有的分片重新分配结束。

##### Description

&emsp;&emsp;该接口将现有的索引收缩为一个拥有更少主分片的新索引。目标索引中请求的主分片数量必须是源索引中分片数量的因子（factor）。例如，具有 8 个主分片的索引可以收缩为具有 4、2 或 1 个主分片，而具有 15 个主分片的索引可以收缩为具有 5、3 或 1 个主分片。如果索引中的分片数是质数（prime number），则只能收缩为单个主分片。在收缩之前，索引中每个分片（主分片或副本）必须存在于同一节点上。

&emsp;&emsp;data stream当前的write index不能收缩。若要收缩当前的write index，data  stream必须先[rolled over](#Rollover)，这样新的write Index会被创建，那么之前的write index就可以收缩了。

###### How shrinking works

&emsp;&emsp;收缩操作：

1. 创建一个跟源索引有相同定义的目标索引，但是有更少的主分片
2. 建立源索引跟目标索引的hard-links segments。（如果系统不支持hard-linking，那么所有的段会拷贝到新的索引，这样花费更多的处理时间。同样的如果使用了多个数据路径，并且它们不在同一个磁盘上，由于hardlink不支持跨磁盘，那么就要求完全拷贝段文件）
3. 恢复后的目标索引就像是一个被关闭的索引，并且刚刚被打开

###### Shrink an index

&emsp;&emsp;若要将源索引`my_source_index`收缩到名为`my_target_index`的目标索引中，可以发起下面的请求：

```text
POST /my_source_index/_shrink/my_target_index
{
  "settings": {
    "index.routing.allocation.require._name": null, 
    "index.blocks.write": null 
  }
}
```

&emsp;&emsp;第4行，清除目标索引中来自源索引的索引设置`allocation requirement`
&emsp;&emsp;第5行，清除目标索引中来自源索引的索引设置`index wrtie block`

&emsp;&emsp;一旦目标索引添加到集群状态，上面的请求就会马上返回。他不会等待收缩操作开始。

> IMPORTANT：只有满足以下的要求，索引才可以被收缩：
> - 目标索引必须不存在
> - 源索引必须比目标索引有更多的主分片
> - 目标索引中的主分片数量必须是源索引的因子。源索引必须比目标索引有更多的主分片
> - 如果目标索引是单个分片，那么源索引中所有分片的文档数量总和不能超过2,147,483,519，因为这是单个分片能处理的最大文档数量
> - 处理收缩操作的节点必须要有足够多空闲磁盘来容纳现有索引的第二个副本

&emsp;&emsp;`_shrink`操作类似[create index API](#Create index API)，接受对目标索引提供`settings`和`aliases`参数：

```text
POST /my_source_index/_shrink/my_target_index
{
  "settings": {
    "index.number_of_replicas": 1,
    "index.number_of_shards": 1, 
    "index.codec": "best_compression" 
  },
  "aliases": {
    "my_search_indices": {}
  }
}
```

&emsp;&emsp;第5行，目标索引的主分片数量。必须是源索引的因子
&emsp;&emsp;第6行，`best compression`只有对新写入到目标索引的数据生效，比如通过[force-merging](#Force merge APIs)将分片合并为单个段。

###### Monitor the shrink process

&emsp;&emsp;收缩处理进度可以通过[\_cat recovery ](#cat recovery API) API查看，或者可以调用[cluster health API](#Cluster health API)以及参数`wait_for_status`设置为`yellow`来等待所有的主分片分配结束。

&emsp;&emsp;只要目标索引被添加到集群状态中，在分片分配结束之前，该接口就会马上返回。在这个时间点，所有的分片处于`unassigned`状态。如果因为任何原因导致目标索引不能分配到收缩节点上，索引的主分片仍然会处于`unassigned`状态直到可以被分配到那个节点上。

&emsp;&emsp;一旦主分片分配结束，就切换成`initializing`状态，然后收缩处理就开始了。当收缩操作完成，就切换成`active`状态。在这个时间点，Elasticsearch会尽量分配副本分片并且可能决策出将主分片重新分配到其他节点。

###### Wait for active shards

&emsp;&emsp;由于收缩操作创建一个新的索引来收缩分片，因此在索引创建时的[wait for active shards](#Create index API)设置也适用于收缩索引操作。

##### Path parameters

- `<index>`：（Required, string）待收缩的源索引名称
- `<target-index>`：（Required, string）待创建的目标索引名称
  - 索引名称必须满足下面的规则：
    - 只允许小写
    - 不能包含\, /, \*, ?, ", <, >, |, \` \` (space character), `,` , \#
    - 在7.0之前允许包含`:`，7.0之后不被支持
    - 不能以`-`, `_`, `+`开头
    - 不能有 `.` 或者`..`
    - 不能超过255个字节，有些字符用多个字节表示，所以更容易超过255个字节的限制
    - 以`.`开头的索引名被弃用了，除了 [hidden indices](#Index Modules) 以及被插件使用的内部的索引名

##### Query parameters

- wait_for_active_shards：（Optional, string） 操作开始前已经启用的shard copy（主分片跟副本分片）的数量。设置成`all`或者一个正整数（不能超过索引的分片总数（`number_of_replicas + 1`）），默认值1，即主分片。见[Active shards](#Index API)。
- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Response body

- aliases：（Optional, object of objects）索引的别名
  - `<alias>`：（Required, object）别名的key，索引别名支持[date math](#Date math support in system and index alias names-1)，这个对象中包含了别名的选项。支持空对象
    - filter: （Optional, [time units](#API conventions)） 用来限制文档访问的DSL语句。
    - index_routing（: （Optional, string） 用于索引阶段到指定的分片进行写入索引，这个值会覆盖用于写入索引操作的参数`routing`
    - is_hidden: （Optional, Boolean） 如果为true，那么别名是 [hidden](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-split-index.html#split-index-api-path-params)，默认为false，所有这个别名的索引都要有相同的`is_hidden`值。
    - is_write_index: （Optional, Boolean） 如果为true，这个索引是这个别名中的[write index](#Aliases)，默认为false。
    - routing: （Optional, string） 用来索引阶段或查询阶段路由到指定分片
    - search_routing: （Optional, string） 用于查询阶段到指定的分片进行查询,这个值会覆盖用于查询操作的参数`routing`
- settings：（Optional, [index setting object](#Index Settings)）索引的配置，见[Index Settings](#Index modules)
- max_primary_shard_size：（Optional,[byte units](#Byte size units)）目标索引的最大主分片大小。用于找到目标索引的最佳分片数。当设置了此参数时，目标索引中每个分片的存储大小不会超过该参数。目标索引的分片数仍将是源索引分片数的因子，但如果该参数小于源索引中的单个分片大小，则目标索引的分片数将等于源索引的分片数。例如，当将此参数设置为50GB时，如果源索引有60个主分片，总计为100GB，则目标索引将具有2个主分片，每个分片大小为50GB；如果源索引有60个主分片，总计为1000GB，则目标索引将有20个主分片；如果源索引有60个主分片，总计为4000GB，则目标索引仍将有60个主分片。该参数与设置中的 number_of_shards 冲突，只能设置其中一个。

#### Simulate index API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-simulate-index.html)

> WARNING：这个功能目前处于技术预览阶段，未来的版本中可能会进行更改或移除。Elastic会努力修复任何问题，但是技术预览中的功能不受官方正式发布（GA）功能的支持服务级别协议（SLA）约束。

&emsp;&emsp;返回索引的配置，这些配置从匹配到的[Index template](#Index templates)索引模板中获得。

```text
POST /_index_template/_simulate_index/my-index-000001
```

##### Request

```text
POST /_index_template/_simulate_index/<index>
```

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage_index_templates`或者`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Path Parameters

- `<index>`：（Required, string）待模拟的索引名称

##### Query parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Request body

- overlapping：（array）同样匹配了索引名称，但是被优先级更高的模板替代。如果没有模板被替代那么这个字段是一个空的数组
  - name：（string）被替代的模板的名称
  - index_patterns：（array）被替代的模板中定义的`index_pattern`内容
- template：（object）应用到索引的settings、mappings以及aliases
  - aliases：（Optional, object of objects）待添加的别名
    - 如果索引模板中定义了`data_stream`，则它们是data stream别名，否则就是索引别名。Data stream忽略了`index_routing`、`routing`以及`search_routing`选项
      - `<alias>`：（Required, object）别名的名称，索引别名支持[date math](#Date math support in system and index alias names-1)，这个对象中包含了别名的选项。支持空对象
        - filter: （Optional, [time units](#API conventions)） 用来限制文档访问的DSL语句。
        - index_routing（: （Optional, string） 用于索引阶段到指定的分片进行写入索引，这个值会覆盖用于写入索引操作的参数`routing`
        - is_hidden: （Optional, Boolean） 如果为true，那么别名是 [hidden](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-split-index.html#split-index-api-path-params)，默认为false，所有这个别名的索引都要有相同的`is_hidden`值。
        - is_write_index: （Optional, Boolean） 如果为true，这个索引是这个别名中的[write index](#Aliases)，默认为false。
        - routing: （Optional, string） 用来索引阶段或查询阶段路由到指定分片
        - search_routing: （Optional, string） 用于查询阶段到指定的分片进行查询,这个值会覆盖用于查询操作的参数`routing`
  - mappings：（Optional, [mapping object](#Mapping)）
    - Field names
    - [Field data types](#Field data types)
    - [Mapping parameters](#Mapping parameters)
    - 如果没有应用mapping，响应中会忽略这个字段
  - settings：（Optional, [index setting object](#Index Settings)）索引的配置，见[Index Settings](#Index modules)
    - 如果没有应用mapping，响应中会忽略这个字段

##### Examples

&emsp;&emsp;下面的例子展示的是应用到`my-index-000001`索引的现有模板的配置信息。

```text
PUT /_component_template/ct1                    
{
  "template": {
    "settings": {
      "index.number_of_shards": 2
    }
  }
}

PUT /_component_template/ct2                    
{
  "template": {
    "settings": {
      "index.number_of_replicas": 0
    },
    "mappings": {
      "properties": {
        "@timestamp": {
          "type": "date"
        }
      }
    }
  }
}

PUT /_index_template/final-template             
{
  "index_patterns": ["my-index-*"],
  "composed_of": ["ct1", "ct2"],
  "priority": 5
}

POST /_index_template/_simulate_index/my-index-000001 
```

&emsp;&emsp;第1行，创建一个组件模版（`ct1`），定义主分片数量为2。
&emsp;&emsp;第10行，创建第二个组件模版（`ct2`）定义0个副本分片以及一个mapping
&emsp;&emsp;第26行，创建一个索引模板（final-template）并且使用了两个组件模版
&emsp;&emsp;第33行，展示将应用到`my-index-000001`的配置信息

&emsp;&emsp;响应中展示了由`final-template`提供的settings、mappings以及aliases：

```text
{
  "template" : {
    "settings" : {
      "index" : {
        "number_of_shards" : "2",
        "number_of_replicas" : "0",
        "routing" : {
          "allocation" : {
            "include" : {
              "_tier_preference" : "data_content"
            }
          }
        }
      }
    },
    "mappings" : {
      "properties" : {
        "@timestamp" : {
          "type" : "date"
        }
      }
    },
    "aliases" : { }
  },
  "overlapping" : [
    {
      "name" : "template_1",
      "index_patterns" : [
        "my-index-*"
      ]
    }
  ]
}
```

#### Simulate index template API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-simulate-template.html)

&emsp;&emsp;返回将被某个索引模板（[index template](#Index templates)）应用的索引配置信息。

```text
POST /_index_template/_simulate/template_1
```

##### Request

```text
POST /_index_template/_simulate/<index-template>
```

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage_index_templates`或者`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Path parameters

- `<index-template>`：（Optional, string）待模拟的索引模板名称。将一个模板配置添加到集群之前先进下测试，忽略这个参数并且在请求内容中指定模版配置

##### Query parameters

- create：（Optional, Boolean）如果为`true`，如果请求中的模板通过测试并且不存在其他有相同`index patterns`（模版中的`index_patterns`字段）时才会被使用。如果为`false`，则会使用优先级最高的那个模版。注意的是不管是哪个值，都不会将模板永久的添加/更新到集群中，这只是用来模拟。默认为`false`
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`

##### Request body

- data_stream：（Optional, object）如果请求中有这个字段，说明这个模板用来创建data streams以及它们的backing indices。支持空的对象（即`data_stream: {}`）
  - Data stream要求满足匹配的模板中要有一个`data_stream`字段。见[create an index template](#Create an index template)
    - hidden：（Boolean）如果为`true`，data stream是[hidden](#Hidden data streams and indices-1)
    - allow_custom_routing：（Boolean）如果为`true`，流入到data stream的数据允许自定义写请求([custom routing](#\_routing field))
- index_patterns：（Required, array of strings）通配符（`*`）数组，用于对创建中的data streams和indices进行匹配
  - Elasticsearch中有一些内置的索引模板。若要防止跟这些模板发生冲突（被这些模板匹配），见[Avoid index pattern collisions](#Index templates)
- \_meta：（Optional, object）跟索引模板相关的，可选的用户自定义的元数据。可能有许多内容。这个字段不是由Elasticsearch自动生成的
- priority：（Optional, integer）当data stream或者index创建时，决定了优先匹配哪些索引。总是选择优先级最高的模版。如果索引模板中没有指定优先级，即默认为优先级为`0`（最低的优先级）。Elasticsearch不会自动生成这个字段
- template：（Optional, object）待应用（apply）的模版。它可能包含了`aliases`、`mappings`、`settings`这三个配置
    - aliases：（Optional, object of objects）待添加的别名
      - 如果索引模板中定义了`data_stream`，则它们是data stream别名，否则就是索引别名。Data stream忽略了`index_routing`、`routing`以及`search_routing`选项
        - `<alias>`：（Required, object）别名的名称，索引别名支持[date math](#Date math support in system and index alias names-1)，这个对象中包含了别名的选项。支持空对象
          - filter: （Optional, [time units](#API conventions)） 用来限制文档访问的DSL语句。
          - index_routing（: （Optional, string） 用于索引阶段到指定的分片进行写入索引，这个值会覆盖用于写入索引操作的参数`routing`
          - is_hidden: （Optional, Boolean） 如果为true，那么别名是 [hidden](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-split-index.html#split-index-api-path-params)，默认为false，所有这个别名的索引都要有相同的`is_hidden`值。
          - is_write_index: （Optional, Boolean） 如果为true，这个索引是这个别名中的[write index](#Aliases)，默认为false。
          - routing: （Optional, string） 用来索引阶段或查询阶段路由到指定分片
          - search_routing: （Optional, string） 用于查询阶段到指定的分片进行查询,这个值会覆盖用于查询操作的参数`routing`
    - mappings：（Optional, [mapping object](#Mapping)）
      - Field names
      - [Field data types](#Field data types)
      - [Mapping parameters](#Mapping parameters)
    - settings：（Optional, [index setting object](#Index Settings)）索引的配置，见[Index Settings](#Index modules)
- version：（Optional,  integer）用来管理索引模板版本信息。这个值不会由Elasticsearch自动生成

##### Response body

- overlapping：（array）同样匹配了索引名称，但是被优先级更高的模板替代。如果没有模板被替代那么这个字段是一个空的数组
  - name：（string）被替代的模板的名称
  - index_patterns：（array）被替代的模板中定义的`index_pattern`内容
- template：（object）应用到索引的settings、mappings以及aliases
  - aliases：（Optional, object of objects）待添加的别名
    - 如果索引模板中定义了`data_stream`，则它们是data stream别名，否则就是索引别名。Data stream忽略了`index_routing`、`routing`以及`search_routing`选项
      - `<alias>`：（Required, object）别名的名称，索引别名支持[date math](#Date math support in system and index alias names-1)，这个对象中包含了别名的选项。支持空对象
        - filter: （Optional, [time units](#API conventions)） 用来限制文档访问的DSL语句。
        - index_routing（: （Optional, string） 用于索引阶段到指定的分片进行写入索引，这个值会覆盖用于写入索引操作的参数`routing`
        - is_hidden: （Optional, Boolean） 如果为true，那么别名是 [hidden](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-split-index.html#split-index-api-path-params)，默认为false，所有这个别名的索引都要有相同的`is_hidden`值。
        - is_write_index: （Optional, Boolean） 如果为true，这个索引是这个别名中的[write index](#Aliases)，默认为false。
        - routing: （Optional, string） 用来索引阶段或查询阶段路由到指定分片
        - search_routing: （Optional, string） 用于查询阶段到指定的分片进行查询,这个值会覆盖用于查询操作的参数`routing`
  - mappings：（Optional, [mapping object](#Mapping)）
    - Field names
    - [Field data types](#Field data types)
    - [Mapping parameters](#Mapping parameters)
  - settings：（Optional, [index setting object](#Index Settings)）索引的配置，见[Index Settings](#Index modules)

##### Example

###### Simulating an existing template

&emsp;&emsp;下面的例子中创建并且模拟了一个组件模版：

```text
PUT /_component_template/ct1                   
{
  "template": {
    "settings": {
      "index.number_of_shards": 2
    }
  }
}

PUT /_component_template/ct2                    
{
  "template": {
    "settings": {
      "index.number_of_replicas": 0
    },
    "mappings": {
      "properties": {
        "@timestamp": {
          "type": "date"
        }
      }
    }
  }
}

PUT /_index_template/final-template            
{
  "index_patterns": ["my-index-*"],
  "composed_of": ["ct1", "ct2"],
  "priority": 5
}

POST /_index_template/_simulate/final-template 
```

&emsp;&emsp;第1行，创建一个组件模版（`ct1`），定义主分片数量为2。
&emsp;&emsp;第10行，创建第二个组件模版（`ct2`）定义0个副本分片以及一个mapping
&emsp;&emsp;第26行，创建一个索引模板（final-template）并且使用了两个组件模版
&emsp;&emsp;第33行，展示了由`final-template`应用后的配置

&emsp;&emsp;响应中展示了由`final-template`应用的settings、mappings以及aliases：

```text
{
  "template" : {
    "settings" : {
      "index" : {
        "number_of_shards" : "2",  
        "number_of_replicas" : "0", 
        "routing" : {
          "allocation" : {
            "include" : {
              "_tier_preference" : "data_content"
            }
          }
        }
      }
    },
    "mappings" : {                 
      "properties" : {
        "@timestamp" : {
          "type" : "date"
        }
      }
    },
    "aliases" : { }
  },
  "overlapping" : [ ]
}
```

&emsp;&emsp;第5行，来自`ct1`中主分片的数量
&emsp;&emsp;第6行，来自`ct2`中副本分片的数量
&emsp;&emsp;第16行，来自`ct1`中的mappings

###### Simulating an arbitrary template configuration

&emsp;&emsp;若要将模板添加到集群之前，先看下它里面的设置，你可以在请求中测试模板配置。如果存在更高等级的模版，那么则使用现有的模板而不是使用请求体中指定的。

```text
POST /_index_template/_simulate
{
  "index_patterns": ["my-index-*"],
  "composed_of": ["ct2"],
  "priority": 10,
  "template": {
    "settings": {
      "index.number_of_replicas": 1
    }
  }
}
```

&emsp;&emsp;会在响应中的`overlapping`中有相同的`index_patterns`但是优先级低的模版。

```text
{
  "template" : {
    "settings" : {
      "index" : {
        "number_of_replicas" : "1",
        "routing" : {
          "allocation" : {
            "include" : {
              "_tier_preference" : "data_content"
            }
          }
        }
      }
    },
    "mappings" : {
      "properties" : {
        "@timestamp" : {
          "type" : "date"
        }
      }
    },
    "aliases" : { }
  },
  "overlapping" : [
    {
      "name" : "final-template",
      "index_patterns" : [
        "my-index-*"
      ]
    }
  ]
}
```

#### Split index API
(8.2) [link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-split-index.html)

&emsp;&emsp;将现有索引切分为拥有更多主分片的新索引。

```text
POST /my-index-000001/_split/split-my-index-000001
{
  "settings": {
    "index.number_of_shards": 2
  }
}
```

##### Request

```text
POST /<index>/_split/<target-index>
PUT /<index>/_split/<target-index>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，对于这个索引，你必须要有管理权限[manage index privilege](#Security privileges)。

&emsp;&emsp;在开始切分这个索引前需要满足下面的条件

- 索引必须是只读
- [cluster health](#Cluster health API)的状态必须是绿色的

&emsp;&emsp;你可以通过下面的方式将索引设置为只读:

```text
PUT /my_source_index/_settings
{
  "settings": {
    "index.blocks.write": true 
  }
}
```

&emsp;&emsp;不允许写入的索引跟deleting index一样仍然允许进行元数据metadata的更改。

&emsp;&emsp;当前处于数据流[data streams](#Data streams)的write index不能被切分，如果要切分当前的write index，那么数据流必须先要执行[roll over](#Data streams)，使得一个新的write index被创建并且之前的write index就可以被切分了。

##### Description

&emsp;&emsp;索引切分API允许你将现有的索引切分到新的索引中，原本的一个主分片会被切分到两个或者多个主分片中。

&emsp;&emsp;索引被切分的次数取决于`index.number_of_routing_shards`，该值用于在内部将文档使用一致性哈希分布到分片上。例如，一个拥有5个分片，并且`index.number_of_routing_shards`的值设置为30时，那么每个分片可以按照因子2或者3进行划分，换句话说，可以按照下面的方式进行划分：

- 5 -> 10 -> 30 （5个分片中的每个分片先分为2份，然后10个分片的每个分片再分为3份）
- 5 -> 15 -> 30（5个分片中的每个分片先分为3份，然后15个分片中的每个分片再分为2份）
- 5 -> 30（5个分片中的每个分片分为6份）

&emsp;&emsp;`index.number_of_routing_shards`是一个[static](#Static)索引设置，只能在索引创建时期或者[关闭](#Open index API)时设置。

###### Index creation example

&emsp;&emsp;下面[create index API](#Create index API)中创建了名为`my-index-000001`的索引，并且设置`index.number_of_routing_shards`的值为30。

```text
PUT /my-index-000001
{
  "settings": {
    "index": {
      "number_of_routing_shards": 30
    }
  }
}
```

&emsp;&emsp;`index.number_of_routing_shards`的默认值取决于原先的索引的主分片数量。默认情况下允许按照因子2进行切分直到最大切分数量为1024。然而原先的分片数量会被考虑进来。比如5个主分片的索引可以被切分为10, 20, 40, 80, 160, 320, 或者最大值640 (一次或者多次切分)。

&emsp;&emsp;如果原先的索引的主分片数量是1（或者多个分片的索引被收缩[shrunk](#Shrink index API)到一个主分片），那么它会被切分为比1大的一个任意数值。默认的 routing shards的数量会被应用（apply）到新的索引中。

##### How splitting works

&emsp;&emsp;切分操作：

1. 创建一个跟源索引（source index）定义相同的目标索引（target index），但是目标索引拥有更多主分片数量。
2. 建立源索引到目标索引的硬链接（hard-linking）（如果操作系统不支持硬链接，那么所有的段会被复制到新的索引中，这是一个开销较大的过程）。
3. 对所有的文档进行hash操作，在low level files都创建后，删除那些属于不同的分片的文档
4. 恢复目标索引，就像是一个关闭的索引刚刚被重新打开那样

##### Why doesn’t Elasticsearch support incremental resharding?

&emsp;&emsp;从N个分片到N+1分片，又名增量分片，确实是许多键值存储支持的功能。 添加新分片并仅向该新分片中推送新数据不是一种选择：这可能是一个索引瓶颈，鉴于文档的\_id属于哪个分片，这是获取、删除和更新请求所必需的，这将变得相当复杂。 这意味着我们需要使用不同的散列方案重新平衡现有数据。

&emsp;&emsp;键值存储最常用的高效方法是使用一致的散列。 当分片数量从N增加到N+1时，一致的哈希只需要迁移1/N的键。 然而，Elasticsearch的存储单位是Lucene的索引。 由于其面向搜索的数据结构，占用Lucene索引的很大一部分，无论是只有5%的文档，删除它们并在另一个碎片上索引通常比键值存储的成本高得多。 如上一节所述，当通过乘法因子增加分片数量时，这一成本是合理的：这允许Elasticsearch在本地执行拆分，这种允许在索引级别执行拆分，而不是对需要移动的文档进行重新索引，并使用硬链接进行高效的文件复制。

&emsp;&emsp;对于仅追加数据，可以通过创建新索引并将新数据推送到其中，同时添加涵盖读取操作的旧索引和新索引的别名，从而获得更大的灵活性。 假设新旧索引中分别有M和N个分片，与搜索具有M+N分片的索引相比，这没有开销。

##### Split an index

&emsp;&emsp;执行下面的请求将名为`my_source_index`的索引切分到名为`my_target_index`

```text
POST /my_source_index/_split/my_target_index
{
  "settings": {
    "index.number_of_shards": 2
  }
}
```

&emsp;&emsp;上述的请求在索引`my_target_index`被添加到集群后就会返回，它不需要等待开始切分操作。

>IMPORTANT：
>索引在满足下面的要求后才能被切分
>
>1. 目标索引必须不存在
>2. 源索引必须比目标索引的主分片数量少
>3. 目标索引的主分片数量必须是源索引的倍数
>4. 处理切分的节点必须有足够的磁盘空间，要能容纳现有索引以及一份拷贝的大小

&emsp;&emsp;对于目标索引，API`_split`跟[create index api](#Create index API)一样接收参数`settings`和`aliases`

```text
POST /my_source_index/_split/my_target_index
{
  "settings": {
    "index.number_of_shards": 5 
  },
  "aliases": {
    "my_search_indices": {}
  }
}
```

&emsp;&emsp;第4行中，目标索引的主分片数量一定要大于源索引的主分片数量。

> NOTE：Mappings may not be specified in the`_split` request.

##### Monitor the split process

&emsp;&emsp;切分过程可以通过[\_cat recovery API](#cat recovery API)进行监控，也可以通过调用[cluster health API](#Cluster health API) ，等待所有主分片被分配完成后，参数`wait_for_status`被设置为`yellow`。

&emsp;&emsp;`_split`接口在目标索引添加到集群后，所有分片被分配前就会马上返回。在这个时间点，所有的分片处于`unassigned`状态，如果目标索引因为任何原因导致无法被分配，主分片会一直`unassigned`，直到在这个节点上能被分配。

&emsp;&emsp;一旦主分片被分配好了，它的状态就会变为`initializing`，然后切分操作就会开始。当切分操作结束后，主分片的状态就会变为`active`，这个时候，Elasticsearch就会尝试对主分片进行副本分片的分配，并将主分片迁移到其他节点。

##### Wait for active shards

&emsp;&emsp;因为切分操作会创建一个新的索引用于对分片的切分，索引创建中的[wait for active shards](#Create index API)这个设置同样应用于索引的切分。

##### Path parameters

- `<index>`：（Required, string）待切分的源索引名称。

- `<target-index>：（Required, string）目标索引的名称。
  - 索引名称必须满足下面的规范：
    - 只允许小写
    - 不能包含\, /, \*, ?, ", <, >, |, \` \` (space character), `,` , \#
    - 在7.0之前允许包含`:`，7.0之后不被支持
    - 不能以`-`, `_`, `+`开头
    - 不能有 `.` 或者`..`
    - 不能超过255个字节，有些字符用多个字节表示，所以更容易超过255个字节的限制
    - 以`.`开头的索引名被弃用了，除了 [hidden indices](#Index Modules) 以及被插件使用的内部的索引名

##### Query parameters

- wait_for_active_shards：（Optional, string） 开始切分前处于active状态的主分片数量。设置成`all`或者一个正整数，默认值1. 见[Active shards](#Index API)。
- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Request body

- aliases：（Optional, object of objects）该参数用于生成的索引。
  - `<alias>`：（Required, object） 这个字段用来描述索引别名，索引别名支持 [date math](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/date-math-index-names.html)。

    - filter: （Optional, [time units](#API conventions)） 用来限制文档访问的DSL语句。
    - index_routing（: （Optional, string） 用于索引阶段到指定的分片进行写入索引，这个值会覆盖用于写入索引操作的参数`routing`
    - is_hidden: （Optional, Boolean） 如果为true，那么别名是 [hidden](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-split-index.html#split-index-api-path-params)，默认为false，所有这个别名的索引都要有相同的`is_hidden`值。
    - is_write_index: （Optional, Boolean） 如果为true，这个索引是这个别名中的[write index](#Aliases)，默认为false。
    - routing: （Optional, string） 用来索引阶段或查询阶段路由到指定分片
    - search_routing: （Optional, string） 用于查询阶段到指定的分片进行查询,这个值会覆盖用于查询操作的参数`routing`

- settings：（Optional, [index setting object](#Index Modules)）为目标索引的一些配置信息，见 [Index Settings](#Index Modules)。

#### Index template exists API（legacy）
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-template-exists-v1.html)

>IMPORTANT：这篇文档介绍legacy index templates。在Elasticsearch7.8中legacy index templates已被弃用并且使用可组合的模板（composable  template）代替。更多关于composable templates的信息见[Index templates](#Index templates)。

&emsp;&emsp;查看某个[legacy](#Create or update index template API（legacy）)索引模板是否存在：

```text
HEAD /_template/template_1
```

##### Request

```text
HEAD /_template/<index-template>
```

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage_index_templates`或者`manage` [cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;使用这个接口判断一个或多个索引模板是否存在。

&emsp;&emsp;索引模板定义了 [settings](#Index Settings)、[mappings](#Mapping)、[aliases](#Aliases)，可以自动应用到新创建的索引上。

##### Path parameters

- `<index-template>`：（Required, string）用逗号隔开的索引模板的名称来限制请求。支持使用通配符（`*`）表达式

##### Query parameters

- flat_settings：（Optional, Boolean）如果为`true`，以铺开的格式返回。默认值为`false`。
- local：（Optional, Boolean）如果为`true`，则只从local node获取信息。默认是`false`，意味着从master node获取信息
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。

##### Response codes

- 200：说明指定的索引模板都存在
- 404：说明有一个或多个索引模版不存在


#### List dangling indices API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/dangling-indices-list.html#dangling-indices-list-api-example)

&emsp;&emsp;该API用于列出dangling index。

##### Request

```text
GET /_dangling
```

##### Prerequisites

- 如果开启了Elasticsearch security feature，你必须有`manage`的[cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;如果Elasticsearch遇到的索引数据不在当前集群状态中，那么这些索引会被认为是dangling Index。例如，当Elasticsearch节点下线时你删除了超过`cluster.indices.tombstones.size`数量的索引就有可能发生。

&emsp;&emsp;可以使用这个API列出dangling Index，你也可以[import](#Import dangling index API)或者[delete](#Delete dangling index API)这些索引。

##### Examples

&emsp;&emsp;API返回下面的响应：

```text
{
  "dangling_indices": [
   {
    "index_name": "my-index-000001",
    "index_uuid": "zmM4e0JtBkeUjiHD-MihPQ",
    "creation_date_millis": 1589414451372,
    "node_ids": [
      "pL47UN3dAb2d5RCWP6lQ3e"
    ]
   }
  ]
}
```


#### Open index API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-open-close.html)

&emsp;&emsp;打开一个关闭的索引。对于data streams，这个接口打开所有关闭的backing indices。

```text
POST /my-index-000001/_open
```

##### Request

```text
POST /<target>/_open
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有data stream、Index或者别名上的`manage`的管理权限[ index privilege](#Security privileges)。

##### Description

&emsp;&emsp;你可以使用这个接口重新打开一个关闭的索引。如果请求目标是一个data  stream，则会打开流中所有的关闭的backing indices。

&emsp;&emsp;当打开或者关闭一个索引时，对于重启索引分片，master要负责反应索引新的状态。分片才能进行正常的恢复操作。集群会对打开/关闭的索引自动进行replicate，保证总是有足够的shard  copies。

&emsp;&emsp;你可以打开/关闭多个索引。如果请求中显示引用了缺失的索引则会抛出错误。当然可以通过`ignore_unavailable=true`来关闭这个行为。

&emsp;&emsp;你可以显示指定要打开/关闭的索引名称。若要使用`_all`、`*`或者其他通配符表达式打开/关闭索引，需要将`action.destructive_requires_name`修改为`false`。可以通过cluster update settings API更新这个参数。

&emsp;&emsp;关闭的索引会消耗大量的磁盘空间，这可能在受控环境中引发问题。可以通过cluster settings API，将 `cluster.indices.close.enable` 设置为 `false` 来禁用关闭索引的功能。默认值为 `true`。

###### Wait for active shards

&emsp;&emsp;由于打开/关闭索引都会分配分片，创建索引时使用的[wait_for_active_shards](#Create-index-API)也会应用到`_open`和`_close`的索引动作上。


##### Path parameters

- `<target>`：（Optional, string）用逗号隔开的，或者是通配符表达式的索引名称列表用来限制请求
  - 默认你必须显示指定索引名称来限制请求。若要使用`_all`或者`*`或者通配符表达式，你必须通过[cluster update settings](#Cluster update settings API)API或者`elasticsearch.yml`将`action.destructive_requires_name`修改为`false`

##### Query parameters

- allow_no_indices：（Optional, Boolean）如果为`false`，当通配符表达式、[index alias](#Aliases)或者`all`匹配缺失索引或者已关闭的索引则返回一个错误。即使请求找到了打开的索引也可能会返回错误。比如，请求中指定了`foo*, bar*`，但如果找到以`foo`开头的索引，但是没找到以`bar`开头的索引则会返回一个错误。默认为`true`
- expand_wildcards：（Optional, string）决定在`<target>`参数中如果有通配符模式时将如何去匹配data streams和indices。支持使用逗号隔开的值，例如`open, hidden`。默认是`all`。合法值有：
  - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target syntax-1)
  - open：匹配打开的data streams和indices
  - closed：匹配关闭的data streams和indices
  - hidden：匹配隐藏的data streams和indices。必须和`open`、`closed`中的一个或全部组合使用
  - none：不展开通配符模式
  默认值为`open`
- ignore_unavailable：（Optional, Boolean）如果为`false`，请求中指定index如果缺失的话或者已关闭会返回一个错误。默认是`false`
- wait_for_active_shards：（Optional, string） 操作开始前已经启用的shard copy（主分片跟副本分片）的数量。设置成`all`或者一个正整数（不能超过索引的分片总数（`number_of_replicas + 1`）），默认值1，即主分片。见[Active shards](#Index API)。
- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Example

&emsp;&emsp;下面的例子重新打开了一个名为`my-index-000001`的索引。

```text
POST /my-index-000001/_open
```

&emsp;&emsp;这个接口返回以下结果：

```text
{
  "acknowledged" : true,
  "shards_acknowledged" : true
}
```

#### Resolve index API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-resolve-index-api.html)

&emsp;&emsp;解析指定的indices、aliases、data streams的Index pattern或者名称。支持多个pattern以及远端集群

```text
GET /_resolve/index/my-index-*
```

##### Request

```text
GET /_resolve/index/<name>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有这个data stream、index、alias的`view_index_metadata`或者`manage`的[index privilege](#Indices privileges)才能使用这个接口。

##### Path parameters

- `<name>`：（Required, string）用逗号隔开的indices、aliases、data streams的Index pattern或者名称。在[remote clusters](#Remote clusters)上的资源可以使用`<cluster>:<name>`语法来指定

##### Query parameters

- expand_wildcards：（Optional, string）通配符模式可以匹配的索引类型。如果请求目标是data stream，还会检测通配符表达式是否会匹配隐藏的data streams。支持多值，例如`open`, `hidden`。合法值有：
  - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target syntax-1)
  - open：匹配打开的，非隐藏的索引。同样匹配非隐藏的data stream
  - closed：匹配关闭的，非隐藏的索引。同样匹配非隐藏的data stream。Data stream不能关闭
  - hidden：匹配隐藏的data streams和indices。必须和`open`、`closed`中的一个或全部组合使用
  - none：不展开通配符模式
  默认值为`all`。

##### Example

```text
GET /_resolve/index/f*,remoteCluster1:bar*?expand_wildcards=all
```

&emsp;&emsp;这个接口返回以下响应：

```text
{
  "indices": [                                 
    {
      "name": "foo_closed",
      "attributes": [
        "closed"                               
      ]
    },
    {
      "name": "freeze-index",
      "aliases": [
        "f-alias"
      ],
      "attributes": [
        "open"
      ]
    },
    {
      "name": "remoteCluster1:bar-01",
      "attributes": [
        "open"
      ]
    }
  ],
  "aliases": [                                 
    {
      "name": "f-alias",
      "indices": [
        "freeze-index",
        "my-index-000001"
      ]
    }
  ],
  "data_streams": [                            
    {
      "name": "foo",
      "backing_indices": [
        ".ds-foo-2099.03.07-000001"
      ],
      "timestamp_field": "@timestamp"
    }
  ]
}
```

&emsp;&emsp;第2行，所有匹配了提供的名称或者表达式的索引
&emsp;&emsp;第6行，index attribute可以是`open, closed, hidden, system, frozen`
&emsp;&emsp;第25行，所有匹配了提供的名称或者表达式的别名
&emsp;&emsp;第34行，所有匹配了提供的名称或者表达式的data stream

#### Refresh API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-refresh.html)

#### Unfreeze index API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-update-settings.html)

> WARNING：**Deprecated in 7.14**
> 在8.0中，我们移除了冻结索引的能力。在之前的版本中，冻结索引能降低内存开销。然而由于[recent improvements in heap memory usage](https://www.elastic.co/cn/blog/significantly-decrease-your-elasticsearch-heap-memory-usage)使得冻结索引不再有用了。你可以使用这个API解冻那些在7.x中冻结的索引，并且这些索引没有关联frozen data tier。

&emsp;&emsp;解冻一个索引。

##### Request

```text
POST /<index>/_unfreeze
```
##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有index alias、Index或者别名上的`manage`的管理权限[ index privilege](#Security privileges)。

##### Path parameters

- `<index>`：（Required, string）索引的标识符

##### Example

&emsp;&emsp;下面的请求解冻了一个索引：

```text
POST /my-index-000001/_unfreeze
```

#### Update index settings API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-update-settings.html)

&emsp;&emsp;实时更新一个[dynamic index setting](#Index Settings)。

&emsp;&emsp;对于data stream，索引设置（Index setting）默认应用到所有的backing indices上。

```text
PUT /my-index-000001/_settings
{
  "index" : {
    "number_of_replicas" : 2
  }
}
```

##### Request

```text
PUT /<target>/_settings
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有data stream、Index或者别名上的`manage`的管理权限[ index privilege](#Security privileges)。

##### Path parameters

- `<target>`：（Optional, string）用逗号隔开的data stream或indices的名称来限制请求。支持通配符（`*`）。若要获取所有的data streams和indices，可以忽略这个参数或者使用`*`、`_all`

##### Query parameters

- allow_no_indices：（Optional, Boolean）如果为`false`，当通配符表达式、[index alias](#Aliases)或者`all`匹配缺失索引或者已关闭的索引则返回一个错误。即使请求找到了打开的索引也可能会返回错误。比如，请求中指定了`foo*, bar*`，但如果找到以`foo`开头的索引，但是没找到以`bar`开头的索引则会返回一个错误。默认为`false`
- expand_wildcards：（Optional, string）决定在`<target>`参数中如果有通配符模式时将如何去匹配data streams和indices。支持使用逗号隔开的值，例如`open, hidden`。默认是`all`。合法值有：
  - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target syntax-1)
  - open：匹配打开的data streams和indices
  - closed：匹配关闭的data streams和indices
  - hidden：匹配隐藏的data streams和indices。必须和`open`、`closed`中的一个或全部组合使用
  - none：不展开通配符模式
  默认值为`all`。
- flat_settings：（Optional, Boolean）如果为`true`，以铺开的格式返回。默认值为`false`。
- ignore_unavailable：（Optional, Boolean）如果为`false`，请求中指定的data stream或者index如果缺失的话会返回一个错误。默认是`false`
- preserve_existing：（Optional, Boolean）如果为`true`，现有的索引设置不会发生变更。默认为`false`
- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Request body

- settings：（Optional, [Index setting object](#Index Settings)）索引的配置选项。见[Index Settings](#Index Settings)。

##### Example

##### Reset an index setting

&emsp;&emsp;若要将设置还原为默认值，使用`null`，例如：

```text
PUT /my-index-000001/_settings
{
  "index" : {
    "refresh_interval" : null
  }
}
```

&emsp;&emsp;可以通过[Index modules](#Index Settings)查看所有live Index可以更新的动态设置。若不想要现有的设置被更改，可以将请求参数`preserve_existing`设置为`true`。

##### Bulk indexing usage

&emsp;&emsp;这个Update index settings API可以用来动态更改索引设置使得bulk indexing性能更高，然后将回到实时索引状态。在bulk indexing之前，使用：

```text
PUT /my-index-000001/_settings
{
  "index" : {
    "refresh_interval" : "-1"
  }
}
```

&emsp;&emsp;(另一个优化选项就是不开启副本分片，写入后再开启，不过这种方式取决于用户场景)

&emsp;&emsp;随后一旦bulk indexing完成，再更新设置（恢复到默认值）：

```text
PUT /my-index-000001/_settings
{
  "index" : {
    "refresh_interval" : "1s"
  }
}
```

&emsp;&emsp;并且应该调用一次force merge：

```text
POST /my-index-000001/_forcemerge?max_num_segments=5
```

###### Update index analysis

&emsp;&emsp;你只在在关闭的索引上定义新的analyzer。

&emsp;&emsp;若要添加一个analyzer，你必须关闭索引，定义好analyzer然后重新打开索引。

> NOTE：你不能关闭data stream的writer Index
> 若要更新data stream的write Index的analyzer以及以后的backing indices，可以通过[index template used by the stream](#Create an index template)更新analyzer。然后通过[roll over the data stream](#Manually roll over a data stream)将新的analyzer应用到流中的write Index以及以后的backing indices上。在rollover后，将会影响查询以及新添加到流中的数据。然而，它不会影响data stream中的backing indices或现有的数据。
> 
> 若要为现有的backing indices更改analyzer，你必须创建一个新的data stream然后reindex你的数据。见[Use reindex to change mappings or settings](#Use reindex to change mappings or settings)

&emsp;&emsp;例如，下面的命令将名为`content`的analyzer添加到`my-index-000001`索引中：

```text
POST /my-index-000001/_close

PUT /my-index-000001/_settings
{
  "analysis" : {
    "analyzer":{
      "content":{
        "type":"custom",
        "tokenizer":"whitespace"
      }
    }
  }
}

POST /my-index-000001/_open
```

#### Update mapping API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/indices-put-mapping.html)

&emsp;&emsp;添加新的字段到一个现有的data stream或index中。你可以使用这个接口修改现有字段（existing field）的查询设置（search setting）。

&emsp;&emsp;对于data stream，这些变更默认应用到backing indices上。

```text
PUT /my-index-000001/_mapping
{
  "properties": {
    "email": {
      "type": "keyword"
    }
  }
}
```

##### Request

```text
PUT /<target>/_mapping
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须在data stream、index、alias上有`manage` [Index privilege](#Indices privileges)。
- [7.9] Deprecated in 7.9.If the request targets an index or index alias, you can also update its mapping with the create, create_doc, index, or write index privilege.

##### Path parameters

- `<target>`：（Optional, string）用逗号隔开的data stream、indices和aliases的名称来限制请求。支持通配符（`*`）。若要获取所有的data streams、indices和aliases，可以忽略这个参数或者使用`*`、`_all`

##### Query parameters

- allow_no_indices：（Optional, Boolean）如果为`false`，当通配符表达式、[index alias](#Aliases)或者`all`匹配缺失索引或者已关闭的索引则返回一个错误。即使请求找到了打开的索引也可能会返回错误。比如，请求中指定了`foo*, bar*`，但如果找到以`foo`开头的索引，但是没找到以`bar`开头的索引则会返回一个错误。默认为`false`
- expand_wildcards：（Optional, string）决定在`<target>`参数中如果有通配符模式时将如何去匹配data streams和indices。支持使用逗号隔开的值，例如`open, hidden`。默认是`all`。合法值有：
  - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target syntax-1)
  - open：匹配打开的data streams和indices
  - closed：匹配关闭的data streams和indices
  - hidden：匹配隐藏的data streams和indices。必须和`open`、`closed`中的一个或全部组合使用
  - none：不展开通配符模式
  默认值为`all`。
- ignore_unavailable：（Optional, Boolean）如果为`false`，请求中指定的data stream或者index如果缺失的话会返回一个错误。默认是`false`
- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- write_index_only（Optional, Boolean）如果为`true`，mappings只会应用到当前的write index上。默认为`false`

##### Response body

- properties：（Required, [mappings object](#Mapping)）字段的mappings。对于新的字段，mappings中包括：
  - Field names
  - [Field data types](#Field data types)
  - [Mapping parameters](#Mapping parameters)
  &emsp;&emsp;对于现有的字段，见[Change the mapping of an existing field](#Change the mapping of an existing field)。

##### Example

###### Example with single target

&emsp;&emsp;该接口要求一个现有的data stream或index。下面的[create index](#Create index API)请求创建了名为`publications`的索引并且没有mapping。

```text
PUT /publications
```

&emsp;&emsp;下面的请求中向索引`publications`中新增了名为`title`，[text](#Text type family)类型的字段。

```text
PUT /publications/_mapping
{
  "properties": {
    "title":  { "type": "text"}
  }
}
```

###### Multiple targets

&emsp;&emsp;该接口可以通过单次请求应用到多个data streams或indices。例如，你可以同一时间向`my-index-000001`跟`my-index-000002`调用该接口：

```text
# Create the two indices
PUT /my-index-000001
PUT /my-index-000002

# Update both mappings
PUT /my-index-000001,my-index-000002/_mapping
{
  "properties": {
    "user": {
      "properties": {
        "name": {
          "type": "keyword"
        }
      }
    }
  }
}
```

###### Add new properties to an existing object field

&emsp;&emsp;你可以使用这个这个接口，将新的属性（properties）添加到现有的[object](#Object field type)字段中。若要观察它是如何工作，见下面的例子：

&emsp;&emsp;使用[create index](#Create index API)接口创建一个名为`name`的object类型的字段以及`name`下名为`first`的`text`类型的子字段。

```text
PUT /my-index-000001
{
  "mappings": {
    "properties": {
      "name": {
        "properties": {
          "first": {
            "type": "text"
          }
        }
      }
    }
  }
}
```

&emsp;&emsp;使用当前接口在`name`下添加一个新的名为`last`的`text`类型的子字段。

```text
PUT /my-index-000001/_mapping
{
  "properties": {
    "name": {
      "properties": {
        "last": {
          "type": "text"
        }
      }
    }
  }
}
```

###### Add multi-fields to an existing field

&emsp;&emsp;[multi-fields](#fields)可以让你对同一个字段定义不同的mappings 类型。你可以使用该接口更新mappings 参数`fields`，为现有的字段开启multi-fields。

&emsp;&emsp;若要观察它是如何工作，见下面的例子：

&emsp;&emsp;使用[create index](#Create index API)创建一个索引，索引中定义了名为`city`的`text`类型的字段

```text
PUT /my-index-000001
{
  "mappings": {
    "properties": {
      "city": {
        "type": "text"
      }
    }
  }
}
```

&emsp;&emsp;`text`类型的字段用于全文检索，`keyword`类型的字段不会被分词但更适合用于排序或聚合。

&emsp;&emsp;使用该接口为`city`字段开启multi-filed，下面的请求添加了名为`city.raw`，`keyword`类型的multi-field，他可以用于排序

```text
PUT /my-index-000001/_mapping
{
  "properties": {
    "city": {
      "type": "text",
      "fields": {
        "raw": {
          "type": "keyword"
        }
      }
    }
  }
}
```

###### Change supported mapping parameters for an existing field

&emsp;&emsp;[mapping parameter](#Mapping parameters)中的文档指明了是否可以使用当前接口更新这些mapping参数。比如你可以使用这个接口更新[ignore_above](#ignore_above)参数。

&emsp;&emsp;若要观察它是如何工作，见下面的例子：

&emsp;&emsp;使用[create index](#Create index API)接口创建一个索引，它包含名为`user_id`、`keyword`类型的字段。`user_id`字段有一个`ignore_above`参数，该值为`20`。

```text
PUT /my-index-000001
{
  "mappings": {
    "properties": {
      "user_id": {
        "type": "keyword",
        "ignore_above": 20
      }
    }
  }
}
```

&emsp;&emsp;使用该接口将`ignore_above`的值改为`100`。

```text
PUT /my-index-000001/_mapping
{
  "properties": {
    "user_id": {
      "type": "keyword",
      "ignore_above": 100
    }
  }
}
```

###### Change the mapping of an existing field

&emsp;&emsp;除了支持修改[mapping parameters](#Mapping parameters)，你不能更改现有字段的mapping类型。修改现有字段的类型可能使已经索引的数据无效。

&emsp;&emsp;如果你需要修改data stream的backing indices中字段的类型，见[Change mappings and settings for a data stream](#Change mappings and settings for a data stream)。

&emsp;&emsp;如果你需要修改其他索引中的字段的类型，你只能创建一个新的索引，并且使用正确的mapping类型，让通过[reindex](#Reindex API)重新写入数据。

&emsp;&emsp;若要观察它是如何工作，见下面的例子。

&emsp;&emsp;使用[create index](#Create index API)接口创建一个新的索引，它包含名为`user_id`，`long`类型的字段。

```text
PUT /my-index-000001
{
  "mappings" : {
    "properties": {
      "user_id": {
        "type": "long"
      }
    }
  }
}
```

&emsp;&emsp;使用[index](#Index API)接口写入一些带有`user_id`字段数据的文档。

```text
POST /my-index-000001/_doc?refresh=wait_for
{
  "user_id" : 12345
}

POST /my-index-000001/_doc?refresh=wait_for
{
  "user_id" : 12346
}
```

&emsp;&emsp;若要将`user_id`字段修改为`keyword`类型，则使用create index API创建一个新的索引，并且定义正确mapping。

```text
PUT /my-new-index-000001
{
  "mappings" : {
    "properties": {
      "user_id": {
        "type": "keyword"
      }
    }
  }
}
```

&emsp;&emsp;使用[reindex](#Reindex API) API从旧索引中拷贝到新索引中。

```text
POST /_reindex
{
  "source": {
    "index": "my-index-000001"
  },
  "dest": {
    "index": "my-new-index-000001"
  }
}
```

###### Rename a field

&emsp;&emsp;重命名字段的名称会让旧名称的字段对应的数据失效。你可以添加一个[alias](#Alias field type)来作为替代方案。

&emsp;&emsp;例如，使用[create index](#Create index API)创建一个索引，它包含了名为`user_identifier`的字段。

```text
PUT /my-index-000001
{
  "mappings": {
    "properties": {
      "user_identifier": {
        "type": "keyword"
      }
    }
  }
}
```

&emsp;&emsp;使用该接口向现有的字段`user_identifier`中添加名为`user_id`的字段的别名（field alias）。

```text
PUT /my-index-000001/_mapping
{
  "properties": {
    "user_id": {
      "type": "alias",
      "path": "user_identifier"
    }
  }
}
```


### Ingest APIs
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ingest-apis.html)

&emsp;&emsp;使用ingest API来管理[ingest pipeline](#Ingest pipelines)和processor的相关的任务以及资源。

#### Ingest pipeline APIs

&emsp;&emsp;使用下面的接口来创建。管理以及测试ingest pipeline：

- [Create or update pipeline](#Create or update pipeline API) 创建或更新一个pipeline
- [Get pipeline](#Get pipeline API) 来获取pipeline的配置
- [Delete pipeline](#Delete pipeline API) 来删除一个pipeline
- [Simulate pipeline](#Simulate pipeline API) 来测试一个pipeline

#### Stat APIs

&emsp;&emsp;使用下面的接口获取ingest 处理的统计信息：

- [GeoIP stats](#GeoIP stats API)用来下载用于[geoip processor](#GeoIP processor)的GeoIP数据库的统计信息

#### Create or update pipeline API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/put-pipeline-api.html)

&emsp;&emsp;创建或者更新一个[ingest pipeline](#Ingest pipelines)。使用这个接口做出的变更能立即生效。

```text
PUT _ingest/pipeline/my-pipeline-id
{
  "description" : "My optional pipeline description",
  "processors" : [
    {
      "set" : {
        "description" : "My optional processor description",
        "field": "my-keyword-field",
        "value": "foo"
      }
    }
  ]
}
```

##### Request

```text
PUT /_ingest/pipeline/<pipeline>
```

##### Prerequisites

- 如果开启了Elasticsearch security feature，你必须有`manage_pipeline`、`manage_ingest_pipelines`、`manage`的[cluster privilege](#Cluster privileges)才能管理ingest pipeline。

##### Path parameters

- `<pipeline>`：（Required, string）待创建/更新的ingest pipeline的ID

##### Query parameters

- if_version：（Optional, integer）只有pipeline是这个版本号才会执行操作。如果指定了该值并且更新生成，pipeline的版本号会增加
- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Response body

- description：（Optional, string）ingest pipeline的描述
- on_failure：（Optional, array of [processor](#Ingest processor reference) objects）某个processor失败后立即运行的processors
  - 每一个processor支持一个processor-level的`on_failure`值。如果某个processor没有`on_failure`值并且运行失败，Elasticsearch会使用pipeline-level作为一个fallback。这个参数中的processor会按照制定顺序有序运行。Elasticsearch不会尝试运行pipeline中剩余的processor。
- processors：（Required,array of [processor](#Ingest processor reference) objects）用来在写入索引前对文档执行转化的processor。按照指定的顺序有序执行
- version：（Optional,integer）外部系统用来追踪ingest pipeline的版本号
  - 见请求参数`if_version`查看如何使用该参数
- `_meta`：（Optional, object）可选的关于ingest pipeline的元数据。可以是任何内容。不会由Elasticsearch自动生成

##### Example

###### Pipeline metadata

&emsp;&emsp;你可以使用`_meta`参数添加任意的元数据到某个pipeline中。这是用户定义的对象，存储在集群状态中，因此尽量简短些。

&emsp;&emsp;`_meta`参数是可选的并且不会由Elasticsearch自动生成

&emsp;&emsp;若要移除`_meta`，更新pipeline并且不指定该值。

```text
PUT /_ingest/pipeline/my-pipeline-id
{
  "description" : "My optional pipeline description",
  "processors" : [
    {
      "set" : {
        "description" : "My optional processor description",
        "field": "my-keyword-field",
        "value": "foo"
      }
    }
  ],
  "_meta": {
    "reason": "set my-keyword-field to foo",
    "serialization": {
      "class": "MyPipeline",
      "id": 10
    }
  }
}
```
#### Get pipeline API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/get-pipeline-api.html)

&emsp;&emsp;返回一个或多个ingest pipeline的信息。则个API返回pipeline的本地引用。

```text
GET /_ingest/pipeline/my-pipeline-id
```
##### Request

```text
GET /_ingest/pipeline/<pipeline>
GET /_ingest/pipeline
```

##### Prerequisites

- 如果开启了Elasticsearch security feature，你必须有`read_pipeline`、`manage_pipeline`、`manage_ingest_pipelines`、`manage`的[cluster privilege](#Cluster privileges)才能管理ingest pipeline。

##### Path parameters

- `<pipeline>`：（Optional, string）用逗号隔开的pipeline的ID列表。支持使用通配符表达式
  - 若要获取所有的ingest pipeline，忽略这个参数或者使用`*`

##### Query parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Example

###### Get information for a specific ingest pipeline

```text
GET /_ingest/pipeline/my-pipeline-id
```

&emsp;&emsp;这个接口返回下面的响应：

```text
{
  "my-pipeline-id" : {
    "description" : "describe pipeline",
    "version" : 123,
    "processors" : [
      {
        "set" : {
          "field" : "foo",
          "value" : "bar"
        }
      }
    ]
  }
}
```

#### Delete pipeline API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/delete-pipeline-api.html)

&emsp;&emsp;删除一个或多个ingest pipeline。

```text
DELETE /_ingest/pipeline/my-pipeline-id
```

##### Request

```text
DELETE /_ingest/pipeline/<pipeline>
```

##### Prerequisites

- 如果开启了Elasticsearch security feature，你必须有`manage_pipeline`、`manage_ingest_pipelines`、`manage`的[cluster privilege](#Cluster privileges)才能管理ingest pipeline。

##### Path parameters

- `<pipeline>`：（Optional, string）用逗号隔开的pipeline的ID列表来限制请求。
  - 若要删除集群中所有的ingest pipeline，使用`*`

##### Query parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Example

###### Delete a specific ingest pipeline

```text
DELETE /_ingest/pipeline/pipeline-one
```

###### Delete ingest pipelines using a wildcard expression

```text
DELETE /_ingest/pipeline/pipeline-*
```

###### Delete all ingest pipelines

```text
DELETE /_ingest/pipeline/*
```

#### GeoIP stats API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/geoip-stats-api.html)

&emsp;&emsp;获取用于[geoip processor](#GeoIP processor)的 GeoIP2 数据库的下载统计信息。

```text
GET _ingest/geoip/stats
```

##### Request

```text
GET _ingest/geoip/stats
```

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`monitor`、`manage` [cluster privilege](#Cluster privileges)来使用这个API>
- 如果[ingest.geoip.downloader.enabled](#GeoIP processor)禁用了，该接口返回zero values以及空的`nodes`对象

##### Response body

- stats：（object）下载的GeoIP数据库的统计信息
  - successful_downloads：（integer）成功下载数据库的总次数
  - failed_downloads：（integer）下载数据库失败的总次数
  - total_download_time：（integer）下载数据库所花费的总毫秒数
  - database_count：（integer）当前可用于使用的数据库数量
  - skipped_updates：（integer）跳过的数据库更新的总次数
- nodes：（object）下载的每一个节点的GeoIP2数据库信息
  - `<node_id>`：（object）节点上下载的数据库。key就是节点ID
    - databases：（array of object）
      - name：（string）数据库的名称
    - files_in_temp：（array of strings）下载的数据库文件，包括相关的许可证文件。Elasticsearch将这些内容存储在节点的[temporary directory:](#Important Elasticsearch configuration)：`$ES_TMPDIR/geoip-databases/<node_id>`

#### Simulate pipeline API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/simulate-pipeline-api.html)

&emsp;&emsp;对提供的文档集合执行一个ingest pipeline。

```text
POST /_ingest/pipeline/my-pipeline-id/_simulate
{
  "docs": [
    {
      "_index": "index",
      "_id": "id",
      "_source": {
        "foo": "bar"
      }
    },
    {
      "_index": "index",
      "_id": "id",
      "_source": {
        "foo": "rab"
      }
    }
  ]
}

```

##### Request

```text
POST /_ingest/pipeline/<pipeline>/_simulate
GET /_ingest/pipeline/<pipeline>/_simulate
POST /_ingest/pipeline/_simulate
GET /_ingest/pipeline/_simulate
```

##### Prerequisites

- 如果开启了Elasticsearch security feature，你必须有`read_pipeline`、`manage_pipeline`、`manage_ingest_pipelines`、`manage`的[cluster privilege](#Cluster privileges)才能管理ingest pipeline。

##### Description

&emsp;&emsp;这个接口对请求体中提供的文档集合执行指定的pipeline。
&emsp;&emsp;你可以对提供的文档执行一个现有的pipeline或者在请求体中定义一个pipeline。

##### Path parameters

- `<pipeline>`：（Required\*, string）待测试的pipeline。如果你不在请求体中指定一个`pipeline`，那这个参数是必须要提供的

##### Query parameters

- verbose：（Optional, Boolean）如果为`true`，响应中包含pipeline执行的每一个processor的输出数据

##### Response body

- pipeline：（Required\*, object）待测试的pipeline。如果你不在请求体中指定一个`pipeline`，那这个参数是必须要提供的。如果你同时在请求参数跟请求体中指定了，那么接口只使用请求参数中的值
  - description：（Optional, string）ingest pipeline的描述
  - on_failure：（Optional, array of [processor](#Ingest processor reference) objects）某个processor失败后立即运行的processors
    - 每一个processor支持一个processor-level的`on_failure`值。如果某个processor没有`on_failure`值并且运行失败，Elasticsearch会使用pipeline-level作为一个fallback。这个参数中的processor会按照制定顺序有序运行。Elasticsearch不会尝试运行pipeline中剩余的processor。
  - processors：（Required,array of [processor](#Ingest processor reference) objects）用来在写入索引前对文档执行转化的processor。按照指定的顺序有序执行
  - version：（Optional,integer）外部系统用来追踪ingest pipeline的版本号
    - 见请求参数`if_version`查看如何使用该参数
  - `_meta`：（Optional, object）可选的关于ingest pipeline的元数据。可以是任何内容。不会由Elasticsearch自动生成
- docs：（Required, array of objects）样例文档用来在pipeline中测试
  - `_id`：（Optional, string）文档的唯一标示。这个ID在`_index`中必须是唯一的（相同的`_index`的文档，这个`_id`要唯一）
  - `_index`：（Optional, string）包含文档的索引名称
  - `_routing`：（Optional, string）用来将文档发送到指定主分片的值，见[\_routing](#\_routing field)
  - `_source`：（Required, object）文档JSON类型的内容

##### Example

###### Specify a pipeline as a path parameter

```text
POST /_ingest/pipeline/my-pipeline-id/_simulate
{
  "docs": [
    {
      "_index": "index",
      "_id": "id",
      "_source": {
        "foo": "bar"
      }
    },
    {
      "_index": "index",
      "_id": "id",
      "_source": {
        "foo": "rab"
      }
    }
  ]
}
```

&emsp;&emsp;该接口返回下面内容：

```text
{
   "docs": [
      {
         "doc": {
            "_id": "id",
            "_index": "index",
            "_source": {
               "field2": "_value",
               "foo": "bar"
            },
            "_ingest": {
               "timestamp": "2017-05-04T22:30:03.187Z"
            }
         }
      },
      {
         "doc": {
            "_id": "id",
            "_index": "index",
            "_source": {
               "field2": "_value",
               "foo": "rab"
            },
            "_ingest": {
               "timestamp": "2017-05-04T22:30:03.188Z"
            }
         }
      }
   ]
}
```

###### Specify a pipeline in the request body

```text
POST /_ingest/pipeline/_simulate
{
  "pipeline" :
  {
    "description": "_description",
    "processors": [
      {
        "set" : {
          "field" : "field2",
          "value" : "_value"
        }
      }
    ]
  },
  "docs": [
    {
      "_index": "index",
      "_id": "id",
      "_source": {
        "foo": "bar"
      }
    },
    {
      "_index": "index",
      "_id": "id",
      "_source": {
        "foo": "rab"
      }
    }
  ]
}
```

&emsp;&emsp;该接口返回下面内容：

```text
{
   "docs": [
      {
         "doc": {
            "_id": "id",
            "_index": "index",
            "_source": {
               "field2": "_value",
               "foo": "bar"
            },
            "_ingest": {
               "timestamp": "2017-05-04T22:30:03.187Z"
            }
         }
      },
      {
         "doc": {
            "_id": "id",
            "_index": "index",
            "_source": {
               "field2": "_value",
               "foo": "rab"
            },
            "_ingest": {
               "timestamp": "2017-05-04T22:30:03.188Z"
            }
         }
      }
   ]
}
```

###### View verbose results

&emsp;&emsp;你可以使用该接口查看pipeline中每一个processor是如何影响被提取（ingest）的文档的。若要查看在模拟请求中每一个processor的中间结果，你可以在请求中添加`verbose` 参数。

```text
POST /_ingest/pipeline/_simulate?verbose=true
{
  "pipeline" :
  {
    "description": "_description",
    "processors": [
      {
        "set" : {
          "field" : "field2",
          "value" : "_value2"
        }
      },
      {
        "set" : {
          "field" : "field3",
          "value" : "_value3"
        }
      }
    ]
  },
  "docs": [
    {
      "_index": "index",
      "_id": "id",
      "_source": {
        "foo": "bar"
      }
    },
    {
      "_index": "index",
      "_id": "id",
      "_source": {
        "foo": "rab"
      }
    }
  ]
}
```

&emsp;&emsp;该接口返回下面内容：

```text
{
  "docs" : [
    {
      "processor_results" : [
        {
          "processor_type" : "set",
          "status" : "success",
          "doc" : {
            "_index" : "index",
            "_id" : "id",
            "_source" : {
              "field2" : "_value2",
              "foo" : "bar"
            },
            "_ingest" : {
              "pipeline" : "_simulate_pipeline",
              "timestamp" : "2020-07-30T01:21:24.251836Z"
            }
          }
        },
        {
          "processor_type" : "set",
          "status" : "success",
          "doc" : {
            "_index" : "index",
            "_id" : "id",
            "_source" : {
              "field3" : "_value3",
              "field2" : "_value2",
              "foo" : "bar"
            },
            "_ingest" : {
              "pipeline" : "_simulate_pipeline",
              "timestamp" : "2020-07-30T01:21:24.251836Z"
            }
          }
        }
      ]
    },
    {
      "processor_results" : [
        {
          "processor_type" : "set",
          "status" : "success",
          "doc" : {
            "_index" : "index",
            "_id" : "id",
            "_source" : {
              "field2" : "_value2",
              "foo" : "rab"
            },
            "_ingest" : {
              "pipeline" : "_simulate_pipeline",
              "timestamp" : "2020-07-30T01:21:24.251863Z"
            }
          }
        },
        {
          "processor_type" : "set",
          "status" : "success",
          "doc" : {
            "_index" : "index",
            "_id" : "id",
            "_source" : {
              "field3" : "_value3",
              "field2" : "_value2",
              "foo" : "rab"
            },
            "_ingest" : {
              "pipeline" : "_simulate_pipeline",
              "timestamp" : "2020-07-30T01:21:24.251863Z"
            }
          }
        }
      ]
    }
  ]
}
```

### Index lifecycle management APIs
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/index-lifecycle-management-api.html)

&emsp;&emsp;你可以使用下面的API设置策略来自动的管理索引的生命周期。更多关于索引生命周期index lifecycle management (ILM)的信息见[ILM: Manage the index lifecycle](#ILM: Manage the index lifecycle)

##### Policy management APIs

- [Create lifecycle policy](#Create or update lifecycle policy API)
- [Get lifecycle policy](#Get lifecycle policy API)
- [Delete lifecycle policy](#Delete lifecycle policy API)

##### Index management APIs

- [Move index to step](#Move to lifecycle step API)
- [Retry policy on indices](#Retry policy execution API)
- [Remove policy from index](#Remove policy from index API)

##### Operation management APIs

- [Get ILM operation mode](#Get index lifecycle management status API)
- [Start ILM](#Start index lifecycle management API)
- [Stop ILM](#Stop index lifecycle management API)
- [Explain API](#Explain lifecycle API)
- [Migrate to data tiers routing API](#Migrate to data tiers routing APIl)

#### Create or update lifecycle policy API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-put-lifecycle.html)

&emsp;&emsp;创建或更新生命周期策略。见[Index lifecycle](#Index lifecycle)了解策略组成的定义

##### Request

```text
PUT _ilm/policy/<policy_id>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有`manage_ilm`[cluster privilege](#Cluster privileges)来使用这个API。你必须在所有的索引上有`manage` [Index privilege](#Indices privileges)才能通过`policy`管理。ILM（索引生命周期管理）作为最后更新策略的用户执行操作。ILM仅拥有最后一次策略更新时分配给该用户的[role](#Defining roles)。

##### Description

&emsp;&emsp;创建一个生命周期策略。如果指定的策略已经存在，那么就替换该策略并提高版本号。

> NOTE：只有最新版本的策略才会被存储，你不能倒退到之前的版本。


##### Path parameters

- `<policy_id>`：（Required, string）策略的标识符

##### Query parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Examples

&emsp;&emsp;下面的例子创建了一个名为`my_policy`的新策略。另外，你可以使用`_meta`参数添加任意的元数据到策略中，这个`_meta`参数是可选的并且不会自动生成，不会被Elasticsearch使用。若要取消`_meta`，可以替换策略而不需要再指定一个。若要检查`_meta`，你可以使用[Get lifecycle policy](#Get lifecycle policy API)。

```text
PUT _ilm/policy/my_policy
{
  "policy": {
    "_meta": {
      "description": "used for nginx log",
      "project": {
        "name": "myProject",
        "department": "myDepartment"
      }
    },
    "phases": {
      "warm": {
        "min_age": "10d",
        "actions": {
          "forcemerge": {
            "max_num_segments": 1
          }
        }
      },
      "delete": {
        "min_age": "30d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```


&emsp;&emsp;如果请求成功，你会收到如下结果：

```text
{
  "acknowledged": true
}
```

#### Get lifecycle policy API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-get-lifecycle.html)

&emsp;&emsp;获取一个生命周期策略。

##### Request

```text
GET _ilm/policy
GET _ilm/policy/<policy_id>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有`read_ilm`或`manage_ilm`[cluster privilege](#Cluster privileges)来使用这个API。更多信息叫[Security privileges](#Security privileges)

##### Description

&emsp;&emsp;返回指定策略的定义。包含了策略版本和最后一次改动的时间。如果没有指定策略，那么返回所有定义好的策略。

##### Path parameters

- `<policy_id>`：（Optional, string）策略的标识符

##### Query parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Examples

&emsp;&emsp;下面的例子获取了一个名为`my_policy`的生命周期策略：

```text
GET _ilm/policy/my_policy
```

&emsp;&emsp;如果请求成功，响应中的内容包含了策略定义：

```text
{
  "my_policy": {
    "version": 1, 
    "modified_date": 82392349, 
    "policy": {
      "phases": {
        "warm": {
          "min_age": "10d",
          "actions": {
            "forcemerge": {
              "max_num_segments": 1
            }
          }
        },
        "delete": {
          "min_age": "30d",
          "actions": {
            "delete": {
              "delete_searchable_snapshot": true
            }
          }
        }
      }
    },
    "in_use_by" : { 
      "indices" : [],
      "data_streams" : [],
      "composable_templates" : []
    }
  }
}
```

&emsp;&emsp;第3行，当策略更新后，策略版本号则会提高
&emsp;&emsp;第4行，上一次策略更改时间
&emsp;&emsp;第25行，哪些索引、data streams或者模版正使用这个策略

#### Delete lifecycle policy API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-delete-lifecycle.html)

&emsp;&emsp;删除一个索引生命周期策略。

##### Request

```text
DELETE _ilm/policy/<policy_id>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有`manage_ilm`[cluster privilege](#Cluster privileges)来使用这个API。更多信息叫[Security privileges](#Security privileges)

##### Description

&emsp;&emsp;删除指定的生命周期策略定义。你不能删除一个正在被使用的策略。如果策略正在被用于管理任意的索引，请求会失败并且返回一个错误。

##### Path parameters

- `<policy_id>`：（Required, string）策略的标识符

##### Query parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`

##### Examples

&emsp;&emsp;下面的例子删除了一个名为`my_policy`的策略：

```text
DELETE _ilm/policy/my_policy
```

&emsp;&emsp;当成功删除了一个策略，你会收到以下结果：

```text
{
  "acknowledged": true
}
```

#### Move to lifecycle step API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-move-to-step.html)

&emsp;&emsp;触发执行生命周期策略中的指定的一个步骤。

##### Request

```text
POST _ilm/move/<index>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有`manage_ilm`[cluster privilege](#Cluster privileges)来使用这个API。更多信息叫[Security privileges](#Security privileges)

##### Description

> WARNING：这个操作可能会导致数据丢失。手动将某个索引移动到一个指定步骤，即使该步骤已经执行过也会再次执行。 这是一项潜在的破坏性操作，应将此视为专家级别的API。

&emsp;&emsp;手动将某个索引移动到一个指定步骤并且执行该步骤。你必须在请求体中执行当前步骤和需要执行的步骤。

&emsp;&emsp;如果当前步骤不匹配索引当前正在执行的步骤，请求则会失败。这样的目的是防止索引从一个非期望的步骤移动到下一步。

&emsp;&emsp;当指定了索引被移动的目标阶段（`next_step`）后，`name`或者`action`跟`name`是可选的。如果只指定了阶段（共有Hot、Warm、Cold、Frozen、Delete这五个阶段，索引刚被管理时处于一个`new`阶段），索引会被移动到目标阶段的第一个动作的第一步。如果阶段跟动作都制定了，那么索引会被移动到指定阶段的指定动作的第一步。只指定动作是合法的，索引不能被移动不是策略的一部分的步骤。

##### Path parameters

- `<index>`：（Required, string）索引的标识符

##### Query parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Request body

- current_step：（Required, object）
  - phase：（Required, string）当前阶段的名称，必须匹配这个阶段，通过[explain](#Explain lifecycle API)获取
  - action：（Required, string）当前动作的名称，必须匹配这个动作，通过[explain](#Explain lifecycle API)获取
  - name：（Required, string）当前步骤的名称。必须匹配这个步骤，通过[explain](#Explain lifecycle API)获取。如果ILM执行动作时遇到了问题，则会暂定策略的执行，然后转到`ERROR`步骤。如果解决了问题并且尝试让策略继续下去，你必须将`ERROR`指定为当前步骤。更多信息见[ILM error handling](#Troubleshooting index lifecycle management errors)
- next_step（Required, boject）
  - phase：（Required, string）你想要执行/恢复的包含动作的阶段名称
  - action：（Required, string）你要执行/恢复的动作名称
  - name：（Required, string）你想要移动并且执行的步骤

##### Examples

&emsp;&emsp;下面的例子将名为`my-index-000001`的索引从初始化步骤移动到`forcemerge`步骤：

```text
POST _ilm/move/my-index-000001
{
  "current_step": { 
    "phase": "new",
    "action": "complete",
    "name": "complete"
  },
  "next_step": { 
    "phase": "warm",
    "action": "forcemerge", 
    "name": "forcemerge" 
  }
}
```

&emsp;&emsp;第3行，期望索引正处于的步骤
&emsp;&emsp;第8行，你想要执行的步骤
&emsp;&emsp;第10行，索引将被移动到某个动作（可选）
&emsp;&emsp;第11行，索引将被移到某一步骤（可选）
&emsp;&emsp;如果请求成功，你会收到如下结果：

```text
{
  "acknowledged": true
}
```

&emsp;&emsp;如果索引不在`current_step`中指定的`new`阶段，请求将失败。

#### Remove policy from index API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-remove-policy.html)

&emsp;&emsp;从某个Index或者data stream的backing indices中移除被分配的生命周期策略。

##### Request

```text
POST <target>/_ilm/remove
```
##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有`manage_ilm`[cluster privilege](#Cluster privileges)来使用这个API。更多信息叫[Security privileges](#Security privileges)

##### Description

&emsp;&emsp;对于indices，这个接口将移除indices被分配的生命周期策略并且停止管理策略中指定的索引

&emsp;&emsp;对于data streams，这个接口将移backing indices被分配的生命周期策略并且停止管理策略中指定的索引

##### Path parameters

- `<target>`：（Required, string）用逗号隔开的indices、data stream、aliases的目标名称。支持通配符（`*`）。若要全部的data stream和indices，使用`*`或者`_all`

##### Query parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Examples

&emsp;&emsp;下面的例子将移除名为`my-index-000001`的索引，它被分配的策略。

```text
POST my-index-000001/_ilm/remove
```

&emsp;&emsp;如果请求成功，你会收到以下结果：

```text
{
  "has_failures" : false,
  "failed_indexes" : []
}
```

#### Retry policy execution API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-retry-policy.html)

&emsp;&emsp;对正处于`ERROR`步骤的索引，重试它对应的策略。

##### Request

```text
POST <index>/_ilm/retry
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有`manage_ilm`[cluster privilege](#Cluster privileges)来使用这个API。更多信息叫[Security privileges](#Security privileges)

##### Description

&emsp;&emsp;将策略回到错误发生的地方然后继续步骤。使用[ILM Explain API ](#Explain lifecycle API)查看某个索引是否处于`ERROR`步骤。

##### Path parameters

- `<index>`：（Required, string）用逗号隔开的待重试的索引标识符

##### Query parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Examples

&emsp;&emsp;下面的例子对名为`my-index-000001`的索引进行重试：

```text
POST my-index-000001/_ilm/retry
```

&emsp;&emsp;如果请求成功，你会收到以下结果：

```text
{
  "acknowledged": true
}
```

#### Get index lifecycle management status API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-get-status.html)

&emsp;&emsp;获取当前索引生命周期index lifecycle management (ILM)的状态。

&emsp;&emsp;你可以通过[start ILM](#Start index lifecycle management API)和[stop ILM](#Stop index lifecycle management API)来分别启动和停止ILM

##### Request

```text
GET /_ilm/status
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有`read_ilm`或`manage_ilm`[cluster privilege](#Cluster privileges)来使用这个API。更多信息叫[Security privileges](#Security privileges)

##### Query parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Request body

- operation_mode：（string）当前ILM的操作模式
  - RUNNING：ILM运行中
  - STOPPING：ILM正在完成敏感操作（sensitive actions），比如说[shrink](#Shrink)，正在处理中。当这个动作完成后，ILM将会移动到`STOPPED`
  - STOPPED：ILM不在运行中

##### Examples

&emsp;&emsp;下面的例子获取ILM插件状态：

```text
GET _ilm/status
```

&emsp;&emsp;如果请求成功，响应中的内容会展示操作模式（operation mode）：

```text
{
  "operation_mode": "RUNNING"
}
```



#### Explain lifecycle API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-explain-lifecycle.html)

&emsp;&emsp;为一个或多个索引获取当前生命周期状态。对于data streams，这个接口为流中的backing indices获取当前生命周期状态。

##### Request

```text
GET <target>/_ilm/explain
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须在这些索引上有`view_index_metadata`或者`manage_ilm`[cluster privilege](#Cluster privileges)或者同时包含来使用这个API。更多信息叫[Security privileges](#Security privileges)

##### Description

&emsp;&emsp;获取索引当前的生命周期状态，比如当前执行中的阶段，动作和步骤。展示索引进入时间、运行中的阶段的定义、以及关于任何相关的错误信息。

##### Path parameters

- `<target>`：（Optional, string）用逗号隔开的一个或多个data stream、Index或者alias。支持通配符(\*)。若要查询所有的data stream、Index，则不指定这个参数或者使用`*`、`_all`
- 
##### Query parameters

- only_managed：（Optional, Boolean）过滤出只由ILM管理的索引
- only_errors：（Optional, Boolean）过滤出只由ILM管理的索引并且处于一个错误状态，要么由于在执行策略时遇到了一个错误，要么尝试使用了一个不存在的策略
- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Examples

&emsp;&emsp;下面的例子获取了名为`my-index-000001`的索引的生命周期状态：

```text
GET my-index-000001/_ilm/explain
```

&emsp;&emsp;当索引刚刚由ILM接管，`explain`中会显示索引处于管理中并且处于`new`阶段：

```text
{
  "indices": {
    "my-index-000001": {
      "index": "my-index-000001",
      "index_creation_date_millis": 1538475653281,  
      "time_since_index_creation": "15s",           
      "managed": true,                              
      "policy": "my_policy",                        
      "lifecycle_date_millis": 1538475653281,       
      "age": "15s",                                 
      "phase": "new",
      "phase_time_millis": 1538475653317,           
      "action": "complete"
      "action_time_millis": 1538475653317,          
      "step": "complete",
      "step_time_millis": 1538475653317             
    }
  }
}
```

&emsp;&emsp;第5行，当索引创建后，这个时间戳用来决定什么时候开始rollover
&emsp;&emsp;第6行，索引创建后到现在的时间（通过`max_mag`计算什么时候开始rollover）
&emsp;&emsp;第7行，显示索引是否被ILM管理。如果没有被管理，其他字段将不会显示
&emsp;&emsp;第8行，正用于管理这个索引的ILM的策略名称
&emsp;&emsp;第9行，用于`min_age`的时间戳
&emsp;&emsp;第10行，索引的年龄（用来计算什么时候进入下一阶段）
&emsp;&emsp;第12行，当前阶段是什么时候进入的
&emsp;&emsp;第14行，当前动作是什么时候进入的
&emsp;&emsp;第16行，当前步骤是什么时候进入的

&emsp;&emsp;一个某个策略在索引上运行，响应中会包含一个`phase_execution`对象来展示当前阶段的定义。对策略进行修改不会影响这个索引，直到当前阶段完成。

```text
{
  "indices": {
    "test-000069": {
      "index": "test-000069",
      "index_creation_date_millis": 1538475653281,
      "time_since_index_creation": "25.14s",
      "managed": true,
      "policy": "my_lifecycle3",
      "lifecycle_date_millis": 1538475653281,
      "lifecycle_date": "2018-10-15T13:45:21.981Z",
      "age": "25.14s",
      "phase": "hot",
      "phase_time_millis": 1538475653317,
      "phase_time": "2018-10-15T13:45:22.577Z",
      "action": "rollover",
      "action_time_millis": 1538475653317,
      "action_time": "2018-10-15T13:45:22.577Z",
      "step": "attempt-rollover",
      "step_time_millis": 1538475653317,
      "step_time": "2018-10-15T13:45:22.577Z",
      "phase_execution": {
        "policy": "my_lifecycle3",
        "phase_definition": { 
          "min_age": "0ms",
          "actions": {
            "rollover": {
              "max_age": "30s"
            }
          }
        },
        "version": 3, 
        "modified_date": "2018-10-15T13:21:41.576Z", 
        "modified_date_in_millis": 1539609701576 
      }
    }
  }
}
```

&emsp;&emsp;第23行，当索引进入到这个阶段，显示这个阶段的JSON格式的定义
&emsp;&emsp;第31行，加载的策略版本
&emsp;&emsp;第32行，上一次策略修改时间
&emsp;&emsp;第33行，上一层策略修改时间（epoch time）

&emsp;&emsp;如果ILM正在等待某个步骤的完成，响应中会包含在索引上正在执行中的步骤状态信息。

```text
{
  "indices": {
    "test-000020": {
      "index": "test-000020",
      "index_creation_date_millis": 1538475653281,
      "time_since_index_creation": "4.12m",
      "managed": true,
      "policy": "my_lifecycle3",
      "lifecycle_date_millis": 1538475653281,
      "lifecycle_date": "2018-10-15T13:45:21.981Z",
      "age": "4.12m",
      "phase": "warm",
      "phase_time_millis": 1538475653317,
      "phase_time": "2018-10-15T13:45:22.577Z",
      "action": "allocate",
      "action_time_millis": 1538475653317,
      "action_time": "2018-10-15T13:45:22.577Z",
      "step": "check-allocation",
      "step_time_millis": 1538475653317,
      "step_time": "2018-10-15T13:45:22.577Z",
      "step_info": { 
        "message": "Waiting for all shard copies to be active",
        "shards_left_to_allocate": -1,
        "all_shards_active": false,
        "number_of_replicas": 2
      },
      "phase_execution": {
        "policy": "my_lifecycle3",
        "phase_definition": {
          "min_age": "0ms",
          "actions": {
            "allocate": {
              "number_of_replicas": 2,
              "include": {
                "box_type": "warm"
              },
              "exclude": {},
              "require": {}
            },
            "forcemerge": {
              "max_num_segments": 1
            }
          }
        },
        "version": 2,
        "modified_date": "2018-10-15T13:20:02.489Z",
        "modified_date_in_millis": 1539609602489
      }
    }
  }
}
```

&emsp;&emsp;第21行，正在处理中的步骤状态

&emsp;&emsp;如果在执行策略中某一步骤时发生错误，则索引会移动到`ERROR`步骤，那么你必须采取行动使得能让其进入下一步骤。有些步骤在特定环境下会安全的自动重试。若要帮助你诊断问题，这个接口的响应中会显示失败的步骤，提供错误相关信息，以及尝试重试这个失败步骤的信息。

```text
{
  "indices": {
    "test-000056": {
      "index": "test-000056",
      "index_creation_date_millis": 1538475653281,
      "time_since_index_creation": "50.1d",
      "managed": true,
      "policy": "my_lifecycle3",
      "lifecycle_date_millis": 1538475653281,
      "lifecycle_date": "2018-10-15T13:45:21.981Z",
      "age": "50.1d",
      "phase": "hot",
      "phase_time_millis": 1538475653317,
      "phase_time": "2018-10-15T13:45:22.577Z",
      "action": "rollover",
      "action_time_millis": 1538475653317,
      "action_time": "2018-10-15T13:45:22.577Z",
      "step": "ERROR",
      "step_time_millis": 1538475653317,
      "step_time": "2018-10-15T13:45:22.577Z",
      "failed_step": "check-rollover-ready", 
      "is_auto_retryable_error": true, 
      "failed_step_retry_count": 1, 
      "step_info": { 
        "type": "cluster_block_exception",
        "reason": "index [test-000057/H7lF9n36Rzqa-KfKcnGQMg] blocked by: [FORBIDDEN/5/index read-only (api)",
        "index_uuid": "H7lF9n36Rzqa-KfKcnGQMg",
        "index": "test-000057"
      },
      "phase_execution": {
        "policy": "my_lifecycle3",
        "phase_definition": {
          "min_age": "0ms",
          "actions": {
            "rollover": {
              "max_age": "30s"
            }
          }
        },
        "version": 3,
        "modified_date": "2018-10-15T13:21:41.576Z",
        "modified_date_in_millis": 1539609701576
      }
    }
  }
}
```

&emsp;&emsp;第21行，发生错误的步骤
&emsp;&emsp;第22行，指示是否重试这个步骤可以解决这个错误，如果为`true`，ILM会自动重试这一步骤
&emsp;&emsp;第23行，尝试重试执行失败步骤的次数
&emsp;&emsp;第24行，出现的问题信息

#### Start index lifecycle management API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-start.html)

&emsp;&emsp;启动索引生命周期管理插件（index lifecycle management (ILM) plugin）。

##### Request

```text
POST /_ilm/start
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有`manage_ilm`[cluster privilege](#Cluster privileges)来使用这个API。更多信息叫[Security privileges](#Security privileges)

##### Description

&emsp;&emsp;如果ILM插件停止了，则启动它。当形成集群后，ILM会自动启动ILM。只有在使用[Stop ILM API](#Stop index lifecycle management API)停止ILM插件后才有必要重新启动它。

##### Path parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Examples

&emsp;&emsp;下面的例子启动了ILM插件。

```text
POST _ilm/start
```

&emsp;&emsp;如果请求成功，你会收到以下结果：

```text
{
  "acknowledged": true
}
```

#### Stop index lifecycle management API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-stop.html)

&emsp;&emsp;停用生命周期管理插件（index lifecycle management (ILM) plugin）。

##### Request

```text
POST /_ilm/stop
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有`manage_ilm`[cluster privilege](#Cluster privileges)来使用这个API。更多信息叫[Security privileges](#Security privileges)

##### Description

&emsp;&emsp;暂停所有的生命周期管理操作并且停止ILM插件。当你想要对集群进行维护并且需要防止ILM对你的索引执行任何操作时是非常有用的。

&emsp;&emsp;这个接口在被确认后会马上返回，但是插件可能还会继续运行直到处理中的操作完成并且插件能被正确的停止。使用[Get ILM Status API](#Get index lifecycle management status API)查看ILM是否运行中。

##### Path parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Examples

&emsp;&emsp;下面的例子停用了ILM插件。

```text
POST _ilm/stop
```

&emsp;&emsp;如果请求没有遇到错误，你会收到如下结果：

```text
{
  "acknowledged": true
}
```

#### Migrate to data tiers routing API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-migrate-to-data-tiers.html)


### Machine learning APIs
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ml-apis.html)

#### Open anomaly detection jobs API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ml-open-job.html)

#### Start datafeeds API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ml-start-datafeed.html)

#### Set upgrade mode API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ml-set-upgrade-mode.html)

### Node lifecycle APIs
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/node-lifecycle-api.html)

### Rollup APIs
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rollup-apis.html)


#### Create rollup jobs API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rollup-put-job.html)

#### Delete rollup jobs API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rollup-delete-job.html)

#### Get rollup jobs API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rollup-get-job.html)

#### Get rollup job capabilities API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rollup-get-rollup-caps.html)

#### Get rollup index capabilities API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rollup-get-rollup-index-caps.html)

#### Rollup search
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rollup-search.html)

#### Start rollup jobs API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rollup-start-job.html)

#### Stop rollup jobs API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/rollup-stop-job.html)

### Script APIs
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/script-apis.html)

&emsp;&emsp;使用下面的API来管理、存储以及测试你的[script](#Scripting)。

##### Script support APIs

&emsp;&emsp;使用script support APIs获取支持的脚本内容和脚本语言。

- [Get script contexts](#Get script contexts API)
- [Get script languages](#Get script languages API)

##### Stored script APIs

&emsp;&emsp;使用stored script APIs来管理[stored scripts](#Store and retrieve scripts)和[search templates.](#Search templates)。

- [Create or update stored script](#Create or update stored script API)
- [Get stored script](#Get stored script API)
- [Delete stored script](#Delete stored script API)

##### Painless APIs

&emsp;&emsp;在生成中使用之前，先使用[Painless execute API ](https://www.elastic.co/guide/en/elasticsearch/painless/8.2/painless-execute-api.html)安全的测试Painless脚本。

#### Create or update stored script API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/create-stored-script-api.html#create-stored-script-api)

&emsp;&emsp;创建或更新一个[stored script](#Store and retrieve scripts)或者[search template](#Search templates)。

```text
PUT _scripts/my-stored-script
{
  "script": {
    "lang": "painless",
    "source": "Math.log(_score * 2) + params['my_modifier']"
  }
}
```

##### Request

```text
PUT _scripts/<script-id>
POST _scripts/<script-id>
PUT _scripts/<script-id>/<context>
POST _scripts/<script-id>/<context>
```

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage` [cluster privilege](#Cluster privileges)来使用这个API

##### Path parameters

- `<script-id>`：（Required, string）stored script 或 search template的标识符。在集群中必须是唯一的。
- `<context>`：（Optional, string）脚本或search template中待运行的内容。为了防止出错，这个接口会马上编译脚本内容或模板

##### Query parameters
- context：（Optional, string）脚本或search template中待运行的内容。为了防止出错，这个接口会马上编译脚本内容或模板
  - 如果你同时在请求参数中指定了`<context>`，该接口只使用请求参数中的参数
- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Request body

- script：（Required,object）包含脚本或search template，参数以及脚本语言
  - lang：（Required, string）[Script language](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-scripting.html#scripting-available-languages)。对于Search template，使用的是`mustache`
  - params：（Optional,object）键值对用来替换模板中的Mustache变量。key是变量的名称。值是变量的值
  - source：（Required\*,object）对于脚本来说就是包含脚本的字符串
    - 对于search template来说，是包含了search template的对象。支持跟[search API](#Search API)相同的参数。同样支持[Mustache](https://mustache.github.io/)变量

#### Delete stored script API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/delete-stored-script-api.html)

&emsp;&emsp;删除一个[stored Script](#Store and retrieve scripts)或[search template](#Search templates)。

##### Request

```text
DELETE _scripts/<script-id>
```
##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage` [cluster privilege](#Cluster privileges)来使用这个API

##### Path parameters

- `<script-id>`：（Required, string）stored script或search template的标识符

##### Query parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

#### Get script contexts API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/get-script-contexts-api.html)

&emsp;&emsp;获取支持的脚本内容和它们的方法。

```text
GET _script_context
```

##### Request

```text
GET _script_context
```

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage` [cluster privilege](#Cluster privileges)来使用这个API

#### Get script languages API
（8,2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/get-script-languages-api.html)

&emsp;&emsp;获取支持的[script languages](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-scripting.html#scripting-available-languages)和它们的上下文。

##### Request

```text
GET _script_language
```

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage` [cluster privilege](#Cluster privileges)来使用这个API


#### Get stored script API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/get-stored-script-api.html)

&emsp;&emsp;获取一个[stored Script](#Store and retrieve scripts)或[search template](#Search templates)。

```text
GET _scripts/my-stored-script
```

##### Request

```text
GET _scripts/<script-id>
```

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage` [cluster privilege](#Cluster privileges)来使用这个API

##### Path parameters

- `<script-id>`：（Required, string）stored script或search template的标识符

##### Query parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

### Search APIs
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search.html)

##### Search templates（Search APIs）

- [Search template](#Search template API)
- [Multi search template](#Multi search template API)
- [Render search template](#Render search template API)

#### Search API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-search.html)

##### Query parameters
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-search.html#request-body-search-query)


###### allow_partial_search_results

###### ccs_minimize_roundtrips

###### preference

###### query

###### search_type

#### Async search
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/async-search.html#submit-async-search)

&emsp;&emsp;该接口可以让你执行异步的查询请求，监视查询的过程，并且当可用时获取部分结果。

##### Submit async search API

&emsp;&emsp;异步执行一个查询请求。它跟[search API](#Search API)有相同的请求参数跟请求内容。

```text
POST /sales*/_async_search?size=0
{
  "sort": [
    { "date": { "order": "asc" } }
  ],
  "aggs": {
    "sale_date": {
      "date_histogram": {
        "field": "date",
        "calendar_interval": "1d"
      }
    }
  }
}
```

&emsp;&emsp;响应中包含了正在执行查询的标识符。你可以使用这个ID随后获取查询的最终结果。当前可见的查询结果作为[response](#Search API)对象中的一部分返回。

```text
{
  "id" : "FmRldE8zREVEUzA2ZVpUeGs2ejJFUFEaMkZ5QTVrSTZSaVN3WlNFVmtlWHJsdzoxMDc=", 
  "is_partial" : true, 
  "is_running" : true, 
  "start_time_in_millis" : 1583945890986,
  "expiration_time_in_millis" : 1584377890986,
  "response" : {
    "took" : 1122,
    "timed_out" : false,
    "num_reduce_phases" : 0,
    "_shards" : {
      "total" : 562, 
      "successful" : 3, 
      "skipped" : 0,
      "failed" : 0
    },
    "hits" : {
      "total" : {
        "value" : 157483, 
        "relation" : "gte"
      },
      "max_score" : null,
      "hits" : [ ]
    }
  }
}
```

&emsp;&emsp;第2行，异步搜索的标识符，用来监视查询进程，获取查询结果或者删除这次查询
&emsp;&emsp;第3行，当这个Query不处于运行中，会表明查询是否失败或者在所有分片上都成功完成。当Query处于执行中，`is_partial`总是为`true`
&emsp;&emsp;第4行，查询是否处于执行中还是已经完成
&emsp;&emsp;第12行，将在多少个分片上进行查询
&emsp;&emsp;第13行，在多少个分片成功完成查询
&emsp;&emsp;第19行，当前多少文档匹配了Query，这些文档属于已经成功完成查询的分片

> NOTE：尽管查询不处于运行中，因此 `is_running` 被设置为 `false`，但结果可能是部分的。这种情况发生在一些分片返回结果后搜索失败，或者协调异步搜索的节点停止运行的情况下。

&emsp;&emsp;可以通过设置 `wait_for_completion_timeout` 参数来实现阻塞并等待，直至搜索操作在指定的超时时间内完成，默认这个超时时间是1秒。如果在超时时间内完成异步查询，响应中不会包含Id，因为结果不会存储在集群中。`keep_on_completion`参数，默认为`false`，当查询在`wait_for_completion_timeout`完成时，可以设置为`true`以后的查询存储查询结果。

&emsp;&emsp;你也可以通过`keep_alive`参数指定异步查询应该保持多久，默认是`5d`（5天）。以后的异步查询和其他保存的查询结果将在这个周期后删除。

> NOTE：当你对搜索结果进行排序，而这些结果的排序依据（即主要排序字段）是一个已经被索引的字段时，Elasticsearch可以根据每个分片持有该字段的最小值和最大值来对这些分片进行排序。因此，根据你请求的排序标准，部分结果可以变得可用，即使搜索还没有完全完成。

&emsp;&emsp;异步搜索支持跟Search API相同的[参数](#Search API)，但有些参数的默认值是不同的：

- `batched_reduce_size`的默认值是`5`：该值会影响部分结果可见的频率，当分片结果被归并（reduce）后才可见。当协调节点收到一定数量的不同的分片结果时就会执行归并（默认是`5`）
- `request_cache`默认为`true`
- pre_filter_shard_size 默认为1且不能更改：这是为了执行一个预过滤往返操作，以从每个分片检索统计信息，从而跳过那些肯定不包含与查询匹配的任何文档的分片。
- `ccs_minimize_roundtrips` 默认为`false`，这也是唯一支持的值。

> WARNING：异步搜索不支持[scroll](#Scroll search results)，也不支持包含[suggest section](#Suggesters)的查询请求。只有在[ccs_minimize_roundtrips](#Minimize network roundtrips)设置为`false`时才支持CCS查询

> WARNING：默认情况下Elasticsearch不允许存储超过10Mb的异步查询结果，尝试这么做会导致出现错误。可以通过修改集群设置search.max_async_search_response_size来修改存储异步查询结果允许的最大值

##### Get async search

&emsp;&emsp;该接口根据之前提交的异步查询返回的ID进行查询。如果开启了Elasticsearch security features。指定的异步查询若要访问结果需要受限于[the user or API key that submitted it](#Resource sharing check for users and API keys)。

```text
GET /_async_search/FmRldE8zREVEUzA2ZVpUeGs2ejJFUFEaMkZ5QTVrSTZSaVN3WlNFVmtlWHJsdzoxMDc=
```

```text
{
  "id" : "FmRldE8zREVEUzA2ZVpUeGs2ejJFUFEaMkZ5QTVrSTZSaVN3WlNFVmtlWHJsdzoxMDc=",
  "is_partial" : true, 
  "is_running" : true, 
  "start_time_in_millis" : 1583945890986,
  "expiration_time_in_millis" : 1584377890986, 
  "response" : {
    "took" : 12144,
    "timed_out" : false,
    "num_reduce_phases" : 46, 
    "_shards" : {
      "total" : 562,
      "successful" : 188, 
      "skipped" : 0,
      "failed" : 0
    },
    "hits" : {
      "total" : {
        "value" : 456433,
        "relation" : "eq"
      },
      "max_score" : null,
      "hits" : [ ]
    },
    "aggregations" : { 
      "sale_date" :  {
        "buckets" : []
      }
    }
  }
}
```

&emsp;&emsp;第2行，当这个Query不处于运行中，会表明查询是否失败或者在所有分片上都成功完成。当Query处于执行中，`is_partial`总是为`true`
&emsp;&emsp;第3行，查询是否处于执行中还是已经完成
&emsp;&emsp;第6行，异步查询的过期时间
&emsp;&emsp;第10行，表示执行归并的结果的数量。如果相比较上一次的查询结果，这个值变大了，那么你可以在查询响应中获取更多额外的结果
&emsp;&emsp;第13行，执行这个Query的分片数量。注意的是为了分片的结果能包含在查询响应中，它们首先会被归并（reduce）
&emsp;&emsp;第25行，部分聚合结果，来自那些已经完成Query的分片的结果

&emsp;&emsp;在调用 Get Async Search API 时，也可以提供 `wait_for_completion_timeout` 参数，以便等待搜索完成直到提供的超时时间。如果在超时到期之前可用，将返回最终结果，否则一旦超时到期，将返回当前可用的结果。默认情况下没有设置超时，这意味着将返回当前可用的结果，无需任何额外等待。

&emsp;&emsp;`keep_alive` 参数指定异步搜索在集群中应该保持可用的时间长短。未指定时，将使用对应的提交异步请求时设置的 `keep_alive`。否则，可以覆盖此值并延长请求的有效性。当此期限到期时，如果搜索仍在进行中，则会被取消。如果搜索已完成，其保存的结果将被删除。

##### Get async search status

&emsp;&emsp;该接口不会获取查询结果，而是根据ID只显示之前提交的异步查询的状态。如果开启了Elasticsearch security features，访问该接口受限于[monitoring_user role](#Built-in roles)。

```text
GET /_async_search/status/FmRldE8zREVEUzA2ZVpUeGs2ejJFUFEaMkZ5QTVrSTZSaVN3WlNFVmtlWHJsdzoxMDc=
```

```text
{
  "id" : "FmRldE8zREVEUzA2ZVpUeGs2ejJFUFEaMkZ5QTVrSTZSaVN3WlNFVmtlWHJsdzoxMDc=",
  "is_running" : true,
  "is_partial" : true,
  "start_time_in_millis" : 1583945890986,
  "expiration_time_in_millis" : 1584377890986,
  "_shards" : {
      "total" : 562,
      "successful" : 188, 
      "skipped" : 0,
      "failed" : 0
  }
}
```

&emsp;&emsp;第9行，表示目前执行了Query的分片数量

```text
{
  "id" : "FmRldE8zREVEUzA2ZVpUeGs2ejJFUFEaMkZ5QTVrSTZSaVN3WlNFVmtlWHJsdzoxMDc=",
  "is_running" : false,
  "is_partial" : true,
  "start_time_in_millis" : 1583945890986,
  "expiration_time_in_millis" : 1584377890986,
  "_shards" : {
      "total" : 562,
      "successful" : 450,
      "skipped" : 0,
      "failed" : 112
  },
 "completion_status" : 503 
}
```

&emsp;&emsp;第13行，表示异步查询完成但是出错了。

##### Delete async search

&emsp;&emsp;你可以使用这个接口根据ID手动删除一个异步查询。如果查询在运行中，那么它会被取消。否则，已保存的查询结果会被删除。

```text
DELETE /_async_search/FmRldE8zREVEUzA2ZVpUeGs2ejJFUFEaMkZ5QTVrSTZSaVN3WlNFVmtlWHJsdzoxMDc=
```

&emsp;&emsp;如果开启了Elasticsearch security features，删除异步操作受限于：The authenticated user that submitted the original search request. * Users that have the cancel_task cluster privilege。

#### Point in time API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/point-in-time-api.html)

&emsp;&emsp;默认在目标索引的最新可见数据上执行查询请求称为point in time。Elasticsearch PIT是一个轻量级针对于数据状态the state of the data的视图view，在数据初始化后就存在了。在有些场景中，更偏向于使用PIT来执行多次查询。例如，如果[refreshes](#Refresh API)发生在多个search_after请求之间，那么这些请求的结果可能不一致，因为每一次都在最新的PIT上执行查询。

##### Prerequisites

- 如果Elasticsearch security features开启了，你必须要有目标数据流data streams、索引、别名的[read index privilege](#Security privileges)。
- To search a [point in time (PIT)](#Point in time API) for an [alias](#Aliases)，you must have the read index privilege for the alias’s data streams or indices。

##### Examples

&emsp;&emsp;PIT被用来搜索前必须显示的打开，参数`keep_alive`告诉Elasticsearch我们需要保留这个PIT的时间。比如?keep_alive=5m。

```text
POST /my-index-000001/_pit?keep_alive=1m
```

&emsp;&emsp;上述请求的结果会返回一个`_id`，用于设置请求中参数`pit`中的`id`。

```text
POST /_search 
{
    "size": 100,
    "query": {
        "match" : {
            "title" : "elasticsearch"
        }
    },
    "pit": {
	    "id":  "46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA==", 
	    "keep_alive": "1m"  
    }
}
```

1. 使用`pit`查询的请求不能指定`index` 、`routing`等[参数](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-request-body.html#request-body-search-preference)，因为这些参数都已经从PIT中复制过来了。
2. 参数`id`告诉Elasticsearch在哪一个PIT上执行查询
3. 参数`keep_alive`告诉Elasticsearch延长这个PIT的live时间

>IMPORTANT：对于打开PIT的请求以及每一个后续的请求都会收到不一样的id，因此下一次查询总是用这个最新的id

##### Keeping point in time alive

&emsp;&emsp;参数`keep_alive`用于打开PIT的请求以及后续的查询请求，用来延长expand对应的PIT的存活时间time to live，这个值表示需要在这个时间内执行下一次查询，而不是在这个时间内必须处理完所有的结果。

&emsp;&emsp;通常来说，后台的段的合并操作会将一些较小的段合并成一个更大的新的段。一旦这些较小的段（已经被合并了）不再被使用那么就会被删除。然后PIT会使得这些段防止被删除因为它们还在使用中。

>TIP：保留旧的段意味着要求更多的磁盘空间以及文件句柄，确保在你的节点上配置好充足的ample的文件句柄，见[File Descriptors](#File Descriptors)

&emsp;&emsp;此外，如果一个段包含已删除或更新的文档，那么PIT必须跟踪段中的每个文档在初始化搜索请求时是否是live的（没有被删除）。如果你的节点上在某个索引上有很多PIT并且不管的有删除或者更新操作，保证节点上有足够的磁盘空间。注意的是PIT不会阻止它关联的索引被删除。

&emsp;&emsp;你可以使用[nodes stats API](#Nodes stats API)目前打开了多少个PIT。

```text
GET /_nodes/stats/indices/search
```

##### Close point in time API

&emsp;&emsp;当`keep_alive`过期后，PIT会自动的关闭。基于上文中的介绍，保留PIT的开销是很大，当PIT不再需要用于查询时应该马上关闭它。

```text
DELETE /_pit
{
    "id" : "46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA=="
}
```

&emsp;&emsp;上述调用返回下面的内容：

```text
{
   "succeeded": true, 
   "num_freed": 3     
}
```

1. 如果是true，PIT关联的所有search contexts都被成功关闭了
2. 成功关闭的search contexts的数量

##### Search slicing

&emsp;&emsp;当分页查询一个较大数据集时，可以切分成多次查询来消费数据集切分后的数据集。

```text
GET /_search
{
  "slice": {
    "id": 0,                      
    "max": 2                      
  },
  "query": {
    "match": {
      "message": "foo"
    }
  },
  "pit": {
    "id": "46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA=="
  }
}

GET /_search
{
  "slice": {
    "id": 1,
    "max": 2
  },
  "pit": {
    "id": "46ToAwMDaWR5BXV1aWQyKwZub2RlXzMAAAAAAAAAACoBYwADaWR4BXV1aWQxAgZub2RlXzEAAAAAAAAAAAEBYQADaWR5BXV1aWQyKgZub2RlXzIAAAAAAAAAAAwBYgACBXV1aWQyAAAFdXVpZDEAAQltYXRjaF9hbGw_gAAAAA=="
  },
  "query": {
    "match": {
      "message": "foo"
    }
  }
}
```

1. id是切分编号
2. 切分后的数据子集数量

&emsp;&emsp;第一个请求的结果返回属于第一个片的文档(id: 0)，第二个请求的结果返回属于第二个片的文档。由于切片的最大数量被设置为2，所以这两个请求的结果的并集等价于没有切片的时间点搜索的结果。默认情况下，首先在分片上进行分割，然后在每个分片上进行本地分割。本地分割基于Lucene的文档id将分片划分为连续的范围。

&emsp;&emsp;例如，如果分片的数量等于2，用户请求了4个切片，那么切片0和切片2被分配给第一个分片，切片1和切片3被分配给第二个分片。

>IMPORTANT：应该对所有片使用相同的时间点ID。如果使用不同的PIT id，则片可能会重叠和遗漏文档。这是因为拆分标准是基于Lucene文档id的，这些文档id在索引更改时变的不稳定。

#### Search template API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-template-api.html)

&emsp;&emsp;使用[search template](#Search templates)运行一个查询。

```text
GET my-index/_search/template
{
  "id": "my-search-template",
  "params": {
    "query_string": "hello world",
    "from": 0,
    "size": 10
  }
}
```

##### Request

```text
GET <target>/_search/template
GET _search/template
POST <target>/_search/template
POST _search/template
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要在data stream、index或者alias上有`read`的[index privilege](#Indices privileges)。对于CCS，见[Configure remote clusters with security](#Configure remote clusters with security)

##### Path parameters

- `<target>`：（Optional, string）用逗号隔开的data stream、indices以及alias的列表。支持通配符（`*`）。若要查询所有的data streams和indices，可以忽略或者使用`*`

##### Query parameters

- allow_no_indices：（Optional, Boolean）如果为`false`，当通配符表达式、[index alias](#Aliases)或者`all`匹配缺失索引或者已关闭的索引则返回一个错误。即使请求找到了打开的索引也可能会返回错误。比如，请求中指定了`foo*, bar*`，但如果找到以`foo`开头的索引，但是没找到以`bar`开头的索引则会返回一个错误。默认为`true`
- ccs_minimize_roundtrips：（Optional, Boolean）如果为`true`，跨集群搜索（Cross Cluster Search）尝试最小化网络往返次数，以提高搜索效率。默认值为`true`
- expand_wildcards：（Optional, string）通配符模式可以匹配的索引类型。如果请求目标是data stream，还会检测通配符表达式是否会匹配隐藏的data streams。支持多值，例如`open`, `hidden`。合法值有：
  - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target syntax-1)
  - open：匹配打开的，非隐藏的索引。同样匹配非隐藏的data stream
  - closed：匹配关闭的，非隐藏的索引。同样匹配非隐藏的data stream。Data stream不能关闭
  - hidden：匹配隐藏的data streams和indices。必须和`open`、`closed`中的一个或全部组合使用
  - none：不展开通配符模式
  默认值为`all`。

- explain：（Optional, Boolean）如果为`true`，响应中会包含额外关于打分计算的信息并。默认为`false`
- ignore_throttled：（Optional, Boolean）如果为`true`，当索引被限流时，指定的具体的、展开的或别名的索引将不包括在响应中。默认值为`true`
- ignore_unavailable：（Optional, Boolean）如果为`false`，请求中指定的data stream或者index如果缺失的话会返回一个错误。默认是`false`
- preference：（Optional, string）指定在哪个节点或分片上执行。默认是随机
- rest_total_hits_as_int：（Optional, Boolean）如果为`true`，响应返回整数类型的`hits.total`。如果为`false`，返回object类型的`hits.total`。默认为`false`
- routing：（Optional, string）自定义的值用来路由到指定分片
- scroll：（Optional,[time value](#Time units)）为scroll操作保留[search context](#Keeping the search context alive)的保留时间。见[scroll search results](#Scroll search results)
- scroll_size：（Optional,integer）定义了驱动操作（power operation）的scroll请求的大小。默认值为1000
- search_type：（Optional, string）搜索操作的类型。可选项有：
  - `query_then_fetch`
  - `dfs_query_then_fetch`
- typed_keys：（Optional, Boolean）如果为`true`，响应会用它们各自的类型作为前缀，来标识aggregation和suggester的名称。默认为`false`

##### Response body

- explain：（Optional, Boolean）如果为`true`，响应中会包含额外关于打分计算的信息。默认为`false`。
  - 如果你同时在请求参数中指定了 `explain`，那接口只使用请求参数中的参数
- id：（Required\*,string）search template的id。如果未指定`source`，那么必须提供这个参数
- params：（Optional,object）键值对用来替换模板中的Mustache变量。key是变量的名称。值是变量的值
- profile：（Optional, Boolean）如果为`true`，会描述query的执行过程信息。默认为`false`
- source：（Required\*,object）直接提供的search template（而不是通过`id`字段获取）。支持跟[search API]()相同的参数。同样支持[Mustache](https://mustache.github.io/)变量
  - 如果`id`未指定，那么必须提供这个参数


#### Multi search template API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/multi-search-template.html)

&emsp;&emsp;使用单个请求执行多个[templated searches](#Run multiple templated searches)。

```text
GET my-index/_msearch/template
{ }
{ "id": "my-search-template", "params": { "query_string": "hello world", "from": 0, "size": 10 }}
{ }
{ "id": "my-other-search-template", "params": { "query_type": "match_all" }}
```

##### Request

```text
GET <target>/_msearch/template
GET _msearch/template
POST <target>/_msearch/template
POST _msearch/template
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要在data stream、index或者alias上有`read`的[index privilege](#Indices privileges)。对于CCS，见[Configure remote clusters with security](#Configure remote clusters with security)

##### Query parameters

- ccs_minimize_roundtrips：（Optional, Boolean）如果为`true`，跨集群搜索（Cross Cluster Search）尝试最小化网络往返次数，以提高搜索效率。默认值为`true`
- max_concurrent_searches：（Optional,integer）该接口运行查询并行度最大值。默认是
  > max(1, (# of [data nodes](#Data node) * min([search thread pool size](#Thread pools), 10)))

- rest_total_hits_as_int：（Optional, Boolean）如果为`true`，响应返回整数类型的`hits.total`。如果为`false`，返回object类型的`hits.total`。默认为`false`
- search_type：（Optional, string）搜索操作的类型。可选项有：
  - `query_then_fetch`
  - `dfs_query_then_fetch`
- typed_keys：（Optional, Boolean）如果为`true`，响应会用它们各自的类型作为前缀，来标识aggregation和suggester的名称。默认为`false`

##### Request  body

&emsp;&emsp;请求体必须是按照以下格式的换行符分隔的 JSON（NDJSON）：

```text
<header>\n
<body>\n
<header>\n
<body>\n
```

&emsp;&emsp;每一个`header`跟`body`对代表一次查询请求。

&emsp;&emsp;The` <header>` supports the same parameters as the [multi search API's ](#Multi search API)`<header>`. The` <body>` supports the same parameters as the [search template API's ](#Search API) request body.

- `<header>`：（Required,object）用来限制或者更改查询
  - 每一个查询体中都要这个对象但可以是空(`{}`)或者空白行
  - allow_no_indices：（Optional, Boolean）如果为`false`，当通配符表达式、[index alias](#Aliases)或者`all`匹配缺失索引或者已关闭的索引则返回一个错误。即使请求找到了打开的索引也可能会返回错误。比如，请求中指定了`foo*, bar*`，但如果找到以`foo`开头的索引，但是没找到以`bar`开头的索引则会返回一个错误。默认为`true`
  - expand_wildcards：（Optional, string）通配符模式可以匹配的索引类型。如果请求目标是data stream，还会检测通配符表达式是否会匹配隐藏的data streams。支持多值，例如`open`, `hidden`。合法值有：
    - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target syntax-1)
    - open：匹配打开的，非隐藏的索引。同样匹配非隐藏的data stream
    - closed：匹配关闭的，非隐藏的索引。同样匹配非隐藏的data stream。Data stream不能关闭
    - hidden：匹配隐藏的data streams和indices。必须和`open`、`closed`中的一个或全部组合使用
    - none：不展开通配符模式
    默认值为`all`。

  - ignore_unavailable：（Optional, Boolean）如果为`false`，请求中指定的data stream或者index如果缺失的话会返回一个错误。默认是`false`
  - index：（Optional, string or array of strings）待查询的data streams、indices以及alias。支持通配符(`*`)。使用数组指定多个目标
  - preference：（Optional, string）指定在哪个节点或分片上执行。默认是随机
  - request_cache：（Optional, Boolean）如果为`true`，该查询可以使用request cache。默认为索引层的settings。见[Shard request cache settings](#Shard request cache settings)
  - routing：（Optional, string）自定义的[routing value](#_routing field)用来路由到指定分片
- search_type：（Optional, string）是否使用全局的term以及文档词频用于文档打分。可选项  有：
    - `query_then_fetch`：（默认）文档打分只基于当前分片中的term以及文档词频。这种类型通常执行很快但不够精确
    - `dfs_query_then_fetch`：文档打分只基于所有分片中的term以及文档词频。这种类型通常执行很慢但是精确
- `<body>`：（Request,object）用于查询的参数
  - explain：（Optional, Boolean）如果为`true`，响应中会包含额外关于打分计算的信息。默认为`false`
  - id：（Required\*,string）search template的id。如果未指定`source`，那么必须提供这个参数
  - params：（Optional,object）键值对用来替换模板中的Mustache变量。key是变量的名称。值是变量的值
  - profile：（Optional, Boolean）如果为`true`，会描述query的执行过程信息。默认为`false`
  - source：（Required\*,object）直接提供的search template（而不是通过`id`字段获取）。支持跟[search API](#Search API)相同的参数。同样支持[Mustache](https://mustache.github.io/)变量
    - 如果`id`未指定，那么必须提供这个参数

##### Response codes

&emsp;&emsp;如果请求自身出错则返回`400`。如果请求中一个或多个查询失败了，接口返回`200`并且在响应中包含一个`error`对象描述每一个错误。

##### Response body

- reponses：（array of objects）每一个查询的结果，按照请求提交的顺序返回。每一个对象中使用跟[search API](#Search API)的响应中一样的属性
  - 如果查询失败，响应中会包含一个带有错误信息的`error`对象

###### curl requests

&emsp;&emsp;如果提供了文本文件或者文本输入通过`curl`执行，使用`--data-binary`而不是`-d`防止newlines。

```text
$ cat requests
{ "index": "my-index" }
{ "id": "my-search-template", "params": { "query_string": "hello world", "from": 0, "size": 10 }}
{ "index": "my-other-index" }
{ "id": "my-other-search-template", "params": { "query_type": "match_all" }}

$ curl -H "Content-Type: application/x-ndjson" -XGET localhost:9200/_msearch/template --data-binary "@requests"; echo
```

#### Render search template API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/render-search-template-api.html)

&emsp;&emsp;渲染一个[search template](##Search templates)为查询的[request body]()。

```text
POST _render/template
{
  "id": "my-search-template",
  "params": {
    "query_string": "hello world",
    "from": 20,
    "size": 10
  }
}
```

##### Request

```text
GET _render/template
GET _render/template/<template-id>
POST _render/template
POST _render/template/<template-id>
```

##### Prerequisites

&emsp;&emsp;如果开启了Elasticsearch security features，你必须在至少一个index pattern上有`read`的[index privilege](#Indices privileges)。

##### Path parameters

- `<template-id>`：（Required\*, string）待渲染的search template的ID。如果未指定`source`，必须指定该参数或者请求体中的`id`

##### Request body

- id：（Required\*,string）待渲染的search template的ID。如果未指定`source`，那么必须提供这个参数或者`<template-id>`
- params：（Optional,object）键值对用来替换模板中的Mustache变量。key是变量的名称。值是变量的值
- profile：（Optional, Boolean）如果为`true`，会描述query的执行过程信息。默认为`false`
- source：（Required\*,object）直接提供的search template（而不是通过`id`字段获取）。支持跟[search API](#Search API)相同的参数。同样支持[Mustache](https://mustache.github.io/)变量
  - 如果`id`或`<template-id>`未指定，那么必须提供这个参数


#### Search shards API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-shards.html)

#### Suggesters
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-suggesters.html)

##### Completion Suggester

#### Multi search API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-multi-search.html)

#### Validate API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-validate.html)

&emsp;&emsp;
##### Request
##### Prerequisites
##### Description
##### Path parameters
##### Query parameters
##### Response body
##### Example

#### Profile API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-field-caps.html)

#### Field capabilities API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-field-caps.html)

#### Vector tile search API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-vector-tile-api.html)

### Searchable snapshots APIs
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/searchable-snapshots-apis.html)

&emsp;&emsp;你可以使用下面的API执行searchable snapshots的操作。

- [Mount snapshot](#Mount snapshot API)
- [Cache statistics](#Cache stats API)
- [Shard statistics](#Searchable snapshot statistics API)
- [Clear cache](#Clear cache API)

#### Mount snapshot API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/searchable-snapshots-api-mount-snapshot.html)

&emsp;&emsp;挂载一个快照使其成为一个searchable snapshot index。

##### Request

```text
POST /_snapshot/<repository>/<snapshot>/_mount
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`manage`的[cluster privilege](#Cluster privileges)以及`manage`的index privilege来使用这个API。更多信息见[Security privileges](#Security privileges)

##### Path parameters

- `<repository>`：（Required, string）包含待挂载为索引的快照的仓库名称
- `<snapshot>`：（Required, string）待挂载为索引的快照名称

##### Query parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- wait_for_completion：（Optional, Boolean）如果为`true`，当快照完成后才返回一个响应。如果为`false`，当快照初始化结束就返回一个响应。默认值是`false`
- storage：（Optional, string）searchable snapshot index的[Mount option](#Mount options)，可选值有：
  - full_copy (Default)：[Fully mounted index](#Fully mounted index)
  - shard_cache：[Partially mounted index.](#Partially mounted index)

##### Request body

- index：（Required, string）被挂载的快照中索引的名称
  - 如果未指定`renamed_index`，那么`index`的名称将用于创建新的索引
- renamed_index：（Optional, string）将被创建的索引名称
- index_settings：（Optional,object）挂载后，索引的settings
- ignore_index_settings：（Optional,array of strings）挂载后，需要被移除的索引settings

##### Example

&emsp;&emsp;从名为`my_repository`的快照仓库中挂载一个现有的名为`my_snapshot`的快照中的名为`my_docs`的索引。

```text
POST /_snapshot/my_repository/my_snapshot/_mount?wait_for_completion=true
{
  "index": "my_docs", 
  "renamed_index": "docs", 
  "index_settings": { 
    "index.number_of_replicas": 0
  },
  "ignore_index_settings": [ "index.refresh_interval" ] 
}
```

&emsp;&emsp;第3行，挂载的快照中的索引名称
&emsp;&emsp;第4行，待创建的索引名称
&emsp;&emsp;第5行，添加到新的索引的settings
&emsp;&emsp;第8行，挂载快照中的索引后，需要移除这个索引的settings

#### Cache stats API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/searchable-snapshots-api-cache-stats.html)

&emsp;&emsp;为[partially mounted indices](#Partially mounted index)获取关于shared cache的统计信息。

##### Request

```text
GET /_searchable_snapshots/cache/stats
GET /_searchable_snapshots/<node_id>/cache/stats
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`manage`的[cluster privilege](#Cluster privileges)来使用这个API。更多信息见[Security privileges](#Security privileges)

##### Path parameters

- `<node_id>`：（Optional, string）集群中目标节点的名称。例如，`nodeId1`、`nodeId2`。对于节点的选择选项，见[Node specification](#Node specification)

##### Query parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Response body

- nodes：（object）包含请求中指定的节点的统计信息
  - `<node_id>`：（object）包含带有节点标识符的统计信息
    - `shared_cache`：（object）包含shard cache file的信息
      - reads：（long）shared cache被用来读取数据的次数
      - bytes_read_in_bytes：（long）shared cache中读取的总字节数
      - writes：（long）表示有多少次数据从blob存储库写入shared cache
      - bytes_written_in_bytes：（long）写入shared cache的总字节数
      - evictions：（long）从shared cache文件中驱逐（reject）的区域（region，shared cache file（共享缓存文件）被划分的小块或段）数量
      - num_regions：（integer）shared cache文件中的区域数量
      - size_in_bytes：（long）shared cache文件的总大小（以字节为单位）
      - region_size_in_bytes：（long）shared cache文件中一个区字段的大小（以字节为单位）

##### Example

&emsp;&emsp;从所有节点中为partially mounted indices获取shared cache的统计信息：

```text
GET /_searchable_snapshots/cache/stats
```

&emsp;&emsp;响应如下：

```text
{
  "nodes" : {
    "eerrtBMtQEisohZzxBLUSw" : {
      "shared_cache" : {
        "reads" : 6051,
        "bytes_read_in_bytes" : 5448829,
        "writes" : 37,
        "bytes_written_in_bytes" : 1208320,
        "evictions" : 5,
        "num_regions" : 65536,
        "size_in_bytes" : 1099511627776,
        "region_size_in_bytes" : 16777216
      }
    }
  }
}
```

#### Searchable snapshot statistics API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/searchable-snapshots-api-stats.html)

> WARNING：这个功能目前处于技术预览阶段，未来的版本中可能会进行更改或移除。Elastic会努力修复任何问题，但是技术预览中的功能不受官方正式发布（GA）功能的支持服务级别协议（SLA）约束.

&emsp;&emsp;获取searchable snapshots的统计信息。

##### Request

```text
GET /_searchable_snapshots/stats
GET /<target>/_searchable_snapshots/stats
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`manage`的[cluster privilege](#Cluster privileges)以及`manage`的index privilege来使用这个API。更多信息见[Security privileges](#Security privileges)

##### Path parameters

- `<target>`：（Optional, string）用逗号隔开的待获取的data streams、indices列表。若要获取所有data streams、indices的统计信息，则忽略这个参数

##### Example

&emsp;&emsp;获取名为`my-index`的索引的统计信息：

```text
GET /my-index/_searchable_snapshots/stats
```

#### Clear cache API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/searchable-snapshots-api-clear-cache.html)

> WARNING：这个功能目前处于技术预览阶段，未来的版本中可能会进行更改或移除。Elastic会努力修复任何问题，但是技术预览中的功能不受官方正式发布（GA）功能的支持服务级别协议（SLA）约束.

&emsp;&emsp;为[partially mounted indices](#Partially mounted index)清除shared cache中的indices和data streams。

##### Request

```text
POST /_searchable_snapshots/cache/clear
POST /<target>/_searchable_snapshots/cache/clear
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`manage`的[cluster privilege](#Cluster privileges)以及`manage`的index privilege来使用这个API。更多信息见[Security privileges](#Security privileges)

##### Path parameters

- `<target>`：（Optional, string）用逗号隔开的待获取的data streams、indices列表。若要获取所有data streams、indices的统计信息，则忽略这个参数

##### Example

&emsp;&emsp;清除名为`my-index`的缓存：

```text
POST /my-index/_searchable_snapshots/cache/clear
```

### Security APIs
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/security-api.html)

&emsp;&emsp;
##### Request
##### Prerequisites
##### Description
##### Path parameters
##### Query parameters
##### Response body
##### Example

#### Role mappings
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/security-api.html#security-role-mapping-apis)

### Snapshot and restore APIs
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/snapshot-restore-apis.html)

&emsp;&emsp;你可以使用下面的APIs来设置快照仓库、管理快照备份以及将快照回到一个运行中的集群中。

&emsp;&emsp;更多信息见[Snapshot and restore](#Snapshot and restore)。

##### Snapshot repository management APIs

- [Create or update snapshot repository](#Create or update snapshot repository API)
- [Verify snapshot repository](Verify snapshot repository API)
- [Get snapshot repository](#Get snapshot repository API)
- [Delete snapshot repository](#Delete snapshot repository API)
- [Clean up snapshot repository](#Clean up snapshot repository API)

##### Snapshot management APIs

- [Create snapshot](#Create snapshot API)
- [Clone snapshot](#Clone snapshot API)
- [Get snapshot](#Get snapshot API)
- [Get snapshot status](#Get snapshot status API)
- [Restore snapshot](#Restore snapshot API)
- [Delete snapshot](#Delete snapshot API)

#### Create or update snapshot repository API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/put-snapshot-repo-api.html)

&emsp;&emsp;注册或更新一个[snapshot repository](#Register a snapshot repository)。

```text
PUT /_snapshot/my_repository
{
  "type": "fs",
  "settings": {
    "location": "my_backup_location"
  }
}
```

##### Request

```text
PUT /_snapshot/<repository>
POST /_snapshot/<repository>
````
##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage` [cluster privilege](#Cluster privileges)来使用这个API
- 若要注册一个快照仓库，集群的全局元数据必须是可写的。确保没有任何[cluster blocks](#Metadata)阻止写入

##### Path parameters

- `<repository>`：（Required, string）注册或更新的快照仓库名称

##### Query parameter

> IMPORTANT：这个API的有些选项可以通过请求参数（query parameter）或者请求体（query body）指定。如果同时指定了，那么只会使用请求参数。

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。
- verify：（Optional, Boolean）如果为`true`，那么请求会验证是否在所有master和数据节点上都功能正常。如果为`false`，则跳过验证。默认值为`true`
  - 你可以通过[verify snapshot repository API](#Verify snapshot repository API)手动执行验证操作。

##### Request body

- type：（Required, string）仓库类型
  - azure：[Azure repository](#Azure repository)
  - gcs：[Google Cloud Storage repository](#Google Cloud Storage repository)
  - s3：[S3 repository](#S3 repository)
  - fs：[Shared file system repository](#Shared file system repository)
  - source：[Source-only repository](#Source-only repository)
  - url：[Read-only URL repository](#Read-only URL repository)
  
  其他仓库类型可以通过官方插件实现使用
  
  - hdfs：[Hadoop Distributed File System (HDFS) repository](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/repository-hdfs.html)

- settings：（Required, object）仓库设置。根据不同的仓库类型支持的选项各不相同：
  - [Azure repository](#Azure repository)
  - [Google Cloud Storage repository](#Google Cloud Storage repository)
  - [S3 repository](#S3 repository)
  - [Shared file system repository](#Shared file system repository)
  - [Read-only URL repository](#Read-only URL repository)
  - [Source-only repository](#Source-only repository)
  
  其他仓库类型可以通过官方插件实现使用
  
  - [Hadoop Distributed File System (HDFS) repository](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/repository-hdfs.html)
  
- verify：（Optional, Boolean）如果为`true`，那么请求会验证是否在所有master和数据节点上都功能正常。如果为`false`，则跳过验证。默认值为`true`
  - 你可以通过[verify snapshot repository API](#Verify snapshot repository API)手动执行验证操作。

#### Verify snapshot repository API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/verify-snapshot-repo-api.html)

&emsp;&emsp;验证某个快照仓库是否功能正常。见[Verify a repository](#Verify a repository)。

```text
POST /_snapshot/my_repository/_verify
```

##### Request

```text
POST /_snapshot/<repository>/_verify
````

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage` [cluster privilege](#Cluster privileges)来使用这个API

##### Query parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`

##### Response bdy

- nodes：（object）
  - `node_id`：（object）某个节点连接到快照仓库的信息。对象中的key是node的ID
    - name（string）：节点的名称（Human-readable）。你可以在`elasticsearch.yml`中使用[node.name](#Node name setting)属性定义节点的名称。默认是机器的hostname

#### Repository analysis API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/repo-analysis-api.html)

#### Get snapshot repository API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/get-snapshot-repo-api.html)

&emsp;&emsp;获取关于一个或多个已注册的[snapshot repositories](#Register a snapshot repository)信息。

##### Request

```text
GET /_snapshot/<repository>
GET /_snapsho
```

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`monitor_snapshot`、`create_snapshot`、`manage` [cluster privilege](#Cluster privileges)来使用这个API

##### Path parameters

- `<repository>`：（Optional, string）快照仓库名称用逗号隔开来限制请求量。支持通配符`*`包括使用以`-`开头进行排除的组合通配符。
  - 若要获取集群中所有注册的快照仓库，移除这个参数或者使用`*`或`_all`

##### Query parameters

- `local`：（Optional, Boolean）如果为`true`，该请求只从本地节点获取信息。默认是`false`。即从master node获取，默认是`false`。
- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。

##### Response body

- `<repository>`：（object）包含快照仓库的信息。object中的key是快照仓库的名称。
  - type：仓库类型，type的值可以有：
    - fs：Shared file system repository. See [Shared file system repository](#Shared file system repository)
    - source：Source-only repository. See [Source-only repository](#Source-only repositor).
    - url：URL repository. See [Read-only URL repository](#Read-only URL repository).
    &emsp;&emsp;更多官方插件可以有更多的仓库类型：
    - [repository-s3](#S3 repository) for S3 repository support
    - [repository-hdfs](https://www.elastic.co/guide/en/elasticsearch/plugins/8.2/repository-hdfs.html) for HDFS repository support in Hadoop environments
    - [repository-azure](#Azure repository) for Azure storage repositories
    - [repository-gcs](#Google Cloud Storage repository) for Google Cloud Storage repositories

- settings：（object）包含仓库的设置。`settings`中合法属性取决于仓库类型[type](#Create or update snapshot repository API)

##### Examples

```text
GET /_snapshot/my_repository
```

&emsp;&emsp;这个API返回下面的响应：

```text
{
  "my_repository" : {
    "type" : "fs",
    "uuid" : "0JLknrXbSUiVPuLakHjBrQ",
    "settings" : {
      "location" : "my_backup_location"
    }
  }
}
```

#### Delete snapshot repository API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/delete-snapshot-repo-api.html)

&emsp;&emsp;注销一个或多个[snapshot repositories](#Register a snapshot repository)。

&emsp;&emsp;当快照仓库注销后，Elasticsearch只是移除了对该位置的引用，这个仓库仍然存放着快照。快照本身保持不变，且位置不动。

##### Request 

```text
DELETE /_snapshot/my_repository
```
##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage` [cluster privilege](#Cluster privileges)来使用这个API

##### Path parameters

- `<repository>`：（Required, string）待注销的快照仓库的名称。可以使用通配符`*`

##### Query parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`

#### Clean up snapshot repository API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/clean-up-snapshot-repo-api.html)

&emsp;&emsp;触发对快照仓库的审查并且删除不会现有快照引用的陈旧数据。见[Clean up a repository](#Clean up a repository)。

```text
POST /_snapshot/my_repository/_cleanup
```

##### Request

```text
POST /_snapshot/<repository>/_cleanup
```

##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`manage` [cluster privilege](#Cluster privileges)来使用这个API

##### Query parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`

##### Response body

- result：（object）包含清理操作的统计信息
  - deleted_bytes：（integer ）清理操作释放的字节数
  - deleted_blobs：（integer ）在清理过程中从快照仓库移除的二进制大对象（Binary Large OBjects即blobs）的数量

##### Example

```text
POST /_snapshot/my_repository/_cleanup
```

&emsp;&emsp;这个API返回下面的响应：

```text
{
  "results": {
    "deleted_bytes": 20,
    "deleted_blobs": 5
  }
}
```

#### Clone snapshot API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/clone-snapshot-api.html)

#### Create snapshot API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/create-snapshot-api.html)

&emsp;&emsp;指定data stream和Index生成一个快照。见[take a snapshot](#Create a snapshot)。

```text
PUT /_snapshot/my_repository/my_snapshot
```

##### Request

```text
PUT /_snapshot/<repository>/<snapshot>
POST /_snapshot/<repository>/<snapshot>
```
##### Prerequisites

- 如果开启了Elasticsearch security功能，你必须有`create_snapshot`、`manage` [cluster privilege](#Cluster privileges)来使用这个API

##### Path parameters

- `<repository>`：（Required, string）快照仓库的名称
- `<snapshot>`：（Required, string）快照的名称。支持[date math](#Date math support in index and index alias names)。在快照仓库中必须名称唯一

##### Query parameters

- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- wait_for_completion：（Optional, Boolean）如果为`true`，当快照完成后才返回一个响应。如果为`false`，当快照初始化结束就返回一个响应。默认值是`false`

##### Request body

- expand_wildcards：（Optional, string）决定在`indices`参数中如果有通配符模式时将如何去匹配data streams和indices。支持使用逗号隔开的值，例如`open, hidden`。默认是`all`。合法值有：
  - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target syntax-1)
  - open：匹配打开的data streams和indices
  - closed：匹配关闭的data streams和indices
  - hidden：匹配隐藏的data streams和indices。必须和`open`、`closed`中的一个或全部组合使用
  - none：不展开通配符模式
- ignore_unavailable：（Optional, Boolean）如果为`false`，如果任意的data streams或indices丢失或者关闭会导致生成快照失败。如果为`true`，快照会忽略data streams或indices丢失或者关闭。默认值为`false`
- include_global_state：（Optional, Boolean）如果为`true`，快照中会包含集群状态。默认为`true`。集群状态包括：
  - [Persistent cluster settings](#Cluster and node setting types)
  - [Index templates](#Index templates)
  - [Legacy index templates](#Create or update index template API（legacy）)
  - [Ingest pipelines](#Ingest pipelines)
  - [ILM policies](#ILM: Manage the index lifecycle)
  - For snapshots taken after 7.12.0, [feature states](#Feature states)
- indices：（Optional, string or array of strings）写入到快照的data streams和indices，用逗号隔开。支持[multi-target syntax](#Multi-target syntax)。默认是一个空数组（`[]`），包含常规的data streams和常规的indices。若要排除所有的data streams和indices，可以使用`-*`
  - 你不能使用这个参数来包含或者排除[system indices or system data streams](#System indices)，可以转而使用下面的feature_states参数
- feature_states：（Optional, array of strings）将[Feature states ](#Feature states)包含到快照中。若要获取可能的值以及它们的描述，可以使用[get features API](#Get Features API)
  - 如果`include_global_state`为`true`，快照默认包含所有的feature states。否则不包含
  - 注意的是指定一个空数组会产生默认行为。若要排除素有的feature states，并且不用关心`include_global_state`是何值，则可以指定一个只有`none`值的数组（`["none"]`）
- metadata：（Optional, object）任意的附加快照的一些元数据。比如记录哪个人生成了快照，为什么要生成快照，或者其他有用的数据。元数据大小必须小于1024个字节
- partial：（Optional, Boolean）如果为`false`，那么当一个或者多个索引的所有主分片不是都可用，那么整个快照会失败。默认值为`false`。否则允许对可用的分片进行快照并且得到一个不完整的快照

##### Examples

&emsp;&emsp;下面的请求创建了包含`index_1`和`index_2`的快照。

```text
PUT /_snapshot/my_repository/snapshot_2?wait_for_completion=true
{
  "indices": "index_1,index_2",
  "ignore_unavailable": true,
  "include_global_state": false,
  "metadata": {
    "taken_by": "user123",
    "taken_because": "backup before upgrading"
  }
}
```

&emsp;&emsp;这个API返回下面的响应：

```text
{
  "snapshot": {
    "snapshot": "snapshot_2",
    "uuid": "vdRctLCxSketdKb54xw67g",
    "repository": "my_repository",
    "version_id": <version_id>,
    "version": <version>,
    "indices": [],
    "data_streams": [],
    "feature_states": [],
    "include_global_state": false,
    "metadata": {
      "taken_by": "user123",
      "taken_because": "backup before upgrading"
    },
    "state": "SUCCESS",
    "start_time": "2020-06-25T14:00:28.850Z",
    "start_time_in_millis": 1593093628850,
    "end_time": "2020-06-25T14:00:28.850Z",
    "end_time_in_millis": 1593094752018,
    "duration_in_millis": 0,
    "failures": [],
    "shards": {
      "total": 0,
      "failed": 0,
      "successful": 0
    }
  }
}
```

#### Get snapshot API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/get-snapshot-api.html)

&emsp;&emsp;获取一个或多个快照的信息。

```text
GET /_snapshot/my_repository/my_snapshot
```

##### Request

```text
GET /_snapshot/<repository>/<snapshot>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须要有`monitor_snapshot`、`create_snapshot`、`manage`的[cluster privilege](#Cluster privileges)来使用这个API。

##### Path parameters

- `<repository>`：（Optional, string）快照仓库名称用逗号隔开来限制请求量。支持通配符`*`包括使用以`-`开头进行排除的组合通配符。
  - 若要获取集群中所有注册的快照仓库，移除这个参数或者使用`*`或`_all`
- `<Snapshot>`：（Required, string）快照名称用逗号隔开。支持通配符`*`包括使用以`-`开头进行排除的组合通配符。
  - 若要获取某个已注册的仓库中的所有快照，使用通配符（`*`）或者`_all`
  - 使用`_current`获取当前正在运行的快照

> NOTE：所有存在不可用的快照，使用`_all`的请求会失败。将ignore_unavailable(见下文中的请求参数)设置为`true`来返回可见的快照。

##### Query parameters

- master_timeout：（Optional, [time units](#Time units)）周期性的等待连接master node。如果在超时前未收到响应，则请求失败并且返回一个错误。默认是`30s`。
- ignore_unavailable：（Optional, Boolean）如果为`false`，任何快照如果不可见的话则请求返回一个错误。默认为`false`
- verbose：（Optional, Boolean）如果为`true`，返回每一个快照额外的信息，比如生成快照的Elasticsearch版本，快照的开始结束时间，快照中的分片数量。默认是`true`。如果为`false`。则省略这些额外的信息
- index_details：（Optional, Boolean）如果为`true`，返回快照中每一个索引的额外信息，例如索引中的分片数量，索引的大小（单位字节），索引中每一个分片中的段的树林。默认是`false`，意味着这些信息会被省略
- sort：（Optional, string）对结果进行排序。默认是`start_time`。比如根据快照处理的开始时间进行排序
  - start_time：根据快照处理的开始时间排序，如果相同则继续使用快照名称排序
  - duration：根据创建快照花费的时间排序，如果相同则继续使用快照名称排序
  - name：根据快照名称排序
  - repository：根据仓库名称排序，如果相同则继续使用快照名称排序
  - index_count：根据快照中包含的索引数量排序，如果相同则继续使用快照名称排序
  - shard_count：根据快照中包含的分片数量排序，如果相同则继续使用快照名称排序
  - failed_shard_count：根据没能写进快照的分片数量排序，如果相同则继续使用快照名称排序
- size：（Optional, integer）返回的最大快照数量。默认值为`0`意味着没有限制
- order：（Optional, string）排序方式。合法值为`asc`即升序或者`desc`即降序。默认值为`asc`。即默认是升序
- from_sort_value：（Optional, string）从一个有序的字段中的某个值开始检索。当根据快照或仓库名称排序时，该值可以是一个string类型，当根据索引或分片数量排序时，该值可以是一个数值或者毫秒
- after：（Optional, string）该值是一个偏移标识符用于分页查询，它来源于上一次分页查询的响应中`next`字段的值。使用这个参数跟`from_sort_value`是互斥的
- offset：（Optional, integer）该值是一个数值偏移，描述分页查询中的开始位置。使用一个non-zero值，它与`after`参数是互斥的
- slm_policy_filter：（Optional, string）根据快照所属SLM策略的名称，一个或者多个，用逗号隔开来过滤快照。支持通配符(`\*`)包括使用以`-`开头进行排除的组合通配符。例如`*,-policy-a-\*`将会返回所有的快照，以`policy-a-`开头的SLM策略名创建的快照除外。注意的是通配符`*`匹配了所有的由SLM策略创建的快照，不会匹配不是由SLM策略创建的快照。若要包含不是由SLM策略创建的快照，你可以使用特殊的`_none`，那么将会匹配所有不是由SLM策略创建的快照

> NOTE：`after`参数和`next`字段允许在迭代快照时提供一些关于快照的并发创建或删除的一致性保证。它保证了任何在迭代开始时存在且未被并发删除的快照将在迭代过程中被看到。在迭代过程中可能会看到并发创建的快照。

> NOTE：当参数`verbose`为false时，`size`, `order`, `after`, `from_sort_value`, `offset`, `slm_policy_filter` 以及 `sort`参数将不被支持。且verbose=false的请求的排序顺序是未定义的。

##### Response body

- snapshot：（string）快照的名称
- uuid：（string）快照的UUID
- version_id：（int）用来创建快照的Elasticsearch的构建ID
- version：（float）用来创建快照的Elasticsearch的版本号
- indices：（array）快照中包含的索引列表
- index_details：（object）快照中每一个索引的详细信息。索引名作为key。只有设置了请求参数`index_details`才会呈现，并且足够较近版本的Elasticsearch中才包含索引的详细信息
  - shard_count：（integer）索引中分片数量
  - size：（string）索引中分片大小总量。只有设置了请求参数`?human`才会显示
  - size_in_bytes：（long）索引中分片大小总量（单位字节）
  - max_segments_per_shard：（integer）Maximum number of segments per shard in this index snapshot
- data_streams：（array of strings）快照中包含的[data streams](#Data streams)列表
- include_global_state：（Boolean）快照中是否包含当前的集群状态
- feature_states：（array of objects）快照中的[feature states](#Feature states)。只有当快照中包含一个或者多个feature states时才展示
  - feature_name：（string）feature的名称，通过[get features API](#Get Features API)返回
  - indices：（array of strings）feature state中的索引
- start_time：（string）快照处理的开始时间（Date timestamp）
- start_time_in_millis：（long）快照处理的开始时间（时间戳）
- end_time：（string）快照处理的结束时间（Date timestamp）
- end_time_in_millis：（long）快照处理的结束时间（时间戳）
- duration_in_millis：（long）创建快照花费的时间
- failures：（array）创建快照时遇到的失败列表
- shard：（object）快照中包含的分片数量
  - total：（integer）快照中包含的分片数量总和
  - successful：（integer）快照中成功写入的分片数量总和
  - failed：（integer）快照中没能写入的分片数量总和
  - total分片数是指尝试进行快照的分片总数，而successful和failed分别表示成功和失败的分片数
- state：（string）
  - 该字段可以是以下的值：
    - IN_PROGRESS：快照当前正在运行
    - SUCCESS：快照已经完成并且所有的分片都成功存储了
    - FAILED：快照已经完成但是出现了错误并且没能存储任何数据
    - PARTIAL：存储了全局的集群状态，但是至少有一个分片数据没能成功存储。响应中`failure`部分会包含分片为什么没能被正确处理的详细原因
- next：（string）如果请求中包含了一个size limit并且如果有更多的结果。那么`next`会被添加到响应中，随后这个字段的值可以作为下一次请求中的`after`来获取更多的值。
- total：（integer）忽略size limit或者`after`参数，当前请求匹配到的快照数量
- remaining：（integer）由于size limit使得还有一定数量的快照未返回，未返回的数量用该字段描述。随后可以使用`next`的值用于下一次请求更多的结果。

##### Example

&emsp;&emsp;下面的请求返回了快照仓库`my_repository`中名为`snapshot_2`的快照

```text
GET /_snapshot/my_repository/snapshot_2
```

&emsp;&emsp;这个API返回下面的响应：

```text
{
  "snapshots": [
    {
      "snapshot": "snapshot_2",
      "uuid": "vdRctLCxSketdKb54xw67g",
      "repository": "my_repository",
      "version_id": <version_id>,
      "version": <version>,
      "indices": [],
      "data_streams": [],
      "feature_states": [],
      "include_global_state": true,
      "state": "SUCCESS",
      "start_time": "2020-07-06T21:55:18.129Z",
      "start_time_in_millis": 1593093628850,
      "end_time": "2020-07-06T21:55:18.129Z",
      "end_time_in_millis": 1593094752018,
      "duration_in_millis": 0,
      "failures": [],
      "shards": {
        "total": 0,
        "failed": 0,
        "successful": 0
      }
    }
  ],
  "total": 1,
  "remaining": 0
}
```

&emsp;&emsp;下面的请求返回了快照仓库`my_repository`中快照名以`snapshot`开头的快照。

```text
GET /_snapshot/my_repository/snapshot*?size=2&sort=name
```

&emsp;&emsp;这个API返回下面的响应：

```text
{
  "snapshots": [
    {
      "snapshot": "snapshot_1",
      "uuid": "dKb54xw67gvdRctLCxSket",
      "repository": "my_repository",
      "version_id": <version_id>,
      "version": <version>,
      "indices": [],
      "data_streams": [],
      "feature_states": [],
      "include_global_state": true,
      "state": "SUCCESS",
      "start_time": "2020-07-06T21:55:18.129Z",
      "start_time_in_millis": 1593093628850,
      "end_time": "2020-07-06T21:55:18.129Z",
      "end_time_in_millis": 1593094752018,
      "duration_in_millis": 0,
      "failures": [],
      "shards": {
        "total": 0,
        "failed": 0,
        "successful": 0
      }
    },
    {
      "snapshot": "snapshot_2",
      "uuid": "vdRctLCxSketdKb54xw67g",
      "repository": "my_repository",
      "version_id": <version_id>,
      "version": <version>,
      "indices": [],
      "data_streams": [],
      "feature_states": [],
      "include_global_state": true,
      "state": "SUCCESS",
      "start_time": "2020-07-06T21:55:18.130Z",
      "start_time_in_millis": 1593093628851,
      "end_time": "2020-07-06T21:55:18.130Z",
      "end_time_in_millis": 1593094752019,
      "duration_in_millis": 1,
      "failures": [],
      "shards": {
        "total": 0,
        "failed": 0,
        "successful": 0
      }
    }
  ],
  "next": "c25hcHNob3RfMixteV9yZXBvc2l0b3J5LHNuYXBzaG90XzI=",
  "total": 3,
  "remaining": 1
}

```

&emsp;&emsp;接下里的请求来获取剩余的快照（上一个请求中remaining的值为1，说明没有返回所有的快照），使用上面响应中`next`的值作为下一次请求参数`after`的值。

```text
GET /_snapshot/my_repository/snapshot*?size=2&sort=name&after=c25hcHNob3RfMixteV9yZXBvc2l0b3J5LHNuYXBzaG90XzI=
```

&emsp;&emsp;这个API返回下面的响应：

```text
{
  "snapshots": [
    {
      "snapshot": "snapshot_3",
      "uuid": "dRctdKb54xw67gvLCxSket",
      "repository": "my_repository",
      "version_id": <version_id>,
      "version": <version>,
      "indices": [],
      "data_streams": [],
      "feature_states": [],
      "include_global_state": true,
      "state": "SUCCESS",
      "start_time": "2020-07-06T21:55:18.129Z",
      "start_time_in_millis": 1593093628850,
      "end_time": "2020-07-06T21:55:18.129Z",
      "end_time_in_millis": 1593094752018,
      "duration_in_millis": 0,
      "failures": [],
      "shards": {
        "total": 0,
        "failed": 0,
        "successful": 0
      }
    }
  ],
  "total": 3,
  "remaining": 0
}
```

&emsp;&emsp;获取使用偏移值`2`来跳过已经获取的两个快照也能获取相同的结果。

```text
GET /_snapshot/my_repository/snapshot*?size=2&sort=name&offset=2
```

&emsp;&emsp;这个API返回下面的响应：

```text
{
  "snapshots": [
    {
      "snapshot": "snapshot_3",
      "uuid": "dRctdKb54xw67gvLCxSket",
      "repository": "my_repository",
      "version_id": <version_id>,
      "version": <version>,
      "indices": [],
      "data_streams": [],
      "feature_states": [],
      "include_global_state": true,
      "state": "SUCCESS",
      "start_time": "2020-07-06T21:55:18.129Z",
      "start_time_in_millis": 1593093628850,
      "end_time": "2020-07-06T21:55:18.129Z",
      "end_time_in_millis": 1593094752018,
      "duration_in_millis": 0,
      "failures": [],
      "shards": {
        "total": 0,
        "failed": 0,
        "successful": 0
      }
    }
  ],
  "total": 3,
  "remaining": 0
}
```

&emsp;&emsp;下面的请求返回了快照仓库`my_repository`中快照名以`snapshot`开头的快照，除了名为`snapshot_3`的快照。

```text
GET /_snapshot/my_repository/snapshot*,-snapshot_3?sort=name
```

&emsp;&emsp;这个API返回下面的响应：

```text
{
  "snapshots": [
    {
      "snapshot": "snapshot_1",
      "uuid": "dKb54xw67gvdRctLCxSket",
      "repository": "my_repository",
      "version_id": <version_id>,
      "version": <version>,
      "indices": [],
      "data_streams": [],
      "feature_states": [],
      "include_global_state": true,
      "state": "SUCCESS",
      "start_time": "2020-07-06T21:55:18.129Z",
      "start_time_in_millis": 1593093628850,
      "end_time": "2020-07-06T21:55:18.129Z",
      "end_time_in_millis": 1593094752018,
      "duration_in_millis": 0,
      "failures": [],
      "shards": {
        "total": 0,
        "failed": 0,
        "successful": 0
      }
    },
    {
      "snapshot": "snapshot_2",
      "uuid": "vdRctLCxSketdKb54xw67g",
      "repository": "my_repository",
      "version_id": <version_id>,
      "version": <version>,
      "indices": [],
      "data_streams": [],
      "feature_states": [],
      "include_global_state": true,
      "state": "SUCCESS",
      "start_time": "2020-07-06T21:55:18.130Z",
      "start_time_in_millis": 1593093628851,
      "end_time": "2020-07-06T21:55:18.130Z",
      "end_time_in_millis": 1593094752019,
      "duration_in_millis": 1,
      "failures": [],
      "shards": {
        "total": 0,
        "failed": 0,
        "successful": 0
      }
    }
  ],
  "total": 2,
  "remaining": 0
}
```

&emsp;&emsp;当使用默认的升序，并且按照快照名称排序，下面的请求返回了在`snapshot_2`之后的所有快照：

```text
GET /_snapshot/my_repository/*?sort=name&from_sort_value=snapshot_2
```

&emsp;&emsp;这个API返回下面的响应：

```text
{
  "snapshots": [
    {
      "snapshot": "snapshot_2",
      "uuid": "vdRctLCxSketdKb54xw67g",
      "repository": "my_repository",
      "version_id": <version_id>,
      "version": <version>,
      "indices": [],
      "data_streams": [],
      "feature_states": [],
      "include_global_state": true,
      "state": "SUCCESS",
      "start_time": "2020-07-06T21:55:18.130Z",
      "start_time_in_millis": 1593093628851,
      "end_time": "2020-07-06T21:55:18.130Z",
      "end_time_in_millis": 1593094752019,
      "duration_in_millis": 1,
      "failures": [],
      "shards": {
        "total": 0,
        "failed": 0,
        "successful": 0
      }
    },
    {
      "snapshot": "snapshot_3",
      "uuid": "dRctdKb54xw67gvLCxSket",
      "repository": "my_repository",
      "version_id": <version_id>,
      "version": <version>,
      "indices": [],
      "data_streams": [],
      "feature_states": [],
      "include_global_state": true,
      "state": "SUCCESS",
      "start_time": "2020-07-06T21:55:18.129Z",
      "start_time_in_millis": 1593093628850,
      "end_time": "2020-07-06T21:55:18.129Z",
      "end_time_in_millis": 1593094752018,
      "duration_in_millis": 0,
      "failures": [],
      "shards": {
        "total": 0,
        "failed": 0,
        "successful": 0
      }
    }
  ],
  "total": 2,
  "remaining": 0
}
```

&emsp;&emsp;当使用默认的升序，并且按照快照处理的开始时间排序，下面的请求返回了快照名以`snapshot_`开头并且快照处理的开始时间在`1577833200000`(Jan 1st 2020) 之后（包含）的所有快照：

```text
GET /_snapshot/my_repository/snapshot_*?sort=start_time&from_sort_value=1577833200000
```

&emsp;&emsp;这个API返回下面的响应：

```text
{
  "snapshots": [
    {
      "snapshot": "snapshot_1",
      "uuid": "dKb54xw67gvdRctLCxSket",
      "repository": "my_repository",
      "version_id": <version_id>,
      "version": <version>,
      "indices": [],
      "data_streams": [],
      "feature_states": [],
      "include_global_state": true,
      "state": "SUCCESS",
      "start_time": "2020-07-06T21:55:18.128Z",
      "start_time_in_millis": 1593093628849,
      "end_time": "2020-07-06T21:55:18.129Z",
      "end_time_in_millis": 1593093628850,
      "duration_in_millis": 1,
      "failures": [],
      "shards": {
        "total": 0,
        "failed": 0,
        "successful": 0
      }
    },
    {
      "snapshot": "snapshot_2",
      "uuid": "vdRctLCxSketdKb54xw67g",
      "repository": "my_repository",
      "version_id": <version_id>,
      "version": <version>,
      "indices": [],
      "data_streams": [],
      "feature_states": [],
      "include_global_state": true,
      "state": "SUCCESS",
      "start_time": "2020-07-06T21:55:18.130Z",
      "start_time_in_millis": 1593093628851,
      "end_time": "2020-07-06T21:55:18.130Z",
      "end_time_in_millis": 1593093628851,
      "duration_in_millis": 0,
      "failures": [],
      "shards": {
        "total": 0,
        "failed": 0,
        "successful": 0
      }
    },
    {
      "snapshot": "snapshot_3",
      "uuid": "dRctdKb54xw67gvLCxSket",
      "repository": "my_repository",
      "version_id": <version_id>,
      "version": <version>,
      "indices": [],
      "data_streams": [],
      "feature_states": [],
      "include_global_state": true,
      "state": "SUCCESS",
      "start_time": "2020-07-06T21:55:18.131Z",
      "start_time_in_millis": 1593093628852,
      "end_time": "2020-07-06T21:55:18.135Z",
      "end_time_in_millis": 1593093628856,
      "duration_in_millis": 4,
      "failures": [],
      "shards": {
        "total": 0,
        "failed": 0,
        "successful": 0
      }
    }
  ],
  "total": 3,
  "remaining": 0
}
```

#### Get snapshot status API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/get-snapshot-status-api.html)

&emsp;&emsp;获取快照中每一个分片当前状态的详细描述。

```text
GET _snapshot/_status
```

##### Request

```text
GET _snapshot/_status
GET _snapshot/<repository>/_status
GET _snapshot/<repository>/<snapshot>/_status
```

##### Prerequisites


- 如果开启了Elasticsearch security features，你必须要有`monitor_snapshot`、`create_snapshot`、`manage`的[cluster privilege](#Cluster privileges)来使用这个API。

##### Description

&emsp;&emsp;使用这个API来获取快照中每一个分片当前状态的详细描述。

&emsp;&emsp;如果你忽略`<snapshot>`这个请求参数，那么请求只会获取当前运行中的快照的信息。推介这种方式。

&emsp;&emsp;如果有需要，你可以指定`<repository>`和`<snapshot>`来获取指定快照的信息，即使它们不在运行中。

> WARNING：使用这个API获取任意的快照比获取当前运行中的快照需要的开销更高。这个API会读取快照仓库中每一个快照的每一个分片。例如，如果你有100个快照，每个快照有1000个分片。那么这个API请求读取所有快照时会要求100000次读取（100 snapshots * 1000 shards）
> 基于你存储介质的延迟，这种请求可能会需要相当长的时间才能返回。这些请求还可能消耗机器资源，并且在使用云存储时，可能会产生高昂的处理成本

##### Path parameters

- `<repository>`：（Optional, string）快照仓库名称用逗号隔开来限制请求量。如果未指定`<snapshot>`，可以支持通配符`*`。
- `<Snapshot>`：（Required, string）快照名称用逗号隔开用于获取其状态。默认是当前运行中的快照。不支持通配符（`*`）

##### Query parameters

- master_timeout：（Optional, [time units](#Time units)）周期性的等待连接master node。如果在超时前未收到响应，则请求失败并且返回一个错误。默认是`30s`。
- ignore_unavailable：（Optional, Boolean）如果为`false`，任何快照如果不可见的话则请求返回一个错误。默认为`false`
  - 如果为`true`，请求会忽略不可见的快照，比如那些被损坏或者临时不能返回的快照

##### Response body

- repository：（string）包含快照的仓库名称
- snapshot：（string）快照的名称
- uuid：（string）快照的uuid
- state：（string）快照的当前状态
  - FAILED：快照已经完成但是出现了错误并且没能存储任何数据
  - IN_PROGRESS：快照正在运行中
  - PARTIAL：存储了全局的集群状态，但是至少有一个分片数据没能成功存储。响应中failure部分会包含分片为什么没能被正确处理的详细原因
  - SUCCESS：快照已经完成并且所有的分片都成功存储了
- include_global_state：（Boolean）快照中是否会包含集群状态
- shards_stats：（object）快照中分片的一些计数信息
  - initializing：（integer）仍然在初始化的分片数量
  - started：（integer）已经开始但尚未完成的分片数量。这些分片已经开始快照过程，但还没有完成所有必要的步骤。
  - finalizing：（integer）正在完成但尚未完全完成的分片数量
  - done：（integer）成功初始化、开始并完成的分片数量
  - failed：（integer）未能成功包含在快照中的分片数量
  - total：（integer）快照中包含的总分片数量。这个数字是以上所有状态的分片数之和
- stats：（object）提供快照中文件的数量和大小的详细信息
  - incremental：（object）作为增量快照（incremental snapshot）一部分仍需复制的文件的数量和大小。对于已完成的快照，此属性指示作为增量快照一部分而复制的、仓库中原本不存在的文件的数量和大小
  - processed：（object）已经上传到快照的文件的数量和大小。文件上传后，`processed`的file_count和size_in_bytes会在`stats`中相应增加
  - total：（object）快照引用的文件的总数量和大小
  - start_time_in_millis：（long）快照创建过程开始的时间，以毫秒为单位
  - time_in_millis：（long）快照过程完成所需的总时间，以毫秒为单位
- `<index>`：（list of objects）快照中索引相关，以列表展示的信息
  - shards_stats：（object）同上文中的`shards_stats`
  - stats：（object）同上文中的`stats.`
  - shards：（list of objects）快照中索引相关，以列表展示的信息
    - stage：（string）快照中包含的分片的当前状态
      - DONE：成功存储到快照的分片数量
      - FAILURE：没有成功存储到快照的分片数量
      - FINALIZE：在存储到仓库过程中，处于`finalizing`阶段的分片数量
      - INIT：在存储到仓库过程中，处于`initializing`阶段的分片数量
      - STARTED：在存储到仓库过程中，处于`started`阶段的分片数量
    - stats：（object）见上文的`stats`
    - total：（object）快照引用的文件数量和大小
    - start_time_in_millis：（long）见上文中的`start_time_in_millis`
    - time_in_millis：（long）见上文中的`time_in_millis`

##### Example

&emsp;&emsp;下面的请求返回快照仓库`myZ_repository`中名为`snapshot_2`的快照中详细的状态信息。响应中相比较[Get snapshot API](#Get snapshot API)，包含额外的信息，比如分片状态跟文件统计。

```text
GET _snapshot/my_repository/snapshot_2/_status
```

```text
{
  "snapshots" : [
    {
      "snapshot" : "snapshot_2",
      "repository" : "my_repository",
      "uuid" : "lNeQD1SvTQCqqJUMQSwmGg",
      "state" : "SUCCESS",
      "include_global_state" : false,
      "shards_stats" : {
        "initializing" : 0,
        "started" : 0,
        "finalizing" : 0,
        "done" : 1,
        "failed" : 0,
        "total" : 1
      },
      "stats" : {
        "incremental" : {
          "file_count" : 3,
          "size_in_bytes" : 5969
        },
        "total" : {
          "file_count" : 4,
          "size_in_bytes" : 6024
        },
        "start_time_in_millis" : 1594829326691,
        "time_in_millis" : 205
      },
      "indices" : {
        "index_1" : {
          "shards_stats" : {
            "initializing" : 0,
            "started" : 0,
            "finalizing" : 0,
            "done" : 1,
            "failed" : 0,
            "total" : 1
          },
          "stats" : {
            "incremental" : {
              "file_count" : 3,
              "size_in_bytes" : 5969
            },
            "total" : {
              "file_count" : 4,
              "size_in_bytes" : 6024
            },
            "start_time_in_millis" : 1594829326896,
            "time_in_millis" : 0
          },
          "shards" : {
            "0" : {
              "stage" : "DONE",
              "stats" : {
                "incremental" : {
                  "file_count" : 3,
                  "size_in_bytes" : 5969
                },
                "total" : {
                  "file_count" : 4,
                  "size_in_bytes" : 6024
                },
                "start_time_in_millis" : 1594829326896,
                "time_in_millis" : 0
              }
            }
          }
        }
      }
    }
  ]
}
```

#### Restore snapshot API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/restore-snapshot-api.html)

&emsp;&emsp;恢复集群或者data streams和indices的某个[snapshot](#Snapshot and restore)。

```text
POST /_snapshot/my_repository/my_snapshot/_restore
```

##### Request

```text
POST /_snapshot/<repository>/<snapshot>/_restore
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`manage`或者` cluster:admin/snapshot/*`的[cluster privilege](#Cluster privileges)来使用这个API。
- 你可以将快照只恢复到一个运行中的集群的[master node](#Master-eligible node)上。快照仓库必须已注册（[registered](#Register a snapshot repository)）并且对集群可见
- 快照跟集群版本必须兼容。见[Snapshot compatibility](#Snapshot compatibility)
- 若要恢复一个快照，集群的全局元数据必须使可写入的。确保没有任何的[cluster blocks](#Metadata)阻止写入。恢复操作会忽略[index blocks](#Index blocks)
- 在你恢复一个data stream时，确保集群中包含一个[matching index template ](#Create an index template)并且启用了data stream。可以通过Kibana中的[Index Management](#Manage index templates)或者[get index template API](#Get index template API)检查。

```text
GET _index_template/*?filter_path=index_templates.name,index_templates.index_template.index_patterns,index_templates.index_template.data_stream
```

&emsp;&emsp;如果没有这样的模板，你可以[create one](#Create the data stream)或者[restore a cluster state](#Restore an entire cluster)，其中包含了这样的模板。没有一个匹配的索引模板，data stream无法roll over或者创建backing indices

- 如果你的快照中包含App Search或者Workplace Search的数据，确保在恢复快照前已经恢复[Enterprise Search encryption key](https://www.elastic.co/guide/en/enterprise-search/8.2/encryption-keys.html)

##### Path parameters

- `<repository>`：（Optional, string）仓库的名称用来读取待恢复的快照
- `<Snapshot>`：（Required, string）待恢复的快照名称

##### Query parameters

- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- wait_for_completion：（Optional, Boolean）如果为`true`，当恢复操作完成后才返回一个响应。当所有要恢复的索引中的所有主分片都尝试恢复（[recover primary shards](#Monitor a restore)）完成后，这个操作才算是完成。如果一个或者多个恢复尝试失败也是如此
  - 如果为`false`，当恢复操作初始化结束就返回一个响应。默认值是`false`

##### Request body

- ignore_unavailable：（Optional, Boolean）如果为`true`，并且快照中没有`indices`参数中定义的index或者data stream，那么恢复操作会忽略这些。如果为`false`，则会在遇到任意缺失的index或 data stream之后返回一个错误。默认值为`false`
- ignore_index_settings：（Optional, string or array of strings）从快照中恢复的索引设置（Index settings）。你不能使用这个选项来忽略[index.number_of_shards](#indexnumber_of_shards)
 - 对于data streams，这个选项只能应用于恢复backing indices。新的backing indices使用data streams匹配到的索引模板中的配置
- include_aliases：（Optional, Boolean）如果为`true`，请求恢复data streams和indices的别名（Alias）。如果为`false`，则不会恢复别名。默认为`true`
- include_global_state：（Optional, Boolean）如果为`true`，则恢复集群状态。默认是`false`。集群状态包括：
    - [Persistent cluster settings](#Cluster and node setting types)
    - [Index templates](#Index templates)
    - [Legacy index templates](#Create or update index template API（legacy）)
    - [Ingest pipelines](#Ingest pipelines)
    - [ILM policies](#ILM: Manage the index lifecycle)
    - For snapshots taken after 7.12.0, [feature states](#Feature states)
  - 如果`include_global_state`为真，则恢复操作会将集群中的旧版索引模板与快照中包含的模板合并，替换掉任何存在的、名称与快照中的某个模板相匹配的现有模板。它会完全移除所有存在于你的集群中的持久设置、非旧版索引模板、摄取管道和ILM生命周期策略，并用快照中的相应项替换它们。
  
    使用`feature_states`参数来配置如何恢复功能状态。
  
    如果`include_global_state`为真且一个快照是在没有全局状态的情况下创建的，那么恢复请求将失败。
- feature_states：（Optional, array of strings）
- index_settings：（Optional, object）
- indices：（Optional, string or array of strings）
- partial：（Optional, Boolean）
- rename_pattern：（Optional, string）
- rename_replacement：（Optional, string）

#### Delete snapshot API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/delete-snapshot-api.html)

&emsp;&emsp;删除一个[snapshot](#Snapshot and restore)。

```text
DELETE /_snapshot/my_repository/my_snapshot
```

##### Request

```text
DELETE /_snapshot/<repository>/<snapshot>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`manage`的[cluster privilege](#Cluster privileges)来使用这个API。

##### Path parameters

- `<repository>`：（Required, string）待删除的快照所在的快照仓库
- `<snapshot>`：（Required, string）待删除的用逗号隔开的快照名。同样支持通配符（`*`）

##### Example

&emsp;&emsp;下面的请求删除了快照仓库`my_repository`中名为`snapshot_2`和`snapshot_3`的两个快照

```text
DELETE /_snapshot/my_repository/snapshot_2,snapshot_3
```

&emsp;&emsp;上面的API返回以下响应：

```text
{
  "acknowledged" : true
}
```

### Snapshot lifecycle management APIs
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/snapshot-lifecycle-management-api.html)

&emsp;&emsp;你可以使用下面的API设置策略来自动的生成快照并且控制保留时间。

&emsp;&emsp;更多关于snapshot lifecycle management（SLM）的信息见[Automate snapshots with SLM](#Automate snapshots with SLM)。

##### Policy management APIs

- [Create lifecycle policy](#Create or update snapshot lifecycle policy API)
- [Get lifecycle policy](#Get snapshot lifecycle policy API)
- [Delete lifecycle policy](#Delete snapshot lifecycle policy API)

##### Snapshot management APIs

- [Execute snapshot lifecycle policy](#Execute snapshot lifecycle policy API) (take snapshots)
- [Execute snapshot retention policy](#Execute snapshot retention policy API) (delete expired snapshots)

##### Operation management APIs

- [Get SLM status](#Get snapshot lifecycle management status API)
- [Get global and policy-level action statistics](#Get snapshot lifecycle stats API)
- [Start SLM](#Start snapshot lifecycle management API)
- [Stop SLM](#Stop snapshot lifecycle management API)

#### Create or update snapshot lifecycle policy API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/slm-api-put-policy.html)

&emsp;&emsp;创建或者更新一个snapshot lifecycle policy。

##### Request

```text
PUT /_slm/policy/<snapshot-lifecycle-policy-id>
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`manage_slm`的[cluster privilege](#Cluster privileges)以及`manage`的Index privilege来使用这个API。更多信息见[Security privileges](#Security privileges)

##### Description

&emsp;&emsp;`create or update snapshot lifecycle policy API`用来创建或更新一个snapshot lifecycle policy。

&emsp;&emsp;如果策略已存在，这个请求会更新策略的版本。只有最新的策略才会被存储。

##### Path parameters

- `<snapshot-lifecycle-policy-id>`：（Required, string）你想要创建或更新的snapshot lifecycle policy的ID

##### Query parameters

- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Request body

- config：（Required, object）通过策略创建的每一个快照的配置
  - expand_wildcards：（Optional, string）决定在indices参数中如果有通配符模式时将如何去匹配data streams和indices。支持使用逗号隔开的值，例如open, hidden。默认是all。合法值有：
    - all：匹配满足通配符模式的所有data streams和indices，包括[hidden](#Multi-target-syntax-1)
    - open：匹配打开的data streams和indices
    - closed：匹配关闭的data streams和indices
    - hidden：匹配隐藏的data streams和indices。必须和open、closed中的一个或全部组合使用
    - none：不展开通配符模式
  - ignore_unavailable：（Optional, Boolea）如果为`false`，如果任意的data streams或indices丢失或者关闭会导致生成快照失败。如果为`true`，快照会忽略data streams或indices丢失或者关闭。默认值为`false`
  - include_global_state：（Optional, Boolean）如果为`true`，快照中会包含集群状态。默认为`true`。集群状态包括：
    - [Persistent cluster settings](#Cluster and node setting types)
    - [Index templates](#Index templates)
    - [Legacy index templates](#Create or update index template API（legacy）)
    - [Ingest pipelines](#Ingest pipelines)
    - [ILM policies](#ILM: Manage the index lifecycle)
    - For snapshots taken after 7.12.0, [feature states](#Feature states)
  - indices：（Optional, string or array of strings）写入到快照的data streams和indices，用逗号隔开。支持[multi-target syntax](#Multi-target-syntax)。默认是一个空数组（`[]`），包含常规的data streams和常规的indices。若要排除所有的data streams和indices，可以使用`-*`
    - 你不能使用这个参数来包含或者排除[system indices or system data streams](#System indices)，可以转而使用下面的feature_states参数
  - feature_states：（Optional, array of strings）将[Feature states ](#Feature states)包含到快照中。若要获取可能的值以及它们的描述，可以使用[get features API](#Get Features API)
    - 如果`include_global_state`为`true`，快照默认包含所有的feature states。否则不包含
    - 注意的是指定一个空数组会产生默认行为。若要排除素有的feature states，并且不用关心`include_global_state`是何值，则可以指定一个只有`none`值的数组（`["none"]`）
  - metadata：（Optional, object）任意的附加快照的一些元数据。比如记录哪个人生成了快照，为什么要生成快照，或者其他有用的数据。元数据大小必须小于1024个字节
  - partial：（Optional, Boolean）如果为`false`，那么当一个或者多个索引的所有主分片不是都可用，那么整个快照会失败。默认值为`false`。否则允许对可用的分片进行快照并且得到一个不完整的快照
- name：（Required, string）通过策略创建的快照会被自动分配一个名称。支持[Date math](#Date math support in system and index alias names-1)。若要防止出现快照名称冲突，UUID会被自动的添加到每一个快照名称的后面
- repository：（Required, string）用来存储通过策略创建的快照的仓库。这个仓库必须在创建策略之前就已存在。你可以使用[snapshot repository API](#Snapshot module-1)创建一个仓库
- retention：（Optional, object）保留规则（retention rule）用来保留以及删除策略创建的快照
  - expire_after：（Optional, [time units](#Time units)）某个时间段后的快照会被认为是过期的并且可以删除。SLM基于[slm.retention_schedule](#slm.retention_schedule)删除过期的快照
  - max_count：（Optional, string）即使快照尚未过期，也要保留的最大快照数量。如果仓库中的快照数量超过此限制，策略将保留最新的快照并删除较旧的快照。这个限制只包括[state](#Get snapshot API)为`SUCCESS`的快照。
  - min_count：（Optional, integer）即使快照已经过期，也要保留的最小快照数量
- schedule：（Required, [Cron syntax](#Cron expressions-1)）

##### Example

&emsp;&emsp;创建一个名为`daily-snapshots`的生命周期策略：

```text
PUT /_slm/policy/daily-snapshots
{
  "schedule": "0 30 1 * * ?", 
  "name": "<daily-snap-{now/d}>", 
  "repository": "my_repository", 
  "config": { 
    "indices": ["data-*", "important"], 
    "ignore_unavailable": false,
    "include_global_state": false
  },
  "retention": { 
    "expire_after": "30d", 
    "min_count": 5, 
    "max_count": 50 
  }
}
```

&emsp;&emsp;第3行，生成快照的时间，在这个例子中是每天的1:30am
&emsp;&emsp;第4行，每一个快照给定的名称
&emsp;&emsp;第5行，存储快照的仓库
&emsp;&emsp;第6行，额外的快照配置
&emsp;&emsp;第7行，快照中包含的data streams和indices
&emsp;&emsp;第11行，可选的保留快照的配置
&emsp;&emsp;第12行，快照保留30天
&emsp;&emsp;第13行，总是至少保留最新的5个状态为成功的快照，即使快照30天才过期
&emsp;&emsp;第14行，最多保留不超过50个状态为成功的快照，即使快照30天才过期

#### Get snapshot lifecycle policy API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/slm-api-get-policy.html)

&emsp;&emsp;获取一个或者多个snapshot lifecycle policy 的定义以及l最新尝试创建快照成功和失败的信息。

##### Request

```text
GET _slm/policy/<policy-id>
GET _slm/policy
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`manage_slm`的[cluster privilege](#Cluster privileges)来使用这个API。更多信息见[Security privileges](#Security privileges)

##### Description

&emsp;&emsp;返回指定策略的定义以及最新尝试创建快照成功和失败的信息。

##### Path parameters

- `<policy-id>`：（Optional, string）用逗号隔开的snapshot lifecycle policy的ID列表

##### Examples

###### Get a specific policy

&emsp;&emsp;获取名为`daily-snapshots`的策略：

```text
GET _slm/policy/daily-snapshots?human
```

&emsp;&emsp;这个请求返回以下响应：

```text
{
  "daily-snapshots": {
    "version": 1,                                 
    "modified_date": "2099-05-06T01:30:00.000Z",  
    "modified_date_millis": 4081757400000,
    "policy" : {
      "schedule": "0 30 1 * * ?",
      "name": "<daily-snap-{now/d}>",
      "repository": "my_repository",
      "config": {
        "indices": ["data-*", "important"],
        "ignore_unavailable": false,
        "include_global_state": false
      },
      "retention": {
        "expire_after": "30d",
        "min_count": 5,
        "max_count": 50
      }
    },
    "stats": {
      "policy": "daily-snapshots",
      "snapshots_taken": 0,
      "snapshots_failed": 0,
      "snapshots_deleted": 0,
      "snapshot_deletion_failures": 0
    },
    "next_execution": "2099-05-07T01:30:00.000Z", 
    "next_execution_millis": 4081843800000
  }
}
```

&emsp;&emsp;第3行，快照策略的版本号，只有最新版本的策略才会被存储，每次更新策略会增加这个版本号的值
&emsp;&emsp;第4行，上一次策略更改的时间
&emsp;&emsp;第28行，下一次策略执行的时间

###### Get all policy

```text
GET _slm/policy
```

#### Delete snapshot lifecycle policy API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/slm-api-delete-policy.html)

&emsp;&emsp;删除一个现有的snapshot lifecycle policy

##### Request

```text
DELETE /_slm/policy/<snapshot-lifecycle-policy-id>
```
##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`manage_slm`的[cluster privilege](#Cluster privileges)来使用这个API。更多信息见[Security privileges](#Security privileges)

##### Description

&emsp;&emsp;删除指定的生命周期策略定义。可以防止新快照的生成，但是不会取消正在进行中的任务，不会移除之前生成的快照。

##### Path parameters

- `<policy-id>`：（Required, string）待删除的snapshot lifecycle policy的ID

##### Examples

```text
DELETE /_slm/policy/daily-snapshots
```

#### Execute snapshot lifecycle policy API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/slm-api-execute-lifecycle.html)

&emsp;&emsp;不等待定时计划时间，立即根据生命周期策略生成一个快照。

##### Request

```text
PUT /_slm/policy/<snapshot-lifecycle-policy-id>/_execute
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`manage_slm`的[cluster privilege](#Cluster privileges)来使用这个API。更多信息见[Security privileges](#Security privileges)

##### Description

&emsp;&emsp;手动应用快照策略，立即创建一个快照。快照策略通常根据定时计划时间，但是你可能想要手动的，在执行一次升级或者其他维护之前执行一个策略。

##### Path parameters

- `<policy-id>`：（Required, string ）待执行的snapshot lifecycle policy的ID

##### Example

&emsp;&emsp;若要根据名为`daily-snapshots`的策略立即生成一个快照：

```text
POST /_slm/policy/daily-snapshots/_execute
```

&emsp;&emsp;如果成功了，请求会返回生成的快照名：

```text
{
  "snapshot_name": "daily-snap-2019.04.24-gwrqoo2xtea3q57vvg0uea"
}
```

&emsp;&emsp;快照在后台生成。你可以使用[snapshot APIs](#Snapshot lifecycle management APIs)或者[monitor the status of the snapshot](#Monitor a snapshot)来观察快照生成状态。

&emsp;&emsp;若要查看某个策略对应的最新的快照，你可以使用[get snapshot lifecycle policy API](#Get snapshot lifecycle policy API)。

#### Execute snapshot retention policy API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/slm-api-execute-retention.html)

&emsp;&emsp;根据策略的保留规则，删除过期的快照。

##### Request

```text
POST /_slm/_execute_retention
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`manage_slm`的[cluster privilege](#Cluster privileges)来使用这个API。更多信息见[Security privileges](#Security privileges)

##### Description

&emsp;&emsp;手动应用保留策略来强制立即移除过期的快照。保留策略通常根据定时计划时间执行。

##### Examples

&emsp;&emsp;若要强制移除过期的快照：

```text
POST /_slm/_execute_retention
```

&emsp;&emsp;该操作在后台异步运行。

#### Get snapshot lifecycle management status API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/slm-api-get-status.html)

&emsp;&emsp;获取snapshot lifecycle management (SLM)的状态

##### Request

```text
GET /_slm/status
```

##### Description

&emsp;&emsp;返回SLM插件的状态。响应中的`operation_mode`字段展示了三种状态之一：`RUNNING STOPPING STOPPED`。可以使用[stop](#Stop snapshot lifecycle management API)和[start](#Start snapshot lifecycle management API) API来分别暂定以及重启SLM插件。

##### Query parameters

- master_timeout：（Optional, [time units](#API conventions)）连接等待master节点一段时间，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`manage_slm`或`read_slm`的[cluster privilege](#Cluster privileges)以及`manage`的Index privilege来使用这个API。更多信息见[Security privileges](#Security privileges)

##### Examples

```text
GET _slm/status
```

&emsp;&emsp;这个请求返回以下结果：

```text
{
  "operation_mode": "RUNNING"
}
```

#### Get snapshot lifecycle stats API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/slm-api-get-stats.html)

&emsp;&emsp;返回全局的以及策略层级的由SLM（snapshot lifecycle management）执行的动作（操作）的统计信息。

##### Request

```text
GET /_slm/stats
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`manage_slm`的[cluster privilege](#Cluster privileges)来使用这个API。更多信息见[Security privileges](#Security privileges)

##### Examples

```text
GET /_slm/stats
```

&emsp;&emsp;这个请求返回以下响应：

```text
{
  "retention_runs": 13,
  "retention_failed": 0,
  "retention_timed_out": 0,
  "retention_deletion_time": "1.4s",
  "retention_deletion_time_millis": 1404,
  "policy_stats": [ ],
  "total_snapshots_taken": 1,
  "total_snapshots_failed": 1,
  "total_snapshots_deleted": 0,
  "total_snapshot_deletion_failures": 0
}
```

#### Start snapshot lifecycle management API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/slm-api-start.html)

&emsp;&emsp;开启snapshot lifecycle management (SLM)。

##### Request

```text
POST /_slm/start
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`manage_slm`的[cluster privilege](#Cluster privileges)来使用这个API。更多信息见[Security privileges](#Security privileges)

##### Description

&emsp;&emsp;如果SLM插件不在运行中，则启动。当形成集群后，SLM会自动开始。只有通过使用[Stop snapshot lifecycle management](#Stop snapshot lifecycle management API)停止SLM插件后，才有手动启动SLM的必要。

##### Query parameters

- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Examples

&emsp;&emsp;启动SLM插件：

```text
POST _slm/start
```

&emsp;&emsp;如果启动成功，请求返回如下内容：

```text
{
  "acknowledged": true
}
```

#### Stop snapshot lifecycle management API
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/slm-api-stop.html)

&emsp;&emsp;关闭snapshot lifecycle management (SLM)。

##### Request

```text
POST /_slm/stop
```

##### Prerequisites

- 如果开启了Elasticsearch security features，你必须有`manage_slm`的[cluster privilege](#Cluster privileges)来使用这个API。更多信息见[Security privileges](#Security privileges)

##### Description

&emsp;&emsp;暂停所有的snapshot lifecycle management (SLM)的操作并且停止SLM插件。当你想要对集群进行维护并且需要防止SLM对你的data streams和indices执行任何操作时是非常有用的。停止SLM不会停止处理中的快照。即使SLM已经停止了，你可以通过[Execute snapshot lifecycle policy](#Execute snapshot lifecycle policy API)手动的触发快照的生成。

&emsp;&emsp;请求再被确认后会马上返回一个响应，但是在处理中的操作完成并且正确的被停止前，插件可能仍然在运行。

&emsp;&emsp;使用[Get snapshot lifecycle management status](#Get snapshot lifecycle management status API)查看SLM是否运行中。

##### Query parameters

- master_timeout：（Optional,[time units](#Time units)）等待连接master节点的周期值。如果超时前没有收到响应，这个请求会失败并且返回一个错误。默认值是`30s`。
- timeout：（Optional, [time units](#API conventions)）等待返回response，如果没有收到response并且超时了，这次请求视为失败并且返回一个错误，默认值`30s`。

##### Examples

```text
POST _slm/stop
```

### SQL APIs
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/sql-apis.html)

##### Request

##### Prerequisites

##### Query parameters

##### Description

##### Examples

#### SQL search API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/sql-apis.html)


### Transform APIs
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/transform-apis.html)

#### Create transform API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/put-transform.html)

#### Delete transform API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/put-transform.html)

#### Get transforms API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/get-transform.html)

#### Create API key API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/security-api-create-api-key.html)

#### Create or update role mappings API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/security-api-put-role-mapping.html)

#### Create or update roles API 
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/security-api-put-role.html)

&emsp;&emsp;
##### Request
##### Prerequisites
##### Description
##### Path parameters
##### Query parameters
##### Response body
##### Example

#### Get transform statistics API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/get-transform-stats.html)

#### Preview transform API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/preview-transform.html)

#### Reset transform API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/reset-transform.html)

#### Start transform API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/start-transform.html)

#### Stop transforms API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/stop-transform.html)

#### Update transform API
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/update-transform.html)

## Cross-cluster search, clients, and integrations
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ccs-clients-integrations.html)

### Adding and removing nodes
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-discovery-adding-removing-nodes.html)

&emsp;&emsp;见[Add and remove nodes in your cluster](#Add and remove nodes in your cluster)。

### Avoid oversharding
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/avoid-oversharding.html)

&emsp;&emsp;见[Size your shards](#Size your shards)。

### Configure TLS
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/configuring-tls.html#tls-transport)

#### Encrypt internode communication-1

&emsp;&emsp;见[Encrypt internode communication](#Encrypt internode communications with TLS)。

### Cluster-level shard allocation filtering-1
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/allocation-filtering.html)

&emsp;&emsp;见[Cluster-level shard allocation filtering](#Cluster-level shard allocation settings)。

### Cluster-level shard allocation
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/shards-allocation.html)

&emsp;&emsp;见[Cluster-level shard allocation settings](#Cluster-level shard allocation settings)。

### Cron expressions-1
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/cron-expressions.html#cron-expressions)

&emsp;&emsp;见[Cron expressions](#Cron expressions)。


### Dangling indices-1
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-gateway-dangling-indices.html)

&emsp;&emsp;见[Dangling indices](#Dangling indices)。

### Date math support in system and index alias names-1
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/date-math-index-names.html)

&emsp;&emsp;见[Date math support in index and index alias names](#Date math support in index and index alias names)

### fielddata mapping parameter-1
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-gateway-dangling-indices.html)

&emsp;&emsp;见[fielddata mapping parameter](#fielddata mapping parameter)。

### Full cluster restart upgrade
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/restart-upgrade.html)

&emsp;&emsp;当Elasticsearch升级到8.0或者更高版本时，你必须先升级到7.17，即使你优先执行一个full-cluster restart而不是rolling upgrade。更多关于upgrade的信息见[Upgrading to Elastic 8.2.3](https://www.elastic.co/guide/en/elastic-stack/8.2/upgrading-elastic-stack.html)。

### Grok basics
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/grok-basics.html#grok-basics)

&emsp;&emsp;见[Grokking grok](#Grokking grok)。

#### Create users

&emsp;&emsp;参考[elasticsearch-reset-password ](#elasticsearch-reset-password)工具来为内置的用户重置密码。

### Manage existing periodic indices with ILM
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/ilm-with-existing-periodic-indices.html)

&emsp;&emsp;见[Apply policies to existing time series indices](#Apply policies to existing time series indices)。

### Multi-target syntax-1
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/multi-index.html#hidden)

&emsp;&emsp;见[Multi-target syntax](#Multi-target syntax)。

#### Hidden data streams and indices-1

&emsp;&emsp;见[Hidden data streams and indices](#Hidden data streams and indices)。

### Request body search-1
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/search-request-body.html)

#### Stored fields-1

&emsp;&emsp;见[Stored fields](#Stored fields)。

### Snapshot module-1
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-snapshots.html)

&emsp;&emsp;见[Snapshot and restore](#Snapshot and restore)

### Stored scripts-1
（8.2）[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/modules-scripting-stored-scripts.html)

&emsp;&emsp;见[Store and retrieve scripts](#How to write scripts)

#### Repository plugins

&emsp;&emsp;见[Self-managed repository types.](#Self-managed repository types)

#### Rescoring parameter for request body search API-1

&emsp;&emsp;见[Rescore filtered search results](#Rescore filtered search results)。

#### Change index settings during restore

&emsp;&emsp;见[Request body](#index_settings)

### Shard allocation awareness-1
[link](https://www.elastic.co/guide/en/elasticsearch/reference/8.2/allocation-awareness.html)

&emsp;&emsp;见[Shard allocation awareness](#Shard allocation awareness)。



